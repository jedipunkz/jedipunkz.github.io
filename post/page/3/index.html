


    




<!DOCTYPE HTML>

<html>
    <head>
        
            <title>Posts - ジェダイさんのブログ</title>
        

        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1" />
        <meta name="generator" content="Hugo 0.53" />
        


        
        
            
        

        <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Posts"/>
<meta name="twitter:description" content=""/>

        <meta property="og:title" content="Posts" />
<meta property="og:description" content="" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://jedipunkz.github.io/post/" />
<meta property="og:updated_time" content="2015-07-16T00:00:00&#43;00:00"/>

        
<meta itemprop="name" content="Posts">
<meta itemprop="description" content="">


        

        

        
        
            
        

        
        
            <link rel="stylesheet" href="/css/google-font.css" />
            <link rel="stylesheet" href="/css/font-awesome.min.css" />
            <link rel="stylesheet" href="/css/main.css" />
            <link rel="stylesheet" href="/css/add-on.css" />
            <link rel="stylesheet" href="/css/monokai-sublime.css">
        

        

        
        
        
            
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-30563095-1', 'auto');
	
	ga('send', 'pageview');
}
</script>

        
    </head>
    <body>

        
        <div id="wrapper">

    
<header id="header">
    
        <h1><a href="/"></i></a></h1>
    

    <nav class="links">
        <ul>
            
                <li>
                    <a href="">
                        Blog
                    </a>
                </li>
            
                <li>
                    <a href="about/index.html">
                        About
                    </a>
                </li>
            
        </ul>
    </nav>
    <nav class="main">
        <ul>
            
            <li class="search">
                <a class="fa-search" href="#search">Search</a>
                <form id="search" method="get" action="//google.com/search">
                    <input type="text" name="q" placeholder="Search" />
                    <input type="hidden" name="q" value="site:https://jedipunkz.github.io">
                </form>
            </li>
            <li class="menu">
                <a class="fa-bars" href="#menu">Menu</a>
            </li>
        </ul>
    </nav>
</header>


<section id="menu">

    
        <section>
            <form class="search" method="get" action="//google.com/search">
                <input type="text" name="q" placeholder="Search" />
                <input type="hidden" name="q" value="site:https://jedipunkz.github.io">
            </form>
        </section>

    
        <section>
            <ul class="links">
                
                    <li>
                        <a href="">
                            <h3>
                                
                                Blog
                            </h3>
                        </a>
                    </li>
                
                    <li>
                        <a href="about/index.html">
                            <h3>
                                
                                About
                            </h3>
                        </a>
                    </li>
                
            </ul>
        </section>

    
        <section>
            <ul class="links">
                <header>
                    <h3>Recent Posts</h3>
                </header>
                
                    
                

                
                    <li>
                        <a href="https://jedipunkz.github.io/blog/2018/12/31/istio/"><p>Istio, Helm を使って Getting Started 的なアプリをデプロイ</p></a>
                    </li>
                
                    <li>
                        <a href="https://jedipunkz.github.io/blog/2017/07/02/test-kitchen-cluster/"><p>Docker,Test-Kitchen,Ansible でクラスタを構成する</p></a>
                    </li>
                
                    <li>
                        <a href="https://jedipunkz.github.io/blog/2017/04/13/gke-lb/"><p>GCP ロードバランサと GKE クラスタを Terraform を使って構築する</p></a>
                    </li>
                
                    <li>
                        <a href="https://jedipunkz.github.io/blog/2017/02/12/serverless-fission/"><p>Serverless on Kubernetes : Fission を使ってみた</p></a>
                    </li>
                
                    <li>
                        <a href="https://jedipunkz.github.io/blog/2017/01/13/kubernetes-deployments/"><p>Kubernetes Deployments を使ってみた！</p></a>
                    </li>
                
            </ul>
        </section>

    
        
</section>

    
    <div id="main">
        
        
            
        

        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2015/07/16/chef-zero-openstack-allinone/">Chef-ZeroでOpenStack Kiloデプロイ(オールインワン編)</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2015-07-16'>
            July 16, 2015</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは。@jedipunkz です。</p>

<p>久々に openstack-chef-repo を覗いてみたら &lsquo;openstack/openstack-chef-repo&rsquo; とし
て公開されていました。今まで stackforge 側で管理されていましたが &lsquo;openstack&rsquo;
の方に移動したようです。</p>

<p><a href="https://github.com/openstack/openstack-chef-repo">https://github.com/openstack/openstack-chef-repo</a></p>

<p>結構安定してきているのかな？と想い、ちらっと試したのですが案の定、簡単に動作さ
せることが出来ました。</p>

<p>今回はこのレポジトリを使ってオールインワン構成の OpenStack Kilo を作る方法をま
とめていきます。</p>

<h2 id="前提の構成">前提の構成</h2>

<p>このレポジトリは Vagrant で OpenStack を作るための環境一式が最初から用意されて
いますが、Vagrant では本番環境を作ることは出来ないため、Ubuntu ホストを前提と
した記述に差し替えて説明していきます。前提にする構成は下記のとおりです。</p>

<ul>
<li>Uuntu Linux 14.04 x 1 台</li>
<li>ネットワークインターフェース x 3 つ</li>
<li>eth0 : External ネットワーク用</li>
<li>eth1 : Internal (API, Manage) ネットワーク用</li>
<li>eth2 : Guest ネットワーク用</li>
</ul>

<p>特徴としては上記なのですが、eth2 に関してはオールインワンなので必ずしも必要と
いうわけではありません。複数台構成を考慮した設定になっています。</p>

<h2 id="前提のip-アドレス">前提のIP アドレス</h2>

<p>この記事では下記の IP アドレスを前提にします。お手持ちの環境の IP アドレスが違
い場合はそれに合わせて後に示す json ファイルを変更してください。</p>

<ul>
<li>10.0.1.10 (eth0) : external ネットワーク</li>
<li>10.0.2.10 (eth1) : api/management ネットワーク</li>
<li>10.0.3.10 (eth2) : Guest ネットワーク</li>
</ul>

<h2 id="事前の準備">事前の準備</h2>

<p>事前に対象ホスト (OpenStack ホスト) に chef, berkshelf をインストールします。</p>

<pre><code class="language-bash">sudo -i
curl -L https://www.opscode.com/chef/install.sh | bash
</code></pre>

<p>Berkshelf をインストールするのに必要なソフトウェアをインストールします。</p>

<pre><code class="language-bash">apt-get -y install build-essential zlib1g-dev libssl-dev libreadline-dev ruby-dev libxml2-dev libxslt-dev g++
</code></pre>

<p>Berkshelf をインストールします。</p>

<pre><code class="language-bash">/opt/chef/embedded/bin/gem install berkshelf --no-ri --no-rdoc
</code></pre>

<h2 id="デプロイ作業">デプロイ作業</h2>

<p>それでは openstack-chef-repo を取得してデプロイの準備を行います。
ブランチの指定は行わず master ブランチを取得します。Kilo は master ブランチで
管理されています。次のバージョンの開発が始まるタイミングで &lsquo;stable/kilo&rsquo; ブラ
ンチに管理が移されます。</p>

<pre><code class="language-bash">sudo -i
cd ~/
git clone https://github.com/openstack/openstack-chef-repo.git
</code></pre>

<p>次に Berkshelf を使って必要な Cookbooks をダウンロードします。</p>

<pre><code class="language-bash">cd ~/openstack-chef-repo
/opt/chef/embedded/bin/berks vendor ./cookbooks
</code></pre>

<p>Environment を作成します。これは各環境に合わせた設定ファイルのようなもので、各
Cookbooks の Attributes を上書きする仕組みになっています。下記の内容を</p>

<pre><code>openstack-chef-repo/environments/aio-neutron-kilo.json
</code></pre>

<p>というファイル名で保存してください。</p>

<pre><code class="language-json">{
  &quot;name&quot;: &quot;aio-neutron-kilo&quot;,
  &quot;description&quot;: &quot;test&quot;,
  &quot;cookbook_versions&quot;: {
  },
  &quot;json_class&quot;: &quot;Chef::Environment&quot;,
  &quot;chef_type&quot;: &quot;environment&quot;,
  &quot;default_attributes&quot;: {
  },
  &quot;override_attributes&quot;: {
    &quot;mysql&quot;: {
      &quot;bind_address&quot;: &quot;0.0.0.0&quot;,
      &quot;server_root_password&quot;: &quot;mysqlroot&quot;,
      &quot;server_debian_password&quot;: &quot;mysqlroot&quot;,
      &quot;server_repl_password&quot;: &quot;mysqlroot&quot;,
      &quot;allow_remote_root&quot;: true,
      &quot;root_network_acl&quot;: [&quot;10.0.0.0/8&quot;]
    },
    &quot;rabbitmq&quot;: {
      &quot;address&quot;: &quot;0.0.0.0&quot;,
      &quot;port&quot;: &quot;5672&quot;,
      &quot;loopback_users&quot;: []
    },
    &quot;openstack&quot;: {
      &quot;auth&quot;: {
        &quot;validate_certs&quot;: false
      },
      &quot;dashboard&quot;: {
        &quot;session_backend&quot;: &quot;file&quot;
      },
      &quot;block-storage&quot;: {
        &quot;syslog&quot;: {
          &quot;use&quot;: false
        },
        &quot;api&quot;: {
          &quot;ratelimit&quot;: &quot;False&quot;
        },
        &quot;debug&quot;: true,
        &quot;image_api_chef_role&quot;: &quot;os-image&quot;,
        &quot;identity_service_chef_role&quot;: &quot;os-identity&quot;,
        &quot;rabbit_server_chef_role&quot;: &quot;os-ops-messaging&quot;
      },
      &quot;compute&quot;: {
        &quot;rabbit&quot;: {
          &quot;host&quot;: &quot;10.0.2.10&quot;
        },
        &quot;novnc_proxy&quot;: {
          &quot;bind_interface&quot;: &quot;eth1&quot;
        },
        &quot;libvirt&quot;: {
          &quot;virt_type&quot;: &quot;qemu&quot;,
          &quot;bind_interface&quot;: &quot;eth1&quot;
        },
        &quot;novnc_proxy&quot;: {
          &quot;bind_interface&quot;: &quot;eth1&quot;
        },
        &quot;xvpvnc_proxy&quot;: {
          &quot;bind_interface&quot;: &quot;eth1&quot;
        },
        &quot;image_api_chef_role&quot;: &quot;os-image&quot;,
        &quot;identity_service_chef_role&quot;: &quot;os-identity&quot;,
        &quot;nova_setup_chef_role&quot;: &quot;os-compute-api&quot;,
        &quot;rabbit_server_chef_role&quot;: &quot;os-ops-messaging&quot;,
        &quot;network&quot;: {
          &quot;public_interface&quot;: &quot;eth1&quot;,
          &quot;service_type&quot;: &quot;neutron&quot;
        }
      },
      &quot;network&quot;: {
        &quot;debug&quot;: &quot;True&quot;,
        &quot;dhcp&quot;: {
          &quot;enable_isolated_metadata&quot;: &quot;True&quot;
        },
        &quot;metadata&quot;: {
          &quot;nova_metadata_ip&quot;: &quot;10.0.2.10&quot;
        },
        &quot;openvswitch&quot;: {
          &quot;tunnel_id_ranges&quot;: &quot;1:1000&quot;,
          &quot;enable_tunneling&quot;: &quot;True&quot;,
          &quot;tenant_network_type&quot;: &quot;gre&quot;,
          &quot;local_ip_interface&quot;: &quot;eth2&quot;
        },
        &quot;api&quot;: {
          &quot;bind_interface&quot;: &quot;eth1&quot;
        },
        &quot;l3&quot;: {
          &quot;external_network_bridge_interface&quot;: &quot;eth0&quot;
        },
        &quot;service_plugins&quot;: [&quot;neutron.services.l3_router.l3_router_plugin.L3RouterPlugin&quot;]
      },
      &quot;db&quot;: {
        &quot;bind_interface&quot;: &quot;eth1&quot;,
        &quot;compute&quot;: {
          &quot;host&quot;: &quot;10.0.2.10&quot;
        },
        &quot;identity&quot;: {
          &quot;host&quot;: &quot;10.0.2.10&quot;
        },
        &quot;image&quot;: {
          &quot;host&quot;: &quot;10.0.2.10&quot;
        },
        &quot;network&quot;: {
          &quot;host&quot;: &quot;10.0.2.10&quot;
        },
        &quot;volume&quot;: {
          &quot;host&quot;: &quot;10.0.2.10&quot;
        },
        &quot;dashboard&quot;: {
          &quot;host&quot;: &quot;10.0.2.10&quot;
        },
        &quot;telemetry&quot;: {
          &quot;host&quot;: &quot;10.0.2.10&quot;
        },
        &quot;orchestration&quot;: {
          &quot;host&quot;: &quot;10.0.2.10&quot;
        }
      },
      &quot;developer_mode&quot;: true,
      &quot;endpoints&quot;: {
        &quot;compute-api-bind&quot;: {
          &quot;host&quot;: &quot;10.0.2.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;8774&quot;
        },
        &quot;compute-api&quot;: {
          &quot;host&quot;: &quot;10.0.2.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;8774&quot;
        },
        &quot;compute-ec2-admin-bind&quot;: {
          &quot;host&quot;: &quot;10.0.2.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;8773&quot;
        },
        &quot;compute-ec2-admin&quot;: {
          &quot;host&quot;: &quot;10.0.2.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;8773&quot;
        },
       &quot;compute-ec2-api-bind&quot;: {
          &quot;host&quot;: &quot;10.0.2.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;8773&quot;
        },
        &quot;compute-ec2-api&quot;: {
          &quot;host&quot;: &quot;10.0.2.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;8773&quot;
        },
        &quot;compute-xvpvnc&quot;: {
          &quot;host&quot;: &quot;10.0.2.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;6081&quot;
        },
        &quot;compute-novnc-bind&quot;: {
          &quot;host&quot;: &quot;10.0.2.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;6080&quot;
        },
        &quot;compute-novnc&quot;: {
          &quot;host&quot;: &quot;10.0.2.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;6080&quot;
        },
        &quot;compute-vnc&quot;: {
          &quot;host&quot;: &quot;0.0.0.0&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;6080&quot;
        },
        &quot;image-api&quot;: {
          &quot;host&quot;: &quot;10.0.2.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;9292&quot;
        },
        &quot;image-api-bind&quot;: {
          &quot;host&quot;: &quot;10.0.2.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;9292&quot;
        },
        &quot;image-registry&quot;: {
          &quot;host&quot;: &quot;10.0.2.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;9191&quot;
        },
        &quot;image-registry-bind&quot;: {
          &quot;host&quot;: &quot;10.0.2.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;9191&quot;
        },
        &quot;identity-api&quot;: {
          &quot;host&quot;: &quot;10.0.2.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;5000&quot;
        },
        &quot;identity-bind&quot;: {
          &quot;host&quot;: &quot;10.0.2.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;5000&quot;
        },
        &quot;identity-admin&quot;: {
          &quot;host&quot;: &quot;10.0.2.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;35357&quot;
        },
        &quot;identity-internal&quot;: {
          &quot;host&quot;: &quot;10.0.2.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;35357&quot;
        },
        &quot;volume-api-bind&quot;: {
          &quot;host&quot;: &quot;10.0.2.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;8776&quot;
        },
        &quot;volume-api&quot;: {
          &quot;host&quot;: &quot;10.0.2.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;8776&quot;
        },
        &quot;telemetry-api&quot;: {
          &quot;host&quot;: &quot;10.0.2.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;8777&quot;
        },
        &quot;network-api-bind&quot;: {
          &quot;host&quot;: &quot;10.0.2.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;9696&quot;
        },
        &quot;network-api&quot;: {
          &quot;host&quot;: &quot;10.0.2.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;9696&quot;
        },
        &quot;orchestration-api&quot;: {
          &quot;host&quot;: &quot;10.0.2.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;8004&quot;
        },
        &quot;orchestration-api-cfn&quot;: {
          &quot;host&quot;: &quot;10.0.2.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;8000&quot;
        },
        &quot;db&quot;: {
          &quot;host&quot;: &quot;0.0.0.0&quot;,
          &quot;port&quot;: &quot;3306&quot;
        },
        &quot;bind-host&quot;: &quot;0.0.0.0&quot;
      },
      &quot;identity&quot;: {
        &quot;admin_user&quot;: &quot;admin&quot;,
        &quot;bind_interface&quot;: &quot;eth1&quot;,
        &quot;debug&quot;: true
      },
      &quot;image&quot;: {
        &quot;api&quot;: {
          &quot;bind_interface&quot;: &quot;eth1&quot;
        },
        &quot;debug&quot;: true,
        &quot;identity_service_chef_role&quot;: &quot;os-identity&quot;,
        &quot;rabbit_server_chef_role&quot;: &quot;os-ops-messaging&quot;,
        &quot;registry&quot;: {
          &quot;bind_interface&quot;: &quot;eth1&quot;
        },
        &quot;syslog&quot;: {
          &quot;use&quot;: false
        }
      },
      &quot;mq&quot;: {
        &quot;bind_interface&quot;: &quot;eth1&quot;,
        &quot;host&quot;: &quot;10.0.2.10&quot;,
        &quot;user&quot;: &quot;guest&quot;,
        &quot;vhost&quot;: &quot;/nova&quot;,
        &quot;network&quot;: {
          &quot;rabbit&quot;: {
             &quot;host&quot;: &quot;10.0.2.10&quot;,
             &quot;port&quot;: &quot;5672&quot;
          }
        },
        &quot;compute&quot;: {
           &quot;service_type&quot;: &quot;rabbitmq&quot;,
          &quot;rabbit&quot;: {
            &quot;host&quot;: &quot;10.0.2.10&quot;,
            &quot;port&quot;: &quot;5672&quot;
          }
        },
        &quot;block-storage&quot;: {
          &quot;service_type&quot;: &quot;rabbitmq&quot;,
          &quot;rabbit&quot;: {
            &quot;host&quot;: &quot;10.0.2.10&quot;,
            &quot;port&quot;: &quot;5672&quot;
          }
        }
      }
    },
    &quot;queue&quot;: {
      &quot;host&quot;: &quot;10.0.2.10&quot;,
      &quot;user&quot;: &quot;guest&quot;,
      &quot;vhost&quot;: &quot;/nova&quot;
    }
  }
}
</code></pre>

<p>上記ファイルは KVM が使えない環境用に virt_type : qemu にしていますが、KVM が
利用できる環境をご利用であれば該当行を削除してください。デフォルト値の &lsquo;kvm&rsquo;
が入るはずです。</p>

<p>次にデプロイ前に databag 関連の事前操作を行います。Vagrant 用に作成されたファ
イルを除くと&hellip;</p>

<pre><code class="language-ruby">machine 'controller' do
  add_machine_options vagrant_config: controller_config
  role 'allinone-compute'
  role 'os-image-upload'
  chef_environment env
  file('/etc/chef/openstack_data_bag_secret',
       &quot;#{File.dirname(__FILE__)}/.chef/encrypted_data_bag_secret&quot;)
  converge true
end
</code></pre>

<p>となっていて /etc/chef/openstack_data_bag_secret というファイルを事前にコピー
する必要がありそうです。下記のように操作します。</p>

<pre><code class="language-bash">cp .chef/encrypted_data_bag_secret /etc/chef/openstack_data_bag_secret
</code></pre>

<p>デプロイを実行します。</p>

<p>この openstack-chef-repo には .chef ディレクトリが存在していてノード名が記され
ています。&rsquo;nodienode&rsquo; というノード名です。これを利用してそのままデプロイを実行
します。</p>

<pre><code class="language-bash">chef-client -z
knife node -z run_list add nodienode 'role[allinone-compute]'
chef-client -z -E aio-neutron-kilo
</code></pre>

<p>上記の説明を行います。
１行目 chef-client -z で Chef-Zero サーバをメモリ上に起動し、2行目で自ノードへ
run_list を追加しています。最後、3行目でデプロイ実行、となります。</p>

<p>数分待つと OpenStack Kilo が構成されているはずです。</p>

<h2 id="まとめ">まとめ</h2>

<p>Chef-Zero を用いることで Chef サーバを利用せずに楽に構築が行えました。ですが、
OpenStack の複数台構成となるとそれぞれのノードのパラメータを連携させる必要が出
てくるので Chef サーバを用いたほうが良さそうです。今度、時間を見つけて Kilo の
複数台構成についても調べておきます。</p>

<p>また、master ブランチを使用していますので、まだ openstack-chef-repo 自体が流動
的な状態とも言えます。が launchpad で管理されている Bug リストを見ると、ステー
タス Critical, High の Bug が見つからなかったので Kilo に関しては、大きな問題
無く安定してきている感があります。</p>

<p><a href="https://bugs.launchpad.net/openstack-chef">https://bugs.launchpad.net/openstack-chef</a></p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2015/07/16/chef-zero-openstack-allinone/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>

        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2015/06/25/minio/">オブジェクトストレージ minio を使ってみる</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2015-06-25'>
            June 25, 2015</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは、@jedipunkz です。</p>

<p>久々にブログ更新になりましたが、ウォーミングアップで minio というオブジェクト
ストレージを使ってみたメモを記事にしたいと想います。</p>

<p>minio は Minimal Object Storage の名の通り、最小限の小さなオブジェクトストレー
ジになります。公式サイトは下記のとおりです。</p>

<p><a href="http://minio.io/">http://minio.io/</a></p>

<p>Golang で記述されていて Apache License v2 の元に公開されています。</p>

<p>最近、資金調達の話も挙がっていたので、これから一般的になってくるのかもしれません。</p>

<p>早速ですが、minio を動かしてみます。</p>

<h2 id="minio-を起動する">Minio を起動する</h2>

<p>方法は mithub.com/minio/minio の README に書かれていますが、バイナリを持ってき
て実行権限を与えるだけのシンプルな手順になります。</p>

<p>Linux でも Mac でも動作しますが、今回私は Mac 上で動作させました。</p>

<pre><code class="language-bash">% wget https://dl.minio.io:9000/updates/2015/Jun/darwin-amd64/minio
% chmod +x minio
% ./minio mode memory limit 512MB
Starting minio server on: http://127.0.0.1:9000
Starting minio server on: http://192.168.1.123:9000
</code></pre>

<p>起動すると Listening Port と共に EndPoint の URL が表示されます。</p>

<p>次に mc という minio client を使って動作確認します。</p>

<h2 id="mc-を使ってアクセスする">Mc を使ってアクセスする</h2>

<p>mc は下記の URL にあります。</p>

<p><a href="https://github.com/minio/mc">https://github.com/minio/mc</a></p>

<p>こちらもダウンロードして実行権限を付与するのみです。mc は minio だけではなく、
Amazon S3 とも互換性がありアクセス出来ますが、せっかくなので上記で起動した
minio にアクセスします。</p>

<pre><code class="language-bash">% wget https://dl.minio.io:9000/updates/2015/Jun/darwin-amd64/mc
% chmod +x mc
% ./mc config generate
/mc ls  http://127.0.0.1:9000/bucket01
[2015-06-25 16:21:37 JST]     0B testfile
</code></pre>

<p>上記では予め作っておいた bucket01 という名前のバケットの中身を表示しています。
作り方はこれから minio の Golang ライブラリである minio-go を使って作りました。
これから説明します。</p>

<p>また ls コマンドの他にも Usage を確認すると幾つかのサブコマンドが見つかります。</p>

<h2 id="minio-の-golang-ライブラリ-minio-go-を使ってアクセスする">Minio の Golang ライブラリ minio-go を使ってアクセスする</h2>

<p>さて、せっかくのオブジェクトストレージも手作業でファイルやバケットのアクセスを
行うのはもったいないです。ソフトウェアを使って操作してす。</p>

<p>minio のサンプルのコードを参考にして、下記のコードを作成してみました。</p>

<pre><code class="language-go">package main

import (
    &quot;log&quot;
    &quot;os&quot;

    &quot;github.com/minio/minio-go&quot;
)

func main() {
    config := minio.Config{
        // AccessKeyID:     &quot;YOUR-ACCESS-KEY-HERE&quot;,
        // SecretAccessKey: &quot;YOUR-PASSWORD-HERE&quot;,
        Endpoint:        &quot;http://127.0.0.1:9000&quot;,
    }

    s3Client, err := minio.New(config)
    if err != nil {
        log.Fatalln(err)
    }

    err = s3Client.MakeBucket(&quot;bucket01&quot;, minio.BucketACL(&quot;public-read-write&quot;))
    if err != nil {
        log.Fatalln(err)
    }
    log.Println(&quot;Success: I made a bucket.&quot;)

    object, err := os.Open(&quot;testfile&quot;)
    if err != nil {
        log.Fatalln(err)
    }
    defer object.Close()
    objectInfo, err := object.Stat()
    if err != nil {
        object.Close()
        log.Fatalln(err)
    }

    err = s3Client.PutObject(&quot;bucket01&quot;, &quot;testfile&quot;, &quot;application/octet-stream&quot;, objectInfo.Size(), object)
    if err != nil {
        log.Fatalln(err)
    }

    for bucket := range s3Client.ListBuckets() {
        if bucket.Err != nil {
            log.Fatalln(bucket.Err)
        }
        log.Println(bucket.Stat)
    }

    for object := range s3Client.ListObjects(&quot;bucket01&quot;, &quot;&quot;, true) {
        if object.Err != nil {
            log.Fatalln(object.Err)
        }
        log.Println(object.Stat)
    }

}
</code></pre>

<p>簡単ですがコードの説明をします。</p>

<ul>
<li>11行目で config の上書きをします。先ほど起動した minio の EndPoint を記します。</li>
<li>17行目で minio にセッションを張り接続を行っています。</li>
<li>22行目で &lsquo;bucket01&rsquo; というバケットを生成しています。その際にACLも設定</li>
<li>28行目から42行目で &lsquo;testfile&rsquo; というローカルファイルをストレージにPUTしています。</li>
<li>44行目でバケット一覧を表示しています。</li>
<li>51行目で上記で作成したバケットの中のオブジェクト一覧を表示しています。</li>
</ul>

<p>実行結果は下記のとおりです。</p>

<pre><code class="language-bash">2015/06/25 16:56:21 Success: I made a bucket.
2015/06/25 16:56:21 {bucket01 2015-06-25 07:56:21.155 +0000 UTC}
2015/06/25 16:56:21 {&quot;d41d8cd98f00b204e9800998ecf8427e&quot; testfile 2015-06-25
07:56:21.158 +0000 UTC 0 {minio minio} STANDARD}
</code></pre>

<p>バケットの作成とオブジェクトの PUT が正常に行えたことをログから確認できます。</p>

<h2 id="まとめ">まとめ</h2>

<p>上記の通り、今現在出来ることは少ないですが冒頭にも記したとおり資金調達の話も挙
がってきていますので、これからどのような方向に向かうか楽しみでもあります。また
最初から Golang, Python 等のライブラリが用意されているところが今どきだなぁと想
いました。オブジェクトストレージを手作業で操作するケースは現場では殆ど無いと想
いますので、その辺は現在では当たり前になりつつあるかもしれません。ちなみに
Python のライブラリは下記の URL にあります。</p>

<p><a href="https://github.com/minio/minio-py">https://github.com/minio/minio-py</a></p>

<p>以上です。</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2015/06/25/minio/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>

        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2014/12/16/vyos-vxlan/">VyOS で VXLAN を使ってみる</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2014-12-16'>
            December 16, 2014</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは。@jedipunkz です。</p>

<p>VyOS に VXLAN が実装されたと聞いて少し触ってみました。この情報を知ったきっかけ
は @upaa さんの下記の資料です。</p>

<p>参考資料 : <a href="http://www.slideshare.net/upaa/vyos-users-meeting-2-vyosvxlan">http://www.slideshare.net/upaa/vyos-users-meeting-2-vyosvxlan</a></p>

<p>VyOS は御存知の通り実体は Debian Gnu/Linux 系の OS でその上に OSS なミドル
ウェアが搭載されていて CLI でミドルウェアのコンフィギュレーション等が行えるモ
ノになっています。Linux で VXLAN といえば OVS を使ったモノがよく知られています
が VyOS の VXLAN 機能は Linux Kernel の実装を使っているようです。</p>

<h2 id="要件">要件</h2>

<ul>
<li>トンネルを張るためのセグメントを用意</li>
<li>VyOS 1.1.1 (現在最新ステーブルバージョン) が必要</li>
<li>Ubuntu Server 14.04 LTS (同じく Linux VXLAN 搭載バージョン)</li>
</ul>

<h2 id="構成">構成</h2>

<p>特徴</p>

<ul>
<li>マネージメント用セグメント 10.0.1.0/24 を用意</li>
<li>GRE と同じくトンネル終端が必要なのでそのためのセグメント 10.0.2.0/24 を用意</li>
<li>各 eth1 は IP reachable である必要があるので予め IP アドレスの設定と疎通を確認</li>
<li>VXLAN を喋れる Ubuntu 14.04 LTS x 1 台と VyOS 1.1.1 x 2 台で相互に疎通確認</li>
</ul>

<pre><code>+-------------+-------------+------------ Management 10.0.1.0/24
|10.0.0.254   |10.0.0.253   |10.0.0.1
|eth0         |eth0         |eth0
+----------+  +----------+  +----------+ 
|  vyos01  |  |  vyos02  |  |  ubuntu  |
+-+--------+  +----------+  +----------+ 
| |eth1       | |eth1       | |eth1
| |10.0.2.254 | |10.0.2.253 | |10.0.2.1
| +-----------)-+-----------)-+---------- Tunneling 10.0.2.0/24
|             |             |
+-------------+-------------+------------ VXLAN(eth1にlink) 10.0.1.0/24
10.0.1.254     10.0.1.253    10.0.1.1
</code></pre>

<h2 id="設定を投入">設定を投入</h2>

<p>vyos01 の設定を行う。VXLAN の設定に必要なものは&hellip;</p>

<ul>
<li>VNI (VXLAN Network Ideintity)という識別子</li>
<li>Multicast Group Address</li>
<li>互いに IP reachable なトンネルを張るためのインターフェース</li>
</ul>

<p>です。これらを意識して下記の設定を vyos01 に投入します。</p>

<pre><code class="language-bash">$ configure
% set interfaces vxlan vxlan0
% set interfaces vxlan vxlan0 group 239.1.1.1
% set interfaces vxlan vxlan0 vni 42
% set interfaces vxlan vxlan0 address '10.0.1.254/24'
% set interfaces vxlan vxlan0 link eth1
</code></pre>

<p>設定を確認します</p>

<pre><code class="language-bash">% exit
$ show int
...&lt;省略&gt;...
    vxlan vxlan0 {
     address 10.0.1.254/24
     group 239.1.1.1
     link eth1
     vni 42
}
</code></pre>

<p>VyOS の CLI を介さず直 Linux の設定を iproute2 で確認してみましょう。
VNI, Multicast Group Address と共に &lsquo;link eth1&rsquo; で設定したトンネルを終端するための物理 NIC が確認できます。</p>

<pre><code class="language-bash">vyos@vyos01# ip -d link show vxlan0
5: vxlan0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN mode DEFAULT group default
    link/ether 86:24:26:b2:11:5c brd ff:ff:ff:ff:ff:ff promiscuity 0
    vxlan id 42 group 239.1.1.1 dev eth1 port 32768 61000 ttl 16 ageing 300
</code></pre>

<p>vyos02 の設定を同様に行います。</p>

<pre><code class="language-bash">$ congigure
% set interfaces vxlan vxlan0 address '10.0.1.253/24'
% set interfaces vxlan vxlan0 vni 42
% set interfaces vxlan vxlan0 group 239.1.1.1
% set interfaces vxlan vxlan0 link eth1
</code></pre>

<p>設定の確認を行います。</p>

<pre><code class="language-bash">... 省略 ...
vxlan vxlan0 {
     address 10.0.1.254/24
     group 239.1.1.1
     link eth1
     vni 42
}
</code></pre>

<p>同じく Linux の iproute2 で確認します。</p>

<pre><code class="language-bash">vyos@vyos01# ip -d link show vxlan0
5: vxlan0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN mode DEFAULT group default
    link/ether 86:24:26:b2:11:5c brd ff:ff:ff:ff:ff:ff promiscuity 0
    vxlan id 42 group 239.1.1.1 dev eth1 port 32768 61000 ttl 16 ageing 300
</code></pre>

<p>ubuntu ホストの設定を行っていきます。</p>

<p>Ubuntu Server 14.04 LTS であればパッチを当てること無く Linux Kernel の VXLAN 機能を使うことができます。
設定内容は VyOS と同等です。VyOS がこの Linux の実装を使っているのがよく分かります。</p>

<pre><code class="language-bash">sudo modprobe vxlan
sudo ip link add vxlan0 type vxlan id 42 group 239.1.1.1 dev eth1
sudo ip link set up vxlan0
sudo ip a add 10.0.1.1/24 dev vxlan0
</code></pre>

<p>同じく Linux iproute2 で確認を行います。</p>

<pre><code class="language-bash"> ip -d link show vxlan0
5: vxlan0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN mode DEFAULT group default
    link/ether d6:ff:c1:27:69:a0 brd ff:ff:ff:ff:ff:ff promiscuity 0
    vxlan id 42 group 239.1.1.1 dev eth1 port 32768 61000 ageing 300
</code></pre>

<h2 id="疎通確認">疎通確認</h2>

<p>疎通確認を行います。</p>

<p>ubuntu -&gt; vyos01 の疎通確認です。ICMP で疎通が取れることを確認できます。</p>

<pre><code class="language-bash">thirai@ubuntu:~$ ping 10.0.1.254 -c 3
PING 10.0.1.254 (10.0.1.254) 56(84) bytes of data.
64 bytes from 10.0.1.254: icmp_seq=1 ttl=64 time=0.272 ms
64 bytes from 10.0.1.254: icmp_seq=2 ttl=64 time=0.336 ms
64 bytes from 10.0.1.254: icmp_seq=3 ttl=64 time=0.490 ms

--- 10.0.1.254 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 1999ms
rtt min/avg/max/mdev = 0.272/0.366/0.490/0.091 ms
</code></pre>

<p>次に ubuntu -&gt; vyos02 の疎通確認です。</p>

<pre><code class="language-bash">thirai@ubuntu:~$ ping 10.0.1.253 -c 3
PING 10.0.1.253 (10.0.1.253) 56(84) bytes of data.
64 bytes from 10.0.1.253: icmp_seq=1 ttl=64 time=0.272 ms
64 bytes from 10.0.1.253: icmp_seq=2 ttl=64 time=0.418 ms
64 bytes from 10.0.1.253: icmp_seq=3 ttl=64 time=0.451 ms

--- 10.0.1.253 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 1998ms
rtt min/avg/max/mdev = 0.272/0.380/0.451/0.079 ms
</code></pre>

<p>この時点で ubuntu ホストの fdb (forwarding db) の内容を確認します。</p>

<pre><code class="language-bash">$ bridge fdb show dev vxlan0
00:00:00:00:00:00 dst 239.1.1.1 via eth1 self permanent
4e:69:a4:a7:ef:1c dst 10.0.2.253 self
86:24:26:b2:11:5c dst 10.0.2.254 self
</code></pre>

<p>vyos01, vyos02 のトンネル終端 IP アドレスと Mac アドレスが確認できます。ubuntu ホストから見ると
送信先は vyos0[12] の VXLAN インターフェースではなく、あくまでもトンネル終端を行っているインターフェース
になることがわかります。</p>

<h2 id="まとめ">まとめ</h2>

<p>VyOS ver 1.1.0 には VXLAN を物理インターフェースに link する機能に不具合がありそうなので今ら ver 1.1.1 を使うしか
なさそう。とは言え、ver 1.1.1 なら普通に動作しました。</p>

<p>VyOS は仮想ルータという位置付けなので今回紹介したようにインターフェースを VXLAN ネットワークに所属させる
機能があるのみです。VXLAN Trunk を行うような設定はありません。これはハイパーバイザ上で動作させることを前提
に設計されているので仕方ないです..というかスイッチで行うべき機能ですよね..。VM を接続して云々するには OVS
のようなソフトウェアスイッチを使えばできます。</p>

<p>また fdb は時間が経つと情報が消えます。これは VXLAN のメッシュ構造なトンネルがその都度張られているのかどうか
気になるところです。ICMP の送信で一発目のみマルチキャストでその後ユニキャストになることを確認しましたが、その
一発目のマルチキャストでトンネリングがされるものなのでしょうか&hellip;。あとで調べてみます。OVS のように CLI で
トンネルがどのように張られているか確認する手段があれば良いのですが。</p>

<p>以上です。</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2014/12/16/vyos-vxlan/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>

        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2014/12/13/aviator-openstack/">Aviator でモダンに OpenStack を操作する</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2014-12-13'>
            December 13, 2014</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは。@jedipunkz です。</p>

<p>自分は Ruby を普段使うのでいつも Fog というライブラリを使って OpenStack, AWS
を操作していました。Fog を使うとクラウドの操作が Ruby のネイティブコードで行え
るのでシステムコマンド打つよりミス無く済みます。</p>

<p>Fog より後発で Aviator というライブラリが登場してきたので少し使ってみたのです
がまだ未完成なところがあるものの便利な点もあって今後に期待だったので紹介します。</p>

<h2 id="認証情報を-yaml-ファイルに記す">認証情報を yaml ファイルに記す</h2>

<p>接続に必要な認証情報を yaml ファイルで記述します。名前を &lsquo;aviator.yml&rsquo; として
保存。この時に下記のように環境毎に認証情報を別けて書くことができます。こうする
ことでコードの中で開発用・サービス用等と使い分けられます。</p>

<pre><code class="language-yaml">production:
  provider: openstack
  auth_service:
    name: identity
    host_uri: &lt;Auth URL&gt;
    request: create_token
    validator: list_tenants
  auth_credentials:
    username: &lt;User Name&gt;
    password: &lt;Password&gt;
    tenant_name: &lt;Tenant Name&gt;

development:
  provider: openstack
  auth_service:
    name: identity
    host_uri: &lt;Auth URL&gt;
    request: create_token
    validator: list_tenants
  auth_credentials:
    username: &lt;User Name&gt;
    password: &lt;Password&gt;
    tenant_name: &lt;Tenant Name&gt;
</code></pre>

<p>シンタックス確認
+++</p>

<p>次に aviator のシンタックスを確認します。Fog に無い機能で、コマンドラインでシ
ンタックスを確認できてしかも指定可能はパラメータと必須なパラメータと共にサンプ
ルコードまで提供してくれます。公式サイトに&rsquo;サーバ作成&rsquo;のメソッドが掲載されてい
るので、ここでは仮想ディスクを作るシンタックスを確認してみます。</p>

<pre><code class="language-bash">% gem install aviator
% aviator describe openstack volume # &lt;-- 利用可能な機能を確認
Available requests for openstack volume_service:
v1 public list_volume_types
v1 public list_volumes
v1 public delete_volume
v1 public create_volume
v1 public get_volume
v1 public update_volume
  v1 public root
% aviator describe openstack volume v1 public create_volume # &lt;-- シンタックスを確認
:Request =&gt; create_volume

Parameters:
 +---------------------+-----------+
 | NAME                | REQUIRED? |
 +---------------------+-----------+
 | availability_zone   |     N     |
 | display_description |     Y     |
 | display_name        |     Y     |
 | metadata            |     N     |
 | size                |     Y     |
 | snapshot_id         |     N     |
 | volume_type         |     N     |
 +---------------------+-----------+

Sample Code:
  session.volume_service.request(:create_volume) do |params|
    params.volume_type = value
    params.availability_zone = value
    params.snapshot_id = value
    params.metadata = value
    params.display_name = value
    params.display_description = value
    params.size = value
  end
</code></pre>

<p>このように create_volume というメソッドが用意されていて、指定出来るパラメータ・
必須なパラメータが確認できます。必須なモノには &ldquo;Y&rdquo; が REQUIRED に付いています。
またサンプルコードが出力されるので、めちゃ便利です。</p>

<p>では create_volume のシンタックスがわかったので、コードを書いてみましょう。</p>

<p>コードを書いてみる
+++</p>

<pre><code class="language-ruby">#!/usr/bin/env ruby

require 'aviator'
require 'json'

volume_session = Aviator::Session.new(
              :config_file =&gt; '/home/thirai/aviator/aviator.yml',
              :environment =&gt; :production,
              :log_file    =&gt; '/home/thirai/aviator/aviator.log'
            )

volume_session.authenticate

volume_session.volume_service.request(:create_volume) do |params|
  params.display_description = 'testvol'
  params.display_name = 'testvol01'
  params.size = 1
end
puts volume_session.volume_service.request(:list_volumes).body
</code></pre>

<p>6行目で先ほど作成した認証情報ファイル aviator.yml とログ出力ファイル
aviator.log を指定します。12行目で実際に OpenStack にログインしています。</p>

<p>14-18行目はサンプルコードそのままです。必須パラメータの display_description,
display_name, size のみを指定し仮想ディスクを作成しました。最後の puts &hellip; は
実際に作成した仮想ディスク一覧を出力しています。</p>

<p>結果は下記のとおりです。</p>

<pre><code class="language-json">{ volumes: [{ status: 'available', display_name: 'testvol01', attachments: [],
availability_zone: 'az3', bootable: 'false', created_at:
description = 'testvol', volume_type:
'standard', snapshot_id: nil, source_volid: nil, metadata:  }, id:
'3a5f616e-a732-4442-a419-10369111bd4c', size: 1 }] }
</code></pre>

<p>まとめ
+++</p>

<p>サンプルコードやパラメータ一覧等がひと目でわかる aviator はとても便利です。ま
だ利用できるクラウドプラットフォームが OpenStack しかないのと、Neutron の機能
がスッポリ抜けているので、まだ利用するには早いかもです&hellip;。逆に言えばコントリ
ビューションするチャンスなので、もし気になった方がいたら開発に参加してみるのも
いいかもしれません。</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2014/12/13/aviator-openstack/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Categories
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
        <li><a href='/categories/tools'>tools</a></li>
    
</ul>

    </footer>
</article>

        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2014/11/15/chef-zero-openstack-icehouse/">Chef-Zero でお手軽に OpenStack Icehouse を作る</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2014-11-15'>
            November 15, 2014</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは。<a href="https://twitter.com/jedipunkz">@jedipunkz</a> です。</p>

<p>OpenStack Juno がリリースされましたが、今日は Icehouse ネタです。</p>

<p>icehouse 以降、自分の中で OpenStack を自動で作る仕組みが無くなりつつあり、気軽
に OpenStack を作って色々試したい！ッていう時に手段が無く困っていました。例え
ば仕事でちょっと OpenStack 弄りたい！って時に DevStack, RDO しかなく。DevStack
は御存知の通り動かない可能性が結構あるし RDO は Ubuntu/Debian Gnu Linux ベース
じゃないし。</p>

<p>ってことで、以前にも紹介した stackforge 管理の openstack-chef-repo と
Chef-Zero を使って OpenStack Icehouse (Neutron) のオールインワン構成を作る方法
を書きます。ちなみに最近 Chef-Solo が Chef-Zero に置き換わりつつあるらしいです。
Chef-Zero はオンメモリで Chef サーバを起動する仕組みです。Chef-Solo と違って Chef
サーバを扱う時と何も変更無く操作が出来るのでとても楽です。また、Chef サーバを
別途構、構築・管理しなくて良いので、気軽に OpenStack が作れます。</p>

<p>ちなみに stackforge/openstack-chef-repo の README.md に Chef-Zero での構築方法
が書いてありますが、沢山の問題があります。</p>

<ul>
<li>nova-network 構成</li>
<li>API の Endpoint が全て localhost に向いてしまうため外部から操作不可能</li>
<li>各コンポーネントの bind_address が localhost を向いてしまう</li>
<li>berkshelf がそのままでは入らない</li>
</ul>

<p>よって、今回はこれらの問題を解決しつつ &ldquo;オールインワンな Neutron 構成の
Icehouse OpenStack を作る方法&rdquo; を書いていきます。</p>

<h2 id="構成">構成</h2>

<pre><code>+----------------- 10.0.0.0/24 (api/management network)
|
+----------------+
| OpenStack Node |
|   Controller   |
|    Compute     |
+----------------+
|  |
+--(-------------- 10.0.1.0/24 (external network)
   |
   +-------------- 10.0.2.0/24 (guest vm network)
</code></pre>

<p>IP address 達</p>

<ul>
<li>10.0.0.10 (api/manageent network) : eth0</li>
<li>10.0.1.10 (external network) : eth1</li>
<li>10.0.2.10 (guest vm network) : eth2</li>
</ul>

<p>注意 : 操作は全て eth0 経由で行う</p>

<h2 id="前提の環境">前提の環境</h2>

<p>stackforge/openstack-chef-repo の依存している Cookbooks の関係上、upstart 周り
がうまく制御できていないので Ubuntu Server 12.04.x を使います。</p>

<h2 id="インストール方法">インストール方法</h2>

<p>上記のように3つのネットワークインターフェースが付いたサーバを1台用意します。
KVM が利用出来たほうがいいですが使えないくても構いません。KVM リソースが使えな
い場合の修正方法を後に記します。</p>

<p>サーバにログインし root ユーザになります。その後 Chef をオムニバスインストーラ
でインストールします。</p>

<pre><code class="language-bash">% sudo -i
# curl -L https://www.opscode.com/chef/install.sh | bash
</code></pre>

<p>次に stable/icehose ブランチを指定して openstack-chef-repo をクローンします。</p>

<pre><code class="language-bash"># cd ~
# git clone -b stable/icehouse https://github.com/stackforge/openstack-chef-repo
# 
</code></pre>

<p>berkshelf をインストールするのですが依存パッケージが足らないのでここでインストー
ルします。</p>

<pre><code class="language-bash"># apt-get -y install build-essential zlib1g-dev libssl-dev libreadline-dev \
  ruby-dev libxml2-dev libxslt-dev g++
</code></pre>

<p>berkshelf をインストールします。</p>

<pre><code class="language-bash"># /opt/chef/embedded/bin/gem install berkshelf --no-ri --no-rdoc
</code></pre>

<p>次に openstack-chef-repo に依存する Cookbooks を取得します。</p>

<pre><code class="language-bash"># cd ~/openstack-chef-repo
# /opt/chef/embedded/bin/berks vendor ./cookbooks
</code></pre>

<p>~/openstack-chef-repo/environments ディレクトリ配下に neutron-allinone.json と
いうファイル名で作成します。内容は下記の通りです。</p>

<pre><code class="language-json">{                                                                                                                                                      [0/215]
  &quot;name&quot;: &quot;neutron-allinone&quot;,
  &quot;description&quot;: &quot;test&quot;,
  &quot;cookbook_versions&quot;: {
  },
  &quot;json_class&quot;: &quot;Chef::Environment&quot;,
  &quot;chef_type&quot;: &quot;environment&quot;,
  &quot;default_attributes&quot;: {
  },
  &quot;override_attributes&quot;: {
    &quot;mysql&quot;: {
      &quot;bind_address&quot;: &quot;0.0.0.0&quot;,
      &quot;server_root_password&quot;: &quot;root&quot;,
      &quot;server_debian_password&quot;: &quot;root&quot;,
      &quot;server_repl_password&quot;: &quot;root&quot;,
      &quot;allow_remote_root&quot;: true,
      &quot;root_network_acl&quot;: [&quot;10.0.0.0/8&quot;]
    },
    &quot;rabbitmq&quot;: {
      &quot;address&quot;: &quot;10.0.1.10&quot;,
      &quot;port&quot;: &quot;5672&quot;
    },
    &quot;openstack&quot;: {
      &quot;auth&quot;: {
        &quot;validate_certs&quot;: false
      },
      &quot;dashboard&quot;: {
        &quot;session_backend&quot;: &quot;file&quot;
      },
      &quot;block-storage&quot;: {
        &quot;syslog&quot;: {
          &quot;use&quot;: false
        },
        &quot;api&quot;: {
          &quot;ratelimit&quot;: &quot;False&quot;
        },
        &quot;debug&quot;: true,
        &quot;image_api_chef_role&quot;: &quot;os-image&quot;,
        &quot;identity_service_chef_role&quot;: &quot;os-identity&quot;,
        &quot;rabbit_server_chef_role&quot;: &quot;os-ops-messaging&quot;
      },
      &quot;compute&quot;: {
        &quot;rabbit&quot;: {
          &quot;host&quot;: &quot;10.0.1.10&quot;
        },
        &quot;novnc_proxy&quot;: {
          &quot;bind_interface&quot;: &quot;eth0&quot;
        },
        &quot;libvirt&quot;: {
          &quot;bind_interface&quot;: &quot;eth0&quot;,
        },
        &quot;novnc_proxy&quot;: {
          &quot;bind_interface&quot;: &quot;eth0&quot;
        },
        &quot;xvpvnc_proxy&quot;: {
          &quot;bind_interface&quot;: &quot;eth0&quot;
        },
        &quot;image_api_chef_role&quot;: &quot;os-image&quot;,
        &quot;identity_service_chef_role&quot;: &quot;os-identity&quot;,
        &quot;nova_setup_chef_role&quot;: &quot;os-compute-api&quot;,
        &quot;rabbit_server_chef_role&quot;: &quot;os-ops-messaging&quot;,
        &quot;network&quot;: {
          &quot;public_interface&quot;: &quot;eth0&quot;,
          &quot;service_type&quot;: &quot;neutron&quot;
        }
      },
      &quot;network&quot;: {
        &quot;debug&quot;: &quot;True&quot;,
        &quot;dhcp&quot;: {
          &quot;enable_isolated_metadata&quot;: &quot;True&quot;
        },
        &quot;metadata&quot;: {
          &quot;nova_metadata_ip&quot;: &quot;10.0.1.10&quot;
        },
        &quot;openvswitch&quot;: {
          &quot;tunnel_id_ranges&quot;: &quot;1:1000&quot;,
          &quot;enable_tunneling&quot;: &quot;True&quot;,
          &quot;tenant_network_type&quot;: &quot;gre&quot;,
          &quot;local_ip_interface&quot;: &quot;eth2&quot;
        },
        &quot;api&quot;: {
          &quot;bind_interface&quot;: &quot;eth0&quot;
        },
        &quot;l3&quot;: {
          &quot;external_network_bridge_interface&quot;: &quot;eth1&quot;
        },
        &quot;service_plugins&quot;: [&quot;neutron.services.l3_router.l3_router_plugin.L3RouterPlugin&quot;]
      },
      &quot;db&quot;: {
        &quot;bind_interface&quot;: &quot;eth0&quot;,
        &quot;compute&quot;: {
          &quot;host&quot;: &quot;10.0.1.10&quot;
        },
        &quot;identity&quot;: {
          &quot;host&quot;: &quot;10.0.1.10&quot;
        },
        &quot;image&quot;: {
          &quot;host&quot;: &quot;10.0.1.10&quot;
        },
        &quot;network&quot;: {
          &quot;host&quot;: &quot;10.0.1.10&quot;
        },
        &quot;volume&quot;: {
          &quot;host&quot;: &quot;10.0.1.10&quot;
        },
        &quot;dashboard&quot;: {
          &quot;host&quot;: &quot;10.0.1.10&quot;
        },
        &quot;telemetry&quot;: {
          &quot;host&quot;: &quot;10.0.1.10&quot;
        },
        &quot;orchestration&quot;: {
          &quot;host&quot;: &quot;10.0.1.10&quot;
        }
      },
      &quot;developer_mode&quot;: true,
      &quot;endpoints&quot;: {
        &quot;compute-api-bind&quot;: {
          &quot;host&quot;: &quot;10.0.1.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;8774&quot;
        },
        &quot;compute-api&quot;: {
          &quot;host&quot;: &quot;10.0.1.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;8774&quot;
        },
        &quot;compute-ec2-admin-bind&quot;: {
          &quot;host&quot;: &quot;10.0.1.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;8773&quot;
        },
        &quot;compute-ec2-admin&quot;: {
          &quot;host&quot;: &quot;10.0.1.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;8773&quot;
        },
       &quot;compute-ec2-api-bind&quot;: {
          &quot;host&quot;: &quot;10.0.1.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;8773&quot;
        },
        &quot;compute-ec2-api&quot;: {
          &quot;host&quot;: &quot;10.0.1.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;8773&quot;
        },
        &quot;compute-xvpvnc&quot;: {
          &quot;host&quot;: &quot;10.0.1.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;6081&quot;
        },
        &quot;compute-novnc-bind&quot;: {
          &quot;host&quot;: &quot;10.0.1.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;6080&quot;
        },
        &quot;compute-novnc&quot;: {
          &quot;host&quot;: &quot;10.0.1.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;6080&quot;
        },
        &quot;compute-vnc&quot;: {
          &quot;host&quot;: &quot;0.0.0.0&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;6080&quot;
        },
        &quot;image-api&quot;: {
          &quot;host&quot;: &quot;10.0.1.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;9292&quot;
        },
        &quot;image-api-bind&quot;: {
          &quot;host&quot;: &quot;10.0.1.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;9292&quot;
        },
        &quot;image-registry&quot;: {
          &quot;host&quot;: &quot;10.0.1.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;9191&quot;
        },
        &quot;image-registry-bind&quot;: {
          &quot;host&quot;: &quot;10.0.1.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;9191&quot;
        },
        &quot;identity-api&quot;: {
          &quot;host&quot;: &quot;10.0.1.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;5000&quot;
        },
        &quot;identity-bind&quot;: {
          &quot;host&quot;: &quot;10.0.1.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;5000&quot;
        },
        &quot;identity-admin&quot;: {
          &quot;host&quot;: &quot;10.0.1.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;35357&quot;
        },
        &quot;volume-api-bind&quot;: {
          &quot;host&quot;: &quot;10.0.1.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;8776&quot;
        },
        &quot;volume-api&quot;: {
          &quot;host&quot;: &quot;10.0.1.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;8776&quot;
        },
        &quot;telemetry-api&quot;: {
          &quot;host&quot;: &quot;10.0.1.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;8777&quot;
        },
        &quot;network-api-bind&quot;: {
          &quot;host&quot;: &quot;10.0.1.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;9696&quot;
        },
        &quot;network-api&quot;: {
          &quot;host&quot;: &quot;10.0.1.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;9696&quot;
        },
        &quot;orchestration-api&quot;: {
          &quot;host&quot;: &quot;10.0.1.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;8004&quot;
        },
        &quot;orchestration-api-cfn&quot;: {
          &quot;host&quot;: &quot;10.0.1.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;8000&quot;
        }
      },
      &quot;identity&quot;: {
        &quot;admin_user&quot;: &quot;admin&quot;,
        &quot;bind_interface&quot;: &quot;eth0&quot;,
        &quot;debug&quot;: true
      },
      &quot;image&quot;: {
        &quot;api&quot;: {
          &quot;bind_interface&quot;: &quot;eth0&quot;
        },
        &quot;debug&quot;: true,
        &quot;identity_service_chef_role&quot;: &quot;os-identity&quot;,
        &quot;rabbit_server_chef_role&quot;: &quot;os-ops-messaging&quot;,
        &quot;registry&quot;: {
          &quot;bind_interface&quot;: &quot;eth0&quot;
        },
        &quot;syslog&quot;: {
          &quot;use&quot;: false
        },
        &quot;upload_images&quot;: [
          &quot;precise&quot;
        ]
      },
      &quot;mq&quot;: {
        &quot;bind_interface&quot;: &quot;eth0&quot;,
        &quot;host&quot;: &quot;10.0.1.10&quot;,
        &quot;user&quot;: &quot;guest&quot;,
        &quot;vhost&quot;: &quot;/nova&quot;,
        &quot;network&quot;: {
          &quot;rabbit&quot;: {
             &quot;host&quot;: &quot;10.0.1.10&quot;,
             &quot;port&quot;: &quot;5672&quot;
          }
        },
        &quot;compute&quot;: {
           &quot;service_type&quot;: &quot;rabbitmq&quot;,
          &quot;rabbit&quot;: {
            &quot;host&quot;: &quot;10.0.1.10&quot;,
            &quot;port&quot;: &quot;5672&quot;
          }
        },
        &quot;block-storage&quot;: {
          &quot;service_type&quot;: &quot;rabbitmq&quot;,
          &quot;rabbit&quot;: {
            &quot;host&quot;: &quot;10.0.1.10&quot;,
            &quot;port&quot;: &quot;5672&quot;
          }
        }
      }
    },
    &quot;queue&quot;: {
      &quot;host&quot;: &quot;10.0.1.10&quot;,
      &quot;user&quot;: &quot;guest&quot;,
      &quot;vhost&quot;: &quot;/nova&quot;
    }
  }
}
</code></pre>

<p>内容について全て説明するのは難しいですが、このファイルを作成するのが今回一番苦
労した点です。と言うのは、構成を作りつつそれぞれのコンポーネントのコンフィギュ
レーション、エンドポイントのアドレス、バインドアドレス、リスンポート等など、全
てが正常な値になるように Cookbooks を読みつつ作業するからです。この json ファ
イルが完成してしまえば、あとは簡単なのですが。</p>

<p>前述しましたが KVM リソースが使えない環境の場合 Qemu で仮想マシンを稼働するこ
とができます。その場合、下記のように &ldquo;libvirt&rdquo; の項目に &ldquo;virt_type&rdquo; を追記して
ください。</p>

<pre><code class="language-json">        &quot;libvirt&quot;: {
          &quot;bind_interface&quot;: &quot;eth0&quot;,
          &quot;virt_type&quot;: &quot;qemu&quot; # &lt;------ 追記
        },
</code></pre>

<p>それではデプロイしていきます。</p>

<p>ここで &lsquo;allinone&rsquo; はホスト名、&rsquo;allinone-compute&rsquo; は Role 名、neutron-allinone
は先ほど作成した json で指定している environment 名です。</p>

<pre><code class="language-bash"># chef-client -z
# knife node -z run_list add allinone 'role[allinone-compute]'
# chef-client -z -E neutron-allinone
</code></pre>

<p>環境にもよりますが、数分でオールインワンな OpenStack Icehouse が完成します。</p>

<p>まとめ
+++</p>

<p>Chef サーバを使わなくて良いのでお手軽に OpenStack が構築出来ました。この json
ファイルは実は他にも応用出来ると思っています。複数台構成の OpenStack も指定
Role を工夫すれば構築出来るでしょう。が、その場合は chef-zero は使えません。
Chef サーバ構成にする必要が出てきます。</p>

<p>ちなみに OpenStack Paris Summit 2014 で「OpenStack のデプロイに何を使っている
か？」という調査結果が下記になります。Chef が2位ですが Pueppet に大きく離され
ている感があります。Juno 版の openstack-chef-repo も開発が進んでいますので、頑
張って広めていきたいです。</p>

<ul>
<li>1位 Puppet</li>
<li>2位 Chef</li>
<li>3位 Ansible</li>
<li>4位 DevStack</li>
<li>5位 PackStack</li>
<li>6位 Salt</li>
<li>7位 Juju</li>
<li>8位 Crowbar</li>
<li>9位 CFEngine</li>
</ul>

<p>参考 URL : <a href="http://superuser.openstack.org/articles/openstack-user-survey-insights-november-2014">http://superuser.openstack.org/articles/openstack-user-survey-insights-november-2014</a></p>

<p>ちなみに、Puppet を使った OpenStack デプロイも個人的に色々試しています。</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2014/11/15/chef-zero-openstack-icehouse/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>

        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2014/11/04/midostack/">MidoStack を動かしてみる</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2014-11-04'>
            November 4, 2014</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは。@jedipunkz です。</p>

<p>昨晩 Midokura さんが Midonet を OSS 化したとニュースになりました。公式サイトは
下記の URL になっています。Midonet は OpenStack Neutron のプラグインとして動作
するソフトウェアです。</p>

<p><a href="http://www.midonet.org">http://www.midonet.org</a></p>

<p>下記のGithub 上でソースを公開しています。</p>

<p><a href="https://github.com/midonet">https://github.com/midonet</a></p>

<p>本体の midonet と共に midostack というレポジトリがあってどうやら公式サイトの
QuickStart を見ても devstack を交えての簡単な midonet の動作が確認できそう。</p>

<p><a href="https://github.com/midonet/midostack">https://github.com/midonet/midostack</a></p>

<h2 id="早速使ってみる">早速使ってみる</h2>

<p>早速 midostack を使って midonet を体験してみましょう。QuickStart には
Vagrant + VirtualBox を用いた使い方が改定ありますが手元の PC 端末だとリソース
が足らなくて CirrOS VM 一個すら立ち上がりませんでした。よって普通にリソースの
沢山あるサーバで稼働してみます。Vagrantfile 見ても</p>

<pre><code>config.vm.synced_folder &quot;./&quot;, &quot;/midostack&quot;
</code></pre>

<p>としているだけなので、Vagrant ではなくても大丈夫でしょう。</p>

<p>Ubuntu Server 14.04 をインストールしたマシンを用意して midostack を取得します。</p>

<pre><code class="language-bash">% git clone https://github.com/midonet/midostack.git
</code></pre>

<p>midonet_stack.sh を実行します。</p>

<pre><code class="language-bash">% cd midostack
% ./midonet_stack.sh
</code></pre>

<p>暫く待つと Neutron Middonet Plugin が有効になった OpenStack が立ち上がります。
Horizon にアクセスしましょう。ユーザ名 : admin, パスワード : gogomid0 (デフォ
ルト) です。</p>

<p>VM も普通に立ち上がりますし VM 同士の通信も良好です。</p>

<h2 id="neutron-プロセスを確認する">Neutron プロセスを確認する</h2>

<p>Neutron-Server は下記のように立ち上がっています。</p>

<pre><code class="language-bash">16229 pts/13   S+     0:06 python /usr/local/bin/neutron-server --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugins/midonet/midonet.ini
</code></pre>

<p>/etc/neutron/neutron.conf の midonet の指定はこんな感じ。</p>

<pre><code>core_plugin = midonet.neutron.plugin.MidonetPluginV2
api_extensions_path = /opt/stack/midonet/python-neutron-plugin-midonet/midonet/neutron/extensions
</code></pre>

<p>次に /etc/neutron/plugins/midonet/midonet.ini を確認してみましょう。</p>

<pre><code>[midonet]
# MidoNet API server URI
# midonet_uri = http://localhost:8080/midonet-api

# MidoNet admin username
# username = admin

# MidoNet admin password
# password = passw0rd

# ID of the project that MidoNet admin user belongs to
# project_id = 77777777-7777-7777-7777-777777777777

# Virtual provider router ID
# provider_router_id = 00112233-0011-0011-0011-001122334455

# Path to midonet host uuid file
# midonet_host_uuid_path = /etc/midolman/host_uuid.properties

[MIDONET]
project_id = admin
password = gogomid0
username = admin
midonet_uri = http://localhost:8081/midonet-api
</code></pre>

<h2 id="midonet-api-にアクセスする">Midonet API にアクセスする</h2>

<p>Midonet API のリファレンスが下記の URL で公開されていました。</p>

<p><a href="http://docs.midonet.org/docs/v1.8/rest-api/api/rest-api-specification.html">http://docs.midonet.org/docs/v1.8/rest-api/api/rest-api-specification.html</a></p>

<p>早速使ってみましょう。まず Token を得ます。</p>

<pre><code class="language-bash">curl -i 'http://127.0.0.1:5000/v2.0/tokens' -X POST -H &quot;Content-Type: application/json&quot; -H &quot;Accept: application/json&quot;  -d '{&quot;auth&quot;: {&quot;tenantName&quot;: &quot;admin&quot;, &quot;passwordCredentials&quot;: {&quot;username&quot;: &quot;admin&quot;, &quot;password&quot;: &quot;gogomid0&quot;}}}'
</code></pre>

<p>Token ID を取得したら &ldquo;/&rdquo; に対してアクセスしてみましょう。</p>

<pre><code class="language-bash">% curl -i -X GET http://localhost:8081/midonet-api/ -H &quot;User-Agent: python-keystoneclient&quot; -H &quot;X-Auth-Token: &lt;TokenID&gt;&quot;
</code></pre>

<p>レスポンス</p>

<pre><code class="language-json">{
&quot;routerTemplate&quot;: &quot;http://localhost:8081/midonet-api/routers/{id}&quot;,
&quot;portTemplate&quot;: &quot;http://localhost:8081/midonet-api/ports/{id}&quot;,
&quot;vipTemplate&quot;: &quot;http://localhost:8081/midonet-api/vips/{id}&quot;,
&quot;poolTemplate&quot;: &quot;http://localhost:8081/midonet-api/pools/{id}&quot;,
&quot;healthMonitorTemplate&quot;: &quot;http://localhost:8081/midonet-api/health_monitors/{id}&quot;,
&quot;healthMonitors&quot;: &quot;http://localhost:8081/midonet-api/health_monitors&quot;,
&quot;loadBalancers&quot;: &quot;http://localhost:8081/midonet-api/load_balancers&quot;,
&quot;ipAddrGroupTemplate&quot;: &quot;http://localhost:8081/midonet-api/ip_addr_groups/{id}&quot;,
&quot;tenants&quot;: &quot;http://localhost:8081/midonet-api/tenants&quot;,
&quot;tenantTemplate&quot;: &quot;http://localhost:8081/midonet-api/tenants/{id}&quot;,
&quot;portGroupTemplate&quot;: &quot;http://localhost:8081/midonet-api/port_groups/{id}&quot;,
&quot;loadBalancerTemplate&quot;: &quot;http://localhost:8081/midonet-api/load_balancers/{id}&quot;,
&quot;poolMemberTemplate&quot;: &quot;http://localhost:8081/midonet-api/pool_members/{id}&quot;,
&quot;hostVersions&quot;: &quot;http://localhost:8081/midonet-api/versions&quot;,
&quot;version&quot;: &quot;v1.7&quot;,
&quot;bridgeTemplate&quot;: &quot;http://localhost:8081/midonet-api/bridges/{id}&quot;,
&quot;hostTemplate&quot;: &quot;http://localhost:8081/midonet-api/hosts/{id}&quot;,
&quot;uri&quot;: &quot;http://localhost:8081/midonet-api/&quot;,
&quot;vteps&quot;: &quot;http://localhost:8081/midonet-api/vteps&quot;,
&quot;tunnelZoneTemplate&quot;: &quot;http://localhost:8081/midonet-api/tunnel_zones/{id}&quot;,
&quot;ipAddrGroups&quot;: &quot;http://localhost:8081/midonet-api/ip_addr_groups&quot;,
&quot;writeVersion&quot;: &quot;http://localhost:8081/midonet-api/write_version&quot;,
&quot;chainTemplate&quot;: &quot;http://localhost:8081/midonet-api/chains/{id}&quot;,
&quot;vtepTemplate&quot;: &quot;http://localhost:8081/midonet-api/vteps/{ipAddr}&quot;,
&quot;adRouteTemplate&quot;: &quot;http://localhost:8081/midonet-api/ad_routes/{id}&quot;,
&quot;bgpTemplate&quot;: &quot;http://localhost:8081/midonet-api/bgps/{id}&quot;,
&quot;hosts&quot;: &quot;http://localhost:8081/midonet-api/hosts&quot;,
&quot;routeTemplate&quot;: &quot;http://localhost:8081/midonet-api/routes/{id}&quot;,
&quot;ruleTemplate&quot;: &quot;http://localhost:8081/midonet-api/rules/{id}&quot;,
&quot;systemState&quot;: &quot;http://localhost:8081/midonet-api/system_state&quot;,
&quot;vips&quot;: &quot;http://localhost:8081/midonet-api/vips&quot;,
&quot;pools&quot;: &quot;http://localhost:8081/midonet-api/pools&quot;,
&quot;routers&quot;: &quot;http://localhost:8081/midonet-api/routers&quot;,
&quot;bridges&quot;: &quot;http://localhost:8081/midonet-api/bridges&quot;,
&quot;chains&quot;: &quot;http://localhost:8081/midonet-api/chains&quot;,
&quot;portGroups&quot;: &quot;http://localhost:8081/midonet-api/port_groups&quot;,
&quot;poolMembers&quot;: &quot;http://localhost:8081/midonet-api/pool_members&quot;,
&quot;tunnelZones&quot;: &quot;http://localhost:8081/midonet-api/tunnel_zones&quot;
}
</code></pre>

<p>なんとなく引数にこれらの文字列を渡せばいいのだなと分かります。</p>

<p>次に neutron の管理している subnets を確認してみましょう。</p>

<pre><code class="language-bash">% curl -i -X GET http://localhost:8081/midonet-api/neutron/subnets -H &quot;User-Agent: python-keystoneclient&quot; -H &quot;X-Auth-Token: &lt;TokenID&gt;&quot;
</code></pre>

<p>レスポンス</p>

<pre><code class="language-json">[
  {
    &quot;enable_dhcp&quot;: false,
    &quot;tenant_id&quot;: &quot;65f7012145d84ac5afc36572eabe5b09&quot;,
    &quot;host_routes&quot;: [],
    &quot;dns_nameservers&quot;: [],
    &quot;id&quot;: &quot;3dbe5cff-8a8c-4790-85b5-b789d8ede863&quot;,
    &quot;name&quot;: &quot;public-subnet&quot;,
    &quot;cidr&quot;: &quot;200.200.200.0/24&quot;,
    &quot;shared&quot;: false,
    &quot;ip_version&quot;: 4,
    &quot;network_id&quot;: &quot;45269fba-e32f-40b0-a542-f5cfe34ce1a1&quot;,
    &quot;gateway_ip&quot;: &quot;200.200.200.1&quot;,
    &quot;allocation_pools&quot;: [
      {
        &quot;last_ip&quot;: null,
        &quot;first_ip&quot;: null
      }
    ]
  },
  {
    &quot;enable_dhcp&quot;: true,
    &quot;tenant_id&quot;: &quot;f34b4398015546b8b84f50c731ed6c51&quot;,
    &quot;host_routes&quot;: [],
    &quot;dns_nameservers&quot;: [],
    &quot;id&quot;: &quot;3dbcf04a-9738-4b1f-b084-76f2a4b17cbc&quot;,
    &quot;name&quot;: &quot;private-subnet&quot;,
    &quot;cidr&quot;: &quot;10.0.0.0/24&quot;,
    &quot;shared&quot;: false,
    &quot;ip_version&quot;: 4,
    &quot;network_id&quot;: &quot;2edb78c3-0f23-4e29-a3e6-cc97f55baa6a&quot;,
    &quot;gateway_ip&quot;: &quot;10.0.0.1&quot;,
    &quot;allocation_pools&quot;: [
      {
        &quot;last_ip&quot;: null,
        &quot;first_ip&quot;: null
      }
    ]
  }
]
</code></pre>

<p>２つのサブネットが確認出来ました。</p>

<h2 id="まとめ">まとめ</h2>

<p>勉強不足でまだ全く midonet で出来る事がわからない..汗。でもとりあえず動かせた
し、API も引っ張れるのでこれから色々試せそうですね。OSS 化されたことで、コミュ
ニティの間でも使われていくことも想像出来ますし、自分たち技術者としてはとても有
り難いことでした。</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2014/11/04/midostack/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>

        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2014/07/16/chef-container/">Chef-Container Beta を使ってみる</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2014-07-16'>
            July 16, 2014</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは。<a href="https://twitter.com/jedipunkz">@jedipunkz</a> です。</p>

<p>昨晩 Chef が Chef-Container を発表しました。</p>

<ul>
<li><a href="http://www.getchef.com/blog/2014/07/15/release-chef-container-0-2-0-beta/">http://www.getchef.com/blog/2014/07/15/release-chef-container-0-2-0-beta/</a></li>
<li><a href="http://docs.opscode.com/containers.html">http://docs.opscode.com/containers.html</a></li>
</ul>

<p>まだ Beta リリースでバージョンは 0.2.0 です。(gem だと 0.1.1)</p>

<p>Docker を代表とするコンテナ周りの技術が最近、盛んにリリースされていますし、今
後クラウドプラットフォーム上でコンテナを使ってアプリを動かすケースも増えてくる
のではないでしょうか。Dockerfile を使っても Chef-Solo を使ってソフトウェアをデ
プロイ出来るのだけどそれだけだとしんどいので、コンテナに特化した Chef が出てき
たってことだと思います。特徴として SSH でログインしてブートストラップするので
はなくて Runit + Chef-init を用いてコンテナにデプロイすることが挙げられます。</p>

<p>では実際に使ってみたのでその時の手順をまとめてみます。</p>

<h2 id="事前に用意する環境">事前に用意する環境</h2>

<p>下記のソフトウェアを予めインストールしておきましょう。</p>

<ul>
<li>docker</li>
<li>chef</li>
<li>berkshelf</li>
</ul>

<p>ここで注意なのですが後に knife コマンドを使って Docker イメージをビルドします。
つまり root 権限が必要です。rbenv 等を使って ruby, chef をインストールすると、
辛いかもしれませんので OS のパッケージを使ってインストールすると良いと思います。
この辺りは今後改善策が出てくるかも&hellip;。</p>

<p>尚、インストール方法はここでは割愛します。</p>

<h2 id="chef-container-のインストール">Chef-Container のインストール</h2>

<p>下記の2つの Gems をインストールします。</p>

<ul>
<li>knife-container</li>
<li>chef-container</li>
</ul>

<pre><code class="language-bash">% sudo gem install knife-container
% sudo gem install chef-container
</code></pre>

<h2 id="使用方法">使用方法</h2>

<p>まず knife コマンドを使って操作に必要なディレクトリとファイルを生成します。</p>

<pre><code class="language-bash">% knife container docker init chef/ubuntu-12.04 -r 'recipe[apache2]' -z -b
</code></pre>

<p>ここで &lsquo;chef/ubuntu-12.04&rsquo; は Docker のイメージ名です。chef-init 等の環境が予
め入っていました。このイメージ以外では今のところ動作を確認していません..。これは後にまとめで触れます。</p>

<p>上記のコマンドの結果で得られるディレクトリとファイル達です。</p>

<pre><code>.
└── dockerfiles
    └── chef
        └── ubuntu-12.04
            ├── Berksfile
            ├── chef
            │   ├── first-boot.json
            │   └── zero.rb
            └── Dockerfile
</code></pre>

<p>また dockerfiles/chef/ubuntu-12.04/Dockerfile を確認すると&hellip;</p>

<pre><code># BASE chef/ubuntu-12.04:latest
FROM chef/ubuntu-12.04
ADD chef/ /etc/chef/
RUN chef-init --bootstrap
RUN rm -rf /etc/chef/secure/*
ENTRYPOINT [&quot;chef-init&quot;]
CMD [&quot;--onboot&quot;]
</code></pre>

<p>イメージを取得 -&gt; ディレクトリ同期 -&gt; chef-init 実行 -&gt; /etc/chef/secure 配下削除、と
実行しているようです。</p>

<p>次に first-boot.json という名前のファイルを生成します。chef-init が解釈するファ
イルです。</p>

<pre><code class="language-json">{
   &quot;run_list&quot;: [
      &quot;recipe[apache2]&quot;
   ],
   &quot;container_service&quot;: {
      &quot;apache2&quot;: {
         &quot;command&quot;: &quot;/usr/sbin/apache2 -k start&quot;
      }
   }
}
</code></pre>

<p>ではいよいよ knife コマンドで Docker イメージをビルドします。</p>

<pre><code class="language-bash">% sudo knife container docker build chef/ubuntu-12.04 -z
</code></pre>

<p>すると、下記のように Docker イメージが出来上がります。</p>

<pre><code class="language-bash">% sudo docker images
REPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE
chef/ubuntu-12.04   11                  03fd2357596f        4 days ago          397.7 MB
chef/ubuntu-12.04   11.12               03fd2357596f        4 days ago          397.7 MB
chef/ubuntu-12.04   11.12.8             03fd2357596f        4 days ago          397.7 MB
</code></pre>

<p>出来上がったイメージを利用してコンテナを稼働します。</p>

<pre><code class="language-bash">% sudo docker run chef/ubuntu-12.04
% sudo docker ps
CONTAINER ID        IMAGE               COMMAND              CREATED             STATUS              PORTS               NAMES
191cfdaf0bdb        650a89f73ed8        chef-init --onboot   39 minutes ago      Up 39 minutes                           agitated_almeida
</code></pre>

<h2 id="まとめ">まとめ</h2>

<p>コンテナと言っても今現在は Docker のみに対応しているようです。また init の際に指定する Docker イメージ
の中に chef-init が入っている必要がありそうです。Build する前に予めイメージを作っておく必要があるという
のはしんどいので、今後改善されるかもしれません。</p>

<p>そもそも Docker やコンテナ技術の登場で Puppet, Chef を代表とするツール類が不要になるのでは？という議論が
幾つかの場面であったように思います。つまりコンテナのイメージに予めソフトウェアを配布しそれを用いて稼働
することで、マシンが起動した後にデプロイすることが必要ないよね？という発想です。今回紹介したようにコンテナの
イメージを生成するのに Chef を用いるということであれば、また別の議論になりそうです。また稼働したコンテナに
ソフトウェアをデプロイすることも場合によっては必要なので、この辺りの技術の完成度が上がることを期待したいです。</p>

<h2 id="参考-url">参考 URL</h2>

<ul>
<li>CreationLine さんブログ <a href="http://www.creationline.com/lab/5346">http://www.creationline.com/lab/5346</a></li>
<li>公式サイト <a href="http://docs.opscode.com/containers.html">http://docs.opscode.com/containers.html</a></li>
</ul>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2014/07/16/chef-container/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>

        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2014/06/22/jtf2014-ceph/">JTF2014 で Ceph について話してきた！</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2014-06-22'>
            June 22, 2014</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは。<a href="https://twitter.com/jedipunkz">@jedipunkz</a> です。</p>

<p>今日、JTF2014 (July Tech Festa 2014) というイベントで Ceph のことを話してきま
した。Ceph ユーザ会の会員として話してきたのですが Ceph ユーザ会は実は最近発足
したばかりのユーザ会で、まだまだ活動が活発ではありません。もし興味がある方いらっ
しゃいましたら是非参加よろしくお願いしますー。下記の Google Groups になります。</p>

<p><a href="https://groups.google.com/forum/#!forum/ceph-jp">https://groups.google.com/forum/#!forum/ceph-jp</a></p>

<p>ユーザ会としての勉強会として初になるのですが、今回このイベントで自分は
Ceph-Deploy について話してきました。とりあえず皆さんに使ってもらいたかったので
この話をしてきました。が、予定時間がメチャ短かったので超絶早口で頑張った分、皆
さんに理解してもらえなかった気がしてちょっと反省&hellip;。なので、このブログを利用
して少し細くさせてもらいます。</p>

<p>今日の発表資料はこちらです！</p>

<script async class="speakerdeck-embed"
data-id="592a0b90ceb30131a5d25ae3f95c3a1a" data-ratio="1.33333333333333" src="//speakerdeck.com/assets/embed.js"></script>

<p>今日のテーマは 「Ceph-Deploy を使って Ceph を構築してみる」だったのですが、下
記のテーマを持って資料を作っています。</p>

<ul>
<li>単にミニマム構成ではなく運用を考慮した実用性のある構成</li>
<li>OSD, MON, MDS の各プロセスとノード・ディスクの数の関係を知ってもらう</li>
</ul>

<p>特に「実用性のある..」は意識したつもりでした。そのために前提とした構成に下記の
特徴を持たせています。(資料 6 ページ目に構成図があります。確認してみてください。)</p>

<ul>
<li>オブジェクト格納用ディスクは複数/ノードを前提</li>
<li>OSD レプリケーションのためのクラスタネットワークを用いる構成</li>
<li>OSD の扱うジャーナル格納用ディスクは高速な SSD を用いる</li>
<li>MDS は利用する HW リソースの特徴が異なるので別ノードへ配置</li>
</ul>

<p>ストレージ全体を拡張したければ</p>

<ul>
<li>図中 ceph01-03 の様なノードを増設する</li>
<li>ceph01-03 にディスクとそれに対する OSD を増設する</li>
</ul>

<p>ですが、前者がベストでしょう。ノード増設の場合 ceph-deploy を用いて</p>

<ul>
<li>ceph-deploy mon create &lt;新規ホスト名&gt; で MON を稼働</li>
<li>ceph-dploy disk zap, osd create で OSD を稼働</li>
</ul>

<p>で簡単に可能です。MDS の増設も負荷状況を見ながらするといいでしょう。自分はまだ
Ceph を運用していないので、各プロセスがどのようなリソースの消費の仕方をするの
か知りません。MDS がどのような数で運用していくべきなのか。早く運用から得たノウ
ハウが共有されないかなぁと期待しています。</p>

<p>また今回話すのを忘れたのですが SSD をジャーナル格納用ディスクとして用いたのは
ハードディスクに対して高速でアクセス出来ること・またメタデータはファイルオブジェ
クトに対して小容量で済む、といった理由からです。メタデータを扱うのに適している
と思います。また将来的には幾つかの KVS データベースソフトウェアをメタデータ管
理に使う実装がされるそうです。</p>

<p>以上です。皆さん、是非 Ceph を使ってみてください！ また興味のある方はユーザ会
への加入をご検討くださいー。</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2014/06/22/jtf2014-ceph/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Categories
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
        <li><a href='/categories/report'>report</a></li>
    
</ul>

    </footer>
</article>

        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2014/06/13/mesos-marathon-deimos-docker/">Mesos &#43; Marathon &#43; Deimos &#43; Docker を試してみた!</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2014-06-13'>
            June 13, 2014</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは。@jedipunkz です。</p>

<p>以前 Mesos, Docker について記事にしました。</p>

<p><a href="http://jedipunkz.github.io/blog/2013/09/28/mesos-architecture-number-1/">http://jedipunkz.github.io/blog/2013/09/28/mesos-architecture-number-1/</a>
<a href="http://jedipunkz.github.io/blog/2013/10/01/methos-architecture-number-2-docker-on-mesos/">http://jedipunkz.github.io/blog/2013/10/01/methos-architecture-number-2-docker-on-mesos/</a></p>

<p>Twitter で Docker 関連のオーケストレーションツールについて呟いていたら @everpeace さんから
こんな情報をもらいました。</p>

<p><blockquote class="twitter-tweet" lang="ja"><p><a href="https://twitter.com/jedipunkz">@jedipunkz</a> 元々meos-dockerっていうmesos executorがあったんですけど、mesosがcontainer部分をpluggableにしたので、それに合わせてdeimosっていうmesos用のexternal containerizer が作られました。</p>&mdash; Shingo Omura (@everpeace) <a href="https://twitter.com/everpeace/statuses/476998842383347712">2014, 6月 12</a></blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script></p>

<p>Deimos !!! 知らなかった。Mesos の Docker プラグインらしく下記の場所にありました。</p>

<p><a href="https://github.com/mesosphere/deimos">https://github.com/mesosphere/deimos</a></p>

<p>色々調べいたら、こんな資料が見つかりました。どうやらまだ公開されて4日しか経っていないようです。</p>

<p><a href="http://mesosphere.io/learn/run-docker-on-mesosphere/">http://mesosphere.io/learn/run-docker-on-mesosphere/</a></p>

<p>Mesos + Marathon + Deimos + Docker をオールインワン構成で構築する手順が書かれています。</p>

<p>内容はほぼ同じですが、一応自分がやってみて理解したことをまとめたいので下記に記していきます。</p>

<h2 id="構築してみる">構築してみる</h2>

<p>手順をまとめてスクリプトにしました。パッケージは Ubuntu 13.10 用のようですが 14.04 のホスト
で実行出来ました。14.04 のパッケージはまだ見つかっていません。</p>

<pre><code class="language-bash">#!/bin/bash
# disable ipv6
echo 'net.ipv6.conf.all.disable_ipv6 = 1' | sudo tee -a /etc/sysctl.conf
echo 'net.ipv6.conf.default.disable_ipv6 = 1' | sudo tee -a /etc/sysctl.conf
sudo sysctl -p

# install related tools
sudo apt-get update
sudo apt-get -y install curl python-setuptools python-pip python-dev python-protobuf

# install zookeeper
sudo apt-get -y install zookeeperd
echo 1 | sudo dd of=/var/lib/zookeeper/myid

# install docker
sudo apt-get -y install docker.io
sudo ln -sf /usr/bin/docker.io /usr/local/bin/docker
sudo sed -i '$acomplete -F _docker docker' /etc/bash_completion.d/docker.io
sudo docker pull libmesos/ubuntu

# install mesos
curl -fL http://downloads.mesosphere.io/master/ubuntu/13.10/mesos_0.19.0-xcon3_amd64.deb -o /tmp/mesos.deb
sudo dpkg -i /tmp/mesos.deb
sudo mkdir -p /etc/mesos-master
echo in_memory  | sudo dd of=/etc/mesos-master/registry
curl -fL http://downloads.mesosphere.io/master/ubuntu/13.10/mesos_0.19.0-xcon3_amd64.egg -o /tmp/mesos.egg
sudo easy_install /tmp/mesos.egg

# install marathon
curl -fL http://downloads.mesosphere.io/marathon/marathon_0.5.0-xcon2_noarch.deb -o /tmp/marathon.deb
sudo dpkg -i /tmp/marathon.deb

# restart each services
sudo service docker.io restart
sudo service zookeeper restart
sudo service mesos-master restart
sudo service mesos-slave restart

# install deimos
sudo pip install deimos
sudo mkdir -p /etc/mesos-slave

## Configure Deimos as a containerizer
echo /usr/bin/deimos  | sudo dd of=/etc/mesos-slave/containerizer_path
echo external     | sudo dd of=/etc/mesos-slave/isolation
</code></pre>

<h2 id="プロセスの確認">プロセスの確認</h2>

<p>実行が終わると各プロセスが確認出来ます。オプションでどのプロセスが何を見ているか大体
わかりますので見ていきます。</p>

<h4 id="mesos-master">mesos-master</h4>

<p>mesos-master は zookeeper を参照して 5050 番ポートで起動しているようです。</p>

<pre><code class="language-bash">% ps ax | grep mesos-master
 1224 ?        Ssl    0:30 /usr/local/sbin/mesos-master --zk=zk://localhost:2181/mesos --port=5050 --log_dir=/var/log/mesos --registry=in_memory
</code></pre>

<h4 id="mesos-slave">mesos-slave</h4>

<p>mesos-slave は同じく zookeeper を参照して containerizer を deimos として稼働していることが
わかります。</p>

<pre><code class="language-bash">% ps ax | grep mesos-slave
 1225 ?        Ssl    0:12 /usr/local/sbin/mesos-slave --master=zk://localhost:2181/mesos --log_dir=/var/log/mesos --containerizer_path=/usr/bin/deimos --isolation=external
</code></pre>

<h4 id="zookeeper">zookeeper</h4>

<p>zookeeper は OpenJDK7 で稼働している Java プロセスです。</p>

<pre><code class="language-bash">% ps ax | grep zookeeper
 1073 ?        Ssl    1:07 /usr/bin/java -cp /etc/zookeeper/conf:/usr/share/java/jline.jar:/usr/share/java/log4j-1.2.jar:/usr/share/java/xercesImpl.jar:/usr/share/java/xmlParserAPIs.jar:/usr/share/java/netty.jar:/usr/share/java/slf4j-api.jar:/usr/share/java/slf4j-log4j12.jar:/usr/share/java/zookeeper.jar -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.local.only=false -Dzookeeper.log.dir=/var/log/zookeeper -Dzookeeper.root.logger=INFO,ROLLINGFILE org.apache.zookeeper.server.quorum.QuorumPeerMain /etc/zookeeper/conf/zoo.cfg
</code></pre>

<h4 id="docker">docker</h4>

<p>docker が起動していることも確認できます。設定は特にしていないです。</p>

<pre><code class="language-bash">% ps axuw | grep docker
root       831  0.0  0.3 364776 14924 ?        Sl   01:30   0:01 /usr/bin/docker.io -d
</code></pre>

<h2 id="marathon-の-webui-にアクセス">Marathon の WebUI にアクセス</h2>

<p>Marathon の WebUI にアクセスしてみましょう。</p>

<p><img src="http://jedipunkz.github.com/pix/deimos_01.png"></p>

<p>まだ何も Tasks が実行されていないので一覧には何も表示されないと思います。</p>

<h2 id="tasks-の実行">Tasks の実行</h2>

<p>Marathon API に対してクエリを発行することで Mesos の Tasks として Docker コンテナを稼働させることが出来ます。
下記のファイルを ubuntu.json として保存。</p>

<pre><code class="language-json">{
    &quot;container&quot;: {
    &quot;image&quot;: &quot;docker:///libmesos/ubuntu&quot;
  },
  &quot;id&quot;: &quot;ubuntu&quot;,
  &quot;instances&quot;: &quot;1&quot;,
  &quot;cpus&quot;: &quot;.5&quot;,
  &quot;mem&quot;: &quot;512&quot;,
  &quot;uris&quot;: [ ]
}
</code></pre>

<p>下記の通り localhost:8080 が Marathon API の Endpoint になっているのでここに対して作成した JSON を POST します。</p>

<pre><code class="language-bash">% curl -X POST -H &quot;Content-Type: application/json&quot; localhost:8080/v2/apps -d@ubuntu.json
</code></pre>

<p>Tasks の一覧を取得してみます。</p>

<pre><code class="language-bash">% curl -X GET -H &quot;Content-Type: application/json&quot; localhost:8080/v2/apps
{&quot;apps&quot;:[{&quot;id&quot;:&quot;ubuntu&quot;,&quot;cmd&quot;:&quot;&quot;,&quot;env&quot;:{},&quot;instances&quot;:1,&quot;cpus&quot;:0.5,&quot;mem&quot;:512.0,&quot;executor&quot;:&quot;&quot;,&quot;constraints&quot;:[],&quot;uris&quot;:[],&quot;ports&quot;:[13049],&quot;taskRateLimit&quot;:1.0,&quot;container&quot;:{&quot;image&quot;:&quot;docker:///libmesos/ubuntu&quot;,&quot;options&quot;:[]},&quot;version&quot;:&quot;2014-06-13T01:45:58.693Z&quot;,&quot;tasksStaged&quot;:1,&quot;tasksRunning&quot;:0}]}
</code></pre>

<p>Tasks の一覧が JSON で返ってきます。id : ubuntu, インスタンス数 : 1, CPU 0.5, メモリー : 512MB で
Task が稼働していることが確認出来ます。</p>

<p>ここで WebUI 側も見てみましょう。</p>

<p><img src="http://jedipunkz.github.com/pix/deimos_05.png"></p>

<p>一つ Task が稼働していることが確認出来ると思います。</p>

<p><img src="http://jedipunkz.github.com/pix/deimos_04.png"></p>

<p>その Task をクリックすると詳細が表示されます。</p>

<p>次に Tasks のスケーリングを行ってみましょう。
下記の通り ubuntu.json を修正し instances : 2 とする。これによってインスタンス数が2に増えます。</p>

<pre><code class="language-json">{
    &quot;container&quot;: {
    &quot;image&quot;: &quot;docker:///libmesos/ubuntu&quot;
  },
  &quot;id&quot;: &quot;ubuntu&quot;,
  &quot;instances&quot;: &quot;2&quot;,
  &quot;cpus&quot;: &quot;.5&quot;,
  &quot;mem&quot;: &quot;512&quot;,
  &quot;uris&quot;: [ ]
}
</code></pre>

<p>修正した JSON を POST します。</p>

<pre><code class="language-bash">% curl -X PUT -H &quot;Content-Type: application/json&quot; localhost:8080/v2/apps/ubuntu -d@ubuntu.json
</code></pre>

<p>Tasks の一覧を取得し containers が 2 になっていることが確認できます。</p>

<pre><code class="language-bash">% curl -X GET -H &quot;Content-Type: application/json&quot; localhost:8080/v2/apps
{&quot;apps&quot;:[{&quot;id&quot;:&quot;ubuntu&quot;,&quot;cmd&quot;:&quot;&quot;,&quot;env&quot;:{},&quot;instances&quot;:2,&quot;cpus&quot;:0.5,&quot;mem&quot;:512.0,&quot;executor&quot;:&quot;&quot;,&quot;constraints&quot;:[],&quot;uris&quot;:[],&quot;ports&quot;:[17543],&quot;taskRateLimit&quot;:1.0,&quot;container&quot;:{&quot;image&quot;:&quot;docker:///libmesos/ubuntu&quot;,&quot;options&quot;:[]},&quot;version&quot;:&quot;2014-06-13T02:40:04.536Z&quot;,&quot;tasksStaged&quot;:3,&quot;tasksRunning&quot;:0}]}
</code></pre>

<p>最後に Tasks を削除してみましょう。</p>

<pre><code class="language-bash">% curl -X DELETE -H &quot;Content-Type: application/json&quot; localhost:8080/v2/apps/ubuntu
</code></pre>

<p>Tasks が削除されたことを確認します。</p>

<pre><code class="language-bash">% curl -X GET -H &quot;Content-Type: application/json&quot; localhost:8080/v2/apps
{&quot;apps&quot;:[]}
</code></pre>

<h2 id="marathon-api-v2">Marathon API v2</h2>

<p>Marathon API v2 について下記の URL に仕様が載っています。上記に記したクエリ以外にも色々載っているので
動作を確認してみるといいと思います。</p>

<p><a href="https://github.com/mesosphere/marathon/blob/master/docs/api/http/REST_template.md">https://github.com/mesosphere/marathon/blob/master/docs/api/http/REST_template.md</a></p>

<h2 id="まとめ">まとめ</h2>

<p>オールインワン構成が出来ました。また動作確認も無事出来ています。
以前試した時よりも大分、手順が簡潔になった印象があります。また参考資料中に</p>

<p>&ldquo;checkout our other multi-node tutorials on how to scale Docker in your data center.&rdquo;</p>

<p>とありますが、まだ見つかっていません(´・ω・｀)見つかった方教えてくださいー。</p>

<p>以前試した時は Mesos-Master の冗長化が出来なかったので今回こそ Multi Mesos-Masters,
Multi Mesos-Slaves の構成を作ってみたいと思います。</p>

<p>また今月？になって続々と Docker のオーケストレーションツールを各社が公開しています。</p>

<h5 id="centurion">centurion</h5>

<p>New Relic が開発したオーケストレーションツール。
<a href="https://github.com/newrelic/centurion">https://github.com/newrelic/centurion</a></p>

<h5 id="helios">helios</h5>

<p>Spotify が開発したオーケストレーションツール。
<a href="https://github.com/spotify/helios">https://github.com/spotify/helios</a></p>

<h5 id="fleet">fleet</h5>

<p>CoreOS 標準搭載。
<a href="https://github.com/coreos/fleet">https://github.com/coreos/fleet</a></p>

<h5 id="geard">geard</h5>

<p>RedHat が Red Hat Enterprise Linux Atomic Host に搭載しているツール。
<a href="http://openshift.github.io/geard/">http://openshift.github.io/geard/</a></p>

<h5 id="kubernetes">Kubernetes</h5>

<p>Google が開発したオーケストレーションツール。
<a href="https://github.com/GoogleCloudPlatform/kubernetes">https://github.com/GoogleCloudPlatform/kubernetes</a></p>

<h5 id="shipper">shipper</h5>

<p>Python のコードで Docker をオーケストレーション出来るツール。
<a href="https://github.com/mailgun/shipper">https://github.com/mailgun/shipper</a></p>

<p>幾つか試したのですが、まだまだ動く所までいかないツールがありました。github の README にも
&ldquo;絶賛開発中なのでプロダクトレディではない&rdquo; と書かれています。これからでしょう。</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2014/06/13/mesos-marathon-deimos-docker/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>

        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2014/05/29/fog-aws-ec2-elb/">クラウドライブラリ Fog で AWS を操作！..のサンプル</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2014-05-29'>
            May 29, 2014</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは。<a href="https://twitter.com/jedipunkz">@jedipunkz</a> です。</p>

<p>最近 OpenStack でサービスを開発！.. じゃなくて AWS でプロトタイプサービス作っ
ているのですが、Ruby で開発したかったので Fog を使っています。EC2 と ELB の
API を叩くコードになりつつあるのですが、サンプルコードって世の中に中々無いと気
がついたので、このブログ記事にサンプルコードを載せたいと思います。</p>

<h2 id="fog-とは">Fog とは ?</h2>

<p>Fog <a href="http://fog.io/">http://fog.io/</a> はクラウドライブラリソフトウェアです。AWS, Rackspace,
CloudStack, OpenStack .. と数ある世の中のクラウドプラットフォームを扱うために
用意されたソフトウェアです。対応しているプラットフォームの種別は下記を見ると参
考になります。</p>

<p><a href="http://fog.io/about/provider_documentation.html">http://fog.io/about/provider_documentation.html</a></p>

<p>ドキュメントがまだまだ揃っていなく、Fog のコードを覗きながら実装するしかない状
況です。なので「こう使えば良い！」というお手本があまりネット上にも無い気がしま
す。</p>

<p>ドキュメントは一応下記にあります。
が使い方がよくわからない・・！(´；ω；｀)ﾌﾞﾜｯ</p>

<p><a href="http://rubydoc.info/gems/fog/frames/index">http://rubydoc.info/gems/fog/frames/index</a></p>

<h2 id="ec2-インスタンスを使ってみる">EC2 インスタンスを使ってみる</h2>

<p>まずは AWS EC2 の API を叩いて t1.micro インスタンスを立ち上げてみましょう。</p>

<pre><code class="language-ruby">require 'fog'

compute = Fog::Compute.new({
  :provider =&gt; 'AWS',
  :aws_access_key_id =&gt; '....',
  :aws_secret_access_key =&gt; '....',
  :region =&gt; 'ap-northeast-1'
})

server = compute.servers.create(
  :image_id =&gt; 'ami-cedaa2bc',
  :flavor_id =&gt; 't1.micro',
  :key_name =&gt; 'test_key',
  :tags =&gt; {'Name' =&gt; 'test'},
  :groups =&gt; 'ssh-secgroup'
)

server.wait_for { print &quot;.&quot;; ready? }

puts &quot;created instance name :&quot;, server.dns_name
</code></pre>

<h4 id="解説">解説</h4>

<ul>
<li>compute = &hellip; とあるところで接続情報を記しています。</li>
</ul>

<p>&ldquo;ACCESS_KEY_ID&rdquo; や &ldquo;SECRET_ACCESS_KEY&rdquo; はみなさん接続する時にお持ちですよね。それ
とリージョン名やプロバイダ名 (ここでは AWS) を記して AWS の API に接続します。</p>

<ul>
<li>server = &hellip; とあるところで実際にインスタンスを作成しています。</li>
</ul>

<p>ここではインスタンス生成に必要な情報を盛り込んでいます。Flavor 名や AMI イメー
ジ名・SSH 鍵の名前・セキュリティグループ名等です。</p>

<h2 id="便利なメソッド">便利なメソッド</h2>

<p>server = &hellip; でインスタンスを生成すると便利なメソッドを扱って情報を読み込むこ
とが出来ます。</p>

<pre><code class="language-ruby">server.dns_name # =&gt; public な DNS 名を取得
server.private_dns_name # =&gt; private な DNS 名を取得
server.id # =&gt; インスタンス ID を取得
server.availability_zone # =&gt; Availability Zone を取得
server.public_ip_address # =&gt; public な IP アドレスを取得
server.private_ip_address # =&gt; private な IP アドレスを取得
</code></pre>

<p>これは便利&hellip;</p>

<p>モジュール化して利用
+++</p>

<p>毎回コードの中でこれらの接続情報を書くのはしんどいので、Ruby のモジュールを作
りましょう。</p>

<pre><code class="language-ruby">module AWSCompute
  def self.connect()
    conn = Fog::Compute.new({
      :provider =&gt; 'AWS',
      :aws_access_key_id =&gt; '...',
      :aws_secret_access_key =&gt; '...',
      :region =&gt; '...'
    })
    begin
      yield conn
    ensure
      # conn.close
    end
  rescue Errno::ECONNREFUSED
  end
end
</code></pre>

<p>こう書いておくと例えば&hellip;</p>

<h4 id="インスタンスのターミネイト">インスタンスのターミネイト</h4>

<pre><code class="language-ruby">AWSCompute.connect() do |sock|
  server = sock.servers.get(instance_id)
  server.destroy
  return server.id
end
</code></pre>

<h4 id="インスタンスの起動">インスタンスの起動</h4>

<pre><code class="language-ruby">AWSCompute.connect() do |sock|
  server = sock.servers.get(instance_id)
  server.start
  return server.id
end
</code></pre>

<h4 id="インスタンスの停止">インスタンスの停止</h4>

<pre><code class="language-ruby">AWSCompute.connect() do |sock|
  server = sock.servers.get(instance_id)
  server.stop
  return server.id
end
</code></pre>

<p>等と出来ます。</p>

<h2 id="elb-elastic-loadbalancer-を使ってみる">ELB (Elastic LoadBalancer) を使ってみる</h2>

<p>同様に ELB を扱うコードのサンプルも載せておきます。同じくモジュール化して書くと</p>

<pre><code class="language-ruby">module AWSELB
  def self.connect()
    conn = Fog::AWS::ELB.new(
      :aws_access_key_id =&gt; '...',
      :aws_secret_access_key =&gt; '...',
      :region =&gt; '...',
    )
    begin
      yield conn
    ensure
      # conn.close
    end
  rescue Errno::ECONNREFUSED
  end
end
</code></pre>

<p>としておいて&hellip;</p>

<h4 id="elb-の新規作成">ELB の新規作成</h4>

<p>下記のコードで ELB を新規作成出来ます。</p>

<pre><code class="language-ruby">AWSELB.connect() do |sock|
  availability_zone = '...'
  elb_name = '...'
  listeners = [{ &quot;Protocol&quot; =&gt; &quot;HTTP&quot;, &quot;LoadBalancerPort&quot; =&gt; 80, &quot;InstancePort&quot; =&gt; 80, &quot;InstanceProtocol&quot; =&gt; &quot;HTTP&quot; }]
  result = sock.create_load_balancer(availability_zone, elb_name, listeners)
  p result
end
</code></pre>

<p>この状態では ELB に対してインスタンスが紐付けられていないので使えません。下記の操作で
インスタンスを紐付けてみましょう。</p>

<pre><code class="language-ruby">AWSELB.connect() do |sock|
  insntance_id = '...'
  elb_name = '...'
  result = sock.register_instances_with_load_balancer(instance_id, elbname)
  p result
end
</code></pre>

<p>insntance_id には紐付けたいインスタンスの ID を、elb_name には先ほど作成した ELB の名前を
入力します。 この操作を繰り返せば AWS 上にクラスタが構成出来ます。</p>

<p>逆にクラスタからインスタンスの削除したい場合は下記の通り実行します。</p>

<pre><code class="language-ruby">AWSELB.connect() do |sock|
  result = sock.deregister_instances_from_load_balancer(instance_id, elbname)
  p result
end
</code></pre>

<h2 id="まとめ">まとめ</h2>

<p>今回は Fog を紹介しましたが Python 使いの方には libcloud をおすすめします。</p>

<p><a href="https://libcloud.apache.org/">https://libcloud.apache.org/</a></p>

<p>Apache ファウンデーションが管理しているクラウドライブラリです。こちらも複数の
クラウドプラットフォームに対応しているようです。</p>

<p>Fog で OpenStack も操作したことがあるのですが、AWS 用のコードの方が完成度が高
いのか、戻り値などが綺麗に整形されていて扱いやすかったり、メソッドも豊富に用意
されていたりという印象でした。これは&hellip; OpenStack 用の Fog コードにコントリビュー
トするチャンス・・！</p>

<p>皆さんもサンプルコードお持ちでしたら、ブログ等で公開していきましょうー。
ではでは。</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2014/05/29/fog-aws-ec2-elb/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>

        

        
<ul class="actions pagination">
    
        <li><a href="/post/page/2/"
                class="button big previous">Previous Page</a></li>
    

    
        <li><a href="/post/page/4/"
                class="button big next">Next Page</a></li>
    
</ul>

    </div>
    
<section id="sidebar">

    
        <section id="intro">
            
            
            
            <ul class="icons">
                
                    <li><a href="https://jedipunkz.github.io/post/index.xml" type="application/rss+xml"
                        target="_blank" title="RSS" class="fa fa-rss"></a></li>
                
                
            </ul>
        </section>

    
        <section id="recent-posts">
            <ul class="posts">
                <header>
                    <h3>Recent Posts</h3>
                </header>
                
                    
                

                
                    
                

                
                    <li>
                        <article>
                            <header>
                                <h3><a href="https://jedipunkz.github.io/blog/2018/12/31/istio/">Istio, Helm を使って Getting Started 的なアプリをデプロイ</a></h3>
                                
                                    
                                
                                <time class="published" datetime=
                                    '2018-12-31'>
                                    December 31, 2018</time>
                            </header>
                        </article>
                    </li>
                
                    <li>
                        <article>
                            <header>
                                <h3><a href="https://jedipunkz.github.io/blog/2017/07/02/test-kitchen-cluster/">Docker,Test-Kitchen,Ansible でクラスタを構成する</a></h3>
                                
                                    
                                
                                <time class="published" datetime=
                                    '2017-07-02'>
                                    July 2, 2017</time>
                            </header>
                        </article>
                    </li>
                
                    <li>
                        <article>
                            <header>
                                <h3><a href="https://jedipunkz.github.io/blog/2017/04/13/gke-lb/">GCP ロードバランサと GKE クラスタを Terraform を使って構築する</a></h3>
                                
                                    
                                
                                <time class="published" datetime=
                                    '2017-04-13'>
                                    April 13, 2017</time>
                            </header>
                        </article>
                    </li>
                
                    <li>
                        <article>
                            <header>
                                <h3><a href="https://jedipunkz.github.io/blog/2017/02/12/serverless-fission/">Serverless on Kubernetes : Fission を使ってみた</a></h3>
                                
                                    
                                
                                <time class="published" datetime=
                                    '2017-02-12'>
                                    February 12, 2017</time>
                            </header>
                        </article>
                    </li>
                
                    <li>
                        <article>
                            <header>
                                <h3><a href="https://jedipunkz.github.io/blog/2017/01/13/kubernetes-deployments/">Kubernetes Deployments を使ってみた！</a></h3>
                                
                                    
                                
                                <time class="published" datetime=
                                    '2017-01-13'>
                                    January 13, 2017</time>
                            </header>
                        </article>
                    </li>
                

                
                    <li>
                        <ul class="actions">
                            <li><a href=
                            
                                "/post/"
                            
                            class="button">View more posts</a></li>
                        </ul>
                    </li>
                
            </ul>
        </section>

    
    
    
    
        <section id="categories">
            <ul class="posts">
                <header>
                    <h3><a href="/categories/">Categories</a></h3>
                </header>

                
                    
                

                
                    <li>
                        <article>
                            <header>
                                <a href="/categories/infrastructure/">infrastructure</a>
                                <span style="float:right;">110</span>
                            </header>
                        </article>
                    </li>
                
                    <li>
                        <article>
                            <header>
                                <a href="/categories/report/">report</a>
                                <span style="float:right;">9</span>
                            </header>
                        </article>
                    </li>
                
                    <li>
                        <article>
                            <header>
                                <a href="/categories/tools/">tools</a>
                                <span style="float:right;">11</span>
                            </header>
                        </article>
                    </li>
                
            </ul>
        </section>
    

    
        

    
        <section id="footer">
            <ul class="icons">
                
                    <li><a href="https://jedipunkz.github.io/post/index.xml" type="application/rss+xml"
                        target="_blank" title="RSS" class="fa fa-rss"></a></li>
                
                
            </ul>

            <p class="copyright">&copy; ジェダイさんのブログ. テーマデザインは <a href="//github.com/jpescador" target="_blank">Julio Pescador</a>さんによるものです。 </p>
        </section>

</section>

            </div>
        <a id="back-to-top" href="#" class="fa fa-arrow-up fa-border fa-2x"></a>
        

        
        
            
        

        
        
            <script src="/js/jquery.min.js"></script>
            <script src="/js/skel.min.js"></script>
            <script src="/js/util.js"></script>
            <script src="/js/main.js"></script>
            <script src="/js/backToTop.js"></script>
            <script src="/js/highlight.pack.js"></script>
        

        

            
            <script>hljs.initHighlightingOnLoad();</script>
            
    </body>
</html>

