


    




<!DOCTYPE HTML>

<html>
    <head>
        
            <title>Posts - ジェダイさんのブログ</title>
        

        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1" />
        <meta name="generator" content="Hugo 0.53" />
        


        
        
            
        

        <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Posts"/>
<meta name="twitter:description" content=""/>

        <meta property="og:title" content="Posts" />
<meta property="og:description" content="" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://jedipunkz.github.io/post/" />
<meta property="og:updated_time" content="2013-01-01T00:00:00&#43;00:00"/>

        
<meta itemprop="name" content="Posts">
<meta itemprop="description" content="">


        

        

        
        
            
        

        
        
            <link rel="stylesheet" href="/css/google-font.css" />
            <link rel="stylesheet" href="/css/font-awesome.min.css" />
            <link rel="stylesheet" href="/css/main.css" />
            <link rel="stylesheet" href="/css/add-on.css" />
            <link rel="stylesheet" href="/css/monokai-sublime.css">
        

        

        
        
        
            
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-30563095-1', 'auto');
	
	ga('send', 'pageview');
}
</script>

        
    </head>
    <body>

        
        <div id="wrapper">

    
<header id="header">
    
        <h1><a href="/"></i></a></h1>
    

    <nav class="links">
        <ul>
            
                <li>
                    <a href="">
                        Blog
                    </a>
                </li>
            
                <li>
                    <a href="about/index.html">
                        About
                    </a>
                </li>
            
        </ul>
    </nav>
    <nav class="main">
        <ul>
            
            <li class="search">
                <a class="fa-search" href="#search">Search</a>
                <form id="search" method="get" action="//google.com/search">
                    <input type="text" name="q" placeholder="Search" />
                    <input type="hidden" name="q" value="site:https://jedipunkz.github.io">
                </form>
            </li>
            <li class="menu">
                <a class="fa-bars" href="#menu">Menu</a>
            </li>
        </ul>
    </nav>
</header>


<section id="menu">

    
        <section>
            <form class="search" method="get" action="//google.com/search">
                <input type="text" name="q" placeholder="Search" />
                <input type="hidden" name="q" value="site:https://jedipunkz.github.io">
            </form>
        </section>

    
        <section>
            <ul class="links">
                
                    <li>
                        <a href="">
                            <h3>
                                
                                Blog
                            </h3>
                        </a>
                    </li>
                
                    <li>
                        <a href="about/index.html">
                            <h3>
                                
                                About
                            </h3>
                        </a>
                    </li>
                
            </ul>
        </section>

    
        <section>
            <ul class="links">
                <header>
                    <h3>Recent Posts</h3>
                </header>
                
                    
                

                
                    <li>
                        <a href="https://jedipunkz.github.io/blog/2018/12/31/istio/"><p>Istio, Helm を使って Getting Started 的なアプリをデプロイ</p></a>
                    </li>
                
                    <li>
                        <a href="https://jedipunkz.github.io/blog/2017/07/02/test-kitchen-cluster/"><p>Docker,Test-Kitchen,Ansible でクラスタを構成する</p></a>
                    </li>
                
                    <li>
                        <a href="https://jedipunkz.github.io/blog/2017/04/13/gke-lb/"><p>GCP ロードバランサと GKE クラスタを Terraform を使って構築する</p></a>
                    </li>
                
                    <li>
                        <a href="https://jedipunkz.github.io/blog/2017/02/12/serverless-fission/"><p>Serverless on Kubernetes : Fission を使ってみた</p></a>
                    </li>
                
                    <li>
                        <a href="https://jedipunkz.github.io/blog/2017/01/13/kubernetes-deployments/"><p>Kubernetes Deployments を使ってみた！</p></a>
                    </li>
                
            </ul>
        </section>

    
        
</section>

    
    <div id="main">
        
        
            
        

        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2013/01/01/freebsd-on-openstack/">FreeBSD on OpenStack</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2013-01-01'>
            January 1, 2013</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>FreeBSD を OpenStack で管理したいなぁと思って自宅に OpenStack 環境作ってました。</p>

<p>お正月なのに&hellip;</p>

<p>使ったのは folsom ベースの OpenStack (nova-network) と FreeBSD 9.1 です。8 系
の FreeBSD でも大体同じ作業で実現出来るぽいです。あと nova-network でって書い
たのは自宅に quantum だと少し厳しいからです。FlatDHCPManager が調度良かった。</p>

<p>今回のポイントは FreeBSD の HDD, NIC のドライバに virtio を使うように修正する
ところです。OpenStack (KVM) は virtio 前提なので、そうせざるを得なかったです。</p>

<h2 id="今回使ったソフトウェア">今回使ったソフトウェア</h2>

<ul>
<li>OpenStack Folsom (nova-network)</li>
<li>Ubuntu Server 12.10</li>
<li>FreeBSD 9.1 amd64</li>
</ul>

<h2 id="作業方法">作業方法</h2>

<p>準備としてこれらが必要になります。事前に行なってください。</p>

<ul>
<li>FreeBSD-9.1-RELEASE-amd64-disc1.iso ダウンロード</li>
<li>作業ホスト (Ubuntu Server 12.10) に qemu-kvm をインストール</li>
</ul>

<p>freebsd9.img として qcow2 イメージを作成します。</p>

<pre><code>% kvm-img create -f qcow2 freebsd9.img 8G
</code></pre>

<p>作成したイメージファイルに FreeBSD 9.1 をインストールします。</p>

<pre><code>% kvm -m 256 -cdrom ./FreeBSD-9.1-RELEASE-amd64-disc1.iso \
-drive file=./freebsd9.img -boot d -net nic -net user -nographic -vnc :10
</code></pre>

<p>VNC のディスプレイ番号は空いているものを使ってください。空いていれば他でも構い
ません。</p>

<p>VNC viewer をインストールし localhost:10 に接続します。ここでは作業ホストに X
Window System が入っていることを前提に書いていますが、Non-X な方は他のホストか
らその他の VNC ソフトウェアを使ってアクセスしてもらっても構わないです。</p>

<pre><code>% sudo apt-get update
% sudo apt-get gvncviewer
%gvncviewer localhost:10
</code></pre>

<p>インストーラが起動しているのでインストール行なってください。気をつける点として
は一つ。src をインストールしてください。後に virtio をインストールするのに必要
になってくるからです。kmod ファイルをコンパイルしてインストールすることになります。</p>

<p>インストールが終わったら今度は仮想マシンを HDD から起動します。</p>

<pre><code>% kvm -m 256 -drive file=./freebsd9.img -boot c -net nic -net user \
  -nographic -vnc :10
</code></pre>

<p>VNC で再度接続し仮想マシン上で emulators/virtio-kmod をインストールします。</p>

<pre><code>freebsd9% cd /usr/ports/emulators/virtio-kmod/
freebsd9% su
freebsd9# make install clean
</code></pre>

<p>次に virtio ドライバを扱うように HDD, NIC ドライバ周りの設定を変更します。
virtio のインストールが終わった後に「こうしろ」とメッセージが出てきますので
それを参考に行います。一部、僕の環境ではそのままではダメだったので修正して
使いました。</p>

<p>インストールした virtio を起動時に読み込むために下記を追記します。</p>

<pre><code>freebsd9# vi /boot/loader.conf
virtio_load=&quot;YES&quot;
virtio_pci_load=&quot;YES&quot;
virtio_blk_load=&quot;YES&quot;
if_vtnet_load=&quot;YES&quot;
virtio_balloon_load=&quot;YES&quot;
</code></pre>

<p>HDD ドライバを virtio を使うように変更します。これによってデバイス名が変わって
くるので /etc/fstab を編集します。</p>

<pre><code>freebsd9# sed -i.bak -Ee 's|/dev/ada?|/dev/vtbd|' /etc/fstab
</code></pre>

<p>NIC も virtio を使います。僕の環境では ifconfig_re0 でした。これを vtnet0 に変
更します。</p>

<pre><code>freebsd9# cat /etc/rc.conf
ifconfig_vtnet0=&quot;DHCP&quot;
..&lt;snip&gt;..
</code></pre>

<p>qemu の起動方法、もしくはデフォルトのハードウェア定義によって re0 は変わってく
るかもしれません。適宜変更します。</p>

<p>仮想マシンをシャットダウンします。</p>

<pre><code>freebsd9# shutdown -p now
</code></pre>

<p>完成したイメージファイル freebsd9.img を openstack 環境に転送し (openstack 環
境で作業している方は必要無いです) glance に登録します。</p>

<p>環境変数諸々を揃えて&hellip;</p>

<pre><code>% glance add name=&quot;FreeBSD 9&quot; is_public=true container_format=ovf disk_format=qcow2 &lt; freebsd9.img
% glance image-list
+--------------------------------------+------------------------+-------------+------------------+-------------+--------+
| ID                                   | Name                   | Disk Format | Container Format | Size        | Status |
+--------------------------------------+------------------------+-------------+------------------+-------------+--------+
| 1af6f41b-1048-4d78-9715-87935c0bc6ae | FreeBSD 9              | qcow2       | ovf              | 4305584128  | active |
+--------------------------------------+------------------------+-------------+------------------+-------------+--------+
</code></pre>

<p>以上です。</p>

<h2 id="まとめ">まとめ</h2>

<p>FreeBSD は仕事場でもプライベートでもまだまだ現役だし OpenStack で扱えるように
することは僕にとってとても重要でした。なので満足ｗ FreeBSD 8 系では同じ手順で
扱えるらしいのだけど、それ以前のバージョンになるとどうか&hellip; virtio がポイント
なのと、KVM とゲスト OS バージョンって相性がめちゃ有るのでここもポイントになり
そう。ハイパーバイザに VMWare も使えるらしいので今度やってみるかな。VMWare で
も相性はあるけど、KVM ほどシビアにならなくていい印象がある。FreeBSD 3系でも頑
張れば動いたし。</p>

<p>あとは cloud-init 。まだ開発途中だそうです。なので metadata サーバにアクセスし
て、色んな事しようと思ってもまだ難しい。</p>

<p>この年末に Windows も OpenStack に乗せてみたので、そちらの記事も時間があったら
載せようっと。</p>

<h4 id="2013-01-02-追記">2013/01/02 追記</h4>

<p>FreeBSD 8.3 で試してみましたが、全く同じ手順で起動してくれました。ただ、
emulators/virtio-kmod が 8.2 or 9 に対応しているとあったので Makefile を修正し
て virtio-kmod をインストールしています。今のところ何も問題出ていません。</p>

<pre><code class="language-bash"># diff -u /usr/ports/emulators/virtio-kmod/Makefile.org  /usr/ports/emulators/virtio-kmod/Makefile
--- /usr/ports/emulators/virtio-kmod/Makefile.org   2013-01-02 06:02:48.000000000 +0900
+++ /usr/ports/emulators/virtio-kmod/Makefile   2013-01-02 06:02:59.000000000 +0900
@@ -28,7 +28,7 @@
 
 .include &lt;bsd.port.pre.mk&gt;
 
-.if ${OSREL} != &quot;8.2&quot; &amp;&amp; ${OSREL} != &quot;9.0&quot;
+.if ${OSREL} != &quot;8.3&quot; &amp;&amp; ${OSREL} != &quot;9.0&quot;
 IGNORE=not supported $${OSREL} (${OSREL})
 .endif
</code></pre>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2013/01/01/freebsd-on-openstack/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>

        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2012/12/08/knife-fog-openstack-api/">OpenStack API を理解しインフラエンジニアの仕事の変化を感じる</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2012-12-08'>
            December 8, 2012</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>今日は &ldquo;OpenStack Advent Calendar 2012 JP&rdquo; というイベントのために記事を書きた
いと思います。Advent Calendar とはキリスト生誕を祝うため <sup>12</sup>&frasl;<sub>25</sub> まで毎日誰かがブログ
等で特定の話題について述べるもの、らしいです。CloudStack さん, Eucalyptus さん
も今年はやっているそうですね。</p>

<p>イベントサイト : <a href="http://atnd.org/events/34389">http://atnd.org/events/34389</a></p>

<p>では早速！(ただ..CloudStack の Advent Calendar とネタがかぶり気味です..。)</p>

<p>御存知の通り OpenStack は API を提供していてユーザがコードを書くことで
OpenStack のコマンド・Horizon で出来ることは全て可能です。API を叩くのに幾つか
フレームワークが存在します。</p>

<ul>
<li>fog</li>
<li>libcloud</li>
<li>deltacloud</li>
</ul>

<p>などです。</p>

<p>ここでは内部で fog を使っている knife-openstack を利用して API に触れてみよう
かと思います。API を叩くことを想像してもらって、インフラエンジニアの仕事内容の
変化まで述べられたらいいなぁと思っています。</p>

<h2 id="openstack-環境の用意">OpenStack 環境の用意</h2>

<p>予め OpenStack 環境は揃っているものとしますです。お持ちでなければ</p>

<p><a href="http://jedipunkz.github.com/blog/2012/11/10/openstack-folsom-install/">http://jedipunkz.github.com/blog/2012/11/10/openstack-folsom-install/</a></p>

<p>この記事を参考に環境を作ってみて下さい。あ、devstack でも大丈夫です。</p>

<h2 id="chef-knife-openstack-の用意">chef, knife-openstack の用意</h2>

<p>chef, knife-openstack を入れるのは OpenStack 環境でも、別のノードでも構いません。</p>

<p>chef が確か 1.9.2 ベースが推奨だったので今回は 1.9.2-p320 使います。
ruby は rbenv で入れるのがオススメです。knife-openstack, chef のインストールは&hellip;</p>

<pre><code>% sudo apt-get install libreadline-dev libxslt1-dev libxml2-dev
% gem install chef --no-rdoc --no-ri
% gem install knife-openstack --no-rdoc --no-ri
% rbenv rehash # rbenv を使っている際に実行..
</code></pre>

<p>次に knife.rb を用意します。情報として下記を</p>

<ul>
<li>OS_USERNAME : :openstack_username</li>
<li>OS_PASSWORD : :openstack_password</li>
<li>OS_AUTH_URL : :openstack_auth_url</li>
<li>OS_TENANT_NAME : :openstack_tenant</li>
</ul>

<p>追加します。例として下記を参考にしてください。</p>

<pre><code>% mkdir .chef
% ${EDITOR} .chef/knife.rb

knife[:openstack_username] = &quot;demo&quot;
knife[:openstack_password] = &quot;demo&quot;
knife[:openstack_auth_url] = &quot;http://172.16.1.11:5000/v2.0/tokens&quot;
knife[:openstack_tenant] = &quot;service&quot;
</code></pre>

<p>ここで注意なのが OS_AUTH_URL がいつもコマンドラインで扱うものと違い /tokens が
付いています。fog を直に扱う時も同じですがこれが必要です。</p>

<h2 id="ssh-keypair-の用意">ssh keypair の用意</h2>

<p>ssh keypair が必要になってくるので用意します。</p>

<pre><code>% nova keypair-add testkey01 &gt; testkey01
</code></pre>

<h2 id="knife-openstack-の操作方法">knife-openstack の操作方法</h2>

<p>いよいよ knife-openstack を使って OpenStack を操作してみましょう。</p>

<p>まずは flavor のリストを取得します。</p>

<pre><code>% knife openstack flavor list
ID  Name       Virtual CPUs  RAM       Disk
1   m1.tiny    1             512 MB    0 GB
2   m1.small   1             2048 MB   20 GB
3   m1.medium  2             4096 MB   40 GB
4   m1.large   4             8192 MB   80 GB
5   m1.xlarge  8             16384 MB  160 GB
</code></pre>

<p>image リストを取得します。id が必要になります。</p>

<pre><code>% knife openstack image list
ID                                    Name
436deba5-8fab-4bb7-9205-41e33fe22744  Cirros 0.3.0 x86_64
</code></pre>

<p>VM を生成してみましょー。</p>

<pre><code>% knife openstack server create -f 1 -I 436deba5-8fab-4bb7-9205-41e33fe22744 -S testkey01 -N knifetest01
Instance Name: knifetest01
Instance ID: 1e20850d-9572-46c8-a41e-fdf56f4f65a7
SSH Keypair: testkey

Waiting for server............
Flavor: 1
Image: 436deba5-8fab-4bb7-9205-41e33fe22744
</code></pre>

<p>出来たかどうか、チェック。</p>

<pre><code>% knife openstack server list
Instance ID                           Name         Public IP  Private IP  Flavor  Image                                 Keypair   State
1e20850d-9572-46c8-a41e-fdf56f4f65a7  knifetest01                         1       436deba5-8fab-4bb7-9205-41e33fe22744  hogehoge  active
</code></pre>

<p>できました。逆に VM を削除するには</p>

<pre><code>% knife openstack server delete 1e20850d-9572-46c8-a41e-fdf56f4f65a7
Instance ID: 1e20850d-9572-46c8-a41e-fdf56f4f65a7
Instance Name: knifetest01
Flavor: 1
Image: 436deba5-8fab-4bb7-9205-41e33fe22744

Do you really want to delete this server? (Y/N) y
WARNING: Deleted server 1e20850d-9572-46c8-a41e-fdf56f4f65a7
WARNING: Corresponding node and client for the 1e20850d-9572-46c8-a41e-fdf56f4f65a7 server were not deleted and remain registered with the Chef Server
</code></pre>

<p>です。</p>

<p>残念なところとしては knife-openstack 自体はまだまだ機能が充実していません。VM
の基本的な操作くらいしか出来ないので、追加実装したいという方がいらっしゃいまし
たら Pull リクエスト送ると良いのではないでしょうか。</p>

<p>knife-openstack 公式サイト : <a href="https://github.com/opscode/knife-openstack">https://github.com/opscode/knife-openstack</a></p>

<p>まぁ、ここまで書いてアレですが.. 気がついた方もいらっしゃると思います。
knife-openstack で出来ることは openstack コマンド群で全て出来るのであまり意味
はないですよね。ただ fog を使って API を叩くことを想像して欲しくて..( -_- )</p>

<h2 id="fog-単体で操作してみる">Fog 単体で操作してみる</h2>

<p>今日はまだまだ書ける！</p>

<p>knife-openstack では基本的な操作しか出来ませんでしたが fog はより多くの機能を実装しています。(2012/12/08 現在 quantum 周りの開発は未
完成らしいです。floating-ip 周りがぁ。誰かコミットして。)</p>

<p>fog 単体で OpenStack API を操作するには、下記の通り実行します。fog をインストー
ルし&hellip;</p>

<pre><code>% gem install fog --no-rdoc --no-ri
% rbenv rehash
</code></pre>

<p>環境変数を入力し&hellip; (情報は例です)</p>

<pre><code>% cat env
export OS_TENANT_NAME=service
export OS_USERNAME=demo
export OS_PASSWORD=demo
export OS_AUTH_URL=&quot;http://172.16.1.11:5000/v2.0/&quot;
export OS_AUTH_URL_FOG=&quot;http://172.16.1.11:5000/v2.0/tokens&quot;
% source env
</code></pre>

<p>コードを下記のように記述すると VM の生成が行えます。
``` ruby
    #!/usr/bin/env ruby</p>

<pre><code>require 'fog'
require 'pp'

conn = Fog::Compute.new({
  :provider =&gt; 'OpenStack',
  :openstack_api_key =&gt; ENV['OS_PASSWORD'],
  :openstack_username =&gt; ENV[&quot;OS_USERNAME&quot;],
  :openstack_auth_url =&gt; ENV[&quot;OS_AUTH_URL_FOG&quot;],
  :openstack_tenant =&gt; ENV[&quot;OS_TENANT_NAME&quot;]
})

flavor = conn.flavors.find { |f| f.name == 'm1.tiny' }

image_name = 'Cirros 0.3.0 x86_64'
image = conn.images.find { |i| i.name == image_name }

puts &quot;#{'Creating server'} from image #{image.name}...&quot;
server = conn.servers.create :name =&gt; &quot;fogvm-#{Time.now.strftime '%Y%m%d-%H%M%S'}&quot;,
                             :image_ref =&gt; image.id,
                             :flavor_ref =&gt; flavor.id,
                             :key_name =&gt; 'testkey01'
server.wait_for { ready? }
</code></pre>

<p>````</p>

<p>実行 !</p>

<pre><code>% ruby &lt;CODENAME&gt;.rb
Creating server from image Cirros 0.3.0 x86_64...
%
</code></pre>

<p>出来ました。VM が生成されたか先ほどの knife-openstack で確認してみましょう。</p>

<pre><code>% knife openstack server list
Instance ID                           Name         Public IP  Private IP  Flavor  Image                                 Keypair   State
1e20850d-9572-46c8-a41e-fdf56f4f65a7  knifetest01                         1       436deba5-8fab-4bb7-9205-41e33fe22744  hogehoge  active
f5c314a3-e32f-498f-984f-b79078d76a5a  fogvm-20121207-104743                         1       aedef2a1-f820-43a6-96cd-f5361d27df3f  testkey01  active
</code></pre>

<h2 id="まとめと-考察">まとめと 考察</h2>

<p>今回は API をみんな叩いてるねん！ってことに気がついて欲しくて、こんな記事を書
いてみました。実は OpenStack のコマンドも &ndash;debug を付けて (Quantum だけは -v)
実行すると OpenStack の API を叩いているメッセージが出力されると OpenStack ユー
ザ会の方から聞きました。是非やっていてください。</p>

<pre><code>% nova --debug list
% quantum -v net-list
</code></pre>

<p>API を実装するのか、ツールを実装するのか？といった話題が以前の OpenStack
Summit で常に話題になっていたらしいですが、最近は API で決まり、といったと
ころでしょうか。コードを書いてインラフを定義する時代に突入です。ちなみに fog
は AWS も扱えます。また Opscode Chef や Puppet, JuJu 等のデプロイフレームワー
クを使えば VM 上のサービス構築もコードを書くことで出来ます。また、以前ブログに
書いたのですが OpenFlow といった技術を使うとネットワークをコードを書くことで設
計出来る、かもしれない。</p>

<p><a href="http://jedipunkz.github.com/blog/2012/11/21/openflow-trema-handson-report/">http://jedipunkz.github.com/blog/2012/11/21/openflow-trema-handson-report/</a></p>

<p>つまり、コードを書くことで</p>

<ul>
<li>サーバ構築</li>
<li>ネットワーク構築</li>
<li>サービス構築</li>
</ul>

<p>を一貫して行えることになります。</p>

<p>インフラエンジニアの僕としては時代の変化に着いて行かねば！という焦りで一杯です。
もちろんレガシなインフラエンジニアも生き残るのでしょうが、働く場所が限られてき
そう。より高度な(低レイヤからの深い？)技術を有している人が限定された場所で成果
を上げていく印象。僕らの様な一般的なインフラエンジニアは OpenStack, AWS, HP
Cloud, CloudStack の様なクラウドインフラを相手にコードを書く、もしくは抽象化さ
れた技術をより扱いやすいソフトウェアを介して構築・管理していくことになるのでしょ
うか。これから覚えることは山積みですが、楽しい時代です。</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2012/12/08/knife-fog-openstack-api/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>

        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2012/11/21/openflow-trema-handson-report/">OpenFlow Trema ハンズオン参加レポート</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2012-11-21'>
            November 21, 2012</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>InternetWeek2012 で開かれた &ldquo;OpenFlow Trema ハンズオン&rdquo; に参加してきました。</p>

<pre><code>講師   : Trema 開発チーム 鈴木一哉さま, 高宮安仁さま
開催日 : 2012年11月21日
</code></pre>

<p>OpenStack の Quantum Plugin として Trema が扱えるという話だったので興味を持っ
たのがきっかけです。また Ruby で簡潔にネットワークをコード化出来る、という点も
個人的に非常に興味を持ちました。OpenStack, CloudStack 等のクラウド管理ソフトウェ
アが提供する API といい、Opscode Chef, Puppet 等のインフラソフトウェア構築フレー
ムワークといい、この OpenFlow もインフラを形成する技術を抽象化し、技術者がコー
ドを書くことでインフラ構築を行える、という点ではイマドキだなと思います。</p>

<p>Google は既にデータセンター間の通信を 100% 、OpenFlow の仕様に沿った機器・ソフ
トウェアをを独自に実装しさばいているそうですし、我々が利用する日も近いと想像し
ます。</p>

<h2 id="openflow-のモチベーション">OpenFlow のモチベーション</h2>

<p>OpenFlow の登場には理由が幾つかあって、既存のネットワークの抱えている下記の幾
つかの問題を解決するためです。</p>

<ul>
<li>装置仕様の肥大化</li>
<li>多様なプロトコルが標準化</li>
<li>装置のコスト増大</li>
<li>ある意味、自律したシステムが招く複雑さ</li>
</ul>

<p>一方、OpenFlow を利用すると..</p>

<ul>
<li>コモディティ化された HW の利用が可能</li>
<li>OpenFlow はコントローラ (神) が集中管理するので楽な場合もある</li>
<li>ネットワーク運用の自動化が図れる</li>
<li>アプリケーションに合わせた最適化</li>
<li>柔軟な自己修復</li>
</ul>

<p>等のメリットが。</p>

<h2 id="openflow-と-trema-とは">OpenFlow と Trema とは?</h2>

<p>OpenFlow は &lsquo;OpenFlow コントローラ&rsquo;, &lsquo;OpenFlow スイッチ&rsquo; から成る。OpenFlow コ
ントローラと OpenFlow スイッチの間の通信は OpenFlow プロトコルでされる。今日の
話題 Trema はこの..</p>

<ul>
<li>OpenFlow コントローラのフレームワーク</li>
<li>エミュレータ</li>
<li>trema コマンド</li>
</ul>

<p>のセットである。自宅の PC 一台で OpenFlow プログラムが行え、エミュレーションも
行える手軽さ、また Ruby による簡潔な記述が可能でプログラミング初心者でも扱いや
すい、という趣味ユーザにはもってのほかだ。</p>

<h2 id="hellow-trema">Hellow, Trema !</h2>

<p>早速 プログラミングの初歩、Hello World から。</p>

<p>コード hello-trema.rb は</p>

<pre><code>class HelloTrema &lt; Controller
  def start
    puts &quot;Hello, Trema!&quot;
  end
end
</code></pre>

<p>実行&hellip;</p>

<pre><code>% trema run hello-trema.rb
Hello, Trema!
</code></pre>

<p>Trema が提供する Controller クラスを &lsquo;継承&rsquo; し HelloTrema クラスを定義した。
Controller クラスには幾つものハンドラが用意されていて start ハンドラもその一つ。
他にも色んなメソッドが用意されている。詳しくは後ほど。</p>

<h2 id="trema-によるスイッチの起動">Trema によるスイッチの起動</h2>

<p>次にスイッチを起動してみる。Trema は ruby の DSL でコンフィギュレーションを定
義出来る。hello-switch.conf として下記の内容&hellip;</p>

<pre><code>vswitch { dpid &quot;0xabc&quot; }
vswitch { dpid &quot;0x1&quot; }
vswitch { dpid &quot;0x2&quot; }
</code></pre>

<p>hello-switch.rb として</p>

<pre><code>class HelloSwitch &lt; Controller
  def switch_ready dpid
    puts &quot;Hello #{ dpid.to_hex }!&quot;
  end
  def switch_disconnected dpid
    puts &quot;Killed ! :D #{ dpid.to_hex }!&quot;
  end
end
</code></pre>

<p>実行すると</p>

<pre><code>% trema run hello-switch.rb -c hello-switch.conf
Hello 0xabc!
Hello 0x1!
Hello 0x2!
</code></pre>

<p>となる。スイッチを3つ起動したわけだ。switch_ready とは Trema が提供するコント
ローラで定義された &ldquo;スイッチが稼働した時に実行されるハンドラ&rdquo; だ。スイッチが起
動したため &ldquo;Hello ..&rdquo; なるメッセージが出力された、と理解すればいい。</p>

<p>またこの起動中に</p>

<pre><code>% trema kill 0x1
Killed ! :D 0x1!
</code></pre>

<p>と実行することでスイッチを停止出来る。この際 switch_disconnected ハンドラが実
行され上記のメッセージを出力したというわけだ。また逆に再稼働させるには trema
up コマンドを用いる。</p>

<h2 id="その他のハンドラ一覧">その他のハンドラ一覧</h2>

<p>紹介した start, switch_ready 等のハンドラ以外にも下記のモノがある。</p>

<pre><code>start switch_ready switch_disconnected packet_in flow_removed port_status
openflow_error features_reply stats_reply barrier_reply get_config_reply
queue_get_config_reply vendor
</code></pre>

<p>ドキュメントは</p>

<pre><code>http://rubydoc.info/github/trema/trema/master/frames
</code></pre>

<p>にあるので参照すると良い。</p>

<h2 id="l2-スイッチの実装">L2 スイッチの実装</h2>

<p>OpenFlow はネットワーク機器を実装出来るモノなので、ここで L2 スイッチを実装し
てみたい。L2 スイッチの動作は</p>

<ul>
<li>既に ARP テーブルを持っていればパケットを受け流す</li>
<li>自分の ARP テーブルで管理されていないパケットは学習してからパケットを受け流
す</li>
</ul>

<p>が基本だ。これを実装する。</p>

<p>l2-switch.conf として</p>

<pre><code>vswitch { dpid &quot;0xabc&quot; }

vhost (&quot;host1&quot;) {
  ip &quot;192.168.0.1&quot;
  netmask &quot;255.255.0.0&quot;
  mac &quot;00:00:00:01:00:01&quot;
}

vhost (&quot;host2&quot;) {
  ip &quot;192.168.0.2&quot;
  netmask &quot;255.255.0.0&quot;
  mac &quot;00:00:00:01:00:02&quot;
}

link &quot;0xabc&quot;, &quot;host1&quot;
link &quot;0xabc&quot;, &quot;host2&quot;
</code></pre>

<p>ここでは detapath_id &ldquo;0xabc&rdquo; なる仮想スイッチを一つ定義し、サーバホスト host1,
host2 を定義した。それぞれで IP アドレス・MAC アドレスを定義している。また
link により仮想スイッチとサーバホストの I/F を接続している。</p>

<p>いよいよ L2 スイッチのコード。l2-switch.rb として、下記を記述する。</p>

<pre><code>require &quot;fdb&quot;

class LearningSwitch &lt; Controller
  def start
    @fdb = FDB.new
  end

  def packet_in dpid, message
    @fdb.learn message.macsa, message.in_port
    port_no = @fdb.lookup( message.macda )
    if port_no
      flow_mod dpid, message, port_no
      packet_out dpid, message, port_no
    else
      flood dpid, message
    end
  end

  def flow_mod dpid, message, port_no
    send_flow_mod_add(
      dpid,
      :match =&gt; ExactMatch.from( message ),
      :actions =&gt; ActionOutput.new( port_no )
    )
  end

  def flow_mod dpid, message, port_no
    send_flow_mod_add(
      dpid,
      :match =&gt; ExactMatch.from( message ),
      :actions =&gt; ActionOutput.new( port_no )
    )
  end

  def packet_out dpid, message, port_no
    send_packet_out(
      dpid,
      :packet_in =&gt; message,
      :actions =&gt; ActionOutput.new( port_no )
    )
  end

  def flood dpid, message
    packet_out dpid, message, OFPP_FLOOD
  end
end
</code></pre>

<p>まず実行してみる。</p>

<pre><code>% trema run l2-switch.rb -c l2-switch.conf
</code></pre>

<p>異なる shell でパケットの送信と状態表示を行う。</p>

<pre><code>% trema send_packet --source host1 --dest host2
% trema show_stats host1
ip_dst,tp_dst,ip_src,tp_src,n_pkts,n_octets
192.168.0.2,1,192.168.0.1,1,1,50
% trema send_packet --source host1 --dest host2
% trema send_packet --source host2 --dest host1
% trema dump_flows 0xabc
NXST_FLOW reply (xid=0x4):
 cookie=0x2, duration=67.343s, table=0, n_packets=0, n_bytes=0, priority=65535,udp,in_port=1,vlan_tci=0x0000,dl_src=00:00:00:01:00:02,dl_dst=00:00:00:01:00:01,nw_src=192.168.0.2,nw_dst=192.168.0.1,nw_tos=0,tp_src=1,tp_dst=1 actions=output:2
 cookie=0x1, duration=70.339s, table=0, n_packets=0, n_bytes=0, priority=65535,udp,in_port=2,vlan_tci=0x0000,dl_src=00:00:00:01:00:01,dl_dst=00:00:00:01:00:01,nw_src=192.168.0.1,nw_dst=192.168.0.1,nw_tos=0,tp_src=1,tp_dst=1 actions=output:2
</code></pre>

<p>host1 から host2 に対して trema send_packet で通信を行い、host1 の状態を表示し
た。また交互に通信をさせフローをダンプしたのが上記だ。</p>

<p>一番基本なところらしいので、コードを詳しく解説。l2-switch.rb の下記の部分。</p>

<pre><code>  def packet_in dpid, message                  # ---(1)
    @fdb.learn message.macsa, message.in_port  # ---(2)
    port_no = @fdb.lookup( message.macda )     # ---(3)
    if port_no                                 # ---(4)
      flow_mod dpid, message, port_no
      packet_out dpid, message, port_no
    else                                       # ---(5)
      flood dpid, message
    end
  end
</code></pre>

<p>(1) では packet_in ハンドラを利用した。(2) で fdb (floating DB) の learn メソッ
ドで port と mac アドレスの学習を行った。(3) で @fdb にすでに mac アドレスの記
述があれば port_no に値が入る。値が入っていれば (4) を。スイッチのフローテーブ</p>

<p>ルを更新しパケットを packet_out する。入っていなければ (5)を実行しパケットを
flood する。</p>

<p>flow_mod, packet_out, flood はこのプログラム内で定義しているプライベートなメソッ
ドだ。</p>

<h2 id="この時のソフトウェア構成">この時のソフトウェア構成</h2>

<p>このコードを動作させた際に実行したホスト上のプロセスを見てみた。</p>

<ul>
<li>ovs-openflowd</li>
<li>phost</li>
<li>switch_manager</li>
</ul>

<p>が居た。siwtch_manager が 6633 番ポート待ち受けているコントローラ自身だろう。
phost は仮想サーバで ovs-openflowd (OpenvSwitch) が仮想スイッチとして動作して
いると想像出来る。またこの時にネットワークインターフェースが</p>

<ul>
<li>trema0-0</li>
<li>trema0-1</li>
<li>trema1-0</li>
<li>trema1-1</li>
</ul>

<p>と起動していた。これは 0xabc スイッチと host1, host2 との接続で使われている
I/F だろう。</p>

<h2 id="まとめ">まとめ</h2>

<p>Ruby で記述出来るので技術者にとって Trema は優しい。また簡潔な記述が行えるとい
うのも Ruby ならではだろう。github.com には高度な利用サンプルが幾つか掲載され
ている。</p>

<pre><code>https://github.com/trema/apps
</code></pre>

<p>また簡単なサンプル集ということであれば</p>

<pre><code>https://github.com/trema/trema/tree/develop/src/examples
</code></pre>

<p>がうってつけのリファレンスになる。</p>

<p>冒頭でも書いたがスイッチのエミュレータが同封されているので、技術者はすぐに開発
に入ることが出来る。</p>

<p>OpenStack は仮想マシンを管理・構成するため API を提供し、そしてネットワークを管理・
構成するため OpenFlow がある。OpenStack の API を叩くコードを書くのと同様に
OpenFlow を Trema という OpenFlow コントローラフレームワークを用いてコードを書
くことが出来る。インフラエンジニアの仕事の範囲は確実にここ数年で変化し、その変
化に追いつくには &ldquo;コードを書く&rdquo; ことを念頭に置かなくてはならないだろう。</p>

<p>最後に、この機会を与えてくださった 鈴木様・高宮様にお礼を申し上げます。</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2012/11/21/openflow-trema-handson-report/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>

        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2012/11/10/openstack-folsom-install/">OpenStack Folsom 構築スクリプト</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2012-11-10'>
            November 10, 2012</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>※2012/12/04 に内容を修正しました。Network Node を切り出すよう修正。
※213/01/09 に内容を修正しました。パラメータ修正です。</p>

<p>OpenStack の Folsom リリースからメインコンポーネントの仲間入りした Quantum を
理解するのに時間を要してしまったのだけど、もう数十回とインストールを繰り返して
だいぶ理解出来てきました。手作業でインストールしてると日が暮れてしまうのでと思っ
て自分用に bash で構築スクリプトを作ったのだけど、それを公開しようと思います。</p>

<p>OpenStack Folsom の構築に四苦八苦している方に使ってもらえたらと思ってます。</p>

<p><a href="http://jedipunkz.github.com/openstack_folsom_deploy/">http://jedipunkz.github.com/openstack_folsom_deploy/</a></p>

<p>chef や puppet, juju などデプロイのフレームワークは今流行です。ただどれも環境
を予め構築しなくてはいけないので、誰でもすぐに使える環境ってことで bash スクリ
プトで書いています。時間があれば是非 chef の cookbook を書いていきたいです。と
いうか予定です。でも、もうすでに opscode 等は書き始めています。(汗</p>

<p>ではでは、紹介を始めます。</p>

<h2 id="前提の構成">前提の構成</h2>

<pre><code>management segment 172.16.1.0/24
+--------------------------------------------+------------------+-----------------
|                                            |                  |
|                                            |                  |
| eth2 172.16.1.13                           | eth2 172.16.1.12 | eth2 172.24.1.11
+------------+                               +-----------+      +------------+
|            | eth1 ------------------- eth1 |           |      |            |
|  network   | vlan/gre seg = 172.24.17.0/24 |  compute  |      | controller |
|    node    | data segment = 172.16.2.0/24  |   node    |      |    node    |
+------------+ 172.16.2.13       172.16.2.12 +-----------+      +------------+      
| eth0 10.200.8.13                                              | eth0 10.200.8.11
|                                                               |
|                                                               |
+--------------------------------------------+------------------+-----------------
|       public segment 10.200.8.0/24
|
| 10.200.8.1
+-----------+
| GW Router |-&gt; The Internet
+-----------+
</code></pre>

<p>Quantum は、</p>

<ul>
<li>API network</li>
<li>Public network</li>
<li>Data network</li>
<li>Management Network</li>
</ul>

<p>の4つのネットワークセグメントを前提に設計されています。4つ用意するのが大変なの
で今回は</p>

<ul>
<li>API / Management network</li>
<li>Public Network</li>
<li>Data Network</li>
</ul>

<p>と API と Management を兼務させた3つのネットワークセグメントを前提に話続けます。
もちろん Management を追加で切り出しても構わないです。NIC を追加するだけで OK
。</p>

<p>詳しくは、</p>

<p><a href="http://docs.openstack.org/trunk/openstack-network/admin/content/connectivity.html">http://docs.openstack.org/trunk/openstack-network/admin/content/connectivity.html</a></p>

<p>に掲載されています。</p>

<p>また今回は public network に 10.200.<sup>8</sup>&frasl;<sub>24</sub> を使ってます。サービス環境ではここが
グローバルセグメントになります。</p>

<h2 id="インストール対象">インストール対象</h2>

<p>OS は Ubuntu Server 12.04 LTS もしくは 12.10 で動作します。</p>

<p>3つの NIC があるマシンを1台、NIC 2つのマシンを2台を用意します。
controller node x 1台 network node x 1台 + comupte node x n台 の構成で
す。</p>

<ul>
<li>controller : glance, keystone, mysql, horizon, quantum server が稼働する Node</li>
<li>network : quantum dhcp agent, quantum l3 agent, quantum openvswitch agent が稼働する Node</li>
<li>compute : nova, quntum openvswitch agent が稼働する Node</li>
</ul>

<p>上記の構成では下記の通り /etc/network/interface を設定します。controller 側の
設定です。</p>

<pre><code>auto lo
iface lo inet loopback

auto eth0
iface eth0 inet static
    address 10.200.8.11
    netmask 255.255.255.0
    dns-nameservers 8.8.8.8 8.8.4.4
    dns-search cpi.ad.jp

auto eth2
iface eth2 inet static
    address 172.16.1.11
    netmask 255.255.255.0
    gateway 172.16.1.1
</code></pre>

<p>Network Node は&hellip;</p>

<pre><code>auto lo
iface lo inet loopback

auto eth0
iface eth0 inet static
    up ifconfig $IFACE 0.0.0.0 up
    up ip link set $IFACE promisc on
    down ip link set $IFACE promisc off
    down ifconfig $IFACE down
    address 10.200.8.21
    netmask 255.255.255.0
    #gateway 10.200.8.1
    # dns-* options are implemented by the resolvconf package, if installed
    dns-nameservers 8.8.8.8 8.8.4.4
    dns-search cpi.ad.jp

auto eth1
    iface eth1 inet static
    address 172.16.2.13
    netmask 255.255.255.0

auto eth2
iface eth2 inet static
    address 172.16.1.13
    netmask 255.255.255.0
    gateway 172.16.1.1
    dns-nameservers 8.8.8.8 8.8.4.4
</code></pre>

<p>compute Node は&hellip;</p>

<pre><code>auto eth1
iface eth1 inet static
    address 172.16.2.12
    netmask 255.255.255.0

auto eth2
iface eth2 inet static
    address 172.16.1.12
    netmask 255.255.255.0
    gateway 172.16.1.1
    dns-nameservers 8.8.8.8 8.8.4.4
</code></pre>

<p>です。下記のコマンドでネットワークインターフェースを再起動してください。
ここまで用意出来たらいよいよ実行するのみです。</p>

<pre><code>controller% sudo /etc/init.d/networking restart
network   % sudo /etc/init.d/networking restart
compute   % sudo /etc/init.d/networking restart
</code></pre>

<h2 id="スクリプト実行">スクリプト実行</h2>

<p>下記の通りスクリプトを取得して&hellip;</p>

<pre><code>controller% git clone https://github.com/jedipunkz/openstack_folsom_deploy.git
controller% cd openstack_folsom_deploy
</code></pre>

<p>それぞれの構成に合わせて deploy.conf を修正します。上記の構成の場合下記のようになります。</p>

<pre><code>BASE_DIR=`pwd`
CONTROLLER_NODE_IP='172.16.1.11'
CONTROLLER_NODE_PUB_IP='10.200.8.11'
NETWORK_NODE_IP='172.16.1.12'
COMPUTE_NODE_IP='172.16.1.13'
DATA_NIC_COMPUTE='eth1'

MYSQL_PASS='secret'
CINDER_VOLUME='/dev/sda6'
DATA_NIC='eth1'
PUBLIC_NIC='eth0'

NETWORK_TYPE='gre'
INT_NET_GATEWAY='172.24.17.254'
INT_NET_RANGE='172.24.17.0/24'
EXT_NET_GATEWAY='10.200.8.1'
EXT_NET_START='10.200.8.36'
EXT_NET_END='10.200.8.40'
EXT_NET_RANGE='10.200.8.0/24'

OS_IMAGE_URL=&quot;https://launchpad.net/cirros/trunk/0.3.0/+download/cirros-0.3.0-x86_64-disk.img&quot;
OS_IMAGE_NAME=&quot;Cirros 0.3.0 x86_64&quot;
</code></pre>

<p>環境に合わせて設定すれば OK です。上記の前提の構成の場合このような値を入れてい
きます。重要なところだけ説明すると..</p>

<ul>
<li>CONTROLLER_NODE_IP : Controller Node の IP アドレス</li>
<li>NETWROK_NODE_IP : Network Node の IP アドレス</li>
<li>COMPUTE_NODE_IP : Compute Node の IP アドレス</li>
<li>INT<em>NET</em>.. : Quantum で管理する内部ネットワーク</li>
<li>EXT_NET-.. : Quantum で管理する外部ネットワーク</li>
</ul>

<p>です。</p>

<p>deploy.conf を修正したら、各 Node にディレクトリをコピーします。</p>

<pre><code>network% scp -r &lt;CONTROLLER_NODE_IP&gt;:~/openstack_folsom_deploy .
compute% scp -r &lt;CONTROLLER_NODE_IP&gt;:~/openstack_folsom_deploy .
</code></pre>

<p>いよいよ実行。それぞれの Node で順にスクリプトを実行します。</p>

<pre><code>controller% sudo ./deploy.sh controller quantum
network   % sudo ./deploy.sh network quantum
compute   % sudo ./deploy.sh compute quantum
</code></pre>

<p>また構築が終了したら quantum 上にネットワークを作成します。</p>

<pre><code>controller% sudo ./deploy.sh create_network qunatum
</code></pre>

<p>完成です。http://${CONTROLLER_NODE_IP}/horizon/ にアクセスすれば管
理画面が表示されるはずです。</p>

<p>更に compute node を追加したければ&hellip;</p>

<pre><code>compute02% scp -r &lt;CONTROLLER_NODE_IP&gt;:~/openstack_folsom_deploy .
compute02% cd openstack_folsom_deploy
compute02% vim deploy.conf # $COMPUTE_NODE_IP を修正する。その他はそのまま。
compute02% sudo ./deploy.sh compute quantum
</code></pre>

<p>と deploy.conf 内 $COMPUTE_NODE_IP を更新して実行すれば OK です。</p>

<h2 id="floating-ip-の利用">Floating IP の利用</h2>

<p>現在、folsom リリース版には floating ip が horizon 経由で利用できない問題があ
ります。コマンドラインでは利用できるのでその方法を。</p>

<pre><code>% source $HOME/openstackrc
% quantum net-list
% quantun floatingip-create &lt;ext_net_id&gt;
% quantun floatingip-list
% quantum port-list
% quantum floatingip-associate &lt;floatingip_id&gt; &lt;vm_port_id&gt;
</code></pre>

<h2 id="quantum-と-i-o-と-所感">Quantum と I/O と..所感</h2>

<p><a href="http://www.readability.com/read?url=http://docs.openstack.org/trunk/openstack-network/admin/content/services.html">http://www.readability.com/read?url=http://docs.openstack.org/trunk/openstack-network/admin/content/services.html</a></p>

<p>最近、メーリングリストで挙がっている話題。今回の構成だと Quantum は controller
上で稼働し全ての VM がこの Quantum を利用することになります。つまり単一障害点っ
ていうだけではなく I/O が集中するので負荷も上昇する。前リリース版 ESSEX の
nova-network の時は追加する compute node 上全てで nova-network を稼働させ、
node が増えるにつれ I/O も拡張出来るシンプルな構成が組めるモノだったのですが、
Quantum の構成になって、そう簡単にいかなくなった。</p>

<h2 id="オールインワン構成等-ほかの構成について">オールインワン構成等、ほかの構成について</h2>

<p>2013/01/09 に nova-network にも対応しました。</p>

<p>オールインワン構成や quantum に代わって nova-network を使う構成等、その他の構成構築方法
については下記のドキュメントを参考にしてください。</p>

<p><a href="https://github.com/jedipunkz/openstack_folsom_deploy/blob/master/README_jp.md">https://github.com/jedipunkz/openstack_folsom_deploy/blob/master/README_jp.md</a></p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2012/11/10/openstack-folsom-install/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>

        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2012/11/04/swift-tempauth/">Swift で簡単に分散オブジェクトストレージ</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2012-11-04'>
            November 4, 2012</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>最近、OpenStack にどっぷり浸かってる <a href="https://twitter.com/jedipunkz">@jedipunkz</a> です。</p>

<p>Folsom がリリースされて Quantum を理解するのにめちゃ苦労して楽しい真っ最中なのだけど、
今日は OpenStack の中でも最も枯れているコンポーネント Swift を使ったオブジェクトストレー
ジ構築について少し書こうかなぁと思ってます。</p>

<p>最近は OpenStack を構築・デプロイするのに皆、Swift 入れてないのね。仲間はずれ
感たっぷりだけど、一番安定して動くと思ってる。</p>

<p>これを読んで、自宅にオブジェクトストレージを置いちゃおぅ。</p>

<h2 id="構成は">構成は ?&hellip;</h2>

<pre><code>                        +--------+
                        | client |
                        +--------+
                             |
                      +-------------+
                      | swift-proxy |
                      +-------------+ 172.16.0.10
                             |
         +-------------------+-------------------+ 172.16.0.0/24
         |                   |                   |
+-----------------+ +-----------------+ +-----------------+
| swift-storage01 | | swift-storage02 | | swift-storage03 |
+-----------------+ +-----------------+ +-----------------+
172.16.0.11         172.16.0.12         172.16.0.13
</code></pre>

<p>となる。IP アドレスは&hellip;</p>

<ul>
<li>client : 172.16.0.0/24 のどこか</li>
<li>swift-proxy : 172.16.0.10</li>
<li>swift-storage01 : 172.16.0.11</li>
<li>swift-storage02 : 172.16.0.12</li>
<li>swift-storage03 : 172.16.0.13</li>
</ul>

<p>これはサンプル。自宅の環境に合わせて読み替えてください。</p>

<p>全て同じネットワークセグメントに。なので上の図は概念的な図です。通信の流れだけ
把握出来ればいいなぁと。向き書いてないけど..。四角で囲まれているのがノード (サー
バ) です。物理サーバでも仮想マシンでも大丈夫！</p>

<h2 id="マシンの準備">マシンの準備</h2>

<p>Ubuntu Server 12.04 もしくは 12.10 を用意。swift-storage のマシン3台だけは
/dev/sda6 など Disk デバイスを swift 用に用意してあげてください。普通にハード
ディスクのパーティションを切ってあげるだけでいいです。高価な Disk を使うまでも
ないので。それが分散ストレージの良いところ！ Disk ・ノードが壊れてもデータが失
われないんです！</p>

<h2 id="swift-proxy-の構築">swift-proxy の構築</h2>

<p>今回は簡易認証機能の tempauth を使います。Keystone を使った構成を説明しようか
迷ったのだけど、Keystone の構築だけで一つ記事が書けるくらいになるので..、諦め
ました。tempauth は Swift 本体に実装されていますよ。</p>

<p>構築は簡単、まず下記をコピペしてください。</p>

<pre><code>apt-get update
apt-get install swift python-swift
apt-get install python-keystoneclient python-keystone 
mkdir /etc/swift
chown -R swift:swift /etc/swift
export PROXY_LOCAL_NET_IP=10.200.4.133
export POUND_NET=10.200.4.138
apt-get install swift-proxy memcached
perl -pi -e &quot;s/-l 127.0.0.1/-l $PROXY_LOCAL_NET_IP/&quot; /etc/memcached.conf
service memcached restart
cat &gt;/etc/swift/proxy-server.conf &lt;&lt;EOF
[DEFAULT]
#cert_file = /etc/swift/cert.crt
#key_file = /etc/swift/cert.key
bind_port = 8080
workers = 8
user = swift

[pipeline:main]
pipeline = healthcheck cache tempauth proxy-server

[app:proxy-server]
use = egg:swift#proxy
allow_account_management = true
account_autocreate = true

[filter:tempauth]
use = egg:swift#tempauth
user_system_root = testpass .admin
https://$PROXY_LOCAL_NET_IP:8080/v1/AUTH_system

[filter:healthcheck]
use = egg:swift#healthcheck

[filter:cache]
use = egg:swift#memcache
memcache_servers = $PROXY_LOCAL_NET_IP01:11211
EOF
</code></pre>

<p>次に swift.conf とリング情報 (バランシングのための情報) を swift-proxy 上に用意します。これらは、
あとで各すべてのノードに配置するので重要です。これもコピペしてください。</p>

<pre><code>cat &gt; /etc/swift/swift.conf &lt;&lt; EOF
[swift-hash]
# random unique string that can never change (DO NOT LOSE)
swift_hash_path_suffix = `od -t x8 -N 8 -A n &lt;/dev/random`
EOF
cd /etc/swift
swift-ring-builder account.builder create 18 3 1
swift-ring-builder container.builder create 18 3 1
swift-ring-builder object.builder create 18 3 1
export STORAGE_LOCAL_NET_IP01=172.16.0.11
export STORAGE_LOCAL_NET_IP02=172.16.0.12
export STORAGE_LOCAL_NET_IP03=172.16.0.13
export ZONE01=1
export ZONE02=2
export ZONE03=3
export WEIGHT=100
export DEVICE=sda6
swift-ring-builder account.builder add z$ZONE01-$STORAGE_LOCAL_NET_IP01:6002/$DEVICE $WEIGHT
swift-ring-builder container.builder add z$ZONE01-$STORAGE_LOCAL_NET_IP01:6001/$DEVICE $WEIGHT
swift-ring-builder object.builder add z$ZONE01-$STORAGE_LOCAL_NET_IP01:6000/$DEVICE $WEIGHT
swift-ring-builder account.builder add z$ZONE02-$STORAGE_LOCAL_NET_IP02:6002/$DEVICE $WEIGHT
swift-ring-builder container.builder add z$ZONE02-$STORAGE_LOCAL_NET_IP02:6001/$DEVICE $WEIGHT
swift-ring-builder object.builder add z$ZONE02-$STORAGE_LOCAL_NET_IP02:6000/$DEVICE $WEIGHT
swift-ring-builder account.builder add z$ZONE03-$STORAGE_LOCAL_NET_IP03:6002/$DEVICE $WEIGHT
swift-ring-builder container.builder add z$ZONE03-$STORAGE_LOCAL_NET_IP03:6001/$DEVICE $WEIGHT
swift-ring-builder object.builder add z$ZONE03-$STORAGE_LOCAL_NET_IP03:6000/$DEVICE $WEIGHT
swift-ring-builder account.builder
swift-ring-builder container.builder
swift-ring-builder object.builder
swift-ring-builder account.builder rebalance
swift-ring-builder container.builder rebalance
swift-ring-builder object.builder rebalance
</code></pre>

<p>で、起動。</p>

<pre><code>swift-proxy# chown -R swift:swift /etc/swift
swift-proxy# swift-init proxy start
</code></pre>

<p>おしまい。</p>

<h2 id="swift-storage-構築">swift-storage 構築</h2>

<p>swift-storage の構築は各ノードで下記の内容をコピペしてください。今回は3台分だ
けど、マシンが余ってたら4台でも5台でも OK ！</p>

<pre><code>apt-get update
apt-get install swift python-swift
mkdir /etc/swift
chown -R swift:swift /etc/swift

export STORAGE_LOCAL_NET_IP=172.16.0.11

apt-get install swift-account swift-container swift-object xfsprogs
mkfs.xfs -i size=1024 /dev/sda6
echo &quot;/dev/sda6 /srv/node/sda6 xfs noatime,nodiratime,nobarrier,logbufs=8 0 0&quot; &gt;&gt; /etc/fstab
mkdir -p /srv/node/sda6
mount /srv/node/sda6
chown -R swift:swift /srv/node
cat &gt;/etc/rsyncd.conf &lt;&lt;EOF
uid = swift
gid = swift
log file = /var/log/rsyncd.log
pid file = /var/run/rsyncd.pid
address = $STORAGE_LOCAL_NET_IP

[account]
max connections = 2
path = /srv/node/
read only = false
lock file = /var/lock/account.lock

[container]
max connections = 2
path = /srv/node/
read only = false
lock file = /var/lock/container.lock

[object]
max connections = 2
path = /srv/node/
read only = false
lock file = /var/lock/object.lock
EOF
perl -pi -e 's/RSYNC_ENABLE=false/RSYNC_ENABLE=true/' /etc/default/rsync
service rsync start
cat &gt;/etc/swift/account-server.conf &lt;&lt;EOF
[DEFAULT]
bind_ip = $STORAGE_LOCAL_NET_IP
workers = 2

[pipeline:main]
pipeline = account-server

[app:account-server]
use = egg:swift#account

[account-replicator]

[account-auditor]

[account-reaper]
EOF
cat &gt;/etc/swift/container-server.conf &lt;&lt;EOF
[DEFAULT]
bind_ip = $STORAGE_LOCAL_NET_IP
workers = 2

[pipeline:main]
pipeline = container-server

[app:container-server]
use = egg:swift#container

[container-replicator]

[container-updater]

[container-auditor]

[container-sync]
EOF
cat &gt;/etc/swift/object-server.conf &lt;&lt;EOF
[DEFAULT]
bind_ip = $STORAGE_LOCAL_NET_IP
workers = 2

[pipeline:main]
pipeline = object-server

[app:object-server]
use = egg:swift#object

[object-replicator]

[object-updater]

[object-auditor]
EOF
</code></pre>

<p>環境変数 ${STORAGE_LOCAL_NET_IP} を3台毎に変えて、各台で流し込んであげたら、
swift-proxy で生成した /etc/swift.conf とリング情報達を各 swift-storage に
配置します。</p>

<pre><code>swift-proxy     # scp /etc/swift/swift.conf /etc/swift/*ring.gz 172.16.0.11:/tmp/
swift-proxy     # scp /etc/swift/swift.conf /etc/swift/*ring.gz 172.16.0.12:/tmp/
swift-proxy     # scp /etc/swift/swift.conf /etc/swift/*ring.gz 172.16.0.13:/tmp/
swift-storage01 # mv /tmp/swift.conf /tmp/*ring.gz /etc/swift/
swift-storage01 # chown -R swift:swift /etc/swift
swift-storage02 # mv /tmp/swift.conf /tmp/*ring.gz /etc/swift/
swift-storage02 # chown -R swift:swift /etc/swift
swift-storage03 # mv /tmp/swift.conf /tmp/*ring.gz /etc/swift/
swift-storage03 # chown -R swift:swift /etc/swift
</code></pre>

<p>swift-storage を各台で起動します。</p>

<pre><code>swift-storage01 # swift-init all start
swift-storage02 # swift-init all start
swift-storage03 # swift-init all start
</code></pre>

<h2 id="アクセスしてみる">アクセスしてみる</h2>

<p>完成したので、swift クライアントでアクセスしてみる。</p>

<pre><code>% swift -A http://172.16.0.10:8080/auth/v1.0 -U system:root -K testpass stat
Account: AUTH_system
Containers: 4
Objects: 15
Bytes: 23866252
Connection: keep-alive
Accept-Ranges: bytes
</code></pre>

<p>ファイルをアップロード・ダウンロードしてみる。</p>

<pre><code>% swift -A http://172.16.0.10:8080/auth/v1.0 -U system:root -K testpass upload test /etc/hosts
% swift -A http://172.16.0.10:8080/auth/v1.0 -U system:root -K testpass list
% test
% cd /tmp/
% swift -A http://172.16.0.10:8080/auth/v1.0 -U system:root -K testpass download test
% ls /tmp/etc/hosts
</code></pre>

<p>もちろん、HTTP な API なので curl 等の HTTP ブラウザを使ってもアクセスできる！</p>

<pre><code>% curl -k -v -H 'X-Storage-User: system:root' -H 'X-Storage-Pass: testpass' http://172.16.0.10:8080/auth/v1.0
&lt; HTTP/1.1 200 OK
&lt; Server: nginx/1.1.19
&lt; Date: Mon, 03 Sep 2012 04:58:30 GMT
&lt; Content-Length: 0
&lt; Connection: keep-alive
&lt; X-Storage-Url: https://172.16.0.10:8080/v1/AUTH_system
&lt; X-Storage-Token: AUTH_tk8a19f76c9bce4077aee02aef76257020
&lt; X-Auth-Token: AUTH_tk8a19f96c9bce4077aee02aef76257020
&lt;
* Connection #0 to host 172.16.0.10 left intact
* Closing connection #0
* SSLv3, TLS alert, Client hello (1):
</code></pre>

<p>得られた X-Auth-Token, X-Storage-Url を使ってアクセスする。</p>

<pre><code>% curl -k -v -H 'X-Auth-Token: &lt;token-from-x-auth-token-above&gt;' &lt;url-from-x-storage-url-above&gt;
&lt; HTTP/1.1 200 OK
&lt; Server: nginx/1.1.19
&lt; Date: Mon, 03 Sep 2012 04:59:47 GMT
&lt; Content-Type: text/plain; charset=utf-8
&lt; Content-Length: 34
&lt; Connection: keep-alive
&lt; X-Account-Object-Count: 15
&lt; X-Account-Bytes-Used: 23866252
&lt; X-Account-Container-Count: 4
&lt; Accept-Ranges: bytes
&lt;
test
* Connection #0 to host 172.16.0.10 left intact
* Closing connection #0
* SSLv3, TLS alert, Client hello (1):
</code></pre>

<p>さっき放り込んだ &lsquo;test&rsquo; が swift 上にあることが確認できる。</p>

<p><a href="http://cyberduck.ch/">http://cyberduck.ch/</a> ここにある CyberDuck という GUI なツールを使ってもアクセス出来るよ！
HTTPS が必須になるので一工夫する必要があるのだけど、そのあたりは頑張ってみてください。</p>

<p>オンラインでのノードの追加・削除なんてことも出来ます。次回時間があったら解説しますね。</p>

<p>今回は swift-ring-builder コマンドでレプリカ数 &lsquo;3&rsquo; を指定したので、アップロー
ドしたファイル(オブジェクトと言う) は必ず 3 個配置される。なので 1 台の
swift-storage が故障しても大丈夫。ノードを増やせばストレージ全体の容量も増やせ
る。また、swift-proxy は単純な HTTP なので負荷分散機・もしくはソフトウェアのロー
ドバランサを入れれば swift-proxy 自体の冗長も組めるほか、ストレージ I/O の拡張
にもつながる。pound, nginx などを使って冗長組んでみてください。その時に
memcached の内容は共有させてあげる必要があるので、これまた一工夫が必要なのだけ
ど。時間があったら今度解説します。</p>

<p>自宅でも簡単に分散オブジェクトストレージが組める swift。使わない手は無いですよぉ。</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2012/11/04/swift-tempauth/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>

        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2012/10/06/secret-training-of-opscode-chef/">Secret Training of Opscode Chef</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2012-10-06'>
            October 6, 2012</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>昨日、開かれた &ldquo;Opscode Chef のシークレットトレーニング&rdquo; に参加してきました。</p>

<p>場所はうちの会社で KDDI ウェブコミュニケーションズ。主催はクリエーションオンラ
インさんでした。講師は Sean OMeara (@someara) さん。今後 Chef のトレーニングを
日本で開くため、事前に内容についてフィードバックが欲しかったそうで、オープンな
レッスンではありませんでしたが、次回以降、日本でも期待できそうです。</p>

<p>内容は chef の基本・メリット・考え方などを網羅した資料で1時間程進められ、その
後はハンズオンがメインでした。今日は実際にハンズオンの内容を書いていこうかと思
います。</p>

<p>chef workstation 環境は揃っている前提にします。また chef server として opscode
の hosted chef (opscode が提供している chef のホスティングサービス,
chef-server として動作します) を使います。またターゲットホストは当日は ec2 イ
ンスタンスを使いましたが、chef ワークステーションから到達できるホストであれば
何でも良いでしょう。</p>

<p>まずは chef-repo のクローン。講習会で使われたものです。</p>

<pre><code>git clone https://github.com/opscode/chef-repo-workshop-sysadmin.git chef-repo
</code></pre>

<p>予め cookbook が入っています。</p>

<p>次に、manage.opscode.com へアクセスしアカウントを作ります。Free アカウントが誰
でも作れるようになっています。</p>

<p><a href="https://manage.opscode.com">https://manage.opscode.com</a> へアクセス -&gt; Sign Up をクリック -&gt; アカウント情報
を入力 -&gt; submit -&gt; メールにて verify -&gt; 自分のアカウント名をクリック -&gt; Get a
new key をクリックし &lt;アカウント名&gt;.pem をダウンロード -&gt; create a
organization をクリックし Free を選択し、適当な名前で organization を作成。
validation key と knife.rb をダウンロード</p>

<p>これらで得た3つのファイル (2つの pem とknife.rb) を chef-repo/.chef/ 配下に置
きます。これで準備 OK。knife.rb を見ると、opscode の chef ホスティングにアクセ
スするよう記述があります。</p>

<pre><code>% cp &lt;somewhere&gt;/knife.rb &lt;somewhere&gt;/&lt;account&gt;.pem &lt;somewhere&gt;/&lt;account&gt;-validation.pem chef-repo/.chef/
</code></pre>

<p>トレーニング用 chef-repo には予め幾つかの cookbook と role, data_bag が入って
います。これらを knife を使ってアップロードします。</p>

<pre><code>% cd chef-repo
% knife cookbook upload -a
% knife role from file role/*.rb
% knife data bag create users
% knife data bag from file users nagiosadmin.json
</code></pre>

<p>そして bootstrap を実行。.chef/bootstrap ディレクトリに chef-full.erb ファイル
が入っていて bash スクリプトになっている。knife bootstrap でこの bash スクリプ
トをターゲットホスト上で実行することになる。中身はと言うと chef 環境をインストー
ルしているようだ。</p>

<p>chef 環境をどうやってノードに入れているかなぁ。preseed 使うかな。手作業じゃ意
味ないしなぁと考えていたのですが、こうやればいいのですね。参考になります。これ
は簡単。</p>

<p>ではいよいよ bootstrap を実行。</p>

<pre><code>% knife bootstrap &lt;IPADDRESS&gt; -r 'role[base],role[monitoring]' --sudo -x &lt;USER&gt; -P &lt;PASSWD&gt;
</code></pre>

<p>IP アドレス、ssh ユーザ名・パスワードは適宜入れてください。これで cookbooks ディ
レクトリ配下の各種 cookbook が実行されました。中身は nagios とそれに依存する
apache2, openssl, mysql, php 等。</p>

<p>次に cookbook を新たにダウンロードし knife upload してみます。</p>

<pre><code>% knife cookbook site download chef-client
% tar zxvf chef-client-1.2.0.tar.gz -C cookbooks/
% knife cookbook upload chef-client
</code></pre>

<p>「アカウントの pem がない場合 validation が用いられるが、一旦サーバと通信出来た
ら validation は必要なくなる。それどころか wi-fi パスワードのようなものなので
validation は削除することを強くすすめる」と Sean が言っていました。chef 環境は
すでに bootstrap で入っているものの、validation を削除するための recipe を
base という role に追加し、実行してみます。</p>

<p>Sean の言っていたこと、間違っていたら指摘してください( &gt;&lt; ) 英語だったので自信
ないです。</p>

<pre><code>% vim role/base.rb
name &quot;base&quot;
description &quot;Base role applied to all nodes.&quot;
run_list(
  &quot;recipe[apt]&quot;,
  &quot;recipe[nagios::client]&quot;
  &quot;recipe[chef-client::delete_validation]&quot;
)

default_attributes(
  &quot;nagios&quot; =&gt; {
  &quot;server_role&quot; =&gt; &quot;monitoring&quot;
  }
)
</code></pre>

<p>role をアップロード。</p>

<pre><code>% knife role from file roles/base.rb
</code></pre>

<p>ターゲットホストで chef-client を実行。ここで バージョン 10.14 から登場した
why-run を試してから実行。</p>

<pre><code>target# chef-client -Fdoc -lfatal --color --why-run
target# chef-client -Fdoc -lfatal --color
</code></pre>

<p>もしくはワークステーションから knife ssh を使っても良い。</p>

<pre><code>% knife search node &quot;role:base&quot; -a cloud.public_ipv4
% knife ssh &quot;role:base&quot; &quot;sudo chef-client -Fmin&quot; -x ubuntu -P opscodechef -a cloud.public_ipv4
</code></pre>

<p>-a cloud.public_ipv4 とは、AWS EC2 や OpenStack, CloudStack 環境ではインスタン
スの eth0 インターフェースにプライベート IP アドレスが付与されているケースが
殆どなため、グローバル IP アドレスを検索し実行している。</p>

<p>大きな流れはこれでおしまい。</p>

<p>トレーニングでは bento, minitest, cucumber-chef 等の紹介があったが、詳細な内容
については見送られた。昼休み1時間をはさんで計7時間の長丁場だったが、chef 初級
を脱するのには最適なトレーニングだった。これからトレーニング内容や種別を組んで
いくそうなので、内容は変わってくるかもしれない。あとでフィードバックをしなく
ちゃ。</p>

<p>個人的には OpenStack に興味を持っているので chef を使って OpenStack をデプロイ
したい。今年6月には &lsquo;Chef for OpenStack&rsquo; というアナウンスがあり、DELL,
RackSpace, HP 等の大企業がパートナーとして参加することになったと発表があった。
今まで openstack cookbook は長くメンテナンスされてこなかったので、これには期待
している。</p>

<p>最後にこの機会を作って下さった クリエーションオンラインさん、opscode の Sean
さん、mr.devops さん、ありがとうございましたー。貴重な体験でした。</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2012/10/06/secret-training-of-opscode-chef/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Categories
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
        <li><a href='/categories/report'>report</a></li>
    
</ul>

    </footer>
</article>

        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2012/08/29/7th-openstack-meetup/">第7回 OpenStack 勉強会参加レポート</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2012-08-29'>
            August 29, 2012</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>第7回 OpenStack 勉強会に参加してきました。</p>

<pre><code>開催日   : 2012年08月28日
開催場所 : 天王洲アイル ビットアイル
</code></pre>

<p>1年以上前から OpenStack, CloudStack 界隈はウォッチしていたのだけど、実際に構築
してってなると、今月始めばかりで、OpenStack も先週4日間掛けてやっとこさ構築出来たっ
てところ&hellip;orz。前回のブログ記事でへなちょこスクリプト公開しちゃったのを後悔しつ
つ現地に向かいましたw あと、その他に Opscode Chef 等の技術にも興味持って調査し
ていたので、今回の勉強会はまさに直ぐに活かせる内容だった。</p>

<p>では早速、報告があった内容と自分の感想を交えつつ書いていきます。</p>

<h2 id="hp-さんのクラウドサービス-hp-cloud-services">HP さんのクラウドサービス HP Cloud Services</h2>

<pre><code>日本 HP 真壁さま
</code></pre>

<p>HP さんは既に Public クラウドサービスを提供し始めていて Ojbect Storage, CDN 部
分は既にリリース済みだそうだ。compute, block storage 等はベータ版状態でこれか
らリリース。OpenStack ベースな構成で Horizon 部分は自前で開発したもの。既
にサーバ数は万の桁まで到達！ MySQL な DaaS も登場予定だとか。</p>

<p>あと HP だけにクラウドサービスに特化したサーバ機器も出していて、それが HP
Project Moonshot 。ARM/Atom 搭載のサーバで 2,880 nodes/rack が可能だとか！す
げぇ。もちろん電源等のボトルネックとなるリソースは他にも出てきそうだけど。</p>

<p>ノード数って増えると嬉しいのかな？コア数が増えるのは嬉しいけど。</p>

<h2 id="canonical-juju">Canonical JuJu</h2>

<pre><code>Canonical 松本さま
</code></pre>

<p>JuJu は Canonical が提供しているデプロイツールで charms と呼ばれるレシピ集 (っ
て言うと語弊があるのか) に従ってソフトウェアの配布を行うツール。MAAS という物
理サーバのプロビジョニングツールと組み合わせればハードウェアを設置した後のプロ
ビジョニング操作は一気通貫出来る、といったもの。具体的な操作例を挙げてくれたの
で添付してきます。</p>

<pre><code>% juju deploy --repository=/tmp swift-proxy
% juju deploy --repository=/tmp swift-storage
% juju add-relation swift-storage:swift-proxy
% swift-proxy:swift-proxy
% juju add-unit swift-storage # node 追加
</code></pre>

<p>swift-proxy, swift-storage をデプロイし、その後それぞれを関係付けているのが
add-relation。また swift-proxy に対して swift-storage node を追加してくといっ
た操作が add-unit らしい。</p>

<p>Charms と呼ばれるモノの中をのぞかせてもらったが Shell Script と json ファイル
集になっていた。インフラ系のエンジニアに操作してもらうにはこれがベスト、といっ
たところなのだろう。Opscode Chef の様な自由度があるかどうかは、触ってみないと
分からない。時間を見つけて調べてみるかぁ。</p>

<p>ちなにみ今日 MAAS について調べたのですが、これは PXE Boot と DHCP のコンフィギュ
レーションを GUI でするってものなのですね。後に出てくる Crowbar とはだいぶ違う。
間違っていたら指摘してください..。</p>

<p>参考 URL : <a href="https://wiki.ubuntu.com/ServerTeam/MAAS">https://wiki.ubuntu.com/ServerTeam/MAAS</a></p>

<h2 id="redhat-の-openstack-への取り組み">RedHat の OpenStack への取り組み</h2>

<pre><code>RedHat 中井さま
</code></pre>

<p>OpenStack ディストリビューションを提供し始めたことで最近話題になっていたが、い
よいよ今年2012年10月リリース予定の folsom をベースとしたリリースを2013年に控え
ているそうだ。ここで初めて有償サポートが開始されるそう。より簡単に構築が出来て、
技術的な不安定を持っているユーザを取り込んでいくのだろう。面白いネタも貰えた。
将来、Swift 代替で Gluster-FS が扱えるようになる可能性があるそうだ。また KVM
にコミットしているエンジニアを抱えている彼らだが、KVM から直接 Gluster-FS 上の
VM イメージを操作出来るように修正加える案も出ているそうだ。これが実現すれば、
nova ノードのサーバリソースに依存しない大きな Disk イメージを扱うことも可能に
なるだろう。</p>

<p>また、Canonical JuJu, Opscode Chef に並ぶツールの紹介もあった。CloudForms がそ
れなのだが、各社と共通するコンセプトを持っているようだ。開発環境・本番環境への
シームレスなデプロイ、と。</p>

<h2 id="dell-crowbar">DELL Crowbar</h2>

<pre><code>DELL 増月さま
</code></pre>

<p>ある意味、僕にとって一番の収穫だったのが DELL の Crowbar。DELL サーバに依存せ
ず使えるデプロイツールで、IPMI, RAID, BIOS 等のハードウェア構成も自動構築が出
来るそう！また Opscode Chef がベースになっていて barclamp と呼ばれる Chef で
言う Cookbooks を元にソフトウェアをデプロイしていくそうだ。Chef Server 環境が
必須で chef-solo のような操作には対応していなそうなのが残念だった。Web ベース
の GUI インターフェースで操作するらしい。デモも当日見れました。</p>

<p>ハードウェア構成も自動構築出来るツールは Canonical の MAAS があるが、一歩踏み
込んだ構成が組めそう。尚、Opscode の Cookbooks を再利用するのは少し難しい状況
のようだ。この辺りは 2.0 バージョンで改善されるそうだ。Chef との依存関係をより
シンプルなものにするそう。</p>

<p>他ベンダのと差別化を図るのかと思いきや、サーバ機器に依存しないツールを出してく
る DELL さんの思いは、どこにあるのだろう。あと BIOS, RAID 周りをソフトウェアで
プロビジョニングしていく受け口の I/F は IPMI なのかな？質問すればよかった。</p>

<h2 id="gmo-お名前-kvm">GMO お名前 KVM</h2>

<p>先月の OSC で発表になった資料をベースに説明して頂いた。diablo ベースで CentOS
上に構築されているらしい。また griddynamic.net のパッケージ (知らなかった) を
利用して (なぜ素直に Ubuntu 使わないのかな？) 構築したそうだ。griddynamic.net
のパッケージは既にエキスパイアしているらしい&hellip;。nova ノードは既に200台規模。
libvirt, kvm 周りにパッチを独自で当てているそうで、その具体的内容が聞けた。た
だこの辺は OpenStack のアップデートに追いつく作業がめちゃ大変になるだろうなぁ
と想像する。..</p>

<h2 id="全体を通して">全体を通して</h2>

<p>Opscode Chef に並ぶ技術が出始めてきた。OpenStack で nodes を追加するところまで
自動化したとしても vm 上の操作を手作業するわけにはいかないし、必然なのだろう。
よりディストリビューションに依存しない、インフラ系・アプリ系共に理解出来る、自
由度のある、汎用性のある技術を我々が選びながら使っていく必要がありそう。僕らイ
ンフラ系エンジニアの仕事内容も、この辺りにシフトしていく時代はもう目の前まで来
ているだろう。OpenStack を構築する手順が JuJu 等でコマンド一発なのを見て唖然と
したのも確か。誰でもできる操作になるのも必然で、ただどういう操作がされているか
を理解し、必要に応じて改変して開発していく力は身に着けておかないと、インフラ系
は特に、仕事内容が単純化していく一方になる気がする。危機を感じつつチャンスに結
びつけるいい機会なのかなぁ。あとは監視周りも自動コンフィギュレーションされない
と、真の自動化には至らないなぁ。</p>

<p>当日は BitIsle スタッフのみなさん、コミュニティのみなさん、ありがとうございま
したぁー。</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2012/08/29/7th-openstack-meetup/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Categories
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
        <li><a href='/categories/report'>report</a></li>
    
</ul>

    </footer>
</article>

        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2012/08/26/all-in-one-openstack-installation/">OpenStack ESSEX オールインワン インストール</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2012-08-26'>
            August 26, 2012</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>OpenStack のインストールってしんどいなぁ、って感じて devstack
<a href="http://devstack.org/">http://devstack.org/</a> とかで構築して中を覗いていたのですが、そもそも devstack
って再起動してしまえば何も起動してこないし、swift がインストールされないしで。
やっぱり公式のマニュアル見ながらインストールするしかないかぁって&hellip;。感じてい
たのですが&hellip;。</p>

<p><a href="http://docs.openstack.org/essex/openstack-compute/starter/os-compute-starterguide-trunk.pdf">http://docs.openstack.org/essex/openstack-compute/starter/os-compute-starterguide-trunk.pdf</a></p>

<p>このマニュアルの前提は、ネットワーク2セグメント・server1, server2 の計2台が前
提なのですが、環境作るのがしんどいので、オールインワンな構築がしたい！サーバ1台で OpenStack ESSEX を
インストールしたい！で、シェルスクリプトを作ったのでそれを使ったインストール方法を紹介します。</p>

<p><img src="http://jedipunkz.github.com/pix/openstack_thinkpad.jpg"></p>

<p>ぼくの Thinkpad に OpenStack ESSEX をインストールしてブラウザで localhost に接続して
いる画面です。ちゃんと KVM VM が起動して noVNC で接続できています。自己満足やぁ。</p>

<h2 id="前提条件">前提条件</h2>

<ul>
<li>Ubuntu Server 12.04 LTS amd64 がインストールされていること</li>
<li>Intel-VT もしくは AMD−Vなマシン</li>
<li>NIC が一つ以上ついているマシン</li>
<li>/dev/sda6, /dev/sda7 (デバイス名は何でもいい) の2つが未使用で空いていること</li>
</ul>

<p>です。</p>

<h2 id="構成">構成</h2>

<p>1 NIC を前提に eth0 と eth0:0 の2つを想定すると、こんな構成になります。eth0:0
は完全にダミーで IP アドレスは何でもいいです。br100 ブリッジデバイス上で VM が
NW I/F を持ちます。floating range ってのは OpenStack で言うグローバル IP レン
ジ。グローバルである必要は無いですが eth0 と同じレンジの IP アドレスを VM に付
与出来ます。/dev/sda6 が nova-volumes で /dev/sda7 が swift 。なので OS インス
トール時に2つのデバイスを未使用で空けておいてください。</p>

<pre><code>+--+--+--+
|VM|VM|VM|  192.168.4.32/27
+--+--+--+..
+----------+ +--------+
|          | | br100  | 192.168.4.33/27 -&gt; floating range : 10.200.8.32/27
|          | +--------+
|          | | eth0:0 | 192.168.3.1       disk devices
|   Host   | +--------+   (dummy)  +------------------------+
|          |                       | /dev/sda6 nova-volumes |
|          | +--------+            +------------------------+
|          | |  eth0  | ${HOST_IP} | /dev/sda7 swift        |
+----------+ +--------+            +------------------------+
|              nw I/Fs
+----------+
|   CPE    |
+----------+
</code></pre>

<h2 id="インストール手順">インストール手順</h2>

<p>インストール手順は簡単です。</p>

<pre><code>% sudo -i
# git clone git://github.com/jedipunkz/openstack_install.git
</code></pre>

<p>して取得したスクリプトを環境に合わせて環境変数設定します。スクリプト上部のこれ
らの内容を、環境に合わせて設定。${HOST_IP} と ${NOVA_VOLUMES_DEV},
${SWIFT_DEV} だけ気をつければ OK です。その他は内部ネットワークの設定なので何
でもつながります。</p>

<pre><code># -----------------------------------------------------------------
# Environment Parameter
# -----------------------------------------------------------------
HOST_IP='10.200.8.15'
HOST_MASK='255.255.255.0'
HOST_NETWORK='10.200.8.0'
HOST_BROADCAST='10.200.8.255'
GATEWAY='10.200.8.1'
MYSQL_PASS='secret'
FIXED_RANGE='192.168.4.1/27'
FLOATING_RANGE='10.200.8.32/27'
FLAT_NETWORK_DHCP_START='192.168.4.33'
ISCSI_IP_PREFIX='192.168.4'
NOVA_VOLUMES_DEV='/dev/sda6'
SWIFT_DEV='/dev/sda7'
</code></pre>

<p>で実行。</p>

<pre><code># chmod +x openstack_install/openstack_install.sh
# ./openstak_install/openstack_install.sh allinone
( wait some minutes...)
</code></pre>

<p>マシンによりますが、10分弱すると OpenStack が構築されているはずです。</p>

<h2 id="os-イメージと-ssh-キーペアのインストール">OS イメージと SSH キーペアのインストール</h2>

<p>上記の手順で OpenStack は構築されるのですが、VM を起動するには OS イメージが必
要ですよね。これは自分で用意するしかないです。ただ、これは簡単で下記の手順で出
来ます。</p>

<h4 id="os-イメージ作成">OS イメージ作成</h4>

<p>サンプルで Ubuntu Server 12.04 LTS amd64 なイメージをここで作ってみます。</p>

<pre><code># kvm-image create -f qcow2 server.img 5G
# wget http://gb.releases.ubuntu.com//precise/ubuntu-12.04-server-amd64.iso
# kvm -m 256 -cdrom ubuntu-12.04-server-amd64.iso -drive file=server.img,if=virtio,index=0 -boot d -net nic -net user -nographic -vnc :0
</code></pre>

<p>手元の端末の VNC ツールで ${HOST_IP}:0 に接続し OS のインストールを済ませます。
その後、下記のコマンドで HDD から起動してあげて&hellip;</p>

<pre><code># kvm -m 256 -drive file=server.img,if=virtio,index=0 -boot c -net nic -net user -nographic -vnc :0
</code></pre>

<p>再度、VNC で VM に接続して..</p>

<pre><code># sudo rm -rf /etc/udev/rules.d/70-persistent-net.rules
# shutdown -h now
</code></pre>

<p>上記の操作をしたら OS イメージ作成は終わり。その他のディストリビューションでの
イメージ作成については公式マニュアルに書いてあります。</p>

<h4 id="glance-に-os-イメージをインストール">Glance に OS イメージをインストール</h4>

<p>作成した OS イメージを Glance に追加します。先ほどのスクリプトで生成された
/root/.openstack が Glance に接続するために必要なので zsh の場合 source してから&hellip;</p>

<pre><code># source /root/.openstack
# glance add name=&quot;Ubuntu Server 12.04LTS&quot; is_public=true container_format=ovf disk_format=qcow2 &lt; server.img
</code></pre>

<p>で追加出来ます。.openstack は bash でも取得できるのでその際は</p>

<pre><code># . /root/.openstack
</code></pre>

<p>してください。root ユーザ以外でも操作出来ます。</p>

<h4 id="ssh-キーペアの生成とインストール">SSH キーペアの生成とインストール</h4>

<p>VM に割り当てる SSH キーペアを作ってインストールする手順です。</p>

<pre><code># ssh-keygen
# nova keypair-add --pub_key .ssh/id_rsa.pub mykey
# nova keypair-list
</code></pre>

<h4 id="horizon-へ接続">Horizon へ接続</h4>

<p>いよいよ Horizon へ接続です。Horizon は OpenStack ESSEX から取り込まれた Web
UI です。VM の作成・削除、ネットワークの設定等がブラウザで操作出来ます。</p>

<pre><code>http://${HOST_IP
</code></pre>

<p>に接続してユーザ : &lsquo;admin&rsquo;, パスワード &lsquo;admin&rsquo; でログインしてください。</p>

<h2 id="まとめ">まとめ</h2>

<p>OpenStack はしんどいｗ ですが来月 <sup>2012</sup>&frasl;<sub>09</sub> リリース予定の Folsom は &lsquo;Easy
Setup&rsquo; がフューチャされてるそうです。期待。手動で構築していると glance のとこ
ろで ID 地獄にハマりますｗ 今回の手順で all in one な環境ができたら、色々覗い
てみてコンポーネント毎に Node を切り出すってことも考えないといけないと思います。
それぞれは HTTP ベースの API で接続できれば OK なので切り出すこと自体は簡単。
冗長を組む方法は.. これから調べます。keystone, glance を拡張・冗長させるって出
来るのか？難しそう。CloudStack と違って rabbitmq-server でキューイングしてくれ
るので、Node が増えた時の対処は考えれれているよう。</p>

<p>あと、OS イメージではなくて AMI で VM を作る方法もあるのですが AMI の作成方法は Web
を見ていると沢山載っていますので参考にして作ってみてください。</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2012/08/26/all-in-one-openstack-installation/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>

        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2012/08/18/opscode-bootstrap-chef-server/">Opscode Bootstrap を使った Chef-Server 構築</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2012-08-18'>
            August 18, 2012</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>chef-server の構築は少し面倒だと前回の記事
<a href="http://jedipunkz.github.com/blog/2012/08/18/chef-solo/">http://jedipunkz.github.com/blog/2012/08/18/chef-solo/</a> に書いたのですが、
opscode が提供している bootstrap を用いると、構築作業がほぼ自動化出来ます。
今回はこの手順を書いていきます。</p>

<h2 id="chef-のインストール">chef のインストール</h2>

<p>前回同様に rbenv を使って ruby をインストールし chef を gem でインストールして
いきます。</p>

<pre><code>% sudo apt-get update
% sudo apt-get install zlib1g-dev build-essential libssl-dev
% sudo -i
# cd ~
# git clone git://github.com/sstephenson/rbenv.git .rbenv
# echo 'export PATH=&quot;$HOME/.rbenv/bin:$PATH&quot;' &gt;&gt; ~/.zshrc
# echo 'eval &quot;$(rbenv init -)&quot;' &gt;&gt; ~/.zshrc
</code></pre>

<p>ruby-build をインストールします。</p>

<pre><code># mkdir -p ~/.rbenv/plugins
# cd ~/.rbenv/plugins
# git clone git://github.com/sstephenson/ruby-build.git
</code></pre>

<p>ruby-1.9.2 インストール</p>

<pre><code># rbenv install 1.9.2-p290
# rbenv global 1.9.2-p290
# rbenv rehash
</code></pre>

<p>gem を使って、chef, chef-server-api をインストールします</p>

<pre><code># gem install chef
# gem install chef-server-api
</code></pre>

<h2 id="opscode-bootstrap-を使った-chef-server-の構築">opscode bootstrap を使った chef-server の構築</h2>

<p>まずは chef-solo の環境整備。この bootstrap は chef-solo で実行出来るクックブッ
ク集になっています。中を覗くと &lsquo;apache2, chef, chef-server, couchdb, erlang,
rabgitmq&rsquo; などなど必要なミドルウェア群のクックブックが入っていることが判ります。</p>

<p>まずは chef-solo の環境を整備します。</p>

<pre><code># mkdir /etc/chef
# vi /etc/chef/solo.rb
# cat /etc/chef/solo.rb
file_cache_path &quot;/tmp/chef-solo&quot;
cookbook_path &quot;/tmp/chef-solo/cookbooks&quot;
# vi /etc/chef/chef.json
# cat /etc/chef/chef.json
{
  &quot;chef_server&quot;: {
  &quot;server_url&quot;: &quot;http://localhost:4000&quot;
  },
  &quot;run_list&quot;: [ &quot;recipe[chef-server::rubygems-install]&quot; ]
}
</code></pre>

<p>solo.rb で指定したパスは任意のもので構いません。</p>

<p>Amazon S3 上に opscode bootstrap があります。いよいよ chef-solo で bootstrap
を実行します。</p>

<pre><code># chef-solo -c /etc/chef/solo.rb -j /etc/chef/chef.json -r http://s3.amazonaws.com/chef-solo/bootstrap-latest.tar.gz
</code></pre>

<p>これで chef-server 構築は終わりです。実行したホストに couchDB, Erlang などが構
成されていることが分かると思います。</p>

<p>次に knife の環境を整備します。knife は chef の操作を行うためのコマンドライン
ツールです。</p>

<pre><code># mkdir -p ~/.chef
# cp /etc/chef/validation.pem /etc/chef/webui.pem ~/.chef
# sudo chown -R root ~/.chef
# knife configure -i
Where should I put the config file? [~/.chef/knife.rb] 
Please enter the chef server URL: [http://localhost:4000] 
Please enter a clientname for the new client: [root]
Please enter the existing admin clientname: [chef-webui] 
Please enter the location of the existing admin client's private key:
# [/etc/chef/webui.pem] /root/.chef/webui.pem
Please enter the validation clientname: [chef-validator] 
Please enter the location of the validation key:
# [/etc/chef/validation.pem] /root/.chef/validation.pem
Please enter the path to a chef repository (or leave blank): 
WARN: Creating initial API user...
INFO: Created (or updated) client[root]
WARN: Configuration file written to /home/root/.chef/knife.rb
</code></pre>

<p>パラメータはほぼそのまま入力してください。pem ファイルのパス指定だけ先ほど
root ユーザの HOME ディレクトリにインストールしたモノを指定してください。</p>

<p>これで下記のように knife が使えるようになっています。</p>

<pre><code># knife client list
chef-validator
chef-webui
root
</code></pre>

<p>今回は以上です。この chef-server の構築を knife::server というツールで実現する
方法もあります。</p>

<p><a href="http://fnichol.github.com/knife-server/">http://fnichol.github.com/knife-server/</a></p>

<p>同じく bootstrap を指定して構築するツールなのですが、これを使う利点として</p>

<ul>
<li>chef-server 構成のバックアップが取れる</li>
<li>バックアップを元に復元できる</li>
</ul>

<p>があります。これは便利。ただし knife が最初から使える環境に限ります。</p>

<p>これらツールを使うことを前提にしないと、作業が複雑化するため、必須だと思います。
手作業はミスの元ですし。ただ、一度は手作業でやってみて構成を理解することはしな
くてはならないでしょう。couchDB, Rabbitmq などの新しいミドルウェアについての理
解も深めなくてはならないでしょうし、これらをスケールさせることを前提にしておか
ないとノードの数が増える度に負荷が上昇していくので将来困るでしょう。前回の記事
でも触れましたが chef-solo を用いた場合はそれらの心配が無くなります。</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2012/08/18/opscode-bootstrap-chef-server/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>

        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2012/08/18/chef-solo/">chef-solo で学ぶ chef の基本動作</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2012-08-18'>
            August 18, 2012</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>仕事で Opesocd Chef の情報収集をしてたのですが、僕が感じるにこれはインフラエン
ジニアの未来だと。逆に言うとインフラエンジニアの危機。AWS のようなクラウドサー
ビスがあればアプリケーションエンジニアが今までインフラエンジニアが行っていた作
業を自ら出来てしまうからです。</p>

<p>インフラエンジニアなら身に付けるしかない！って僕が感じる Chef について
chef-solo を通して理解するために情報まとめました。</p>

<p>chef には chef-server 構成で動作するものと chef-solo というサーバ無しで動作す
るものがある。chef-server は構築するのが少し大変 (後に方法をブログに書きたい)
なので今回は chef-solo を使ってみる。ちなみに Opscode が chef-server のホスティ
ングサービスを展開している。彼らとしてはこちらがメイン。</p>

<h2 id="chef-solo-の入れ方">chef-solo の入れ方</h2>

<p>opscode が推奨している ruby-1.9.2 をインストールする。rvm は色々問題を招き寄せ
るので rbenv を使って環境整えます。root ユーザ環境内に入れてください。</p>

<p>必要なパッケージをインストール</p>

<pre><code>% sudo apt-get update
% sudo apt-get install build-essential zlib1g-dev libssl-dev
</code></pre>

<p>root ユーザにてrbenv をインストール</p>

<pre><code>% sudo -i
# cd ~
# git clone git://github.com/sstephenson/rbenv.git .rbenv
# echo 'export PATH=&quot;$HOME/.rbenv/bin:$PATH&quot;' &gt;&gt; ~/.zshrc
# echo 'eval &quot;$(rbenv init -)&quot;' &gt;&gt; ~/.zshrc
</code></pre>

<p>ruby-build をインストール</p>

<pre><code># mkdir -p ~/.rbenv/plugins
# cd ~/.rbenv/plugins
# git clone git://github.com/sstephenson/ruby-build.git
</code></pre>

<p>opscode が推奨している ruby バージョン 1.9.2 をインストール</p>

<pre><code># rbenv install 1.9.2-p290
# rbenv global 1.9.2-p290
# rbenv rehash
</code></pre>

<p>capistrano, chef のインストールを gem を使い行う</p>

<pre><code># gem install chef
# rbenv rehash
</code></pre>

<p>これらは &lsquo;root&rsquo; ユーザ環境内に構築する必要がある。chef がそれを前提としている
からだ。また、perl と違い ruby は後方互換性がないので将来のことを考え rbenv で
バージョンを管理し続ける必要がある、と思う。</p>

<h2 id="chef-solo-の設定">chef-solo の設定</h2>

<p>/etc/chef/chef.json ファイルを修正することで、chef-solo で実行する recipe の追
加を行う。これは複数指定することが可能。</p>

<pre><code>{
    &quot;run_list&quot;: [
        &quot;recipe[ntp]&quot;,
    ]
}
</code></pre>

<p>上記は ntp レシピ を追加した例。次に /etc/chef/solo.rb を生成する。これは
chef-solo 動作に必要な PATH 指定を主に行う。</p>

<pre><code>file_cache_path &quot;/tmp/chef-solo&quot;
cookbook_path [&quot;/home/jedipunkz/cookbooks&quot;]
role_path &quot;/home/jedipunkz/role&quot;
log_level :debug
</code></pre>

<p>上記パラメータの説明は下記の通り。</p>

<ul>
<li>file_cache_path : cache 用のディレクトリ指定</li>
<li>cookbook_path : cookbook を配置するディレクトリ指定</li>
<li>role_path : role ディレクトリ指定</li>
<li>log_level : Log Level の指定</li>
</ul>

<h2 id="サンプルクックブックのダウンロードと理解">サンプルクックブックのダウンロードと理解</h2>

<p>サンプルとして opscode が提供している &lsquo;ntp&rsquo; を持ってくる。中身が簡単に理解出来
るものなので最初理解するために持ってくるものとしては最適。</p>

<pre><code>% cd /home/jedipunkz/cookbooks
% git clone https://github.com/opscode-cookbooks/ntp.git
</code></pre>

<p>持ってきたクックブックの構造は</p>

<pre><code>ntp .
    |- attributes
    |- templates
    |- recipes
    |- meradata.rb
    |-..
    |-..
</code></pre>

<p>となっている。attribute/default.rb の一部分を抜粋を記してみた。</p>

<pre><code>default['ntp']['servers']   = %w{ 0.pool.ntp.org 1.pool.ntp.org 2.pool.ntp.org 3.pool.ntp.org }
default['ntp']['packages'] = %w{ ntp ntpdate }
default['ntp']['service'] = &quot;ntp&quot;
default['ntp']['varlibdir'] = &quot;/var/lib/ntp&quot;
default['ntp']['conf_owner'] = &quot;root&quot;
default['ntp']['conf_group'] = &quot;root&quot;
default['ntp']['var_owner'] = &quot;ntp&quot;
default['ntp']['var_group'] = &quot;ntp&quot;
</code></pre>

<p>chef は ruby の DSL で記述するが template や recipe 内で指定するパラメータ集と
なるのが attribute となる。上を見てみると ntp のパッケージ名やディレクトリのオー
ナー情報等が記されている。</p>

<p>templates/default/ntp.conf.erb を見てみると&hellip;</p>

<pre><code>driftfile &lt;%= node['ntp']['driftfile'] %&gt;
statsdir &lt;%= node['ntp']['statsdir'] %&gt;

statistics loopstats peerstats clockstats
filegen loopstats file loopstats type day enable
filegen peerstats file peerstats type day enable
filegen clockstats file clockstats type day enable

&lt;%# If ntp.peers is not empty %&gt;
&lt;% unless node['ntp']['peers'].empty? -%&gt;
  &lt;%# Loop through defined peers, but don't peer with ourself %&gt;
  &lt;% node['ntp']['peers'].each do |ntppeer| -%&gt;
    &lt;% if node['ipaddress'] != ntppeer and node['fqdn'] != ntppeer %&gt;
peer &lt;%= ntppeer %&gt; iburst
restrict &lt;%= ntppeer %&gt; nomodify
    &lt;% end -%&gt;
  &lt;% end -%&gt;
&lt;% end -%&gt;
</code></pre>

<p>これはインストールする /etc/ntp.conf (recipde で後に指定する) の内容そのままだ。
先程も書いたが chef は ruby の DSL 記述が基本なので attribute で指定したパラメー
タを持ってきて、こういったコンフィグファイルを生成出来る。では最後に recipe を
見てみる。この recipe が chef の本体と言っていいところですね。上部の抜粋です。</p>

<pre><code>node['ntp']['packages'].each do |ntppkg|
  package ntppkg
end
</code></pre>

<p>この [&lsquo;ntp&rsquo;][&lsquo;packages&rsquo;] は attributes/default.rb に %w{ ntp ntpdate } と書い
てある。つまり ntp, ntpdate の配列を ntppkg として回して chef resources の
&lsquo;package&rsquo; を使ってインストールしている。resources については chef の公式ドキュ
メントを読むと良い。recipe で使える記述全てが1ページにまとまっている。</p>

<p><a href="http://wiki.opscode.com/display/chef/Resources">http://wiki.opscode.com/display/chef/Resources</a></p>

<p>次に recipe の下部を抜粋してみた。chef resources の template によって ntp.conf
をインストールしている。</p>

<pre><code>template &quot;/etc/ntp.conf&quot; do
  source &quot;ntp.conf.erb&quot;
  owner node['ntp']['conf_owner'] 
  group node['ntp']['conf_group']
  mode &quot;0644&quot;
  notifies :restart, resources(:service =&gt; node['ntp']['service'])
end
</code></pre>

<p>source によって template/default/ntp.conf.erb を呼び出し owner, group でファイ
ルのオーナー情報を、mode でパーミッションを指定している。また修正が入った際に
ntp サービスの再起動を行っているのが最終行だ。</p>

<p>抜粋で例を挙げながらだったが、Resources の記述方法さえ理解してしまえば全てが理
解出来るだろうし、自分でクックブックを作ることも簡単だろう。</p>

<h2 id="chef-solo-の実行">chef-solo の実行</h2>

<p>ではいよいよ chef-solo の実行。</p>

<p>上記で生成した chef.json と solo.rb を指定し chef-solo を実行することで上記
run_list で指定した recipe &lsquo;ntp&rsquo; が実行される。</p>

<pre><code>% sudo -i
# chef-solo -c /etc/chef/solo.rb -j /etc/chef/chef.json
</code></pre>

<h2 id="まとめ">まとめ</h2>

<p>chef-server 構成の組み方は後日ブログで書いてみたい。先日 #DevLOVE に参加した際
にも話題になったが、chef-solo を使うか chef-server 構成を組むか、まだ議論が必
要そう。chef-server 構成を組むことは簡単では無いが普通にエンジニアなら組めるだ
ろう。が、組んだところで拡張性・冗長性・を考えた構成を組むにはまだまだノウハウ
が足りない。また couchDB, rabbitmq など比較的新しいミドルウェアが使われている
ので、これから経験積まないと難しいだろう。それに比べて chef-solo は上記の通り
とてもシンプル。しかも chef-solo を実行する node 自身は必然的に数が増え拡張す
るし、それを受ける apt レポジトリは単純な HTTP なので拡張・冗長は簡単だろう。</p>

<p>また、capistrano と chef-solo を組み合わせることで、role といった概念をもたせ
たり、workstation で一括操作といった利便性も持たせることが出来る。ある意味
chef-solo を使うなら必然的な点になりそう。capistrano との組み合わせについても
後日ブログで書いてみたい。</p>

<p>chef を理解すること自体はそんなには難しくないし、これからの時代に必要になるこ
とは眼に見えているので、学んでおいて損はしないだろう。</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2012/08/18/chef-solo/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>

        

        
<ul class="actions pagination">
    
        <li><a href="/post/page/9/"
                class="button big previous">Previous Page</a></li>
    

    
        <li><a href="/post/page/11/"
                class="button big next">Next Page</a></li>
    
</ul>

    </div>
    
<section id="sidebar">

    
        <section id="intro">
            
            
            
            <ul class="icons">
                
                    <li><a href="https://jedipunkz.github.io/post/index.xml" type="application/rss+xml"
                        target="_blank" title="RSS" class="fa fa-rss"></a></li>
                
                
            </ul>
        </section>

    
        <section id="recent-posts">
            <ul class="posts">
                <header>
                    <h3>Recent Posts</h3>
                </header>
                
                    
                

                
                    
                

                
                    <li>
                        <article>
                            <header>
                                <h3><a href="https://jedipunkz.github.io/blog/2018/12/31/istio/">Istio, Helm を使って Getting Started 的なアプリをデプロイ</a></h3>
                                
                                    
                                
                                <time class="published" datetime=
                                    '2018-12-31'>
                                    December 31, 2018</time>
                            </header>
                        </article>
                    </li>
                
                    <li>
                        <article>
                            <header>
                                <h3><a href="https://jedipunkz.github.io/blog/2017/07/02/test-kitchen-cluster/">Docker,Test-Kitchen,Ansible でクラスタを構成する</a></h3>
                                
                                    
                                
                                <time class="published" datetime=
                                    '2017-07-02'>
                                    July 2, 2017</time>
                            </header>
                        </article>
                    </li>
                
                    <li>
                        <article>
                            <header>
                                <h3><a href="https://jedipunkz.github.io/blog/2017/04/13/gke-lb/">GCP ロードバランサと GKE クラスタを Terraform を使って構築する</a></h3>
                                
                                    
                                
                                <time class="published" datetime=
                                    '2017-04-13'>
                                    April 13, 2017</time>
                            </header>
                        </article>
                    </li>
                
                    <li>
                        <article>
                            <header>
                                <h3><a href="https://jedipunkz.github.io/blog/2017/02/12/serverless-fission/">Serverless on Kubernetes : Fission を使ってみた</a></h3>
                                
                                    
                                
                                <time class="published" datetime=
                                    '2017-02-12'>
                                    February 12, 2017</time>
                            </header>
                        </article>
                    </li>
                
                    <li>
                        <article>
                            <header>
                                <h3><a href="https://jedipunkz.github.io/blog/2017/01/13/kubernetes-deployments/">Kubernetes Deployments を使ってみた！</a></h3>
                                
                                    
                                
                                <time class="published" datetime=
                                    '2017-01-13'>
                                    January 13, 2017</time>
                            </header>
                        </article>
                    </li>
                

                
                    <li>
                        <ul class="actions">
                            <li><a href=
                            
                                "/post/"
                            
                            class="button">View more posts</a></li>
                        </ul>
                    </li>
                
            </ul>
        </section>

    
    
    
    
        <section id="categories">
            <ul class="posts">
                <header>
                    <h3><a href="/categories/">Categories</a></h3>
                </header>

                
                    
                

                
                    <li>
                        <article>
                            <header>
                                <a href="/categories/infrastructure/">infrastructure</a>
                                <span style="float:right;">110</span>
                            </header>
                        </article>
                    </li>
                
                    <li>
                        <article>
                            <header>
                                <a href="/categories/report/">report</a>
                                <span style="float:right;">9</span>
                            </header>
                        </article>
                    </li>
                
                    <li>
                        <article>
                            <header>
                                <a href="/categories/tools/">tools</a>
                                <span style="float:right;">11</span>
                            </header>
                        </article>
                    </li>
                
            </ul>
        </section>
    

    
        

    
        <section id="footer">
            <ul class="icons">
                
                    <li><a href="https://jedipunkz.github.io/post/index.xml" type="application/rss+xml"
                        target="_blank" title="RSS" class="fa fa-rss"></a></li>
                
                
            </ul>

            <p class="copyright">&copy; ジェダイさんのブログ. テーマデザインは <a href="//github.com/jpescador" target="_blank">Julio Pescador</a>さんによるものです。 </p>
        </section>

</section>

            </div>
        <a id="back-to-top" href="#" class="fa fa-arrow-up fa-border fa-2x"></a>
        

        
        
            
        

        
        
            <script src="/js/jquery.min.js"></script>
            <script src="/js/skel.min.js"></script>
            <script src="/js/util.js"></script>
            <script src="/js/main.js"></script>
            <script src="/js/backToTop.js"></script>
            <script src="/js/highlight.pack.js"></script>
        

        

            
            <script>hljs.initHighlightingOnLoad();</script>
            
    </body>
</html>

