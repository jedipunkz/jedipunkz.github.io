+++
title = "Journal 用 SSD を用いた Ceph 構成の構築"
date = "2014-02-27"
slug = "2014/02/27/journal-ssd-ceph-deploy"
Categories = []
+++
こんにちは、@jedipunkz です。

前回、'Ceph のプロセス配置ベストプラクティス' というタイトルで記事を書きました。

<http://jedipunkz.github.io/blog/2014/01/29/ceph-process-best-practice/>

今回はこの記事にあるポリシに従って下記のような特徴を持った構成を作る手順を具体
的に書いていきたいと思います。

* ceph01 - ceph04 の4台構成
* ノードに HDD 2台搭載されていることを前提 (/dev/sdb, /dev/sdc)
* ノードに Journal 用 SSD 1台搭載されていることを前提 (/dev/ssd)
* ceph04 は mds サービス稼働
* ceph05 は ceph-deploy を実行するためのワークステーション
* 最終的に ceph04 から Ceph をマウントする
* mon は ノード単位で稼働
* osd は HDD 単位で稼働
* mds は ceph04 に稼働

構成 : ハードウェアとノードとネットワークの関係
----

```
                                                                                      public network
         +-------------------+-------------------+-------------------+-------------------+---------
         |                   |                   |                   |                   |
+--------+--------+ +--------+--------+ +--------+--------+ +--------+--------+ +--------+--------+
|      ceph01     | |      ceph02     | |      ceph03     | |      ceph04     | |      ceph05     |
| +-----+ +-----+ | | +-----+ +-----+ | | +-----+ +-----+ | |                 | |                 |
| | sdb | | sdc | | | | sdb | | sdc | | | | sdb | | sdc | | |                 | |                 |
| +-----+-+-----+ | | +-----+-+-----+ | | +-----+-+-----+ | |                 | |                 |
| |     ssd     | | | |     ssd     | | | |     ssd     | | |                 | |                 |
| +-------------+ | | +-------------+ | | +-------------+ | |                 | |                 |
+--------+--------+ +--------+--------+ +--------+--------+ +-----------------+ +-----------------+
         |                   |                   |                                    cluster network
         +-------------------+-------------------+-------------------------------------------------
```

構成 : プロセスとノードとネットワークの関係
----

```
                                                                                      public network
         +-------------------+-------------------+-------------------+-------------------+---------
         |                   |                   |                   |                   |
+--------+--------+ +--------+--------+ +--------+--------+ +--------+--------+ +--------+--------+
|      ceph01     | |      ceph02     | |      ceph03     | |      ceph04     | |      ceph05     |
| +-----+ +-----+ | | +-----+ +-----+ | | +-----+ +-----+ | | +-------------+ | |                 |
| | osd | | osd | | | | osd | | osd | | | | osd | | osd | | | |     mds     | | |                 |
| +-----+-+-----+ | | +-----+-+-----+ | | +-----+-+-----+ | | +-------------+ | |                 |
| |     mon     | | | |     mon     | | | |     mon     | | |                 | |                 |
| +-------------+ | | +-------------+ | | +-------------+ | |                 | |                 |
+--------+--------+ +--------+--------+ +--------+--------+ +-----------------+ +-----------------+
         |                   |                   |                                    cluster network
         +-------------------+-------------------+-------------------------------------------------
```

注意 : 上記の図だと ssd : mon が対に見えますが、そうではありません。

では構築方法について書いていきます。

作業用ホストの準備
----

ノンパスフレーズの SSH 公開鍵・秘密鍵を生成する。

``` bash
% ssh-keygen
```

公開鍵をターゲットホスト (ceph01-03) に配置

``` bash
% ssh-copy-id ceph@ceph01
% ssh-copy-id ceph@ceph02
% ssh-copy-id ceph@ceph03
```

ceph-deploy の取得を行う。

``` bash
% git clone https://github.com/ceph/ceph-deploy.git ~/ceph-deploy
```

'python-virtualenv' パッケージをインストールする。

``` bash
% sudo apt-get update ; sudo apt-get -y install python-virtualenv
```

ceph-deploy をブートストラップする

``` bash
% cd ~/ceph-deploy
% ./bootstrap
```

PATH を通す。下記は例。

``` bash
% ${EDITOR} ~/.zshrc
export PATH=$HOME/ceph-deploy:$PATH
```

ホスト名の解決を行う。IP アドレスは例。

``` bash
% sudo ${EDITOR} /etc/hosts
10.200.10.11    ceph01
10.200.10.12    ceph02
10.200.10.13    ceph03
10.200.10.14    ceph04
10.200.10.15    ceph05
```

上記の構成の構築方法
----

以前の記事同様に ceph-deploy をデプロイツールとして用いる。

ceph-deploy に関しては下記の URL を参照のこと。

<https://github.com/ceph/ceph-deploy>

下記の手順でコンフィギュレーションと鍵の生成を行う。またこれからの操作はすべて public network 
上の ceph-deploy 専用 node からの操作とする。

``` bash
% mkdir ~/ceph-work
% cd ~/ceph-work
% ceph-deploy --cluster cluster01 new ceph01 ceph02 ceph03 ceph04
```
   
~/ceph-work ディレクトリ上に cluster01.conf が生成されているので下記の通り
cluster network を扱う形へと追記を行う。

``` bash
public network = <public_network_addr>/<netmask>
cluster network = <cluster_network_addr>/<netmask>

[mon.a]
    host = ceph01
    mon addr = <ceph01_ip_addr>:6789

[mon.b]
    host = ceph02
    mon addr = <ceph02_ip_addr>:6789

[mon.c]
    host = ceph03
    mon addr = <ceph03_ip_addr>:6789

[osd.0]
    public addr = <ceph01_public_ip_addr>
    cluster addr = <ceph01_cluster_ip_addr>

[osd.1]
    public addr = <ceph01_public_ip_addr>
    cluster addr = <ceph01_cluster_ip_addr>

[osd.2]
    public addr = <ceph01_public_ip_addr>
    cluster addr = <ceph01_cluster_ip_addr>

[mds.a]
    host = ceph04
```

ceph の各 nodes へのインストールを行う。ceph はワークステーションである ceph05 にも
インストールしておきます。後に Ceph ストレージをマウントするためです。

``` bash
% ceph-deploy --cluster cluster01 install ceph01 ceph02 ceph03 ceph04 ceph04
```

mon プロセスを各 nodes で稼働する。

``` bash
% ceph-deploy --cluster cluster01 mon create ceph01 ceph02 ceph03
```

鍵の配布を各 nodes に行う。

``` bash
% ceph-deploy --cluster cluster01 gatherkeys ceph01 ceph02 ceph03 ceph04 ceph05
```

disk のリストを確認。

各 node 毎に用いることが可能は disk の一覧を確認する。

``` bash
% ceph-deploy --cluster cluster01 disk list ceph01
% ceph-deploy --cluster cluster01 disk list ceph02
% ceph-deploy --cluster cluster01 disk list ceph03
```

disk の初期化を行う。この作業を行うと指定ディスク上のデータは消去される。

``` bash
% ceph-deploy --cluster cluster01 disk zap ceph01:/dev/sdb ceph01:/dev/sdc
% ceph-deploy --cluster cluster01 disk zap ceph02:/dev/sdb ceph02:/dev/sdc
% ceph-deploy --cluster cluster01 disk zap ceph03:/dev/sdb ceph03:/dev/sdc
```

journal 用の ssd のパーティションを切る。ここでは 10GB 毎に切った 
/dev/ssd1, /dev/ssd2 が存在することを前提に記す。ceph と同時にインストールされた
gdisk を用いる。

``` bash
% sudo gdisk /dev/ssd
```

(注意) 下記の公式ドキュメントでは osd prepare, osc activate の手順が掲載されて
いるがその後の osd create のコマンドにて prepare が実行されるようでこれら2つの
手順を行うと正常に osd create コマンドが実行できなかった。よってこのタイミング
にて osd create を行うものとする。

* <http://ceph.com/docs/master/rados/deployment/ceph-deploy-osd/#prepare-osds>
* <http://ceph.com/docs/dumpling/start/quick-ceph-deploy/>

2 つの disk に対してそれぞれ osd を稼働させる。

``` bash
% ceph-deploy --cluster cluster01 osd create ceph01:sdb:/dev/ssd1 ceph02:sdb:/dev/ssd1 ceph03:sdb:/dev/ssd1
% ceph-deploy --cluster cluster01 osd create ceph01:sdc:/dev/ssd2 ceph02:sdc:/dev/ssd2 ceph03:sdc:/dev/ssd2
```

mds の稼働を行う。ここでは1号機にのみ稼働を行う。

``` bash
% ceph-deploy --cluster cluster01 mds create ceph04
```


クライアントからのマウント方法各種
----

上記で構築した Ceph ストレージを利用する方法を3つ説明する。先に述べたように
POSIX 互換 filesystem として利用が可能。それぞれ mds が稼働しているホストに対
して接続を行う。

#### Block Device としてマウントする方法

ストレージ上に block device を生成しそれをマウントする

``` bash
cephclient% rbd create foo --size 4096
cephclient% sudo modprobe rbd
cephclient% sudo rbd map foo --pool rbd --name client.admin
cephclient% sudo mkfs.ext4 -m0 /dev/rbd/rbd/foo
cephclient% sudo mkdir /mnt/myrbd
cephclinet% sudo mount /dev/rbd/rbd/foo /mnt/myrbd
```

#### Kernel Driver を用いてマウントする方法

kernel Driver を用いてストレージをマウントする

``` bash
cephclient% sudo mkdir /mnt/mycephfs
cephclient% sudo mount -t ceph 10.200.10.26:6789:/ /mnt/mycephfs -o \
            name=admin,secret=`sudo ceph-authtool -p /etc/ceph/cluster01.client.admin.keyring`
```

#### Fuse Driver (ユーザランド) を用いてマウントする方法

ユーザランドソフトウェア FUSE を用いてマウントする方法

``` bash
cephclient% sudo mkdir /home/<username>/cephfs
cephclient% sudo ceph-fuse -m 10.200.10.26:6789 /home/<username>/cephfs
```

