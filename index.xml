<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ジェダイさんのブログ</title>
    <link>http://jedipunkz.github.io/</link>
    <description>Recent content on ジェダイさんのブログ</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 23 Jul 2016 02:40:11 +0900</lastBuildDate>
    <atom:link href="http://jedipunkz.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Go言語でInfluxDBにメトリクスデータを挿入</title>
      <link>http://jedipunkz.github.io/blog/2016/07/23/influxdb-go/</link>
      <pubDate>Sat, 23 Jul 2016 02:40:11 +0900</pubDate>
      
      <guid>http://jedipunkz.github.io/blog/2016/07/23/influxdb-go/</guid>
      <description>

&lt;p&gt;こんにちは。&lt;a href=&#34;https://twitter.com/jedipunkz&#34;&gt;@jedipunkz&lt;/a&gt; です。&lt;/p&gt;

&lt;p&gt;Ansible, Terraform, Chef などのソフトウェアを使ってインフラを定義するのが当たり前になった現在ですが、本当の意味でのソフトウェアによるインフラの定義ってなんだろと最近考えています。aws-sdk や fog などを使ったネイティブな言語でインフラを定義することの意味もあるように感じているからです。某サービスプロバイダのエンジニアはこうした言語によるインフラの定義の一番大きなメリットとして &amp;ldquo;再利用性&amp;rdquo; をあげていました。&lt;/p&gt;

&lt;p&gt;ってことで真の Infrastructure as a Code を実践するために幾つかのソフトウェアでインフラを構成したりってことを勉強していこうかなと思い始めています。&lt;/p&gt;

&lt;p&gt;今回は InfluxDB を Go 言語で操作する方法を紹介したいと思います。&lt;/p&gt;

&lt;h2 id=&#34;情報源&#34;&gt;情報源&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/influxdata/influxdb/tree/master/client&#34;&gt;https://github.com/influxdata/influxdb/tree/master/client&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;今回使う InfluxDB Client です。InfluxDB 自体が Go 言語で書かれていますがクライアントも Go 言語で記述することができます。ここにあるサンプルコードをすこしいじって、今回の記事を書こうと思います。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/shirou/gopsutil&#34;&gt;https://github.com/shirou/gopsutil&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;@shirou さんが作られた psutil の Go 言語版です。CPU, Mem などリソースをモニタするのに便利なので利用します。&lt;/p&gt;

&lt;h2 id=&#34;環境構築&#34;&gt;環境構築&lt;/h2&gt;

&lt;p&gt;環境を作っていきます。InfluxDB と Chronograf を構築するのですが Docker で構築するのが簡単なのでこれを利用します。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;InfluxDB の起動&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;InfluxDB のコンテナを起動します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker run -p 8083:8083 -p 8086:8086 \
      -v $PWD:/var/lib/influxdb \
      influxdb
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;Chronograf の起動&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Chronograf のコンテナを起動します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker run -p 10000:10000 chronograf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;この時点で http://${DOCKER_HOST}:10000/ にアクセスすると Chronograf の UI を確認できます。&lt;/p&gt;

&lt;h2 id=&#34;influxdb-にユーザ-データベースを作成する&#34;&gt;InfluxDB にユーザ・データベースを作成する&lt;/h2&gt;

&lt;p&gt;InfluxDB 上にユーザとデータベースを作成します。言語の中でも作ることが出来ますが、今回は手動で。
Mac OSX を使っている場合 homebrew で influxdb をインストールすることが簡単にできます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;brew install influxdb
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ユーザを作ります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;influx -host 192.168.99.100 -port 8086
&amp;gt; create user foo with password &#39;foo&#39;
&amp;gt; grant all privileges to foo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;データベースを作ります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;influx -host 192.168.99.100 -port 8086
&amp;gt; CREATE DATABASE IF NOT EXISTS square_holes;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;go言語でメモリー使用率を取得し-influxdb-にメトリクスデータを挿入&#34;&gt;Go言語でメモリー使用率を取得し InfluxDB にメトリクスデータを挿入&lt;/h2&gt;

&lt;p&gt;Go 言語でメモリー使用率を取得し得られたメトリクスデータを InfluxDB に挿入するコードを書きます。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-golang&#34;&gt;package main

import (
    &amp;quot;log&amp;quot;
    &amp;quot;time&amp;quot;

    &amp;quot;github.com/influxdata/influxdb/client/v2&amp;quot;
    &amp;quot;github.com/shirou/gopsutil/mem&amp;quot;
)

const (
    MyDB = &amp;quot;square_holes&amp;quot;
    username = &amp;quot;foo&amp;quot;
    password = &amp;quot;foo&amp;quot;
)

func main() {
    for {
        // Make client
        c, err := client.NewHTTPClient(client.HTTPConfig{
            Addr: &amp;quot;http://192.168.99.100:8086&amp;quot;,
            Username: username,
            Password: password,
        })
    
        if err != nil {
            log.Fatalln(&amp;quot;Error: &amp;quot;, err)
        }
    
        // Create a new point batch
        bp, err := client.NewBatchPoints(client.BatchPointsConfig{
            Database:  MyDB,
            Precision: &amp;quot;s&amp;quot;,
        })
    
        if err != nil {
            log.Fatalln(&amp;quot;Error: &amp;quot;, err)
        }
    
        // get mem
        v, _ := mem.VirtualMemory()
    
        // Create a point and add to batch
        tags := map[string]string{&amp;quot;mem&amp;quot;: &amp;quot;mem-total&amp;quot;}
        fields := map[string]interface{}{
            &amp;quot;total&amp;quot;:   v.Total,
            &amp;quot;free&amp;quot;:    v.Free,
        }
        pt, err := client.NewPoint(&amp;quot;memory&amp;quot;, tags, fields, time.Now())
    
        if err != nil {
            log.Fatalln(&amp;quot;Error: &amp;quot;, err)
        }
    
        bp.AddPoint(pt)
    
        // Write the batch
        c.Write(bp)
        time.Sleep(1 * time.Second)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ビルドして実行すると下記のように influxdb 上のデータベースにメトリクスデータが挿入されていることを確認できます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;influx -host 192.168.99.100 -port 8086 -execute &#39;SELECT * FROM memory&#39; -database=square_holes -precision=s | head -8
name: memory
------------
time            mem             free            total
1469207199      mem-total       260071424       8589934592
1469207201      mem-total       260616192       8589934592
1469207205      mem-total       260145152       8589934592
1469208496      mem-total       77627392        8589934592
1469208660      mem-total       213663744       8589934592
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Chronograf の UI で確認してみましょう。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jedipunkz.github.io/pix/influx-go.png&#34; width=&#34;70%&#34;&gt;&lt;/p&gt;

&lt;h2 id=&#34;まとめと考察&#34;&gt;まとめと考察&lt;/h2&gt;

&lt;p&gt;InfluxDB の提供元が出している Telegraf というメトリクスデータの送信エージェントがありますが、同じような動きを Go 言語で簡単に開発できることが分かりました。ネイティブな言語で開発するとより柔軟にデータの送信ができることも期待できます。各言語の aws-sdk, fog, などを使ってインフラを定義することでも同じ効果が得られることが期待できると思います。インフラの状態をメトリクスデータとして時系列DBに挿入して可視化するということは監視のコード化とも言えると思います。果たしてこれらインフラを言語で記述していくことがどれだけ有用なのかまだわかりませんが、いつか現場で実践してみたいと思います。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Test-Kitchen, Docker で Ansible Role 開発サイクル高速化！</title>
      <link>http://jedipunkz.github.io/blog/2016/07/14/test-kitchen-with-ansible/</link>
      <pubDate>Thu, 14 Jul 2016 09:10:57 +0900</pubDate>
      
      <guid>http://jedipunkz.github.io/blog/2016/07/14/test-kitchen-with-ansible/</guid>
      <description>

&lt;p&gt;こんにちは。&lt;a href=&#34;https://twitter.com/jedipunkz&#34;&gt;@jedipunkz&lt;/a&gt; です。&lt;/p&gt;

&lt;p&gt;私もインフラのプロビジョニングツールとして Chef ではなく Ansible を使うことが増えたのですが、Chef を使っていた頃に同じく利用していた test-kitchen が便利だったので ansible と併用できないかと思い試してみました。test-kitchen は Docker コンテナや EC2 等を起動して Chef, Ansible 等で構成をデプロイし serverspec 等のテストツールで構成をテストできるソフトウェアです。AWS EC2 でデプロイしてもいいのですが、EC2 を起動してデプロイして失敗したら削除してのサイクルを回すことを考えるとだいぶ面倒なので Docker + test-kitchen を使ってこのサイクルを高速に回す方がメリットが大きそうです。今回は Docker + test-kitchen を使って Ansible Role (Playbook) を開発するサイクルを高速化する方法を記したいと思います。&lt;/p&gt;

&lt;h2 id=&#34;ソフトウェアの構成&#34;&gt;ソフトウェアの構成&lt;/h2&gt;

&lt;p&gt;構成は、私の場合 Mac OSX を使っているので下記のとおりです。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;test-kitchen&lt;/li&gt;
&lt;li&gt;kitchen-ansible (test-kitchen ドライバ)&lt;/li&gt;
&lt;li&gt;kitchen-docker (test-kitchen ドライバ)&lt;/li&gt;
&lt;li&gt;serverspec&lt;/li&gt;
&lt;li&gt;ansible&lt;/li&gt;
&lt;li&gt;docker (Docker-machine)&lt;/li&gt;
&lt;li&gt;VirtualBox&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Linux でネイティブな Docker を使っている方は以降、読み替えて下さい。読み替えるのはそれほど難しくないと思います。&lt;/p&gt;

&lt;h2 id=&#34;ソフトウェアのインストール&#34;&gt;ソフトウェアのインストール&lt;/h2&gt;

&lt;p&gt;今回は上記ソフトウェアのインストール方法は省きます。test-kitchen, kitchen-ansible, kitchen-docker, serverspec は Ruby で開発されたソフトウェアなので Gemfile 等で管理、ansible は pip 等でインストールしてください。&lt;/p&gt;

&lt;h2 id=&#34;環境作成&#34;&gt;環境作成&lt;/h2&gt;

&lt;p&gt;test-kitchen が稼働するように環境を作っていきます。
作業ディレクトリで kitchen コマンドを使って初期設定を行います。今回は試しに nginx のデプロイを実施したいと思います。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ mkdir -p test-kitchen/nginx test-kitchen/roles
$ cd test-kitchen/nginx
$ kitchen init
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;また上記で作成した roles ディレクトリに ansible-galaxy で nginx の role を取得します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ansible-galaxy install geerlingguy.nginx -p ../roles/nginx
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下記の内容を .kitchen.local.yml として保存してください。
Docker ホストの指定、Provisioner として ansible の指定、Platform として &amp;lsquo;ubuntu:16.04&amp;rsquo; の Docker コンテナの指定を行っています。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;---
driver:
  name: docker
  binary: /usr/local/bin/docker
  socker: tcp://192.168.99.100:2376

provisioner:
  name: ansible_playbook
  playbook: ./site.yml
  roles_path: ../roles
  host_vars_path: ./host_vars
  hosts: kitchen-deploy
  require_ansible_omnibus: false
  ansible_platform: ubuntu
  require_chef_for_busser: true

platforms:
    - name: ubuntu
      driver_config:
        image: ubuntu:16.04
        platform: ubuntu
        require_chef_omnibus: false

suites:
  - name: default
    run_list:
    attributes:
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ここからは上記 .kitchen.local.yml ファイル内で指定したファイルの準備を行っていきます。&lt;/p&gt;

&lt;p&gt;site.yml ファイルの内容を下記のように書いてください。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;---
- hosts: kitchen-deploy
  sudo: yes
  roles:
    - { role: geerlingguy.nginx, tags: nginx }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;host_vars/hosts ファイルを作成します。&amp;rsquo;host_vars&amp;rsquo; ディレクトリは手動で作成してください。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;localhost              ansible_connection=local
[kitchen-deploy]
localhost
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;次に serverspec で行うテストの内容を作成します。
serverspec-init コマンドではインタラクティブに回答しますが、SSH ではなく EXEC(Local) を選択することに注意してください。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ mkdir -p test/integration/default/serverspec
$ cd test/integration/default/serverspec
$ serverspec-init             # &amp;lt;--- インタラクティブに回答 : 1) UNIX, 2) EXEC(Local) を選択
$ rm localhost/sample_spec.rb # &amp;lt;--- 必要ないので削除
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;test/integration/default/serverspec/localhost/nginx_spec.rb として下記の内容を試しに書いてみましょう。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;require &#39;spec_helper&#39;

describe package(&#39;nginx&#39;) do
    it { should be_installed }
end

describe service(&#39;nginx&#39;) do
    it { should be_enabled }
    it { should be_running }
end

describe file(&#39;/etc/nginx/nginx.conf&#39;) do
    it { should be_file }
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下記のようなファイルとディレクトリ構成になっていることを確認しましょう。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;.
├── nginx
│   ├── chefignore
│   ├── host_vars
│   │   └── hosts
│   ├── site.yml
│   └── test
│       └── integration
│           └── default
│               ├── Rakefile
│               └── serverspec
│                   ├── localhost
│                   │   └── nginx_spec.rb
│                   └── spec_helper.rb
└── roles
    └── geerlingguy.nginx
        ├── README.md
        &amp;lt;省略&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;デプロイ-テストを実行する&#34;&gt;デプロイ・テストを実行する&lt;/h2&gt;

&lt;p&gt;環境作成が完了したの Docker コンテナを起動し Ansible でデプロイ、その後 Serverspec でテストしてみます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cd test-kitchen
$ kitchen create  # &amp;lt;--- Docker コンテナ起動
$ kitchen setup   # &amp;lt;--- Ansible デプロイ
$ kitchen verify  # &amp;lt;--- Serverspec テスト
$ kitchen destroy # &amp;lt;--- コンテナ削除
$ kitchen test    # &amp;lt;--- 上の4つのコマンドを一気に実行
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;まとめ&#34;&gt;まとめ&lt;/h2&gt;

&lt;p&gt;Ansible でも test-kitchen を使ってデプロイ・テストが出来ることが分かりました。インスタンスを使ってデプロイ・テストを実施するよりコンテナを使うほうが失敗した際に削除・起動するのも一瞬で終わりますし Ansible 開発が高速化できることも実際に触っていただいてわかっていただけると思います。&lt;/p&gt;

&lt;p&gt;ただ上記の手順ではコンテナの中に Ruby, Chef も一緒にインストールされてしまいます。
test-kitchen 的には下記の記述を .kitchen.local.yml の provisioner: の欄に記述すると Chef のインストールは省けるはず (Ruby は Serverspec で用いる) のですが今現在 (2016/7中旬) では NG でした。これが正常に機能するようになるともっと高速にコンテナデプロイが完了すると思うので残念です。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;require_chef_for_busser: false
require_ruby_for_busser: true
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>イベントドリブンな StackStorm で運用自動化</title>
      <link>http://jedipunkz.github.io/blog/2016/07/02/stackstorm/</link>
      <pubDate>Sat, 02 Jul 2016 23:37:17 +0900</pubDate>
      
      <guid>http://jedipunkz.github.io/blog/2016/07/02/stackstorm/</guid>
      <description>

&lt;p&gt;こんにちは。&lt;a href=&#34;https://twitter.com/jedipunkz&#34;&gt;@jedipunkz&lt;/a&gt; です。&lt;/p&gt;

&lt;p&gt;今回は StackStorm (&lt;a href=&#34;https://stackstorm.com/&#34;&gt;https://stackstorm.com/&lt;/a&gt;) というイベントドリブンオートメーションツールを使ってみましたので
紹介したいと思います。&lt;/p&gt;

&lt;p&gt;クラウドとプロビジョニングツールの登場で昨今はエンジニアがほぼ全ての操作を自動化出来るようになりました。
ですが監視についてはどうでしょうか？監視システムを自動で構築することが出来ても障害発生時に対応を行う
のは手動になっていませんでしょうか。もちろんクラスタ組んでいれば大抵のアラートは放置出来ますが、クラスタ
を組むことが出来ないような箇所はクラウドを使ってもどうしても出てきます。&lt;/p&gt;

&lt;p&gt;そこで登場するのが今回紹介する StackStorm のようなツールかなぁと考えるようになりました。&lt;/p&gt;

&lt;h2 id=&#34;インストール&#34;&gt;インストール&lt;/h2&gt;

&lt;p&gt;インストール手順は下記の URL にあります。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://docs.stackstorm.com/install/index.html&#34;&gt;https://docs.stackstorm.com/install/index.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;自分は CentOS7 を使ったので下記のワンライナーでインストールできました。
password は任意のものを入れてください。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -sSL https://stackstorm.com/packages/install.sh | bash -s -- --user=st2admin --password=foo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;MongoDB, postgreSQL が依存してインストールされます。&lt;/p&gt;

&lt;p&gt;80番ポートで下記のような WEB UI も起動します。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jedipunkz.github.io/pix/stackstorm.png&#34; width=&#34;70%&#34;&gt;&lt;/p&gt;

&lt;h2 id=&#34;stackstorm-の基本&#34;&gt;StackStorm の基本&lt;/h2&gt;

&lt;p&gt;基本を知るために幾つかの要素について説明していきます。&lt;/p&gt;

&lt;p&gt;まず CLI で操作するために TOKEN を取得して環境変数にセットする必要があります。
上記で設定したユーザ名・パスワードを入力してください。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;export ST2_AUTH_TOKEN=`st2 auth -t -p foo st2admin`
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;Action&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Action はイベントが発生した際に実行できるアクションになります。早速アクションの一覧を取得してみましょう。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ st2 action list
+---------------------------------+---------+-------------------------------------------------------------+
| ref                             | pack    | description                                                 |
+---------------------------------+---------+-------------------------------------------------------------+
| chatops.format_execution_result | chatops | Format an execution result for chatops                      |
| chatops.post_message            | chatops | Post a message to stream for chatops                        |
| chatops.post_result             | chatops | Post an execution result to stream for chatops              |
&amp;lt;省略&amp;gt;
| core.http                       | core    | Action that performs an http request.                       |
| core.local                      | core    | Action that executes an arbitrary Linux command on the      |
|                                 |         | localhost.                                                  |
| core.local_sudo                 | core    | Action that executes an arbitrary Linux command on the      |
|                                 |         | localhost.                                                  |
| core.remote                     | core    | Action to execute arbitrary linux command remotely.         |
| core.remote_sudo                | core    | Action to execute arbitrary linux command remotely.         |
| core.sendmail                   | core    | This sends an email                                         |
| core.windows_cmd                | core    | Action to execute arbitrary Windows command remotely.       |
&amp;lt;省略&amp;gt;
| linux.cp                        | linux   | Copy file(s)                                                |
| linux.diag_loadavg              | linux   | Diagnostic workflow for high load alert                     |
| linux.dig                       | linux   | Dig action                                                  |
&amp;lt;省略&amp;gt;
| st2.kv.get                      | st2     | Get value from datastore                                    |
| st2.kv.set                      | st2     | Set value in datastore                                      |
+---------------------------------+---------+-------------------------------------------------------------+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上記のように Linux のコマンドや ChatOps, HTTP でクエリを投げるもの、Key Value の読み書きを行うモノまであります。
上記はかなり咲楽して貼り付けたので本来はもっと沢山のアクションがあります。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Trigger&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Trigger は Action を実行する際のトリガになります。同様に一覧を見てみましょう。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ st2 trigger list
+--------------------------------------+-------+----------------------------------------------------------------+
| ref                                  | pack  | description                                                    |
+--------------------------------------+-------+----------------------------------------------------------------+
| core.st2.generic.actiontrigger       | core  | Trigger encapsulating the completion of an action execution.   |
| core.st2.IntervalTimer               | core  | Triggers on specified intervals. e.g. every 30s, 1week etc.    |
| core.st2.generic.notifytrigger       | core  | Notification trigger.                                          |
| core.st2.DateTimer                   | core  | Triggers exactly once when the current time matches the        |
|                                      |       | specified time. e.g. timezone:UTC date:2014-12-31 23:59:59.    |
| core.st2.action.file_writen          | core  | Trigger encapsulating action file being written on disk.       |
| core.st2.CronTimer                   | core  | Triggers whenever current time matches the specified time      |
|                                      |       | constaints like a UNIX cron scheduler.                         |
| core.st2.key_value_pair.create       | core  | Trigger encapsulating datastore item creation.                 |
| core.st2.key_value_pair.update       | core  | Trigger encapsulating datastore set action.                    |
| core.st2.key_value_pair.value_change | core  | Trigger encapsulating a change of datastore item value.        |
| core.st2.key_value_pair.delete       | core  | Trigger encapsulating datastore item deletion.                 |
| core.st2.sensor.process_spawn        | core  | Trigger indicating sensor process is started up.               |
| core.st2.sensor.process_exit         | core  | Trigger indicating sensor process is stopped.                  |
| core.st2.webhook                     | core  | Trigger type for registering webhooks that can consume         |
|                                      |       | arbitrary payload.                                             |
| linux.file_watch.line                | linux | Trigger which indicates a new line has been detected           |
+--------------------------------------+-------+----------------------------------------------------------------+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;CronTimer はその名の通り Cron であることが分かります。IntervalTimer は同じように一定時間間隔で実行するようです。
その他 webhook や Key Value のペアが生成・削除・変更されたタイミング、等あります。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Rule&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Rule は Trigger が発生して Action を実行する際のルールを記述するものになります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ st2 rule list
+----------------+---------+-------------------------------------------------+---------+
| ref            | pack    | description                                     | enabled |
+----------------+---------+-------------------------------------------------+---------+
| chatops.notify | chatops | Notification rule to send results of action     | True    |
|                |         | executions to stream for chatops                |         |
+----------------+---------+-------------------------------------------------+---------+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;初期では上記の chatops.notify のみがあります。&lt;/p&gt;

&lt;h2 id=&#34;実際に使ってみる&#34;&gt;実際に使ってみる&lt;/h2&gt;

&lt;p&gt;core.local というアクションを実行してみました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ st2 run core.local -- uname -a
id: 5774c022e138230c66f2eefc
status: succeeded
parameters:
  cmd: uname -a
result:
  failed: false
  return_code: 0
  stderr: &#39;&#39;
  stdout: &#39;Linux localhost.localdomain 3.10.0-327.el7.x86_64 #1 SMP Thu Nov 19 22:10:57 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux&#39;
  succeeded: true
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;stdout に実行結果が出力されています。また下記のように実行結果の一覧を得ることが出来ます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ st2 execution list
+----------------------------+---------------+--------------+-------------------------+-----------------------------+-------------------------------+
| id                         | action.ref    | context.user | status                  | start_timestamp             | end_timestamp                 |
+----------------------------+---------------+--------------+-------------------------+-----------------------------+-------------------------------+
|   5774bdbee138230c66f2eeef | core.local    | st2admin     | succeeded (0s elapsed)  | Thu, 30 Jun 2016 06:35:42   | Thu, 30 Jun 2016 06:35:42 UTC |
|                            |               |              |                         | UTC                         |                               |
+----------------------------+---------------+--------------+-------------------------+-----------------------------+-------------------------------+
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;応用した使い方&#34;&gt;応用した使い方&lt;/h2&gt;

&lt;p&gt;上記のように core.local, core.remote 等でホスト上のコマンドを実行できることが分かりました。
ここで応用した使い方をしてみます。と言いますか、上記の基本的な使い方だけでは StackStorm を
使うメリットが無いように思えます。&lt;/p&gt;

&lt;p&gt;下記のような Rule を作成します。ファイル名は st2_sample_rule_webhook_remote_command.yaml とします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;---
    name: &amp;quot;st2_sample_rule_webhook_remote_command&amp;quot;
    pack: &amp;quot;examples&amp;quot;
    description: &amp;quot;Sample rule of webhook.&amp;quot;
    enabled: true

    trigger:
        type: &amp;quot;core.st2.webhook&amp;quot;
        parameters:
            url: &amp;quot;restart&amp;quot;

    criteria:

    action:
        ref: &amp;quot;core.remote&amp;quot;
        parameters:
            hosts: &amp;quot;10.0.1.250&amp;quot;
            username: &amp;quot;thirai&amp;quot;
            private_key: &amp;quot;/root/.ssh/id_rsa&amp;quot;
            cmd: &amp;quot;sudo service cron restart&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;StackStorm の基本要素 Action, Criteria(Rule の基準), Trigger から成っていることが分かります。
Triger は webhoook です。url: &amp;ldquo;restart&amp;rdquo; となっているのは URL : https://&lt;stackstorm_ip_addr&gt;/api/v1/webhooks/restart という名前で
アクセスを受けるようになるという意味です。criteria は今回は無条件で action を実行したいので空行にします。
action では core.remote が選択されていて hosts: &amp;lsquo;10.0.1.250&amp;rsquo; に username で SSH してコマンドを実行しています。&lt;/p&gt;

&lt;p&gt;要するに https://&lt;stackstorm_ip_addr&gt;/api/v1/webhooks/restart というアドレスでリクエストを受けると
10.0.1.250 に &amp;lsquo;foo&amp;rsquo; というユーザで SSH してコマンドを実行する、というルールになっています。&lt;/p&gt;

&lt;p&gt;下記のコマンドで上記の yaml ファイルをルールに読み込みます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;st2 rule create st2_sample_rule_webhook_remote_command.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;実際にリクエストを投げてみましょう。Token は読み替えてください。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -k https://localhost/api/v1/webhooks/restart -d &#39;{}&#39; -H &#39;Content-Type: application/json&#39; -H &#39;X-Auth-Token: &amp;lt;Your_Token&amp;gt;&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;するとリモートホストで &amp;lsquo;cron&amp;rsquo; プロセスの再起動が掛かります。&lt;/p&gt;

&lt;h2 id=&#34;まとめと考察&#34;&gt;まとめと考察&lt;/h2&gt;

&lt;p&gt;StackStorm は紹介した以外にも沢山のアクションがあり応用が効きます。また監視ツールでアラートが発生した際に webhook 通知するようにして
障害対応を自動で行うといった操作も可能な事がわかりました。ChatOps でも応用が可能なことが分かります。従来、ChatOps ではリモートホストで
コマンドなどを実行しようとした場合には Hubot 等のプロセスが稼働しているホストもしくはそのホストから SSH 出来るホストで実行する必要がありましたが
StackStorm を介すことで実行結果の閲覧やルールに従った実行等が可能になります。&lt;/p&gt;

&lt;p&gt;自分はまだ少しのアクション・ルールを試用しただけなのですが、他に良い運用上の応用例がないか探してみようと思います。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Vagrant で Mesosphere DC/OS を構築</title>
      <link>http://jedipunkz.github.io/blog/2016/06/21/mesos-dcos-vagrant/</link>
      <pubDate>Tue, 21 Jun 2016 17:05:25 +0900</pubDate>
      
      <guid>http://jedipunkz.github.io/blog/2016/06/21/mesos-dcos-vagrant/</guid>
      <description>

&lt;p&gt;こんにちは。&lt;a href=&#34;https://twitter.com/jedipunkz&#34;&gt;@jedipunkz&lt;/a&gt; です。&lt;/p&gt;

&lt;p&gt;今回は DC/OS (&lt;a href=&#34;https://dcos.io/&#34;&gt;https://dcos.io/&lt;/a&gt;) を Vagrant を使って構築し評価してみようと思います。
DC/OS はその名の通りデータセンタ OS として利用されることを期待され開発された OS で内部で
Docker と Mesos が稼働しています。&lt;/p&gt;

&lt;p&gt;一昔前に Mesos のマルチノード構成は構築したことあったのですが、DC/OS はデプロイ方法が全く変わっていました。
はじめに想定する構成から説明していきます。&lt;/p&gt;

&lt;h2 id=&#34;想定する構成&#34;&gt;想定する構成&lt;/h2&gt;

&lt;p&gt;本来 DC/OS は public, private ネットワーク構成ですが利用するレポジトリではこのような構成が想定されていました。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;+----+ +----+ +----+ +------+
| m1 | | a1 | | p1 | | boot |
+----+ +----+ +----+ +------+
|      |      |      |
+------+------+------+--------- 192.168.65/24
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;各ノードは下記の通り動作します。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;m1 : Mesos マスタ, Marathon&lt;/li&gt;
&lt;li&gt;a1 : Mesos スレーブ(Private Agent)&lt;/li&gt;
&lt;li&gt;p1 : Mesos スレーブ(Public Agent)&lt;/li&gt;
&lt;li&gt;boot : DC/OS インストレーションノード&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;前提の環境&#34;&gt;前提の環境&lt;/h2&gt;

&lt;p&gt;Vagrant が動作するマシンであれば問題ないと思いますが私は下記の構成で利用しました。
比較的たくさんのマシンリソースを使うのでメモリ 8GB はほしいところです。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Mac OSX&lt;/li&gt;
&lt;li&gt;Vagrant&lt;/li&gt;
&lt;li&gt;Virtualbox&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;事前の準備&#34;&gt;事前の準備&lt;/h2&gt;

&lt;p&gt;事前の手順を記します。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;予め用意された dcos-vagrant を取得する&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ git clone https://github.com/dcos/dcos-vagrant
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;Mac OSX の hosts ファイルをダイナミック編集する Vagrant プラグインをインストール&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ vagrant plugin install vagrant-hostmanager
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;構築手順&#34;&gt;構築手順&lt;/h2&gt;

&lt;p&gt;それでは構築手順です。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;DC/OS が利用する config を環境変数に指定指定&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ export DCOS_CONFIG_PATH=etc/config-1.7.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;DC/OS をレポジトリのルートディレクトリに保存&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;DC/OS 1.7.0 (Early Access)(2016/06/21現在) をダウンロードしてレポジトリのルートディレクトリにインストール&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ cd dcos-vagrant
$ wget https://downloads.dcos.io/dcos/EarlyAccess/dcos_generate_config.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;VagrantConfig を作成&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&amp;lsquo;VagrantConfig.yaml.example&amp;rsquo; というファイルがレポジトリ内ルートディレクトリにあるのでこれを元に下記の通りファイルを生成。元のファイルのままだと比較的大きな CPU/Mem リソースが必要になるので環境に合わせてリソース量を指定。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;m1:
  ip: 192.168.65.90
  cpus: 1
  memory: 512
  type: master
a1:
  ip: 192.168.65.111
  cpus: 1
  memory: 1024
  memory-reserved: 512
  type: agent-private
p1:
  ip: 192.168.65.60
  cpus: 1
  memory: 1024
  memory-reserved: 512
  type: agent-public
  aliases:
  - spring.acme.org
  - oinker.acme.org
boot:
  ip: 192.168.65.50
  cpus: 1
  memory: 1024
  type: boot
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;デプロイを実施&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ vagrant up m1 a1 p1 boot
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;dc-os-の-ui&#34;&gt;DC/OS の UI&lt;/h2&gt;

&lt;p&gt;インストールが完了すると下記のアドレスで DC/OS の UI にアクセスできます。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://m1.dcos/&#34;&gt;http://m1.dcos/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jedipunkz.github.io/pix/dcos-mesos.png&#34; width=&#34;70%&#34;&gt;&lt;/p&gt;

&lt;h2 id=&#34;marathon-の-ui&#34;&gt;Marathon の UI&lt;/h2&gt;

&lt;p&gt;下記のアドレスで Marathon の UI にアクセスできます
Marathon は分散型の Linux Init 機構のようなものです。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://m1.dcos:8080/&#34;&gt;http://m1.dcos:8080/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jedipunkz.github.io/pix/dcos-marathon.png&#34; width=&#34;70%&#34;&gt;&lt;/p&gt;

&lt;h2 id=&#34;chronos-の-ui&#34;&gt;Chronos の UI&lt;/h2&gt;

&lt;p&gt;下記のアドレスで Chronos の UI にアクセスできる
Chronos は分散型のジョブスケジューラーであり cron のようなものです。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://a1.dcos:1370/&#34;&gt;http://a1.dcos:1370/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jedipunkz.github.io/pix/dcos-chronos.png&#34; width=&#34;70%&#34;&gt;&lt;/p&gt;

&lt;h2 id=&#34;marathon-で-redis-サーバを立ち上げる&#34;&gt;Marathon で redis サーバを立ち上げる&lt;/h2&gt;

&lt;p&gt;テストで redis サーバを立ち上げてみる。Mesos-Slave (今回の環境だと a1 ホスト) 上に Docker コンテナとして redis サーバが立ち上がることになる。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Marathon の UI にて &amp;ldquo;Create Application&amp;rdquo; を選択&lt;/li&gt;
&lt;li&gt;General タブの ID に任意の名前を入力&lt;/li&gt;
&lt;li&gt;General タブの Command 欄に &amp;lsquo;redis-server&amp;rsquo; を入力&lt;/li&gt;
&lt;li&gt;Docker Container タブの Image 欄に &amp;lsquo;redis&amp;rsquo; を入力&lt;/li&gt;
&lt;li&gt;&amp;lsquo;Create Application&amp;rsquo; を選択&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;結果、下記の通りアプリケーションが生成される&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jedipunkz.github.io/pix/dcos-marathon-redis.png&#34; width=&#34;70%&#34;&gt;&lt;/p&gt;

&lt;p&gt;※ 20160625 下記追記&lt;/p&gt;

&lt;h2 id=&#34;構成&#34;&gt;構成&lt;/h2&gt;

&lt;p&gt;ここからは Mesosphere DC/OS の内部構成を理解していきます。主となる mesos-master, mesos-slave の構成は下記の通り。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Mesos-Master Node 構成&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;+--------------+
| mesos-master |
+--------------+ +-----------+ +----------+ +-------------+ +-----------+ +-----------+
|   zookeeper  | | mesos-dns | | marathon | | adminRouter | | minuteman | | exhibitor |
+--------------+ +-----------+ +----------+ +-------------+ +-----------+ +-----------+
|                  mesos-master node                                                  |
+-------------------------------------------------------------------------------------+
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;* * * * * * * Mesos-Slave (Mesos-Agent) Node 構成&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;+-------------+ +---+---+---+---+
| m-executor  | | c | c | c | c |
+-------------+ +---+---+---+---+
| mesos-slave | | docker-daemon |
+-------------------------------+
|        mesos-slave node       |
+-------------------------------+
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;各プロセスの役割&#34;&gt;各プロセスの役割&lt;/h2&gt;

&lt;p&gt;上記の図の各要素を参考資料を元にまとめました。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;mesos-master&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;masos-slave node からの情報を受け取り mesos-slave へプロセスを分配する
役割。mesos-master は zookeeper によってリーダー選出により冗長構成が保
たれている。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;mesos-dns&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;mesos フレームワーク内での DNS 解決を行うプロセス。各 mesos-master ノー
ド上に稼働している。通常の DNS でいうコンテンツ DNS (Authoritative
DNS)になっており mesos-master からクラスタ情報を受け取って DNS レコー
ド登録、それを mesos-slave が DNS 参照する。mesos-slave が内部レコード
に無い DNS 名を解決しに来た際にはインターネット上の root DNS へ問い合
わせ実施。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;marathon&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;コンテナオーケストレーションを司り mesos-slave へ支持を出しコンテナを
稼働する役割。各 mesos-master 上で稼働し zookeeper 越しに mesos-master
のカレントリーダを探しだしリクエストを創出。他に下記の機能を持っている。
&amp;lsquo;HA&amp;rsquo;, &amp;lsquo;ロードバランス&amp;rsquo;, &amp;lsquo;アプリのヘルスチェック&amp;rsquo;, &amp;lsquo;Web UI&amp;rsquo;, &amp;lsquo;REST
API&amp;rsquo;, &amp;lsquo;Metrics&amp;rsquo;。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;adminRouter&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;実態は nginx。各サービスの認証と Proxy の役割を担っている。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;minuteman&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;L4 レイヤのロードバランサ。各 mesos-master ノードで稼働。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;zookeeper&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;mesos-master プロセスを冗長構成させるためのソフトウェア。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;exhibitor&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;zookeeper のコンフィギュレーションを実施。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;mesos-slave&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Task を実行する役割。内部で meosos-executor (上記 m-executor) を実行し
ている。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;m-executor (mesos-executor)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;mesos-slave ノード上でサービスのための TASK を稼働させる。&lt;/p&gt;

&lt;h2 id=&#34;起動シーケンス&#34;&gt;起動シーケンス&lt;/h2&gt;

&lt;p&gt;ここからは mesos-master, mesos-slave の起動シーケンスについて、まとめてきます。&lt;/p&gt;

&lt;p&gt;mesos-master&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;exhibitor が起動し zookeeper のコンフィギュレーションを実施し zookeeper を起動&lt;/li&gt;
&lt;li&gt;mesos-master が起動。自分自身をレジスト後、他の mesos-master ノードを探索&lt;/li&gt;
&lt;li&gt;mesos-dns が起動&lt;/li&gt;
&lt;li&gt;mesos-dns が leader.mesos にカレントリーダの mesos-master を登録&lt;/li&gt;
&lt;li&gt;marathon が起動し zookeeper 越しに mesos-master を探索。&lt;/li&gt;
&lt;li&gt;adminRouter が起動し各 UI (mesos, marathon, exhibitor) が閲覧可能に。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;mesos-slave&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;leader.mesos に ping を打って mesos-master のカレントリーダを見つけ出し mesos-slave 稼働。&lt;/li&gt;
&lt;li&gt;mesos-master に対して自分自身を &amp;lsquo;agent&amp;rsquo; として登録。&lt;/li&gt;
&lt;li&gt;mesos-master はその登録された IP アドレスを元に接続を試みる&lt;/li&gt;
&lt;li&gt;mesos-slave が TASK 実行可能な状態に&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;まとめと考察&#34;&gt;まとめと考察&lt;/h2&gt;

&lt;p&gt;一昔前の Mesos + Marathon + Chronos とはだいぶデプロイ方法が変わっていた。だが構成には大きな変化は見られない。
AWS のような public, private ネットワークが別れたプラットフォームでは mesos-slave (DC/OS 的には Agent とも呼ぶ)は public agent, private agent として別けて管理される模様。public agent は AWS の ELB 等で分散され各コンテナ上のアプリにクエリがインターネットからのリクエストに応える。private agent はプライベートネットワーク上に配置されて public agent からのリクエストにも応える。また、mesos-master 達は別途 admin なネットワークに配置するのが Mesosphare の推奨らしい。&lt;/p&gt;

&lt;p&gt;だがしかし public, private を別けずに DC/OS を構成することも可能なように思えた。下記のように p1 を削除して構成して物理・仮想ロードバランサでリクエストを private agent に送出することでも DC/OS は機能するので。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ vagrant up m1 a1 boot
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ちなみに a2, a3, &amp;hellip; と数を増やすことで private agent ノードを増やすことが可能。&lt;/p&gt;

&lt;p&gt;あとマニュアルインストール手順(公式)を実施してみて解ったが、pulic, private ネットワークを各ノードにアタッチして mesos-master, mesos-slave, またその他の各機能はプライベートネットワークを、外部からのリクエストに応えるためのパブリックネットワーク、といった構成も可能でした。&lt;/p&gt;

&lt;h2 id=&#34;参考-url&#34;&gt;参考 URL&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;手順は右記のものを利用。
&lt;a href=&#34;https://github.com/dcos/dcos-vagrant/blob/master/docs/deploy.md&#34;&gt;https://github.com/dcos/dcos-vagrant/blob/master/docs/deploy.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.mesosphere.com/1.7/overview/architecture/&#34;&gt;https://docs.mesosphere.com/1.7/overview/architecture/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.mesosphere.com/1.7/overview/security/&#34;&gt;https://docs.mesosphere.com/1.7/overview/security/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>about</title>
      <link>http://jedipunkz.github.io/about/</link>
      <pubDate>Sun, 01 May 2016 15:00:27 +0900</pubDate>
      
      <guid>http://jedipunkz.github.io/about/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://jedipunkz.github.io/pix/jedipunkz_banner.png&#34; width=&#34;100%&#34;&gt;&lt;/p&gt;

&lt;p&gt;こんにちは。ジェダイさんこと @jedipunkz です。
オンラインゲームの中で &amp;ldquo;ジェダイさん&amp;rdquo; と呼ばれることが多くなったのでタイトルを &amp;ldquo;ジェダイさんのブログ&amp;rdquo; にしました。
インフラエンジニアからインフラ寄りのソフトウェアエンジニアになるため勉強していきます。これから勉強した内容を記事にできればいいなぁと思っています。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Chronograf, Telegraf, Influxdbでサーバとコンテナ情報を可視化する</title>
      <link>http://jedipunkz.github.io/blog/2015/12/28/chronograf-telegraf-influxdb/</link>
      <pubDate>Mon, 28 Dec 2015 00:00:00 +0000</pubDate>
      
      <guid>http://jedipunkz.github.io/blog/2015/12/28/chronograf-telegraf-influxdb/</guid>
      <description>

&lt;p&gt;こんにちは。&lt;a href=&#34;https://twitter.com/jedipunkz&#34;&gt;@jedipunkz&lt;/a&gt; です。&lt;/p&gt;

&lt;p&gt;Influxdb が Influxdata (&lt;a href=&#34;https://influxdata.com/&#34;&gt;https://influxdata.com/&lt;/a&gt;) として生まれ変わり公式の
メトリクス送信エージェント Telegraf と可視化ツール Chronograf をリリースしたので
使ってみました。&lt;/p&gt;

&lt;p&gt;3つのツールの役割は下記のとおりです。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Chronograf : 可視化ツール, Grafana 相当のソフトウェアです&lt;/li&gt;
&lt;li&gt;Telegraf : メトリクス情報を Influxdb に送信するエージェント&lt;/li&gt;
&lt;li&gt;Influxdb : メトリクス情報を格納する時系列データベース&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;以前に cAdvisor, influxdb, grafana を使って Docker コンテナのメトリクスを可視
化する記事を書きましたが telegraf を使うとサーバ情報と合わせて Docker コンテナ
のメトリクスも influxdb に送信することが出来ます。個人的にはそのコンテナ情報の
扱いもサーバ情報と同様に扱ってくれる点に期待しつつ、評価してみました。&lt;/p&gt;

&lt;h2 id=&#34;今回の環境&#34;&gt;今回の環境&lt;/h2&gt;

&lt;p&gt;今回は Ubuntu 15.04 vivid64 を使ってテストしています。&lt;/p&gt;

&lt;h2 id=&#34;influxdb-をインストールして起動&#34;&gt;influxdb をインストールして起動&lt;/h2&gt;

&lt;p&gt;最新リリース版の deb パッケージが用意されていたのでこれを使いました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;wget http://influxdb.s3.amazonaws.com/influxdb_0.9.5.1_amd64.deb
sudo dpkg -i influxdb_0.9.5.1_amd64.deb
sudo service influxdb start
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;telegraf-のインストールと起動&#34;&gt;telegraf のインストールと起動&lt;/h2&gt;

&lt;p&gt;こちらも deb パッケージで。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;wget http://get.influxdb.org/telegraf/telegraf_0.2.4_amd64.deb
sudo dpkg -i telegraf_0.2.4_amd64.deb
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;コンフィギュレーションですが今回は CPU, Disk, Net, Docker のメトリクス情報を送
信するようにしました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[agent]
    interval = &amp;quot;0.1s&amp;quot;

[outputs]

[outputs.influxdb]
    urls = [&amp;quot;http://localhost:8086&amp;quot;]
    database = &amp;quot;telegraf-test&amp;quot;
    user_agent = &amp;quot;telegraf&amp;quot;

[plugins]
[[plugins.cpu]]
  percpu = true
  totalcpu = false
  drop = [&amp;quot;cpu_time*&amp;quot;]

[[plugins.disk]]
  [plugins.disk.tagpass]
    fstype = [ &amp;quot;ext4&amp;quot;, &amp;quot;xfs&amp;quot; ]
    #path = [ &amp;quot;/home*&amp;quot; ]

[[plugins.disk]]
  pass = [ &amp;quot;disk_inodes*&amp;quot; ]
  
[[plugins.docker]]

[[plugins.net]]
  interfaces = [&amp;quot;eth0&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;他にも色々メトリクス情報を取得できそうです、下記のサイトを参考にしてみてください。
&lt;a href=&#34;https://github.com/influxdb/telegraf/blob/0.3.0/CONFIGURATION.md&#34;&gt;https://github.com/influxdb/telegraf/blob/0.3.0/CONFIGURATION.md&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;telegraf を起動します。Docker コンテナのメトリクスを取得するために root ユーザ
で起動する必要があります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo telegraf -config telegraf.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;chronograf-のインストールと起動&#34;&gt;chronograf のインストールと起動&lt;/h2&gt;

&lt;p&gt;こちらも deb でインストールします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;wget https://s3.amazonaws.com/get.influxdb.org/chronograf/chronograf_0.4.0_amd64.deb
sudo dpkg -i chronograf_0.4.0_amd64.deb
sudo /opt/chronograf/chronograf -sample-config &amp;gt; /opt/chronograf/config.toml
sudo service chronograf start
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;グラフの描画&#34;&gt;グラフの描画&lt;/h2&gt;

&lt;p&gt;この状態でブラウザでアクセスしてみましょう。&lt;/p&gt;

&lt;p&gt;http://&amp;lt;ホストのIPアドレス&amp;gt;:10000/&lt;/p&gt;

&lt;p&gt;アクセスすると簡単なガイドが走りますのでここでは設定方法は省きます。Grafana を使った場合と
同様に気をつけるポイントは下記のとおりです。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&amp;lsquo;filter by&amp;rsquo;  に描画したいリソース名を選択(CPU,Disk,Net,Dockerの各リソース)&lt;/li&gt;
&lt;li&gt;Database に telegraf.conf に記した &amp;lsquo;telegraf-test&amp;rsquo; を選択&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;すると下記のようなグラフやダッシュボードが作成されます。下記は CPU 使用率をグ
ラフ化したものです。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jedipunkz.github.io/pix/chronograf_cpu.png&#34; width=&#34;70%&#34;&gt;&lt;/p&gt;

&lt;p&gt;こちらは Docker 関連のグラフ。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jedipunkz.github.io/pix/chronograf_docker.png&#34; width=&#34;70%&#34;&gt;&lt;/p&gt;

&lt;p&gt;複数のグラフを1つのダッシュボードにまとめることもできるようです。&lt;/p&gt;

&lt;p&gt;まとめ
+++&lt;/p&gt;

&lt;p&gt;個人的には Grafana の UI はとてもわかりずらかったので公式の可視化ツールが出てきて良かった
と思っています。操作もとても理解しやすくなっています。Telegraf についても公式のメトリクス
情報送信エージェントということで安心感があります。また Grafana は別途 HTTP サー
バが必要でしたが Chronograf は HTTP サーバも内包しているのでセットアップが簡単
でした。&lt;/p&gt;

&lt;p&gt;ただ configuration guide がまだまだ説明不十分なので凝ったことをしようすとする
とソースを読まなくてはいけないかもしれません。&lt;/p&gt;

&lt;p&gt;いずれにしてもサーバのメトリクス情報と共に cAdvisor 等のソフトウェアを用いなく
てもサーバ上で稼働しているコンテナ周りの情報も取得できたので個人的にはハッピー。
cAdvisor でしか取得できない情報もありそうですが今後、導入を検討する上で評価し
ていきたいと思います。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Weave を使った Docker ネットワーク</title>
      <link>http://jedipunkz.github.io/blog/2015/12/22/weave-docker-network/</link>
      <pubDate>Tue, 22 Dec 2015 00:00:00 +0000</pubDate>
      
      <guid>http://jedipunkz.github.io/blog/2015/12/22/weave-docker-network/</guid>
      <description>

&lt;p&gt;こんにちは。&lt;a href=&#34;https://twitter.com/jedipunkz&#34;&gt;@jedipunkz&lt;/a&gt; です。&lt;/p&gt;

&lt;p&gt;今回は Weave というコンテナ間のネットワークを提供してくれる Docker のネットワークプラ
グインを使ってみました。下記のような沢山の機能があるようです。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Fast Data Path&lt;/li&gt;
&lt;li&gt;Docker Network Plugin&lt;/li&gt;
&lt;li&gt;Security&lt;/li&gt;
&lt;li&gt;Dynamic Netwrok Attachment&lt;/li&gt;
&lt;li&gt;Service Binding&lt;/li&gt;
&lt;li&gt;Fault Tolerance&lt;/li&gt;
&lt;li&gt;etc &amp;hellip;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;この記事では上から幾つか抜粋して、Weave ってどのように動かせるのか？を解説します。
そこから Weave が一体ナニモノなのか理解できればなぁと思います。&lt;/p&gt;

&lt;h2 id=&#34;vagrant-を使った構成&#34;&gt;Vagrant を使った構成&lt;/h2&gt;

&lt;p&gt;この記事では下記の構成を作って色々と試していきます。使う技術は&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Vagrant&lt;/li&gt;
&lt;li&gt;Docker&lt;/li&gt;
&lt;li&gt;Weave&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;です。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;+---------------------+ +---------------------+ +---------------------+
| docker container a1 | | docker container a2 | | docker container a3 |
+---------------------+ +---------------------+ +---------------------+
|    vagrant host 1   | |    vagrant host 2   | |    vagrant host 3   |
+---------------------+-+---------------------+-+---------------------+
|                          Mac or Windows                             |
+---------------------------------------------------------------------+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;特徴としては&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;作業端末(Mac or Windows or Linux)上で Vagrant を動作させる&lt;/li&gt;
&lt;li&gt;各 Vagrant VM 同士はホスト OS のネットワークインターフェース上で疎通が取れる&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;です。&lt;/p&gt;

&lt;h2 id=&#34;vagrantfile-の作成と-host1-2-3-の起動&#34;&gt;Vagrantfile の作成と host1,2,3 の起動&lt;/h2&gt;

&lt;p&gt;上記の3台の構成を下記の Vagrantfile で構築します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Vagrant.configure(2) do |config|
  config.vm.box = &amp;quot;ubuntu/vivid64&amp;quot;

  config.vm.define &amp;quot;host1&amp;quot; do |server|
    server.vm.network &amp;quot;private_network&amp;quot;, ip: &amp;quot;192.168.33.11&amp;quot;
  end

  config.vm.define &amp;quot;host2&amp;quot; do |server|
    server.vm.network &amp;quot;private_network&amp;quot;, ip: &amp;quot;192.168.33.12&amp;quot;
  end

  config.vm.define &amp;quot;host3&amp;quot; do |server|
    server.vm.network &amp;quot;private_network&amp;quot;, ip: &amp;quot;192.168.33.13&amp;quot;
  end

  config.vm.provision :shell, inline: &amp;lt;&amp;lt;-SHELL
apt-get update
apt-get install -y libsqlite3-dev docker.io
curl -L git.io/weave -o /usr/local/bin/weave
chmod a+x /usr/local/bin/weave
  SHELL
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;vagrant コマンドを使って host1, host2, host3 を起動します。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ vagrant up
$ vagrant ssh host1 # &amp;lt;--- host1 に SSH する場合
$ vagrant ssh host2 # &amp;lt;--- host2 に SSH する場合
$ vagrant ssh host3 # &amp;lt;--- host3 に SSH する場合
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;物理ノードまたがったコンテナ間で通信をする&#34;&gt;物理ノードまたがったコンテナ間で通信をする&lt;/h2&gt;

&lt;p&gt;weave でまず物理ノードをまたがったコンテナ間で通信をさせてみましょう。ここでは
上図の host1, host2 を使います。通常、物理ノードまたがっていると各々のホストで
稼働する Docker コンテナは通信し合えませんが weave を使うと通信しあうことが出
来ます。&lt;/p&gt;

&lt;p&gt;まず weave が用いる Docker コンテナを稼働します。下記のように /16 でレンジを切って
更にそこからデフォルトのレンジを指定することが出来ます。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;host1# weave launch --ipalloc-range 10.2.0.0/16 --ipalloc-default-subnet 10.2.1.0/24
host1# eval $(weave env)
host2# weave launch --ipalloc-range 10.2.0.0/16 --ipalloc-default-subnet 10.2.1.0/24 192.168.33.11
host2# eval $(weave env)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;この状態で下記のようなコンテナが稼働します。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;host1# docker ps
CONTAINER ID        IMAGE                        COMMAND                CREATED             STATUS              PORTS               NAMES
c55e96b4bdf9        weaveworks/weaveexec:1.4.0   &amp;quot;/home/weave/weavepr   4 seconds ago       Up 3 seconds                            weaveproxy
394382c9c5d9        weaveworks/weave:1.4.0       &amp;quot;/home/weave/weaver    5 seconds ago       Up 4 seconds                            weave
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;host1, host2 でそれぞれテスト用コンテナを稼働させます。名前を &amp;ndash;name オプションで付けるのを
忘れないようにしてください。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;host1# docker run --name a1 -ti ubuntu
host2# docker run --name a2 -ti ubuntu
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;どちらか一方から ping をもう一方に打ってみましょう。下記では a2 -&amp;gt; a1 の流れで
ping を実行しています。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;root@a2:/# ping 10.2.1.1 -c 3
PING 10.2.1.1 (10.2.1.1) 56(84) bytes of data.
64 bytes from 10.2.1.1: icmp_seq=1 ttl=64 time=0.316 ms
64 bytes from 10.2.1.1: icmp_seq=2 ttl=64 time=0.501 ms
64 bytes from 10.2.1.1: icmp_seq=3 ttl=64 time=0.619 ms

--- 10.2.1.1 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 1999ms
rtt min/avg/max/mdev = 0.316/0.478/0.619/0.127 ms
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;また docker コンテナを起動する時に指定した &amp;ndash;name a1, &amp;ndash;name a2 の名前で ping
を実行してみましょう。これも weave の機能の１つで dns lookup が行えます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root@b2:/# ping a1 -c 3
PING b1.weave.local (10.2.1.1) 56(84) bytes of data.
64 bytes from a1.weave.local (10.2.1.1): icmp_seq=1 ttl=64 time=1.14 ms
64 bytes from a1.weave.local (10.2.1.1): icmp_seq=2 ttl=64 time=0.446 ms
64 bytes from a1.weave.local (10.2.1.1): icmp_seq=3 ttl=64 time=0.364 ms

--- b1.weave.local ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2001ms
rtt min/avg/max/mdev = 0.364/0.653/1.149/0.352 ms
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;結果から、異なる物理ノード(今回は VM)上で動作させた Docker コンテナ同士が通信し合えた
ことがわかります。またコンテナ名の DNS 的は名前解決も可能になりました。&lt;/p&gt;

&lt;h2 id=&#34;ダイナミックにネットワークをアタッチ-デタッチする&#34;&gt;ダイナミックにネットワークをアタッチ・デタッチする&lt;/h2&gt;

&lt;p&gt;次に weave のネットワークを動的(コンテナがオンラインのまま)にアタッチ・デタッ
チすることが出来るので試してみます。&lt;/p&gt;

&lt;p&gt;最初に weave のネットワークに属さない a1-1 という名前のコンテナを作ります。
docker exec で IP アドレスを確認すると eth0, lo のインターフェースしか持っていない
ことが判ります。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;host1# C=$(docker run --name a1-1 -e WEAVE_CIDR=none -dti ubuntu)
host1# docker exec -it a1-1 ip a # &amp;lt;--- docker コンテナ内でコマンド実行
1: lo: &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN group default
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
25: eth0: &amp;lt;BROADCAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state UP group default
    link/ether 02:42:ac:11:00:05 brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.5/16 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::42:acff:fe11:5/64 scope link
       valid_lft forever preferred_lft forever
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;では weave のネットワークを a1-1 コンテナにアタッチしてみましょう。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;host1# weave attach $C
10.2.1.1
host1# docker exec -it a1-1 ip a
1: lo: &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN group default
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
25: eth0: &amp;lt;BROADCAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state UP group default
    link/ether 02:42:ac:11:00:05 brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.5/16 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::42:acff:fe11:5/64 scope link
       valid_lft forever preferred_lft forever
27: ethwe: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1410 qdisc pfifo_fast state UP group default qlen 1000
    link/ether aa:15:06:51:6a:3b brd ff:ff:ff:ff:ff:ff
    inet 10.2.1.1/24 scope global ethwe
       valid_lft forever preferred_lft forever
    inet6 fe80::a815:6ff:fe51:6a3b/64 scope link
       valid_lft forever preferred_lft forever
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上記のようにインターフェース ethwe が付与され最初に指定したデフォルトのサブネッ
ト上の IP アドレスが付きました。&lt;/p&gt;

&lt;p&gt;次に weave ネットワークを複数アタッチしてみましょう。default, 10.2.2.0/24,
10.2.3.0/24 のネットワーク(サブネット)をアタッチします。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;host1# weave attach net:default net:10.2.2.0/24 net:10.2.3.0/24 $C
10.2.1.1 10.2.2.1 10.2.3.1
root@vagrant-ubuntu-vivid-64:~# docker exec -it b3 ip a
1: lo: &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN group default
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
25: eth0: &amp;lt;BROADCAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state UP group default
    link/ether 02:42:ac:11:00:05 brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.5/16 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::42:acff:fe11:5/64 scope link
       valid_lft forever preferred_lft forever
33: ethwe: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1410 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 9a:74:73:1b:24:a9 brd ff:ff:ff:ff:ff:ff
    inet 10.2.1.1/24 scope global ethwe
       valid_lft forever preferred_lft forever
    inet 10.2.2.1/24 scope global ethwe
       valid_lft forever preferred_lft forever
    inet 10.2.3.1/24 scope global ethwe
       valid_lft forever preferred_lft forever
    inet6 fe80::9874:73ff:fe1b:24a9/64 scope link
       valid_lft forever preferred_lft forever
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;結果、ethwe インターフェースに3つの IP アドレスが付与されました。
この様にダイナミックにコンテナに対して weave ネットワークをアタッチすることが出来ます。&lt;/p&gt;

&lt;h2 id=&#34;コンテナ外部から情報を取得する&#34;&gt;コンテナ外部から情報を取得する&lt;/h2&gt;

&lt;p&gt;下記のようにコンテナを起動しているホスト上 (Vagrant VM) からコンテナの情報を取
得する事もできます。シンプルですがオーケストレーション・自動化を行う上で重要な機能に
なりそうな予感がします。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;host1# weave expose
10.2.1.1
host1# weave dns-lookup a2
10.2.1.128
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;ダイナミックに物理ノードを追加し-weave-ネットワークへ&#34;&gt;ダイナミックに物理ノードを追加し weave ネットワークへ&lt;/h2&gt;

&lt;p&gt;物理ノード(今回の場合 vagrant vm)を追加し上記で作成した weave ネットワークへ参
加させることも可能です。なお、今回は上記の vagrant up の時点で追加分の vm (host3)
を既に稼働させています。&lt;/p&gt;

&lt;p&gt;host1 で新しい物理ノードを接続します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;host1# weave connect 192.168.33.12
host1# weave status targets
192.168.33.13
192.168.33.12
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;host3 で weave コンテナ・テストコンテナを起動します。
下記で指定している 192.168.33.11 は host1 の IP アドレスです。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;host3# weave launch --ipalloc-range 10.2.0.0/16 --ipalloc-default-subnet 10.2.1.0/24 192.168.33.11
host3# docker run --name a3 -ti ubuntu
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;host2 の a2 コンテナに ping を打ってみます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;roota3:/# ping a2 -c 3
PING a2.weave.local (10.2.1.128) 56(84) bytes of data.
64 bytes from a2.weave.local (10.2.1.128): icmp_seq=1 ttl=64 time=0.366 ms
64 bytes from a2.weave.local (10.2.1.128): icmp_seq=2 ttl=64 time=0.709 ms
64 bytes from a2.weave.local (10.2.1.128): icmp_seq=3 ttl=64 time=0.569 ms
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;host3 上の a3 コンテナが既存の weave ネットワークに参加し通信出来たことが確認
できました。&lt;/p&gt;

&lt;h2 id=&#34;まとめと考察&#34;&gt;まとめと考察&lt;/h2&gt;

&lt;p&gt;コンフィギュレーションらしきモノを記述することなく Docker コンテナ間の通信
が出来ました。これは自動化する際に優位になるでしょう。また今回紹介したのは
&amp;lsquo;weave net&amp;rsquo; と呼ばれるモノですが他にも &amp;lsquo;weave scope&amp;rsquo;, &amp;lsquo;weave run&amp;rsquo; といったモノ
があります。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://weave.works/product/&#34;&gt;http://weave.works/product/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;また Docker Swarm, Compose と組み合わせる構成も組めるようです。試してみたい方
がいましたら是非。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://weave.works/guides/weave-and-docker-platform/compose-scalable-swarm-cluster-with-weave.html&#34;&gt;http://weave.works/guides/weave-and-docker-platform/compose-scalable-swarm-cluster-with-weave.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;ですが結果、まだ weave をどう自分たちのサービスに組み込めるかは検討が付いてい
ません。&amp;rsquo;出来る&amp;rsquo; と &amp;lsquo;運用できる&amp;rsquo; が別物であることと、コンテナまわりのネットワー
ク機能全般に理解して選定する必要がありそうです。&lt;/p&gt;

&lt;p&gt;参考サイト
+++&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://weave.works/docs/&#34;&gt;http://weave.works/docs/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CodeDeploy, S3 を併用して CircleCI により VPC にデプロイ</title>
      <link>http://jedipunkz.github.io/blog/2015/11/15/circleci-codedeploy/</link>
      <pubDate>Sun, 15 Nov 2015 00:00:00 +0000</pubDate>
      
      <guid>http://jedipunkz.github.io/blog/2015/11/15/circleci-codedeploy/</guid>
      <description>

&lt;p&gt;こんにちは。&lt;a href=&#34;https://twitter.com/jedipunkz&#34;&gt;@jedipunkz&lt;/a&gt; です。&lt;/p&gt;

&lt;p&gt;最近、業務で CircleCI を扱っていて、だいぶ &amp;ldquo;出来ること・出来ないこと&amp;rdquo; や &amp;ldquo;出来ないこと
に対する回避方法&amp;rdquo; 等のノウハウが若干溜まってきたので共有したいなと思います。&lt;/p&gt;

&lt;h2 id=&#34;この記事の前提&#34;&gt;この記事の前提&amp;hellip;&lt;/h2&gt;

&lt;p&gt;ここでは CodeDeploy の設定方法や、CircleCIの設定方法等に関しては記述しませ
ん。あくまで、Tips 的な内容にしています。また運用する上で想定できる問題点と、
それの回避方法等&amp;hellip;についてまとめています。&lt;/p&gt;

&lt;h2 id=&#34;cirlceci-と併用するサービスについて&#34;&gt;CirlceCI と併用するサービスについて&lt;/h2&gt;

&lt;p&gt;CircleCI は Github と連携してレポジトリ内の制御ファイル circle.yml に従ってテ
スト・ビルド・デプロイを実施してくれる CI サービスです。ただ CircleCI は自分た
ちの管理しているシステム外にあるので、AWS VPC を用いていると VPC 内のプライベー
トインスタンスにデプロイするのが難しいです。プロキシ挟んで・・ってことは出来そ
うですがプロキシの運用もしたくない、AWS のインフラリソースに任せることが出来た
らインスタンス・インスタンス上のミドルウェアを運用しなくて済むので運用コストが
省けそう。ってことで&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;AWS S3 (&lt;a href=&#34;https://aws.amazon.com/jp/s3/&#34;&gt;https://aws.amazon.com/jp/s3/&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;AWS CodeDeploy (&lt;a href=&#34;https://aws.amazon.com/jp/codedeploy/&#34;&gt;https://aws.amazon.com/jp/codedeploy/&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;を併用することを考えました。&lt;/p&gt;

&lt;p&gt;S3 は皆さんご存知のオブジェクトストレージです。CircleCI 用のバケットを作って、
ビルドした結果を格納します。私の務めている会社ではプログラミング言語として
Scala を用いているので SBT というツールを使ってビルドします。その結果もバージョ
ニングしつつ S3 バケットに格納できれば、万が一問題が発生した時にバイナリ毎切り
戻すことが出来そうです。&lt;/p&gt;

&lt;p&gt;また CodeDeploy は EC2 インスタンス・またオンプレのインスタンスへコードのデプ
ロイが可能になるサービスです。東京リージョンでは &lt;sup&gt;2015&lt;/sup&gt;&amp;frasl;&lt;sub&gt;08&lt;/sub&gt; から利用が可能になり
ました。これの便利なところは CircleCI 等の CI サービスから簡単に叩けて、VPC 内
のインスタンスに対してもデプロイが可能なところです。&lt;/p&gt;

&lt;p&gt;Tips 的な情報として
+++&lt;/p&gt;

&lt;p&gt;circle.yml という CircleCI の制御ファイルがあります。Git レポジトリ内に格納することで
CircleCI の動作を制御することが出来ます。この記事では circle.yml の紹介をメインとしたい
と思います。&lt;/p&gt;

&lt;h2 id=&#34;git-push-からデプロイまでを自動で行う-circle-yml&#34;&gt;Git push からデプロイまでを自動で行う circle.yml&lt;/h2&gt;

&lt;p&gt;Github への push, merge をトリガーとしてデプロイまでの流れを自動で行う流れを組む場合の
circle.yml を紹介します。全体の流れとしては&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;レポジトリに git push, merge ことがトリガで処理が走る&lt;/li&gt;
&lt;li&gt;circle.yml を元にテスト・ビルド(場合によってはテストのみ) が走る&lt;/li&gt;
&lt;li&gt;S3 バケットにビルドされた結果が格納される&lt;/li&gt;
&lt;li&gt;CodeDeploy が実行され S3 バケット内のビルドされた成果物を対象のインスタンスにデプロイする&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;となります。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yml&#34;&gt;machine:
  environment:
    SBT_VERSION: 0.13.9
    SBT_OPTS: &amp;quot;-Xms512M -Xmx1536M -Xss1M -XX:+CMSClassUnloadingEnabled -XX:MaxPermSize=256M&amp;quot;
  services:
    - docker

dependencies:
  pre:
    - (事前に実行したいコマンド・スクリプトを記述)
  cache_directories:
    - &amp;quot;~/.sbt&amp;quot;

test:
  override:
    - sbt compile

deployment:
  production:
    branch: master
    codedeploy:
      codedeploy-sample:
        application_root: /
        region: ap-northeast-1
        revision_location:
          revision_type: S3
          s3_location:
            bucket: circleci-sample-bucket
            key_pattern: filename-{CIRCLE_BRANCH}-{CIRCLE_BUILD_NUM}.zip
        deployment_group: codedeploy-sample-group
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;それぞれのパラメータの意味&#34;&gt;それぞれのパラメータの意味&lt;/h4&gt;

&lt;p&gt;上記 circle.yml の重要なパラメータのみ説明していきます。
私が務めている会社は Scala を使っていると冒頭に説明しましたがテスト・ビルドに
SBT を使うのでこのような記述になっています。Ruby や Python でも同様に記述でき
ると思いますので読み替えてください。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;machine -&amp;gt; environment : 全体で適用できる環境変数を定義します&lt;/li&gt;
&lt;li&gt;dependencies -&amp;gt; pre : 事前に実行したいコマンド等を定義できます&lt;/li&gt;
&lt;li&gt;test -&amp;gt; overide : テストを実行するコマンドを書きます。&lt;/li&gt;
&lt;li&gt;deployment -&amp;gt; production -&amp;gt; branch : 適用するブランチ名と本番環境であることを記述します。&lt;/li&gt;
&lt;li&gt;&amp;lsquo;codedeploy-sample&amp;rsquo; : CodeDeploy 上にサンプルで作成した &amp;lsquo;Application&amp;rsquo; 名です&lt;/li&gt;
&lt;li&gt;s3_location -&amp;gt; bucket : ビルドした成果物を S3 へ格納する際のバケット名を記します&lt;/li&gt;
&lt;li&gt;s3_location -&amp;gt; key_pattern : S3 バケットに収めるファイル名指定です&lt;/li&gt;
&lt;li&gt;deployment_group : CodeDeploy で定義する &amp;lsquo;Deployment-Group&amp;rsquo; 名です&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;より詳細な説明を読みたい場合は下記の URL に描いてあります。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://circleci.com/docs/configuration&#34;&gt;https://circleci.com/docs/configuration&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;s3-のみににデプロイする例&#34;&gt;S3 のみににデプロイする例&lt;/h2&gt;

&lt;p&gt;上記の circle.yml ではビルドとデプロイを一気に処理するのですが、テスト・ビルドとデプロイを別けて
実行したい場面もありそうです。流れとしては&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;レポジトリに git push, merge ことがトリガで処理が走る&lt;/li&gt;
&lt;li&gt;circle.yml を元にテスト・ビルド(場合によってはテストのみ) が走る&lt;/li&gt;
&lt;li&gt;S3 バケットにビルドされた結果が格納される&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;です。S3 のバケットに格納されたアプリを CodeDeploy を使ってデプロイするのは CodeDeploy の
API を直接叩けば出来そうです。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://docs.aws.amazon.com/codedeploy/latest/APIReference/API_CreateDeployment.html&#34;&gt;http://docs.aws.amazon.com/codedeploy/latest/APIReference/API_CreateDeployment.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;このリファレンスにある &amp;ldquo;CreateDeployment&amp;rdquo; については後に例をあげます。&lt;/p&gt;

&lt;p&gt;ただ、同様のサービスとして TravisCI 等は S3 にのみデプロイを実施する仕組みが用意されているのですが
CircleCI にはこの機能はありませんでした。サポートに問い合わせもしたのですが、あまり良い回答ではありませんでした。&lt;/p&gt;

&lt;p&gt;よって、下記のように awscli をテストコンテナ起動の度にインストールして S3 にアクセスすれば
上記の流れが組めそうです。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yml&#34;&gt;machine:
  environment:
    SBT_VERSION: 0.13.9
    SBT_OPTS: &amp;quot;-Xms512M -Xmx1536M -Xss1M -XX:+CMSClassUnloadingEnabled -XX:MaxPermSize=256M&amp;quot;
  services:
    - docker

dependencies:
  pre:
    - sudo pip install awscli
  cache_directories:
    - &amp;quot;~/.sbt&amp;quot;

test:
  override:
    - sbt compile

deployment:
  master:
    branch: master
    commands:
      - zip -r sample-code-${CIRCLE_BRANCH}-${CIRCLE_BUILD_NUM}.zip .
      - aws s3 cp
        sample-code-${CIRCLE_PROJECT_REPONAME}-${CIRCLE_BRANCH}-${CIRCLE_BUILD_NUM}.zip s3://&amp;lt;バケット名&amp;gt;/&amp;lt;ディレクトリ&amp;gt;/ --region ap-northeast-1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;事前に awscli をインストールしているだけです。&lt;/p&gt;

&lt;p&gt;S3 バケットに格納された成果物を CodeDeploy を使って手動でデプロイするには下記
のコマンドで実施できます。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ aws deploy create-deployment \
  --application-name codedeploy-sample \
  --deployment-config-name CodeDeployDefault.OneAtATime \
  --deployment-group-name codedeploy-sample-group \
  --description &amp;quot;deploy test&amp;quot; \
  --s3-location bucket=&amp;lt;バケット名&amp;gt;,bundleType=zip,key=&amp;lt;ファイル名&amp;gt;
  {
    &amp;quot;deploymentId&amp;quot;: &amp;quot;d-2B4OAMT0B&amp;quot;
   }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;deploymentId は CodeDeploy 上の Application に紐付いた ID です。CodeDeploy の
API を叩くか AWS コンソールで確認可能です。&lt;/p&gt;

&lt;h4 id=&#34;circleci-の問題点とそれの回避方法&#34;&gt;CircleCI の問題点とそれの回避方法&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;production と staging&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;1つのブランチで管理できる circle.yml は1つです。このファイルの中で定義できる &amp;lsquo;本番用&amp;rsquo;, &amp;lsquo;開発用&amp;rsquo; の定義は
deployment -&amp;gt; production, staging の2種類になります。この2つで管理しきれない環境がある場合(例えば staging 以前の
development 環境がある) は、レポジトリのブランチを別けて circle.yml を管理する方法があると思います。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;複数のデプロイ先があるレポジトリの運用&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;同一のレポジトリ内で管理しているコードのデプロイ先が複数ある場合は CodeDeploy 上で1つの Application に対して複数の
Deployment-Group を作成することで対応できます。ただ、cirlce.yml で定義できるデプロイ先は deployment_group: の1つ(
厳密に言うと production, staging の2つ) になるので、こちらもブランチによる circle.yml の別管理で回避できそうです。&lt;/p&gt;

&lt;p&gt;こちらの問題については CircleCI 的にはおそらく「1つのレポジトリで管理するデプロイ先は1つに」というコンセプトなのかもしれません。&lt;/p&gt;

&lt;h4 id=&#34;aws-iam-ユーザにアタッチする-policy-作成&#34;&gt;AWS IAM ユーザにアタッチする Policy 作成&lt;/h4&gt;

&lt;p&gt;IAM ユーザを CircleCI に事前に設定しておくことで直接 AWS のリソースを操作出来るのですが、
そのユーザにアタッチしておくべき Policy について例をあげておきます。&lt;/p&gt;

&lt;p&gt;特定の S3 バケットにオブジェクト Put する Policy&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{
    &amp;quot;Version&amp;quot;: &amp;quot;2012-10-17&amp;quot;,
    &amp;quot;Statement&amp;quot;: [
        {
            &amp;quot;Sid&amp;quot;: &amp;quot;Stmt1444196633000&amp;quot;,
            &amp;quot;Effect&amp;quot;: &amp;quot;Allow&amp;quot;,
            &amp;quot;Action&amp;quot;: [
                &amp;quot;s3:PutObject&amp;quot;
            ],
            &amp;quot;Resource&amp;quot;: [
                &amp;quot;arn:aws:s3:::&amp;lt;S3 バケット名&amp;gt;/*&amp;quot;
            ]
        }
    ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;CodeDeploy の各 Action を実行する Policy&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{
    &amp;quot;Version&amp;quot;: &amp;quot;2012-10-17&amp;quot;,
    &amp;quot;Statement&amp;quot;: [
        {
            &amp;quot;Effect&amp;quot;: &amp;quot;Allow&amp;quot;,
            &amp;quot;Action&amp;quot;: [
                &amp;quot;codedeploy:RegisterApplicationRevision&amp;quot;,
                &amp;quot;codedeploy:GetApplicationRevision&amp;quot;
            ],
            &amp;quot;Resource&amp;quot;: [
                &amp;quot;*&amp;quot;
            ]
        },
        {
            &amp;quot;Effect&amp;quot;: &amp;quot;Allow&amp;quot;,
            &amp;quot;Action&amp;quot;: [
                &amp;quot;codedeploy:CreateDeployment&amp;quot;,
                &amp;quot;codedeploy:GetDeployment&amp;quot;
            ],
            &amp;quot;Resource&amp;quot;: [
                &amp;quot;*&amp;quot;
            ]
        },
        {
            &amp;quot;Effect&amp;quot;: &amp;quot;Allow&amp;quot;,
            &amp;quot;Action&amp;quot;: [
                &amp;quot;codedeploy:GetDeploymentConfig&amp;quot;
            ],
            &amp;quot;Resource&amp;quot;: [
                &amp;quot;*&amp;quot;
            ]
        }
    ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;まとめ&#34;&gt;まとめ&lt;/h2&gt;

&lt;p&gt;CodeDeploy, S3 を併用することで CircleCI を使っても VPC 内のプライベートインス
タンスにデプロイできることが判りました。もし EC2 インスタンスを使っている場合
は他の方法も取れることが判っています。circle.yml 内の pre: で指定出来るコマン
ド・スクリプトで EC2 インスタンスに紐付いているセキュリティグループに穴あけ処
理を記述すれば良さそうです。デプロイが終わったら穴を塞げばいいですね。この辺の
例については国内でもブログ記事にされている方がいらっしゃいますので参考にしてくだ
さい。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>cAdvisor/influxDB/GrafanaでDockerリソース監視</title>
      <link>http://jedipunkz.github.io/blog/2015/09/12/cadvisor-influxdb-grafana-docker/</link>
      <pubDate>Sat, 12 Sep 2015 00:00:00 +0000</pubDate>
      
      <guid>http://jedipunkz.github.io/blog/2015/09/12/cadvisor-influxdb-grafana-docker/</guid>
      <description>

&lt;p&gt;こんにちは。&lt;a href=&#34;https://twitter.com/jedipunkz&#34;&gt;@jedipunkz&lt;/a&gt; です。&lt;/p&gt;

&lt;p&gt;今回は Docker ネタです。Docker 導入するにしても監視はどうする？という話になる
と思うのですが、各 Monitoring as a Service を使うにしてもエージェント入れない
といけないしお金掛かることもあるし..で、調べていたら cAdvisor というキーワード
が出てきました。今回は cAdvisor を使ってコンテナの監視が出来ないか、について書
いていきたいと想います。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;cAdvisor とは ?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;cAdvisor は Kubernates で用いられているコンポーネントで単体でも利用可能とのこ
と。Google が開発しています。また Docker コンテナの監視においてこの cAdvisor
は一般化しつつあるようです。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/google/cadvisor&#34;&gt;https://github.com/google/cadvisor&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;収集したメトリクスの保存&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;cAdvisor 自体も Docker で起動して、同ホスト上に起動している Docker コンテナの
リソースをモニタリングしてくれます。そのメトリクスデータは幾つかの DB に保存出
来るのですが、そのうちの一つが influxDB です。influxDB は時系列データベースで
す。システムのメトリクスデータを収めるのにちょうどいいデータベースになります。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://influxdb.com/&#34;&gt;https://influxdb.com/&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;DB に収めたメトリクスの可視化&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;influxDB に収めたメトリクスデータを可視化するのが Grafana です。Grafana のデー
タソースは influxDB の他にも幾つかあり Elasticsearch, KairosDB, Graphite 等が
それです。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://grafana.org/&#34;&gt;http://grafana.org/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;では早速試してみましょう。&lt;/p&gt;

&lt;h2 id=&#34;前提の環境&#34;&gt;前提の環境&lt;/h2&gt;

&lt;p&gt;今回は Vagrant を使います。また Vagrant 上で上記の3つのソフトウェアを Docker
で稼働します。またどうせなので docker-compose を使って3つのコンテナを一斉に立
ち上げてみましょう。&lt;/p&gt;

&lt;h2 id=&#34;vagrantfile-の準備&#34;&gt;VagrantFile の準備&lt;/h2&gt;

&lt;p&gt;下記のような VagrantFile を作成します。各ソフトウェアはそれぞれ WebUI を持って
いて、そこに手元のコンピュータから接続するため forwarded_port しています。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# -*- mode: ruby -*-
# vi: set ft=ruby :

Vagrant.configure(2) do |config|
    config.vm.box = &amp;quot;ubuntu/trusty64&amp;quot;
    config.vm.network &amp;quot;forwarded_port&amp;quot;, guest: 8080, host: 8080
    config.vm.network &amp;quot;forwarded_port&amp;quot;, guest: 8083, host: 8083
    config.vm.network &amp;quot;forwarded_port&amp;quot;, guest: 3000, host: 3000
    config.vm.network &amp;quot;private_network&amp;quot;, ip: &amp;quot;192.168.33.10&amp;quot;
end
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;docker-コンテナの起動と-docker-compose-yml-の準備&#34;&gt;Docker コンテナの起動と docker-compose.yml の準備&lt;/h2&gt;

&lt;p&gt;Vagrant を起動し docker, docker-compose のインストールを行います。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ vagrant up
$ vagrant ssh
vagrant$ sudo apt-get update ; sudo apt-get -y install curl
vagrant$ curl -sSL https://get.docker.com/ | sh
vagrant$ sudo -i
vagrant# export VERSION_NUM=1.4.0
vagrant# curl -L https://github.com/docker/compose/releases/download/VERSION_NUM/docker-compose-`uname -s`-`uname -m` &amp;gt; /usr/local/bin/docker-compose
vagrant# chmod +x /usr/local/bin/docker-compose
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;次に docker-compose.yml を作成します。上記3つのソフトウェアが稼働するコンテナ
を起動するため下記のように記述しましょう。カレントディレクトリに作成します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;InfluxSrv:
    image: &amp;quot;tutum/influxdb:0.8.8&amp;quot;
    ports:
        - &amp;quot;8083:8083&amp;quot;
        - &amp;quot;8086:8086&amp;quot;
    expose:
        - &amp;quot;8090&amp;quot;
        - &amp;quot;8099&amp;quot;
    environment:
        - PRE_CREATE_DB=cadvisor
cadvisor:
    image: &amp;quot;google/cadvisor:0.16.0&amp;quot;
    volumes:
        - &amp;quot;/:/rootfs:ro&amp;quot;
        - &amp;quot;/var/run:/var/run:rw&amp;quot;
        - &amp;quot;/sys:/sys:ro&amp;quot;
        - &amp;quot;/var/lib/docker/:/var/lib/docker:ro&amp;quot;
    links:
        - &amp;quot;InfluxSrv:influxsrv&amp;quot;
    ports:
        - &amp;quot;8080:8080&amp;quot;
    command: &amp;quot;-storage_driver=influxdb -storage_driver_db=cadvisor -storage_driver_host=influxsrv:8086 -storage_driver_user=root -storage_driver_password=root -storage_driver_secure=False&amp;quot;
grafana:
    image: &amp;quot;grafana/grafana:2.1.3&amp;quot;
    ports:
        - &amp;quot;3000:3000&amp;quot;
    environment:
        - INFLUXDB_HOST=localhost
        - INFLUXDB_PORT=8086
        - INFLUXDB_NAME=cadvisor
        - INFLUXDB_USER=root
        - INFLUXDB_PASS=root
    links:
        - &amp;quot;InfluxSrv:influxsrv&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;コンテナの起動
+++&lt;/p&gt;

&lt;p&gt;docker コンテナを立ち上げます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vagrant$ docker-compose -d
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;influxdb-の-webui-に接続する&#34;&gt;influxDB の WebUI に接続する&lt;/h2&gt;

&lt;p&gt;それでは起動したコンテナのうち一つ influxDB の WebUI に接続していましょう。
上記の VagrantFile では IP アドレスを 192.168.33.10 と指定しました。&lt;/p&gt;

&lt;p&gt;URL : &lt;a href=&#34;http://192.168.33.10:8083&#34;&gt;http://192.168.33.10:8083&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;データベースに接続します。&lt;/p&gt;

&lt;p&gt;ユーザ名 : root
パスワード : root&lt;/p&gt;

&lt;p&gt;接続するとデータベース作成画面に飛びますので Database Datails 枠に &amp;ldquo;cadvisor&amp;rdquo;
と入力、その他の項目はデフォルトのままで &amp;ldquo;Create Database&amp;rdquo; をクリックします。&lt;/p&gt;

&lt;h2 id=&#34;cadvisor-の-webui-に接続する&#34;&gt;cAdvisor の WebUI に接続する&lt;/h2&gt;

&lt;p&gt;続いて cAdvisor の WebUI に接続してみましょう。&lt;/p&gt;

&lt;p&gt;URL : &lt;a href=&#34;http://192.168.33.10:8080&#34;&gt;http://192.168.33.10:8080&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;ここでは特に作業の必要はありません。コンテナの監視が行われグラフが描画されてい
ることを確認します。&lt;/p&gt;

&lt;h2 id=&#34;grafana-の-webui-に接続する&#34;&gt;Grafana の WebUI に接続する&lt;/h2&gt;

&lt;p&gt;最後に Grafana の WebUI です。&lt;/p&gt;

&lt;p&gt;URL : &lt;a href=&#34;http://192.168.33.10:3000&#34;&gt;http://192.168.33.10:3000&lt;/a&gt;
ユーザ名 : admin
パスワード : admin&lt;/p&gt;

&lt;p&gt;まずデータソースの設定を行います。左上のアイコンをクリックし &amp;ldquo;Data Sources&amp;rdquo; を
選択します。次に &amp;ldquo;Add New Data Source&amp;rdquo; ボタンをクリックします。&lt;/p&gt;

&lt;p&gt;下記の情報を入力しましょう。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Name : influxdb&lt;/li&gt;
&lt;li&gt;Type : influxDB 0.8.x&lt;/li&gt;
&lt;li&gt;Url  : &lt;a href=&#34;http://influxsrv:8086&#34;&gt;http://influxsrv:8086&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Access : proxy&lt;/li&gt;
&lt;li&gt;Basic Auth User admin&lt;/li&gt;
&lt;li&gt;Basic Auth Password admin&lt;/li&gt;
&lt;li&gt;Database : cadvisor&lt;/li&gt;
&lt;li&gt;User : root&lt;/li&gt;
&lt;li&gt;Password : root&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;さて最後にグラフを作成していきます。左メニューの &amp;ldquo;Dashboard&amp;rdquo; を選択し上部の
&amp;ldquo;Home&amp;rdquo; ボランを押し &amp;ldquo;+New&amp;rdquo; を押します。&lt;/p&gt;

&lt;p&gt;下記の画面を参考にし値に入力していきます。&lt;/p&gt;

&lt;p&gt;Metrics を選択しネットワークの受信転送量をグラフにしています。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;series : &amp;lsquo;stats&amp;rsquo;&lt;/li&gt;
&lt;li&gt;alias : RX Bytes&lt;/li&gt;
&lt;li&gt;select mean(rx_bytes)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;同じく送信転送量もグラフにします。Add Query を押すと追加できます。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;series : &amp;lsquo;stats&amp;rsquo;&lt;/li&gt;
&lt;li&gt;alias : TX Bytes&lt;/li&gt;
&lt;li&gt;select mean(tx_bytes)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;http://jedipunkz.github.io/pix/grafana_input_data.png&#34; width=&#34;80%&#34;&gt;&lt;/p&gt;

&lt;p&gt;時間が経過すると下記のようにグラフが描画されます。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jedipunkz.github.io/pix/grafana_graph.png&#34; width=&#34;80%&#34;&gt;&lt;/p&gt;

&lt;h2 id=&#34;まとめと考察&#34;&gt;まとめと考察&lt;/h2&gt;

&lt;p&gt;3つのソフトウェア共に開発が活発であり、cAdvisor は特に Docker コンテナの監視と
して一般化しつつあるよう。Kubernates の一部ということもありそう簡単には廃れな
いと想います。コンテナの中にエージェント等を入れることもなく、これで Docker コ
ンテナのリソース監視が出来そう。ただサービス監視は別途考えなくてはいけないなぁ
という印象です。また、今回 docker-compose に記した各コンテナのバージョンは
Docker Hub を確認すると別バージョンもあるので時期が経ってこのブログ記事をご覧
になった方は修正すると良いと想います。ただこの記事を書いている時点では
influxDB の 0.9.x 系では動作しませんでした。よって latest ではなくバージョン指
定で記してあります。&lt;/p&gt;

&lt;h2 id=&#34;参考にしたサイト&#34;&gt;参考にしたサイト&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://qiita.com/atskimura/items/4c4aaaaa554e2814e938&#34;&gt;http://qiita.com/atskimura/items/4c4aaaaa554e2814e938&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.brianchristner.io/how-to-setup-docker-monitoring/&#34;&gt;https://www.brianchristner.io/how-to-setup-docker-monitoring/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Knife-ZeroでOpenStack Kiloデプロイ(複数台編)</title>
      <link>http://jedipunkz.github.io/blog/2015/07/20/knife-zero-openstack-kilo/</link>
      <pubDate>Mon, 20 Jul 2015 00:00:00 +0000</pubDate>
      
      <guid>http://jedipunkz.github.io/blog/2015/07/20/knife-zero-openstack-kilo/</guid>
      <description>

&lt;p&gt;こんにちは。@jedipunkz です。&lt;/p&gt;

&lt;p&gt;前回 OpenStack Kilo のオールインワン構成を Chef-Zero を使ってデプロイする方法
を書きましたが、複数台構成についても調べたので結果をまとめていきます。&lt;/p&gt;

&lt;p&gt;使うのは openstack/openstack-chef-repo です。下記の URL にあります。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/openstack/openstack-chef-repo&#34;&gt;https://github.com/openstack/openstack-chef-repo&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;この中に Vagrant を使ったファイルが存在しますが、実機でのデプロイには全く役に
立ちません。自分で Environment ファイルを作成する必要があります。今回は前提の
構成を作って、それに合わせた Environment ファイルを記します。ほぼスタンダード
な構成にしましたので、自分の環境に合わせて修正するのも比較的簡単だと想います。
参考にしてください。&lt;/p&gt;

&lt;p&gt;今回は knife-zero を使ってデプロイします。Chef サーバが必要なく、knife-zero を
使うホスト上のオンメモリで Chef サーバが稼働するので準備がほとんど必要ありません。&lt;/p&gt;

&lt;p&gt;早速ですが、構成と準備・そしてデプロイ作業を記していきます。&lt;/p&gt;

&lt;h2 id=&#34;前提の構成&#34;&gt;前提の構成&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;   +------------+
   | GW Router  |
+--+------------+
|  |
|  +--------------+--------------+---------------------------- public network
|  | eth0         | eth0
|  +------------+ +------------+ +------------+ +------------+
|  | Controller | |  Network   | |  Compute   | | Knife-Zero | 
|  +------------+ +-------+----+ +------+-----+ +------------+
|  | eth1         | eth1  |      | eth1 |       | eth1 
+--+--------------+-------)------+------)-------+------------- api/management network
                          | eth2        | eth2
                          +-------------+--------------------- guest network
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;特徴としては&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;public, api/management, guest の3つのネットワークに接続された OpenStack ホスト&lt;/li&gt;
&lt;li&gt;Controller, Network, Compute の最小複数台構成&lt;/li&gt;
&lt;li&gt;knife-zero を実行する &amp;lsquo;Knife-Zero&amp;rsquo; ホスト&lt;/li&gt;
&lt;li&gt;Knife-zero ホストは api/management network のみに接続で可&lt;/li&gt;
&lt;li&gt;デプロイは api/management network を介して行う&lt;/li&gt;
&lt;li&gt;public, api/management network はインターネットへの疎通が必須&lt;/li&gt;
&lt;li&gt;OS は Ubuntu 14.04 amd64&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;とくに api/management network がインターネットへの疎通が必要なところに注意して
ください。デプロイは knife-zero ホストで実行しますが、各ノードへログインしデプ
ロイする際にインターネット上からパッケージの取得を試みます。&lt;/p&gt;

&lt;p&gt;また api/management network を2つに分離するのも一般的ですが、ここでは一本にま
とめています。&lt;/p&gt;

&lt;h2 id=&#34;ip-アドレス&#34;&gt;IP アドレス&lt;/h2&gt;

&lt;p&gt;IP アドレスは下記を前提にします。&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;interface&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;IP addr&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Controller eth0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;10.0.1.10&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Controller eth1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;10.0.2.10&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Network eth0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;10.0.1.11&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Network eth1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;10.0.2.11&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Network eth2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;10.0.3.11&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Compute eth1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;10.0.2.12&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Compute eth2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;10.0.3.12&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Knife-Zero eth1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;10.0.2.13&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;ネットワークインターフェース設定&#34;&gt;ネットワークインターフェース設定&lt;/h2&gt;

&lt;p&gt;それぞれのホストで下記のようにネットワークインターフェースを設定します。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Controller ホスト&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;eth0, 1 を使用します。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;auto eth0
iface eth0 inet static
    address 10.0.1.10
    netmask 255.255.255.0
    gateway 10.0.1.254
    dns-nameservers 8.8.8.8
    dns-search jedihub.com

auto eth1
iface eth1 inet static
    address 10.0.2.10
    netmask 255.255.255.0

auto eth2
iface eth2 inet manual
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;Network ホスト&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;eth0, 1, 2 全てを使用します。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;auto eth0
iface eth0 inet static
        up ifconfig $IFACE 0.0.0.0 up
        up ip link set $IFACE promisc on
        down ip link set $IFACE promisc off
        down ifconfig $IFACE down
        address 10.0.1.11
        netmask 255.255.255.0

auto eth1
iface eth1 inet static
        address 10.0.2.11
        netmask 255.255.255.0
        gateway 10.0.2.248
        dns-nameservers 8.8.8.8
        dns-search jedihub.com

auto eth2
iface eth2 inet static
        address 10.0.3.11
        netmask 255.255.255.0
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;Compute ホスト&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;eth1, 2 を使用します。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;auto eth0
iface eth0 inet manual

auto eth1
iface eth1 inet static
        address 10.0.2.12
        netmask 255.255.255.0
        gateway 10.0.2.248
        dns-nameservers 8.8.8.8
        dns-search jedihub.com

auto eth2
iface eth2 inet static
        address 10.0.3.12
        netmask 255.255.255.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;これらの作業は knife-zero からログインし eth1 を介して行ってください。でないと
接続が切断される可能性があります。&lt;/p&gt;

&lt;h2 id=&#34;準備&#34;&gt;準備&lt;/h2&gt;

&lt;p&gt;knife-zero ホストに chef, knife-zero, berkshelf が入っている必要があるので、こ
こでインストールしていきます。&lt;/p&gt;

&lt;p&gt;knife-zero ホストに chef をインストールします。Omnibus パッケージを使って手っ
取り早く環境を整えます。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo -i
curl -L https://www.opscode.com/chef/install.sh | bash
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Berkshelf をインストールするのに必要なソフトウェアをインストールします。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;apt-get -y install build-essential zlib1g-dev libssl-dev libreadline-dev ruby-dev libxml2-dev libxslt-dev g++
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Berkshelf をインストールします。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;/opt/chef/embedded/bin/gem install berkshelf --no-ri --no-rdoc
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最後に knife-zero をインストールします。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;/opt/chef/embedded/bin/gem install knife-zero --no-ri --no-rdoc
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;デプロイ作業&#34;&gt;デプロイ作業&lt;/h2&gt;

&lt;p&gt;それでは openstack-chef-repo を取得してデプロイの準備を行います。
ブランチの指定は行わず master ブランチを取得します。Kilo は master ブランチで
管理されています。次のバージョンの開発が始まるタイミングで &amp;lsquo;stable/kilo&amp;rsquo; ブラ
ンチに管理が移されます。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo -i
cd ~/
git clone https://github.com/openstack/openstack-chef-repo.git
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;次に Berkshelf を使って必要な Cookbooks をダウンロードします。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd ~/openstack-chef-repo
/opt/chef/embedded/bin/berks vendor ./cookbooks
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Environment を作成します。これは各環境に合わせた設定ファイルのようなもので、各
Cookbooks の Attributes を上書きする仕組みになっています。下記の内容を&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;openstack-chef-repo/environments/multi-neutron-kilo.json
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;というファイル名で保存してください。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{
  &amp;quot;name&amp;quot;: &amp;quot;multi-neutron-kilo&amp;quot;,
  &amp;quot;description&amp;quot;: &amp;quot;test&amp;quot;,
  &amp;quot;cookbook_versions&amp;quot;: {
  },
  &amp;quot;json_class&amp;quot;: &amp;quot;Chef::Environment&amp;quot;,
  &amp;quot;chef_type&amp;quot;: &amp;quot;environment&amp;quot;,
  &amp;quot;default_attributes&amp;quot;: {
  },
  &amp;quot;override_attributes&amp;quot;: {
    &amp;quot;mysql&amp;quot;: {
      &amp;quot;bind_address&amp;quot;: &amp;quot;0.0.0.0&amp;quot;,
      &amp;quot;server_root_password&amp;quot;: &amp;quot;mysqlroot&amp;quot;,
      &amp;quot;server_debian_password&amp;quot;: &amp;quot;mysqlroot&amp;quot;,
      &amp;quot;server_repl_password&amp;quot;: &amp;quot;mysqlroot&amp;quot;,
      &amp;quot;allow_remote_root&amp;quot;: true,
      &amp;quot;root_network_acl&amp;quot;: [&amp;quot;10.0.0.0/8&amp;quot;]
    },
    &amp;quot;rabbitmq&amp;quot;: {
      &amp;quot;address&amp;quot;: &amp;quot;0.0.0.0&amp;quot;,
      &amp;quot;port&amp;quot;: &amp;quot;5672&amp;quot;,
      &amp;quot;loopback_users&amp;quot;: []
    },
    &amp;quot;openstack&amp;quot;: {
      &amp;quot;auth&amp;quot;: {
        &amp;quot;validate_certs&amp;quot;: false
      },
      &amp;quot;dashboard&amp;quot;: {
        &amp;quot;session_backend&amp;quot;: &amp;quot;file&amp;quot;
      },
      &amp;quot;block-storage&amp;quot;: {
        &amp;quot;syslog&amp;quot;: {
          &amp;quot;use&amp;quot;: false
        },
        &amp;quot;api&amp;quot;: {
          &amp;quot;ratelimit&amp;quot;: &amp;quot;False&amp;quot;
        },
        &amp;quot;debug&amp;quot;: true,
        &amp;quot;image_api_chef_role&amp;quot;: &amp;quot;os-image&amp;quot;,
        &amp;quot;identity_service_chef_role&amp;quot;: &amp;quot;os-identity&amp;quot;,
        &amp;quot;rabbit_server_chef_role&amp;quot;: &amp;quot;os-ops-messaging&amp;quot;
      },
      &amp;quot;compute&amp;quot;: {
        &amp;quot;rabbit&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;
        },
        &amp;quot;novnc_proxy&amp;quot;: {
          &amp;quot;bind_interface&amp;quot;: &amp;quot;eth1&amp;quot;
        },
        &amp;quot;libvirt&amp;quot;: {
          &amp;quot;virt_type&amp;quot;: &amp;quot;qemu&amp;quot;,
          &amp;quot;bind_interface&amp;quot;: &amp;quot;eth1&amp;quot;
        },
        &amp;quot;novnc_proxy&amp;quot;: {
          &amp;quot;bind_interface&amp;quot;: &amp;quot;eth1&amp;quot;
        },
        &amp;quot;xvpvnc_proxy&amp;quot;: {
          &amp;quot;bind_interface&amp;quot;: &amp;quot;eth1&amp;quot;
        },
        &amp;quot;image_api_chef_role&amp;quot;: &amp;quot;os-image&amp;quot;,
        &amp;quot;identity_service_chef_role&amp;quot;: &amp;quot;os-identity&amp;quot;,
        &amp;quot;nova_setup_chef_role&amp;quot;: &amp;quot;os-compute-api&amp;quot;,
        &amp;quot;rabbit_server_chef_role&amp;quot;: &amp;quot;os-ops-messaging&amp;quot;,
        &amp;quot;network&amp;quot;: {
          &amp;quot;public_interface&amp;quot;: &amp;quot;eth1&amp;quot;,
          &amp;quot;service_type&amp;quot;: &amp;quot;neutron&amp;quot;
        }
      },
      &amp;quot;network&amp;quot;: {
        &amp;quot;debug&amp;quot;: &amp;quot;True&amp;quot;,
        &amp;quot;dhcp&amp;quot;: {
          &amp;quot;enable_isolated_metadata&amp;quot;: &amp;quot;True&amp;quot;
        },
        &amp;quot;metadata&amp;quot;: {
          &amp;quot;nova_metadata_ip&amp;quot;: &amp;quot;10.0.2.10&amp;quot;
        },
        &amp;quot;openvswitch&amp;quot;: {
          &amp;quot;tunnel_id_ranges&amp;quot;: &amp;quot;1:1000&amp;quot;,
          &amp;quot;enable_tunneling&amp;quot;: &amp;quot;True&amp;quot;,
          &amp;quot;tenant_network_type&amp;quot;: &amp;quot;gre&amp;quot;,
          &amp;quot;tunnel_types&amp;quot;: &amp;quot;gre&amp;quot;,
          &amp;quot;tunnel_type&amp;quot;: &amp;quot;gre&amp;quot;,
          &amp;quot;bridge_mappings&amp;quot;: &amp;quot;physnet1:br-eth2&amp;quot;,
          &amp;quot;bridge_mapping_interface&amp;quot;: &amp;quot;br-eth2:eth2&amp;quot;
        },
        &amp;quot;ml2&amp;quot;: {
          &amp;quot;tenant_network_types&amp;quot;: &amp;quot;gre&amp;quot;,
          &amp;quot;mechanism_drivers&amp;quot;: &amp;quot;openvswitch&amp;quot;,
          &amp;quot;tunnel_id_ranges&amp;quot;: &amp;quot;1:1000&amp;quot;,
          &amp;quot;enable_security_group&amp;quot;: &amp;quot;True&amp;quot;
        },
        &amp;quot;api&amp;quot;: {
          &amp;quot;bind_interface&amp;quot;: &amp;quot;eth1&amp;quot;
        },
        &amp;quot;l3&amp;quot;: {
          &amp;quot;external_network_bridge_interface&amp;quot;: &amp;quot;eth0&amp;quot;
        },
        &amp;quot;service_plugins&amp;quot;: [&amp;quot;neutron.services.l3_router.l3_router_plugin.L3RouterPlugin&amp;quot;]
      },
      &amp;quot;db&amp;quot;: {
        &amp;quot;bind_interface&amp;quot;: &amp;quot;eth1&amp;quot;,
        &amp;quot;compute&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;
        },
        &amp;quot;identity&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;
        },
        &amp;quot;image&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;
        },
        &amp;quot;network&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;
        },
        &amp;quot;volume&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;
        },
        &amp;quot;dashboard&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;
        },
        &amp;quot;telemetry&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;
        },
        &amp;quot;orchestration&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;
        }
      },
      &amp;quot;developer_mode&amp;quot;: true,
      &amp;quot;endpoints&amp;quot;: {
        &amp;quot;network-openvswitch&amp;quot;: {
          &amp;quot;bind_interface&amp;quot;: &amp;quot;eth1&amp;quot;
        },
        &amp;quot;compute-api-bind&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;8774&amp;quot;
        },
        &amp;quot;compute-api&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;8774&amp;quot;
        },
        &amp;quot;compute-ec2-admin-bind&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;8773&amp;quot;
        },
        &amp;quot;compute-ec2-admin&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;8773&amp;quot;
        },
       &amp;quot;compute-ec2-api-bind&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;8773&amp;quot;
        },
        &amp;quot;compute-ec2-api&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;8773&amp;quot;
        },
        &amp;quot;compute-xvpvnc&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;6081&amp;quot;
        },
        &amp;quot;compute-novnc-bind&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;6080&amp;quot;
        },
        &amp;quot;compute-novnc&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;6080&amp;quot;
        },
        &amp;quot;compute-vnc&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;0.0.0.0&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;6080&amp;quot;
        },
        &amp;quot;image-api&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;9292&amp;quot;
        },
        &amp;quot;image-api-bind&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;9292&amp;quot;
        },
        &amp;quot;image-registry&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;9191&amp;quot;
        },
        &amp;quot;image-registry-bind&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;9191&amp;quot;
        },
        &amp;quot;identity-api&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;5000&amp;quot;
        },
        &amp;quot;identity-bind&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;5000&amp;quot;
        },
        &amp;quot;identity-admin&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;35357&amp;quot;
        },
        &amp;quot;identity-internal&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;35357&amp;quot;
        },
        &amp;quot;volume-api-bind&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;8776&amp;quot;
        },
        &amp;quot;volume-api&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;8776&amp;quot;
        },
        &amp;quot;telemetry-api&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;8777&amp;quot;
        },
        &amp;quot;network-api-bind&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.11&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;9696&amp;quot;
        },
        &amp;quot;network-api&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.11,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;9696&amp;quot;
        },
        &amp;quot;block-storage-api-bind&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;8776&amp;quot;,
          &amp;quot;bind_interface&amp;quot;: &amp;quot;eth1&amp;quot;
        },
        &amp;quot;block-storage-api&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;8776&amp;quot;
        },
        &amp;quot;orchestration-api&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;8004&amp;quot;
        },
        &amp;quot;orchestration-api-cfn&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;8000&amp;quot;
        },
        &amp;quot;db&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;0.0.0.0&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;3306&amp;quot;
        },
        &amp;quot;bind-host&amp;quot;: &amp;quot;0.0.0.0&amp;quot;
      },
      &amp;quot;identity&amp;quot;: {
        &amp;quot;admin_user&amp;quot;: &amp;quot;admin&amp;quot;,
        &amp;quot;bind_interface&amp;quot;: &amp;quot;eth1&amp;quot;,
        &amp;quot;debug&amp;quot;: true
      },
      &amp;quot;image&amp;quot;: {
        &amp;quot;api&amp;quot;: {
          &amp;quot;bind_interface&amp;quot;: &amp;quot;eth1&amp;quot;
        },
        &amp;quot;debug&amp;quot;: true,
        &amp;quot;identity_service_chef_role&amp;quot;: &amp;quot;os-identity&amp;quot;,
        &amp;quot;rabbit_server_chef_role&amp;quot;: &amp;quot;os-ops-messaging&amp;quot;,
        &amp;quot;registry&amp;quot;: {
          &amp;quot;bind_interface&amp;quot;: &amp;quot;eth1&amp;quot;
        },
        &amp;quot;syslog&amp;quot;: {
          &amp;quot;use&amp;quot;: false
        }
      },
      &amp;quot;mq&amp;quot;: {
        &amp;quot;bind_interface&amp;quot;: &amp;quot;eth1&amp;quot;,
        &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;,
        &amp;quot;user&amp;quot;: &amp;quot;guest&amp;quot;,
        &amp;quot;vhost&amp;quot;: &amp;quot;/nova&amp;quot;,
        &amp;quot;network&amp;quot;: {
          &amp;quot;rabbit&amp;quot;: {
             &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;,
             &amp;quot;port&amp;quot;: &amp;quot;5672&amp;quot;
          }
        },
        &amp;quot;compute&amp;quot;: {
           &amp;quot;service_type&amp;quot;: &amp;quot;rabbitmq&amp;quot;,
          &amp;quot;rabbit&amp;quot;: {
            &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;,
            &amp;quot;port&amp;quot;: &amp;quot;5672&amp;quot;
          }
        },
        &amp;quot;block-storage&amp;quot;: {
          &amp;quot;service_type&amp;quot;: &amp;quot;rabbitmq&amp;quot;,
          &amp;quot;rabbit&amp;quot;: {
            &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;,
            &amp;quot;port&amp;quot;: &amp;quot;5672&amp;quot;
          }
        }
      }
    },
    &amp;quot;queue&amp;quot;: {
      &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;,
      &amp;quot;user&amp;quot;: &amp;quot;guest&amp;quot;,
      &amp;quot;vhost&amp;quot;: &amp;quot;/nova&amp;quot;
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上記ファイルでは virt_type : qemu に設定していますが、KVM リソースを利用出来る
環境であればここを削除してください。デフォルトの &amp;lsquo;kvm&amp;rsquo; が適用されます。また気
をつけることは IP アドレスとネットワークインターフェース名です。環境に合わせて
設定していきましょう。今回は前提構成に合わせて environemnt ファイルを作ってい
ます。&lt;/p&gt;

&lt;p&gt;次に openstack-chef-repo/.chef/encrypted_data_bag_secret というファイルが
knife-zero ホストにあるはずです。これをデプロイ対象の3ノードに事前に転送してお
く必要があります。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;scp openstack-chef-repo/.chef/encrypted_data_bag_secret 10.0.2.10:/tmp/
scp openstack-chef-repo/.chef/encrypted_data_bag_secret 10.0.2.11:/tmp/
scp openstack-chef-repo/.chef/encrypted_data_bag_secret 10.0.2.12:/tmp/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;対象ホストにて&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir /etc/chef
mv /tmp/encrypted_data_bag_secret /etc/chef/openstack_data_bag_secret
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ではいよいよデプロイです。&lt;/p&gt;

&lt;p&gt;Controller ホストへのデプロイ&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;knife zero bootstrap 10.0.2.10 -N kilo01 -r &#39;role[os-compute-single-controller-no-network]&#39; -E multi-neutron-kilo -x &amp;lt;USERNAME&amp;gt; --sudo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Network ホストへのデプロイ&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;knife zero bootstrap 10.0.2.11 -N kilo02 -r &#39;role[os-client]&#39;,&#39;role[os-network]&#39; -E multi-neutron-kilo -x &amp;lt;USERNAME&amp;gt; --sudo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Compute ノードへのデプロイ&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;knife zero bootstrap 10.0.2.12 -N kilo03 -r &#39;role[os-compute-worker]&#39; -E multi-neutron-kilo -x &amp;lt;USERNAME&amp;gt; --sudo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;これで完了です。admin/mypass というユーザ・パスワードでログインが可能です。&lt;/p&gt;

&lt;h2 id=&#34;まとめ&#34;&gt;まとめ&lt;/h2&gt;

&lt;p&gt;openstack-chef-repo を使って OpenStack Kilo の複数台構成をデプロイ出来ました。重要なのは Environment をどうやって作るか？ですが、
私は 作成 -&amp;gt; デプロイ -&amp;gt; 修正 -&amp;gt; デプロイ -&amp;gt;&amp;hellip;. を繰り返して作成しています。何度実行しても不具合は発生しない設計なクックブックに
なっていますので、このような作業が可能になります。また、「ここの設定を追加したい」という時は&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;該当の template を探す&lt;/li&gt;
&lt;li&gt;該当のパラメータを確認する&lt;/li&gt;
&lt;li&gt;recipe 内で template にどうパラメータを渡しているか確認する&lt;/li&gt;
&lt;li&gt;attribute なり、変数なりを修正するための方法を探す&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;と行います。比較的難しい作業になるのですが、自らの環境に合わせた Environment を作成するにはこれらの作業が必須となってきます。&lt;/p&gt;

&lt;p&gt;以上、複数台構成のデプロイ方法についてでした。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Chef-ZeroでOpenStack Kiloデプロイ(オールインワン編)</title>
      <link>http://jedipunkz.github.io/blog/2015/07/16/chef-zero-openstack-allinone/</link>
      <pubDate>Thu, 16 Jul 2015 00:00:00 +0000</pubDate>
      
      <guid>http://jedipunkz.github.io/blog/2015/07/16/chef-zero-openstack-allinone/</guid>
      <description>

&lt;p&gt;こんにちは。@jedipunkz です。&lt;/p&gt;

&lt;p&gt;久々に openstack-chef-repo を覗いてみたら &amp;lsquo;openstack/openstack-chef-repo&amp;rsquo; とし
て公開されていました。今まで stackforge 側で管理されていましたが &amp;lsquo;openstack&amp;rsquo;
の方に移動したようです。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/openstack/openstack-chef-repo&#34;&gt;https://github.com/openstack/openstack-chef-repo&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;結構安定してきているのかな？と想い、ちらっと試したのですが案の定、簡単に動作さ
せることが出来ました。&lt;/p&gt;

&lt;p&gt;今回はこのレポジトリを使ってオールインワン構成の OpenStack Kilo を作る方法をま
とめていきます。&lt;/p&gt;

&lt;h2 id=&#34;前提の構成&#34;&gt;前提の構成&lt;/h2&gt;

&lt;p&gt;このレポジトリは Vagrant で OpenStack を作るための環境一式が最初から用意されて
いますが、Vagrant では本番環境を作ることは出来ないため、Ubuntu ホストを前提と
した記述に差し替えて説明していきます。前提にする構成は下記のとおりです。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Uuntu Linux 14.04 x 1 台&lt;/li&gt;
&lt;li&gt;ネットワークインターフェース x 3 つ&lt;/li&gt;
&lt;li&gt;eth0 : External ネットワーク用&lt;/li&gt;
&lt;li&gt;eth1 : Internal (API, Manage) ネットワーク用&lt;/li&gt;
&lt;li&gt;eth2 : Guest ネットワーク用&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;特徴としては上記なのですが、eth2 に関してはオールインワンなので必ずしも必要と
いうわけではありません。複数台構成を考慮した設定になっています。&lt;/p&gt;

&lt;h2 id=&#34;前提のip-アドレス&#34;&gt;前提のIP アドレス&lt;/h2&gt;

&lt;p&gt;この記事では下記の IP アドレスを前提にします。お手持ちの環境の IP アドレスが違
い場合はそれに合わせて後に示す json ファイルを変更してください。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;10.0.1.10 (eth0) : external ネットワーク&lt;/li&gt;
&lt;li&gt;10.0.2.10 (eth1) : api/management ネットワーク&lt;/li&gt;
&lt;li&gt;10.0.3.10 (eth2) : Guest ネットワーク&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;事前の準備&#34;&gt;事前の準備&lt;/h2&gt;

&lt;p&gt;事前に対象ホスト (OpenStack ホスト) に chef, berkshelf をインストールします。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo -i
curl -L https://www.opscode.com/chef/install.sh | bash
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Berkshelf をインストールするのに必要なソフトウェアをインストールします。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;apt-get -y install build-essential zlib1g-dev libssl-dev libreadline-dev ruby-dev libxml2-dev libxslt-dev g++
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Berkshelf をインストールします。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;/opt/chef/embedded/bin/gem install berkshelf --no-ri --no-rdoc
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;デプロイ作業&#34;&gt;デプロイ作業&lt;/h2&gt;

&lt;p&gt;それでは openstack-chef-repo を取得してデプロイの準備を行います。
ブランチの指定は行わず master ブランチを取得します。Kilo は master ブランチで
管理されています。次のバージョンの開発が始まるタイミングで &amp;lsquo;stable/kilo&amp;rsquo; ブラ
ンチに管理が移されます。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo -i
cd ~/
git clone https://github.com/openstack/openstack-chef-repo.git
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;次に Berkshelf を使って必要な Cookbooks をダウンロードします。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd ~/openstack-chef-repo
/opt/chef/embedded/bin/berks vendor ./cookbooks
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Environment を作成します。これは各環境に合わせた設定ファイルのようなもので、各
Cookbooks の Attributes を上書きする仕組みになっています。下記の内容を&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;openstack-chef-repo/environments/aio-neutron-kilo.json
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;というファイル名で保存してください。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{
  &amp;quot;name&amp;quot;: &amp;quot;aio-neutron-kilo&amp;quot;,
  &amp;quot;description&amp;quot;: &amp;quot;test&amp;quot;,
  &amp;quot;cookbook_versions&amp;quot;: {
  },
  &amp;quot;json_class&amp;quot;: &amp;quot;Chef::Environment&amp;quot;,
  &amp;quot;chef_type&amp;quot;: &amp;quot;environment&amp;quot;,
  &amp;quot;default_attributes&amp;quot;: {
  },
  &amp;quot;override_attributes&amp;quot;: {
    &amp;quot;mysql&amp;quot;: {
      &amp;quot;bind_address&amp;quot;: &amp;quot;0.0.0.0&amp;quot;,
      &amp;quot;server_root_password&amp;quot;: &amp;quot;mysqlroot&amp;quot;,
      &amp;quot;server_debian_password&amp;quot;: &amp;quot;mysqlroot&amp;quot;,
      &amp;quot;server_repl_password&amp;quot;: &amp;quot;mysqlroot&amp;quot;,
      &amp;quot;allow_remote_root&amp;quot;: true,
      &amp;quot;root_network_acl&amp;quot;: [&amp;quot;10.0.0.0/8&amp;quot;]
    },
    &amp;quot;rabbitmq&amp;quot;: {
      &amp;quot;address&amp;quot;: &amp;quot;0.0.0.0&amp;quot;,
      &amp;quot;port&amp;quot;: &amp;quot;5672&amp;quot;,
      &amp;quot;loopback_users&amp;quot;: []
    },
    &amp;quot;openstack&amp;quot;: {
      &amp;quot;auth&amp;quot;: {
        &amp;quot;validate_certs&amp;quot;: false
      },
      &amp;quot;dashboard&amp;quot;: {
        &amp;quot;session_backend&amp;quot;: &amp;quot;file&amp;quot;
      },
      &amp;quot;block-storage&amp;quot;: {
        &amp;quot;syslog&amp;quot;: {
          &amp;quot;use&amp;quot;: false
        },
        &amp;quot;api&amp;quot;: {
          &amp;quot;ratelimit&amp;quot;: &amp;quot;False&amp;quot;
        },
        &amp;quot;debug&amp;quot;: true,
        &amp;quot;image_api_chef_role&amp;quot;: &amp;quot;os-image&amp;quot;,
        &amp;quot;identity_service_chef_role&amp;quot;: &amp;quot;os-identity&amp;quot;,
        &amp;quot;rabbit_server_chef_role&amp;quot;: &amp;quot;os-ops-messaging&amp;quot;
      },
      &amp;quot;compute&amp;quot;: {
        &amp;quot;rabbit&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;
        },
        &amp;quot;novnc_proxy&amp;quot;: {
          &amp;quot;bind_interface&amp;quot;: &amp;quot;eth1&amp;quot;
        },
        &amp;quot;libvirt&amp;quot;: {
          &amp;quot;virt_type&amp;quot;: &amp;quot;qemu&amp;quot;,
          &amp;quot;bind_interface&amp;quot;: &amp;quot;eth1&amp;quot;
        },
        &amp;quot;novnc_proxy&amp;quot;: {
          &amp;quot;bind_interface&amp;quot;: &amp;quot;eth1&amp;quot;
        },
        &amp;quot;xvpvnc_proxy&amp;quot;: {
          &amp;quot;bind_interface&amp;quot;: &amp;quot;eth1&amp;quot;
        },
        &amp;quot;image_api_chef_role&amp;quot;: &amp;quot;os-image&amp;quot;,
        &amp;quot;identity_service_chef_role&amp;quot;: &amp;quot;os-identity&amp;quot;,
        &amp;quot;nova_setup_chef_role&amp;quot;: &amp;quot;os-compute-api&amp;quot;,
        &amp;quot;rabbit_server_chef_role&amp;quot;: &amp;quot;os-ops-messaging&amp;quot;,
        &amp;quot;network&amp;quot;: {
          &amp;quot;public_interface&amp;quot;: &amp;quot;eth1&amp;quot;,
          &amp;quot;service_type&amp;quot;: &amp;quot;neutron&amp;quot;
        }
      },
      &amp;quot;network&amp;quot;: {
        &amp;quot;debug&amp;quot;: &amp;quot;True&amp;quot;,
        &amp;quot;dhcp&amp;quot;: {
          &amp;quot;enable_isolated_metadata&amp;quot;: &amp;quot;True&amp;quot;
        },
        &amp;quot;metadata&amp;quot;: {
          &amp;quot;nova_metadata_ip&amp;quot;: &amp;quot;10.0.2.10&amp;quot;
        },
        &amp;quot;openvswitch&amp;quot;: {
          &amp;quot;tunnel_id_ranges&amp;quot;: &amp;quot;1:1000&amp;quot;,
          &amp;quot;enable_tunneling&amp;quot;: &amp;quot;True&amp;quot;,
          &amp;quot;tenant_network_type&amp;quot;: &amp;quot;gre&amp;quot;,
          &amp;quot;local_ip_interface&amp;quot;: &amp;quot;eth2&amp;quot;
        },
        &amp;quot;api&amp;quot;: {
          &amp;quot;bind_interface&amp;quot;: &amp;quot;eth1&amp;quot;
        },
        &amp;quot;l3&amp;quot;: {
          &amp;quot;external_network_bridge_interface&amp;quot;: &amp;quot;eth0&amp;quot;
        },
        &amp;quot;service_plugins&amp;quot;: [&amp;quot;neutron.services.l3_router.l3_router_plugin.L3RouterPlugin&amp;quot;]
      },
      &amp;quot;db&amp;quot;: {
        &amp;quot;bind_interface&amp;quot;: &amp;quot;eth1&amp;quot;,
        &amp;quot;compute&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;
        },
        &amp;quot;identity&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;
        },
        &amp;quot;image&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;
        },
        &amp;quot;network&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;
        },
        &amp;quot;volume&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;
        },
        &amp;quot;dashboard&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;
        },
        &amp;quot;telemetry&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;
        },
        &amp;quot;orchestration&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;
        }
      },
      &amp;quot;developer_mode&amp;quot;: true,
      &amp;quot;endpoints&amp;quot;: {
        &amp;quot;compute-api-bind&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;8774&amp;quot;
        },
        &amp;quot;compute-api&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;8774&amp;quot;
        },
        &amp;quot;compute-ec2-admin-bind&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;8773&amp;quot;
        },
        &amp;quot;compute-ec2-admin&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;8773&amp;quot;
        },
       &amp;quot;compute-ec2-api-bind&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;8773&amp;quot;
        },
        &amp;quot;compute-ec2-api&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;8773&amp;quot;
        },
        &amp;quot;compute-xvpvnc&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;6081&amp;quot;
        },
        &amp;quot;compute-novnc-bind&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;6080&amp;quot;
        },
        &amp;quot;compute-novnc&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;6080&amp;quot;
        },
        &amp;quot;compute-vnc&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;0.0.0.0&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;6080&amp;quot;
        },
        &amp;quot;image-api&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;9292&amp;quot;
        },
        &amp;quot;image-api-bind&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;9292&amp;quot;
        },
        &amp;quot;image-registry&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;9191&amp;quot;
        },
        &amp;quot;image-registry-bind&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;9191&amp;quot;
        },
        &amp;quot;identity-api&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;5000&amp;quot;
        },
        &amp;quot;identity-bind&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;5000&amp;quot;
        },
        &amp;quot;identity-admin&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;35357&amp;quot;
        },
        &amp;quot;identity-internal&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;35357&amp;quot;
        },
        &amp;quot;volume-api-bind&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;8776&amp;quot;
        },
        &amp;quot;volume-api&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;8776&amp;quot;
        },
        &amp;quot;telemetry-api&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;8777&amp;quot;
        },
        &amp;quot;network-api-bind&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;9696&amp;quot;
        },
        &amp;quot;network-api&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;9696&amp;quot;
        },
        &amp;quot;orchestration-api&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;8004&amp;quot;
        },
        &amp;quot;orchestration-api-cfn&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;8000&amp;quot;
        },
        &amp;quot;db&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;0.0.0.0&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;3306&amp;quot;
        },
        &amp;quot;bind-host&amp;quot;: &amp;quot;0.0.0.0&amp;quot;
      },
      &amp;quot;identity&amp;quot;: {
        &amp;quot;admin_user&amp;quot;: &amp;quot;admin&amp;quot;,
        &amp;quot;bind_interface&amp;quot;: &amp;quot;eth1&amp;quot;,
        &amp;quot;debug&amp;quot;: true
      },
      &amp;quot;image&amp;quot;: {
        &amp;quot;api&amp;quot;: {
          &amp;quot;bind_interface&amp;quot;: &amp;quot;eth1&amp;quot;
        },
        &amp;quot;debug&amp;quot;: true,
        &amp;quot;identity_service_chef_role&amp;quot;: &amp;quot;os-identity&amp;quot;,
        &amp;quot;rabbit_server_chef_role&amp;quot;: &amp;quot;os-ops-messaging&amp;quot;,
        &amp;quot;registry&amp;quot;: {
          &amp;quot;bind_interface&amp;quot;: &amp;quot;eth1&amp;quot;
        },
        &amp;quot;syslog&amp;quot;: {
          &amp;quot;use&amp;quot;: false
        }
      },
      &amp;quot;mq&amp;quot;: {
        &amp;quot;bind_interface&amp;quot;: &amp;quot;eth1&amp;quot;,
        &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;,
        &amp;quot;user&amp;quot;: &amp;quot;guest&amp;quot;,
        &amp;quot;vhost&amp;quot;: &amp;quot;/nova&amp;quot;,
        &amp;quot;network&amp;quot;: {
          &amp;quot;rabbit&amp;quot;: {
             &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;,
             &amp;quot;port&amp;quot;: &amp;quot;5672&amp;quot;
          }
        },
        &amp;quot;compute&amp;quot;: {
           &amp;quot;service_type&amp;quot;: &amp;quot;rabbitmq&amp;quot;,
          &amp;quot;rabbit&amp;quot;: {
            &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;,
            &amp;quot;port&amp;quot;: &amp;quot;5672&amp;quot;
          }
        },
        &amp;quot;block-storage&amp;quot;: {
          &amp;quot;service_type&amp;quot;: &amp;quot;rabbitmq&amp;quot;,
          &amp;quot;rabbit&amp;quot;: {
            &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;,
            &amp;quot;port&amp;quot;: &amp;quot;5672&amp;quot;
          }
        }
      }
    },
    &amp;quot;queue&amp;quot;: {
      &amp;quot;host&amp;quot;: &amp;quot;10.0.2.10&amp;quot;,
      &amp;quot;user&amp;quot;: &amp;quot;guest&amp;quot;,
      &amp;quot;vhost&amp;quot;: &amp;quot;/nova&amp;quot;
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上記ファイルは KVM が使えない環境用に virt_type : qemu にしていますが、KVM が
利用できる環境をご利用であれば該当行を削除してください。デフォルト値の &amp;lsquo;kvm&amp;rsquo;
が入るはずです。&lt;/p&gt;

&lt;p&gt;次にデプロイ前に databag 関連の事前操作を行います。Vagrant 用に作成されたファ
イルを除くと&amp;hellip;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;machine &#39;controller&#39; do
  add_machine_options vagrant_config: controller_config
  role &#39;allinone-compute&#39;
  role &#39;os-image-upload&#39;
  chef_environment env
  file(&#39;/etc/chef/openstack_data_bag_secret&#39;,
       &amp;quot;#{File.dirname(__FILE__)}/.chef/encrypted_data_bag_secret&amp;quot;)
  converge true
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;となっていて /etc/chef/openstack_data_bag_secret というファイルを事前にコピー
する必要がありそうです。下記のように操作します。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cp .chef/encrypted_data_bag_secret /etc/chef/openstack_data_bag_secret
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;デプロイを実行します。&lt;/p&gt;

&lt;p&gt;この openstack-chef-repo には .chef ディレクトリが存在していてノード名が記され
ています。&amp;rsquo;nodienode&amp;rsquo; というノード名です。これを利用してそのままデプロイを実行
します。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;chef-client -z
knife node -z run_list add nodienode &#39;role[allinone-compute]&#39;
chef-client -z -E aio-neutron-kilo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上記の説明を行います。
１行目 chef-client -z で Chef-Zero サーバをメモリ上に起動し、2行目で自ノードへ
run_list を追加しています。最後、3行目でデプロイ実行、となります。&lt;/p&gt;

&lt;p&gt;数分待つと OpenStack Kilo が構成されているはずです。&lt;/p&gt;

&lt;h2 id=&#34;まとめ&#34;&gt;まとめ&lt;/h2&gt;

&lt;p&gt;Chef-Zero を用いることで Chef サーバを利用せずに楽に構築が行えました。ですが、
OpenStack の複数台構成となるとそれぞれのノードのパラメータを連携させる必要が出
てくるので Chef サーバを用いたほうが良さそうです。今度、時間を見つけて Kilo の
複数台構成についても調べておきます。&lt;/p&gt;

&lt;p&gt;また、master ブランチを使用していますので、まだ openstack-chef-repo 自体が流動
的な状態とも言えます。が launchpad で管理されている Bug リストを見ると、ステー
タス Critical, High の Bug が見つからなかったので Kilo に関しては、大きな問題
無く安定してきている感があります。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://bugs.launchpad.net/openstack-chef&#34;&gt;https://bugs.launchpad.net/openstack-chef&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>オブジェクトストレージ minio を使ってみる</title>
      <link>http://jedipunkz.github.io/blog/2015/06/25/minio/</link>
      <pubDate>Thu, 25 Jun 2015 00:00:00 +0000</pubDate>
      
      <guid>http://jedipunkz.github.io/blog/2015/06/25/minio/</guid>
      <description>

&lt;p&gt;こんにちは、@jedipunkz です。&lt;/p&gt;

&lt;p&gt;久々にブログ更新になりましたが、ウォーミングアップで minio というオブジェクト
ストレージを使ってみたメモを記事にしたいと想います。&lt;/p&gt;

&lt;p&gt;minio は Minimal Object Storage の名の通り、最小限の小さなオブジェクトストレー
ジになります。公式サイトは下記のとおりです。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://minio.io/&#34;&gt;http://minio.io/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Golang で記述されていて Apache License v2 の元に公開されています。&lt;/p&gt;

&lt;p&gt;最近、資金調達の話も挙がっていたので、これから一般的になってくるのかもしれません。&lt;/p&gt;

&lt;p&gt;早速ですが、minio を動かしてみます。&lt;/p&gt;

&lt;h2 id=&#34;minio-を起動する&#34;&gt;Minio を起動する&lt;/h2&gt;

&lt;p&gt;方法は mithub.com/minio/minio の README に書かれていますが、バイナリを持ってき
て実行権限を与えるだけのシンプルな手順になります。&lt;/p&gt;

&lt;p&gt;Linux でも Mac でも動作しますが、今回私は Mac 上で動作させました。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;% wget https://dl.minio.io:9000/updates/2015/Jun/darwin-amd64/minio
% chmod +x minio
% ./minio mode memory limit 512MB
Starting minio server on: http://127.0.0.1:9000
Starting minio server on: http://192.168.1.123:9000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;起動すると Listening Port と共に EndPoint の URL が表示されます。&lt;/p&gt;

&lt;p&gt;次に mc という minio client を使って動作確認します。&lt;/p&gt;

&lt;h2 id=&#34;mc-を使ってアクセスする&#34;&gt;Mc を使ってアクセスする&lt;/h2&gt;

&lt;p&gt;mc は下記の URL にあります。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/minio/mc&#34;&gt;https://github.com/minio/mc&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;こちらもダウンロードして実行権限を付与するのみです。mc は minio だけではなく、
Amazon S3 とも互換性がありアクセス出来ますが、せっかくなので上記で起動した
minio にアクセスします。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;% wget https://dl.minio.io:9000/updates/2015/Jun/darwin-amd64/mc
% chmod +x mc
% ./mc config generate
/mc ls  http://127.0.0.1:9000/bucket01
[2015-06-25 16:21:37 JST]     0B testfile
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上記では予め作っておいた bucket01 という名前のバケットの中身を表示しています。
作り方はこれから minio の Golang ライブラリである minio-go を使って作りました。
これから説明します。&lt;/p&gt;

&lt;p&gt;また ls コマンドの他にも Usage を確認すると幾つかのサブコマンドが見つかります。&lt;/p&gt;

&lt;h2 id=&#34;minio-の-golang-ライブラリ-minio-go-を使ってアクセスする&#34;&gt;Minio の Golang ライブラリ minio-go を使ってアクセスする&lt;/h2&gt;

&lt;p&gt;さて、せっかくのオブジェクトストレージも手作業でファイルやバケットのアクセスを
行うのはもったいないです。ソフトウェアを使って操作してす。&lt;/p&gt;

&lt;p&gt;minio のサンプルのコードを参考にして、下記のコードを作成してみました。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;package main

import (
    &amp;quot;log&amp;quot;
    &amp;quot;os&amp;quot;

    &amp;quot;github.com/minio/minio-go&amp;quot;
)

func main() {
    config := minio.Config{
        // AccessKeyID:     &amp;quot;YOUR-ACCESS-KEY-HERE&amp;quot;,
        // SecretAccessKey: &amp;quot;YOUR-PASSWORD-HERE&amp;quot;,
        Endpoint:        &amp;quot;http://127.0.0.1:9000&amp;quot;,
    }

    s3Client, err := minio.New(config)
    if err != nil {
        log.Fatalln(err)
    }

    err = s3Client.MakeBucket(&amp;quot;bucket01&amp;quot;, minio.BucketACL(&amp;quot;public-read-write&amp;quot;))
    if err != nil {
        log.Fatalln(err)
    }
    log.Println(&amp;quot;Success: I made a bucket.&amp;quot;)

    object, err := os.Open(&amp;quot;testfile&amp;quot;)
    if err != nil {
        log.Fatalln(err)
    }
    defer object.Close()
    objectInfo, err := object.Stat()
    if err != nil {
        object.Close()
        log.Fatalln(err)
    }

    err = s3Client.PutObject(&amp;quot;bucket01&amp;quot;, &amp;quot;testfile&amp;quot;, &amp;quot;application/octet-stream&amp;quot;, objectInfo.Size(), object)
    if err != nil {
        log.Fatalln(err)
    }

    for bucket := range s3Client.ListBuckets() {
        if bucket.Err != nil {
            log.Fatalln(bucket.Err)
        }
        log.Println(bucket.Stat)
    }

    for object := range s3Client.ListObjects(&amp;quot;bucket01&amp;quot;, &amp;quot;&amp;quot;, true) {
        if object.Err != nil {
            log.Fatalln(object.Err)
        }
        log.Println(object.Stat)
    }

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;簡単ですがコードの説明をします。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;11行目で config の上書きをします。先ほど起動した minio の EndPoint を記します。&lt;/li&gt;
&lt;li&gt;17行目で minio にセッションを張り接続を行っています。&lt;/li&gt;
&lt;li&gt;22行目で &amp;lsquo;bucket01&amp;rsquo; というバケットを生成しています。その際にACLも設定&lt;/li&gt;
&lt;li&gt;28行目から42行目で &amp;lsquo;testfile&amp;rsquo; というローカルファイルをストレージにPUTしています。&lt;/li&gt;
&lt;li&gt;44行目でバケット一覧を表示しています。&lt;/li&gt;
&lt;li&gt;51行目で上記で作成したバケットの中のオブジェクト一覧を表示しています。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;実行結果は下記のとおりです。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;2015/06/25 16:56:21 Success: I made a bucket.
2015/06/25 16:56:21 {bucket01 2015-06-25 07:56:21.155 +0000 UTC}
2015/06/25 16:56:21 {&amp;quot;d41d8cd98f00b204e9800998ecf8427e&amp;quot; testfile 2015-06-25
07:56:21.158 +0000 UTC 0 {minio minio} STANDARD}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;バケットの作成とオブジェクトの PUT が正常に行えたことをログから確認できます。&lt;/p&gt;

&lt;h2 id=&#34;まとめ&#34;&gt;まとめ&lt;/h2&gt;

&lt;p&gt;上記の通り、今現在出来ることは少ないですが冒頭にも記したとおり資金調達の話も挙
がってきていますので、これからどのような方向に向かうか楽しみでもあります。また
最初から Golang, Python 等のライブラリが用意されているところが今どきだなぁと想
いました。オブジェクトストレージを手作業で操作するケースは現場では殆ど無いと想
いますので、その辺は現在では当たり前になりつつあるかもしれません。ちなみに
Python のライブラリは下記の URL にあります。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/minio/minio-py&#34;&gt;https://github.com/minio/minio-py&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;以上です。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>VyOS で VXLAN を使ってみる</title>
      <link>http://jedipunkz.github.io/blog/2014/12/16/vyos-vxlan/</link>
      <pubDate>Tue, 16 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>http://jedipunkz.github.io/blog/2014/12/16/vyos-vxlan/</guid>
      <description>

&lt;p&gt;こんにちは。@jedipunkz です。&lt;/p&gt;

&lt;p&gt;VyOS に VXLAN が実装されたと聞いて少し触ってみました。この情報を知ったきっかけ
は @upaa さんの下記の資料です。&lt;/p&gt;

&lt;p&gt;参考資料 : &lt;a href=&#34;http://www.slideshare.net/upaa/vyos-users-meeting-2-vyosvxlan&#34;&gt;http://www.slideshare.net/upaa/vyos-users-meeting-2-vyosvxlan&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;VyOS は御存知の通り実体は Debian Gnu/Linux 系の OS でその上に OSS なミドル
ウェアが搭載されていて CLI でミドルウェアのコンフィギュレーション等が行えるモ
ノになっています。Linux で VXLAN といえば OVS を使ったモノがよく知られています
が VyOS の VXLAN 機能は Linux Kernel の実装を使っているようです。&lt;/p&gt;

&lt;h2 id=&#34;要件&#34;&gt;要件&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;トンネルを張るためのセグメントを用意&lt;/li&gt;
&lt;li&gt;VyOS 1.1.1 (現在最新ステーブルバージョン) が必要&lt;/li&gt;
&lt;li&gt;Ubuntu Server 14.04 LTS (同じく Linux VXLAN 搭載バージョン)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;構成&#34;&gt;構成&lt;/h2&gt;

&lt;p&gt;特徴&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;マネージメント用セグメント 10.0.1.0/24 を用意&lt;/li&gt;
&lt;li&gt;GRE と同じくトンネル終端が必要なのでそのためのセグメント 10.0.2.0/24 を用意&lt;/li&gt;
&lt;li&gt;各 eth1 は IP reachable である必要があるので予め IP アドレスの設定と疎通を確認&lt;/li&gt;
&lt;li&gt;VXLAN を喋れる Ubuntu 14.04 LTS x 1 台と VyOS 1.1.1 x 2 台で相互に疎通確認&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;+-------------+-------------+------------ Management 10.0.1.0/24
|10.0.0.254   |10.0.0.253   |10.0.0.1
|eth0         |eth0         |eth0
+----------+  +----------+  +----------+ 
|  vyos01  |  |  vyos02  |  |  ubuntu  |
+-+--------+  +----------+  +----------+ 
| |eth1       | |eth1       | |eth1
| |10.0.2.254 | |10.0.2.253 | |10.0.2.1
| +-----------)-+-----------)-+---------- Tunneling 10.0.2.0/24
|             |             |
+-------------+-------------+------------ VXLAN(eth1にlink) 10.0.1.0/24
10.0.1.254     10.0.1.253    10.0.1.1
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;設定を投入&#34;&gt;設定を投入&lt;/h2&gt;

&lt;p&gt;vyos01 の設定を行う。VXLAN の設定に必要なものは&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;VNI (VXLAN Network Ideintity)という識別子&lt;/li&gt;
&lt;li&gt;Multicast Group Address&lt;/li&gt;
&lt;li&gt;互いに IP reachable なトンネルを張るためのインターフェース&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;です。これらを意識して下記の設定を vyos01 に投入します。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ configure
% set interfaces vxlan vxlan0
% set interfaces vxlan vxlan0 group 239.1.1.1
% set interfaces vxlan vxlan0 vni 42
% set interfaces vxlan vxlan0 address &#39;10.0.1.254/24&#39;
% set interfaces vxlan vxlan0 link eth1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;設定を確認します&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;% exit
$ show int
...&amp;lt;省略&amp;gt;...
    vxlan vxlan0 {
     address 10.0.1.254/24
     group 239.1.1.1
     link eth1
     vni 42
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;VyOS の CLI を介さず直 Linux の設定を iproute2 で確認してみましょう。
VNI, Multicast Group Address と共に &amp;lsquo;link eth1&amp;rsquo; で設定したトンネルを終端するための物理 NIC が確認できます。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;vyos@vyos01# ip -d link show vxlan0
5: vxlan0: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1450 qdisc noqueue state UNKNOWN mode DEFAULT group default
    link/ether 86:24:26:b2:11:5c brd ff:ff:ff:ff:ff:ff promiscuity 0
    vxlan id 42 group 239.1.1.1 dev eth1 port 32768 61000 ttl 16 ageing 300
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;vyos02 の設定を同様に行います。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ congigure
% set interfaces vxlan vxlan0 address &#39;10.0.1.253/24&#39;
% set interfaces vxlan vxlan0 vni 42
% set interfaces vxlan vxlan0 group 239.1.1.1
% set interfaces vxlan vxlan0 link eth1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;設定の確認を行います。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;... 省略 ...
vxlan vxlan0 {
     address 10.0.1.254/24
     group 239.1.1.1
     link eth1
     vni 42
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同じく Linux の iproute2 で確認します。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;vyos@vyos01# ip -d link show vxlan0
5: vxlan0: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1450 qdisc noqueue state UNKNOWN mode DEFAULT group default
    link/ether 86:24:26:b2:11:5c brd ff:ff:ff:ff:ff:ff promiscuity 0
    vxlan id 42 group 239.1.1.1 dev eth1 port 32768 61000 ttl 16 ageing 300
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ubuntu ホストの設定を行っていきます。&lt;/p&gt;

&lt;p&gt;Ubuntu Server 14.04 LTS であればパッチを当てること無く Linux Kernel の VXLAN 機能を使うことができます。
設定内容は VyOS と同等です。VyOS がこの Linux の実装を使っているのがよく分かります。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo modprobe vxlan
sudo ip link add vxlan0 type vxlan id 42 group 239.1.1.1 dev eth1
sudo ip link set up vxlan0
sudo ip a add 10.0.1.1/24 dev vxlan0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同じく Linux iproute2 で確認を行います。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt; ip -d link show vxlan0
5: vxlan0: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1450 qdisc noqueue state UNKNOWN mode DEFAULT group default
    link/ether d6:ff:c1:27:69:a0 brd ff:ff:ff:ff:ff:ff promiscuity 0
    vxlan id 42 group 239.1.1.1 dev eth1 port 32768 61000 ageing 300
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;疎通確認&#34;&gt;疎通確認&lt;/h2&gt;

&lt;p&gt;疎通確認を行います。&lt;/p&gt;

&lt;p&gt;ubuntu -&amp;gt; vyos01 の疎通確認です。ICMP で疎通が取れることを確認できます。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;thirai@ubuntu:~$ ping 10.0.1.254 -c 3
PING 10.0.1.254 (10.0.1.254) 56(84) bytes of data.
64 bytes from 10.0.1.254: icmp_seq=1 ttl=64 time=0.272 ms
64 bytes from 10.0.1.254: icmp_seq=2 ttl=64 time=0.336 ms
64 bytes from 10.0.1.254: icmp_seq=3 ttl=64 time=0.490 ms

--- 10.0.1.254 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 1999ms
rtt min/avg/max/mdev = 0.272/0.366/0.490/0.091 ms
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;次に ubuntu -&amp;gt; vyos02 の疎通確認です。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;thirai@ubuntu:~$ ping 10.0.1.253 -c 3
PING 10.0.1.253 (10.0.1.253) 56(84) bytes of data.
64 bytes from 10.0.1.253: icmp_seq=1 ttl=64 time=0.272 ms
64 bytes from 10.0.1.253: icmp_seq=2 ttl=64 time=0.418 ms
64 bytes from 10.0.1.253: icmp_seq=3 ttl=64 time=0.451 ms

--- 10.0.1.253 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 1998ms
rtt min/avg/max/mdev = 0.272/0.380/0.451/0.079 ms
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;この時点で ubuntu ホストの fdb (forwarding db) の内容を確認します。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ bridge fdb show dev vxlan0
00:00:00:00:00:00 dst 239.1.1.1 via eth1 self permanent
4e:69:a4:a7:ef:1c dst 10.0.2.253 self
86:24:26:b2:11:5c dst 10.0.2.254 self
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;vyos01, vyos02 のトンネル終端 IP アドレスと Mac アドレスが確認できます。ubuntu ホストから見ると
送信先は vyos0[12] の VXLAN インターフェースではなく、あくまでもトンネル終端を行っているインターフェース
になることがわかります。&lt;/p&gt;

&lt;h2 id=&#34;まとめ&#34;&gt;まとめ&lt;/h2&gt;

&lt;p&gt;VyOS ver 1.1.0 には VXLAN を物理インターフェースに link する機能に不具合がありそうなので今ら ver 1.1.1 を使うしか
なさそう。とは言え、ver 1.1.1 なら普通に動作しました。&lt;/p&gt;

&lt;p&gt;VyOS は仮想ルータという位置付けなので今回紹介したようにインターフェースを VXLAN ネットワークに所属させる
機能があるのみです。VXLAN Trunk を行うような設定はありません。これはハイパーバイザ上で動作させることを前提
に設計されているので仕方ないです..というかスイッチで行うべき機能ですよね..。VM を接続して云々するには OVS
のようなソフトウェアスイッチを使えばできます。&lt;/p&gt;

&lt;p&gt;また fdb は時間が経つと情報が消えます。これは VXLAN のメッシュ構造なトンネルがその都度張られているのかどうか
気になるところです。ICMP の送信で一発目のみマルチキャストでその後ユニキャストになることを確認しましたが、その
一発目のマルチキャストでトンネリングがされるものなのでしょうか&amp;hellip;。あとで調べてみます。OVS のように CLI で
トンネルがどのように張られているか確認する手段があれば良いのですが。&lt;/p&gt;

&lt;p&gt;以上です。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Aviator でモダンに OpenStack を操作する</title>
      <link>http://jedipunkz.github.io/blog/2014/12/13/aviator-openstack/</link>
      <pubDate>Sat, 13 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>http://jedipunkz.github.io/blog/2014/12/13/aviator-openstack/</guid>
      <description>

&lt;p&gt;こんにちは。@jedipunkz です。&lt;/p&gt;

&lt;p&gt;自分は Ruby を普段使うのでいつも Fog というライブラリを使って OpenStack, AWS
を操作していました。Fog を使うとクラウドの操作が Ruby のネイティブコードで行え
るのでシステムコマンド打つよりミス無く済みます。&lt;/p&gt;

&lt;p&gt;Fog より後発で Aviator というライブラリが登場してきたので少し使ってみたのです
がまだ未完成なところがあるものの便利な点もあって今後に期待だったので紹介します。&lt;/p&gt;

&lt;h2 id=&#34;認証情報を-yaml-ファイルに記す&#34;&gt;認証情報を yaml ファイルに記す&lt;/h2&gt;

&lt;p&gt;接続に必要な認証情報を yaml ファイルで記述します。名前を &amp;lsquo;aviator.yml&amp;rsquo; として
保存。この時に下記のように環境毎に認証情報を別けて書くことができます。こうする
ことでコードの中で開発用・サービス用等と使い分けられます。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;production:
  provider: openstack
  auth_service:
    name: identity
    host_uri: &amp;lt;Auth URL&amp;gt;
    request: create_token
    validator: list_tenants
  auth_credentials:
    username: &amp;lt;User Name&amp;gt;
    password: &amp;lt;Password&amp;gt;
    tenant_name: &amp;lt;Tenant Name&amp;gt;

development:
  provider: openstack
  auth_service:
    name: identity
    host_uri: &amp;lt;Auth URL&amp;gt;
    request: create_token
    validator: list_tenants
  auth_credentials:
    username: &amp;lt;User Name&amp;gt;
    password: &amp;lt;Password&amp;gt;
    tenant_name: &amp;lt;Tenant Name&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;シンタックス確認
+++&lt;/p&gt;

&lt;p&gt;次に aviator のシンタックスを確認します。Fog に無い機能で、コマンドラインでシ
ンタックスを確認できてしかも指定可能はパラメータと必須なパラメータと共にサンプ
ルコードまで提供してくれます。公式サイトに&amp;rsquo;サーバ作成&amp;rsquo;のメソッドが掲載されてい
るので、ここでは仮想ディスクを作るシンタックスを確認してみます。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;% gem install aviator
% aviator describe openstack volume # &amp;lt;-- 利用可能な機能を確認
Available requests for openstack volume_service:
v1 public list_volume_types
v1 public list_volumes
v1 public delete_volume
v1 public create_volume
v1 public get_volume
v1 public update_volume
  v1 public root
% aviator describe openstack volume v1 public create_volume # &amp;lt;-- シンタックスを確認
:Request =&amp;gt; create_volume

Parameters:
 +---------------------+-----------+
 | NAME                | REQUIRED? |
 +---------------------+-----------+
 | availability_zone   |     N     |
 | display_description |     Y     |
 | display_name        |     Y     |
 | metadata            |     N     |
 | size                |     Y     |
 | snapshot_id         |     N     |
 | volume_type         |     N     |
 +---------------------+-----------+

Sample Code:
  session.volume_service.request(:create_volume) do |params|
    params.volume_type = value
    params.availability_zone = value
    params.snapshot_id = value
    params.metadata = value
    params.display_name = value
    params.display_description = value
    params.size = value
  end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;このように create_volume というメソッドが用意されていて、指定出来るパラメータ・
必須なパラメータが確認できます。必須なモノには &amp;ldquo;Y&amp;rdquo; が REQUIRED に付いています。
またサンプルコードが出力されるので、めちゃ便利です。&lt;/p&gt;

&lt;p&gt;では create_volume のシンタックスがわかったので、コードを書いてみましょう。&lt;/p&gt;

&lt;p&gt;コードを書いてみる
+++&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;#!/usr/bin/env ruby

require &#39;aviator&#39;
require &#39;json&#39;

volume_session = Aviator::Session.new(
              :config_file =&amp;gt; &#39;/home/thirai/aviator/aviator.yml&#39;,
              :environment =&amp;gt; :production,
              :log_file    =&amp;gt; &#39;/home/thirai/aviator/aviator.log&#39;
            )

volume_session.authenticate

volume_session.volume_service.request(:create_volume) do |params|
  params.display_description = &#39;testvol&#39;
  params.display_name = &#39;testvol01&#39;
  params.size = 1
end
puts volume_session.volume_service.request(:list_volumes).body
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;6行目で先ほど作成した認証情報ファイル aviator.yml とログ出力ファイル
aviator.log を指定します。12行目で実際に OpenStack にログインしています。&lt;/p&gt;

&lt;p&gt;14-18行目はサンプルコードそのままです。必須パラメータの display_description,
display_name, size のみを指定し仮想ディスクを作成しました。最後の puts &amp;hellip; は
実際に作成した仮想ディスク一覧を出力しています。&lt;/p&gt;

&lt;p&gt;結果は下記のとおりです。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{ volumes: [{ status: &#39;available&#39;, display_name: &#39;testvol01&#39;, attachments: [],
availability_zone: &#39;az3&#39;, bootable: &#39;false&#39;, created_at:
description = &#39;testvol&#39;, volume_type:
&#39;standard&#39;, snapshot_id: nil, source_volid: nil, metadata:  }, id:
&#39;3a5f616e-a732-4442-a419-10369111bd4c&#39;, size: 1 }] }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;まとめ
+++&lt;/p&gt;

&lt;p&gt;サンプルコードやパラメータ一覧等がひと目でわかる aviator はとても便利です。ま
だ利用できるクラウドプラットフォームが OpenStack しかないのと、Neutron の機能
がスッポリ抜けているので、まだ利用するには早いかもです&amp;hellip;。逆に言えばコントリ
ビューションするチャンスなので、もし気になった方がいたら開発に参加してみるのも
いいかもしれません。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Chef-Zero でお手軽に OpenStack Icehouse を作る</title>
      <link>http://jedipunkz.github.io/blog/2014/11/15/chef-zero-openstack-icehouse/</link>
      <pubDate>Sat, 15 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>http://jedipunkz.github.io/blog/2014/11/15/chef-zero-openstack-icehouse/</guid>
      <description>

&lt;p&gt;こんにちは。&lt;a href=&#34;https://twitter.com/jedipunkz&#34;&gt;@jedipunkz&lt;/a&gt; です。&lt;/p&gt;

&lt;p&gt;OpenStack Juno がリリースされましたが、今日は Icehouse ネタです。&lt;/p&gt;

&lt;p&gt;icehouse 以降、自分の中で OpenStack を自動で作る仕組みが無くなりつつあり、気軽
に OpenStack を作って色々試したい！ッていう時に手段が無く困っていました。例え
ば仕事でちょっと OpenStack 弄りたい！って時に DevStack, RDO しかなく。DevStack
は御存知の通り動かない可能性が結構あるし RDO は Ubuntu/Debian Gnu Linux ベース
じゃないし。&lt;/p&gt;

&lt;p&gt;ってことで、以前にも紹介した stackforge 管理の openstack-chef-repo と
Chef-Zero を使って OpenStack Icehouse (Neutron) のオールインワン構成を作る方法
を書きます。ちなみに最近 Chef-Solo が Chef-Zero に置き換わりつつあるらしいです。
Chef-Zero はオンメモリで Chef サーバを起動する仕組みです。Chef-Solo と違って Chef
サーバを扱う時と何も変更無く操作が出来るのでとても楽です。また、Chef サーバを
別途構、構築・管理しなくて良いので、気軽に OpenStack が作れます。&lt;/p&gt;

&lt;p&gt;ちなみに stackforge/openstack-chef-repo の README.md に Chef-Zero での構築方法
が書いてありますが、沢山の問題があります。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;nova-network 構成&lt;/li&gt;
&lt;li&gt;API の Endpoint が全て localhost に向いてしまうため外部から操作不可能&lt;/li&gt;
&lt;li&gt;各コンポーネントの bind_address が localhost を向いてしまう&lt;/li&gt;
&lt;li&gt;berkshelf がそのままでは入らない&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;よって、今回はこれらの問題を解決しつつ &amp;ldquo;オールインワンな Neutron 構成の
Icehouse OpenStack を作る方法&amp;rdquo; を書いていきます。&lt;/p&gt;

&lt;h2 id=&#34;構成&#34;&gt;構成&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;+----------------- 10.0.0.0/24 (api/management network)
|
+----------------+
| OpenStack Node |
|   Controller   |
|    Compute     |
+----------------+
|  |
+--(-------------- 10.0.1.0/24 (external network)
   |
   +-------------- 10.0.2.0/24 (guest vm network)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;IP address 達&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;10.0.0.10 (api/manageent network) : eth0&lt;/li&gt;
&lt;li&gt;10.0.1.10 (external network) : eth1&lt;/li&gt;
&lt;li&gt;10.0.2.10 (guest vm network) : eth2&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;注意 : 操作は全て eth0 経由で行う&lt;/p&gt;

&lt;h2 id=&#34;前提の環境&#34;&gt;前提の環境&lt;/h2&gt;

&lt;p&gt;stackforge/openstack-chef-repo の依存している Cookbooks の関係上、upstart 周り
がうまく制御できていないので Ubuntu Server 12.04.x を使います。&lt;/p&gt;

&lt;h2 id=&#34;インストール方法&#34;&gt;インストール方法&lt;/h2&gt;

&lt;p&gt;上記のように3つのネットワークインターフェースが付いたサーバを1台用意します。
KVM が利用出来たほうがいいですが使えないくても構いません。KVM リソースが使えな
い場合の修正方法を後に記します。&lt;/p&gt;

&lt;p&gt;サーバにログインし root ユーザになります。その後 Chef をオムニバスインストーラ
でインストールします。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;% sudo -i
# curl -L https://www.opscode.com/chef/install.sh | bash
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;次に stable/icehose ブランチを指定して openstack-chef-repo をクローンします。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# cd ~
# git clone -b stable/icehouse https://github.com/stackforge/openstack-chef-repo
# 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;berkshelf をインストールするのですが依存パッケージが足らないのでここでインストー
ルします。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# apt-get -y install build-essential zlib1g-dev libssl-dev libreadline-dev \
  ruby-dev libxml2-dev libxslt-dev g++
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;berkshelf をインストールします。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# /opt/chef/embedded/bin/gem install berkshelf --no-ri --no-rdoc
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;次に openstack-chef-repo に依存する Cookbooks を取得します。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# cd ~/openstack-chef-repo
# /opt/chef/embedded/bin/berks vendor ./cookbooks
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;~/openstack-chef-repo/environments ディレクトリ配下に neutron-allinone.json と
いうファイル名で作成します。内容は下記の通りです。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{                                                                                                                                                      [0/215]
  &amp;quot;name&amp;quot;: &amp;quot;neutron-allinone&amp;quot;,
  &amp;quot;description&amp;quot;: &amp;quot;test&amp;quot;,
  &amp;quot;cookbook_versions&amp;quot;: {
  },
  &amp;quot;json_class&amp;quot;: &amp;quot;Chef::Environment&amp;quot;,
  &amp;quot;chef_type&amp;quot;: &amp;quot;environment&amp;quot;,
  &amp;quot;default_attributes&amp;quot;: {
  },
  &amp;quot;override_attributes&amp;quot;: {
    &amp;quot;mysql&amp;quot;: {
      &amp;quot;bind_address&amp;quot;: &amp;quot;0.0.0.0&amp;quot;,
      &amp;quot;server_root_password&amp;quot;: &amp;quot;root&amp;quot;,
      &amp;quot;server_debian_password&amp;quot;: &amp;quot;root&amp;quot;,
      &amp;quot;server_repl_password&amp;quot;: &amp;quot;root&amp;quot;,
      &amp;quot;allow_remote_root&amp;quot;: true,
      &amp;quot;root_network_acl&amp;quot;: [&amp;quot;10.0.0.0/8&amp;quot;]
    },
    &amp;quot;rabbitmq&amp;quot;: {
      &amp;quot;address&amp;quot;: &amp;quot;10.0.1.10&amp;quot;,
      &amp;quot;port&amp;quot;: &amp;quot;5672&amp;quot;
    },
    &amp;quot;openstack&amp;quot;: {
      &amp;quot;auth&amp;quot;: {
        &amp;quot;validate_certs&amp;quot;: false
      },
      &amp;quot;dashboard&amp;quot;: {
        &amp;quot;session_backend&amp;quot;: &amp;quot;file&amp;quot;
      },
      &amp;quot;block-storage&amp;quot;: {
        &amp;quot;syslog&amp;quot;: {
          &amp;quot;use&amp;quot;: false
        },
        &amp;quot;api&amp;quot;: {
          &amp;quot;ratelimit&amp;quot;: &amp;quot;False&amp;quot;
        },
        &amp;quot;debug&amp;quot;: true,
        &amp;quot;image_api_chef_role&amp;quot;: &amp;quot;os-image&amp;quot;,
        &amp;quot;identity_service_chef_role&amp;quot;: &amp;quot;os-identity&amp;quot;,
        &amp;quot;rabbit_server_chef_role&amp;quot;: &amp;quot;os-ops-messaging&amp;quot;
      },
      &amp;quot;compute&amp;quot;: {
        &amp;quot;rabbit&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.1.10&amp;quot;
        },
        &amp;quot;novnc_proxy&amp;quot;: {
          &amp;quot;bind_interface&amp;quot;: &amp;quot;eth0&amp;quot;
        },
        &amp;quot;libvirt&amp;quot;: {
          &amp;quot;bind_interface&amp;quot;: &amp;quot;eth0&amp;quot;,
        },
        &amp;quot;novnc_proxy&amp;quot;: {
          &amp;quot;bind_interface&amp;quot;: &amp;quot;eth0&amp;quot;
        },
        &amp;quot;xvpvnc_proxy&amp;quot;: {
          &amp;quot;bind_interface&amp;quot;: &amp;quot;eth0&amp;quot;
        },
        &amp;quot;image_api_chef_role&amp;quot;: &amp;quot;os-image&amp;quot;,
        &amp;quot;identity_service_chef_role&amp;quot;: &amp;quot;os-identity&amp;quot;,
        &amp;quot;nova_setup_chef_role&amp;quot;: &amp;quot;os-compute-api&amp;quot;,
        &amp;quot;rabbit_server_chef_role&amp;quot;: &amp;quot;os-ops-messaging&amp;quot;,
        &amp;quot;network&amp;quot;: {
          &amp;quot;public_interface&amp;quot;: &amp;quot;eth0&amp;quot;,
          &amp;quot;service_type&amp;quot;: &amp;quot;neutron&amp;quot;
        }
      },
      &amp;quot;network&amp;quot;: {
        &amp;quot;debug&amp;quot;: &amp;quot;True&amp;quot;,
        &amp;quot;dhcp&amp;quot;: {
          &amp;quot;enable_isolated_metadata&amp;quot;: &amp;quot;True&amp;quot;
        },
        &amp;quot;metadata&amp;quot;: {
          &amp;quot;nova_metadata_ip&amp;quot;: &amp;quot;10.0.1.10&amp;quot;
        },
        &amp;quot;openvswitch&amp;quot;: {
          &amp;quot;tunnel_id_ranges&amp;quot;: &amp;quot;1:1000&amp;quot;,
          &amp;quot;enable_tunneling&amp;quot;: &amp;quot;True&amp;quot;,
          &amp;quot;tenant_network_type&amp;quot;: &amp;quot;gre&amp;quot;,
          &amp;quot;local_ip_interface&amp;quot;: &amp;quot;eth2&amp;quot;
        },
        &amp;quot;api&amp;quot;: {
          &amp;quot;bind_interface&amp;quot;: &amp;quot;eth0&amp;quot;
        },
        &amp;quot;l3&amp;quot;: {
          &amp;quot;external_network_bridge_interface&amp;quot;: &amp;quot;eth1&amp;quot;
        },
        &amp;quot;service_plugins&amp;quot;: [&amp;quot;neutron.services.l3_router.l3_router_plugin.L3RouterPlugin&amp;quot;]
      },
      &amp;quot;db&amp;quot;: {
        &amp;quot;bind_interface&amp;quot;: &amp;quot;eth0&amp;quot;,
        &amp;quot;compute&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.1.10&amp;quot;
        },
        &amp;quot;identity&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.1.10&amp;quot;
        },
        &amp;quot;image&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.1.10&amp;quot;
        },
        &amp;quot;network&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.1.10&amp;quot;
        },
        &amp;quot;volume&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.1.10&amp;quot;
        },
        &amp;quot;dashboard&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.1.10&amp;quot;
        },
        &amp;quot;telemetry&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.1.10&amp;quot;
        },
        &amp;quot;orchestration&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.1.10&amp;quot;
        }
      },
      &amp;quot;developer_mode&amp;quot;: true,
      &amp;quot;endpoints&amp;quot;: {
        &amp;quot;compute-api-bind&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.1.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;8774&amp;quot;
        },
        &amp;quot;compute-api&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.1.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;8774&amp;quot;
        },
        &amp;quot;compute-ec2-admin-bind&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.1.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;8773&amp;quot;
        },
        &amp;quot;compute-ec2-admin&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.1.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;8773&amp;quot;
        },
       &amp;quot;compute-ec2-api-bind&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.1.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;8773&amp;quot;
        },
        &amp;quot;compute-ec2-api&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.1.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;8773&amp;quot;
        },
        &amp;quot;compute-xvpvnc&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.1.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;6081&amp;quot;
        },
        &amp;quot;compute-novnc-bind&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.1.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;6080&amp;quot;
        },
        &amp;quot;compute-novnc&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.1.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;6080&amp;quot;
        },
        &amp;quot;compute-vnc&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;0.0.0.0&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;6080&amp;quot;
        },
        &amp;quot;image-api&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.1.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;9292&amp;quot;
        },
        &amp;quot;image-api-bind&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.1.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;9292&amp;quot;
        },
        &amp;quot;image-registry&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.1.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;9191&amp;quot;
        },
        &amp;quot;image-registry-bind&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.1.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;9191&amp;quot;
        },
        &amp;quot;identity-api&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.1.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;5000&amp;quot;
        },
        &amp;quot;identity-bind&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.1.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;5000&amp;quot;
        },
        &amp;quot;identity-admin&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.1.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;35357&amp;quot;
        },
        &amp;quot;volume-api-bind&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.1.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;8776&amp;quot;
        },
        &amp;quot;volume-api&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.1.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;8776&amp;quot;
        },
        &amp;quot;telemetry-api&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.1.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;8777&amp;quot;
        },
        &amp;quot;network-api-bind&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.1.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;9696&amp;quot;
        },
        &amp;quot;network-api&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.1.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;9696&amp;quot;
        },
        &amp;quot;orchestration-api&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.1.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;8004&amp;quot;
        },
        &amp;quot;orchestration-api-cfn&amp;quot;: {
          &amp;quot;host&amp;quot;: &amp;quot;10.0.1.10&amp;quot;,
          &amp;quot;scheme&amp;quot;: &amp;quot;http&amp;quot;,
          &amp;quot;port&amp;quot;: &amp;quot;8000&amp;quot;
        }
      },
      &amp;quot;identity&amp;quot;: {
        &amp;quot;admin_user&amp;quot;: &amp;quot;admin&amp;quot;,
        &amp;quot;bind_interface&amp;quot;: &amp;quot;eth0&amp;quot;,
        &amp;quot;debug&amp;quot;: true
      },
      &amp;quot;image&amp;quot;: {
        &amp;quot;api&amp;quot;: {
          &amp;quot;bind_interface&amp;quot;: &amp;quot;eth0&amp;quot;
        },
        &amp;quot;debug&amp;quot;: true,
        &amp;quot;identity_service_chef_role&amp;quot;: &amp;quot;os-identity&amp;quot;,
        &amp;quot;rabbit_server_chef_role&amp;quot;: &amp;quot;os-ops-messaging&amp;quot;,
        &amp;quot;registry&amp;quot;: {
          &amp;quot;bind_interface&amp;quot;: &amp;quot;eth0&amp;quot;
        },
        &amp;quot;syslog&amp;quot;: {
          &amp;quot;use&amp;quot;: false
        },
        &amp;quot;upload_images&amp;quot;: [
          &amp;quot;precise&amp;quot;
        ]
      },
      &amp;quot;mq&amp;quot;: {
        &amp;quot;bind_interface&amp;quot;: &amp;quot;eth0&amp;quot;,
        &amp;quot;host&amp;quot;: &amp;quot;10.0.1.10&amp;quot;,
        &amp;quot;user&amp;quot;: &amp;quot;guest&amp;quot;,
        &amp;quot;vhost&amp;quot;: &amp;quot;/nova&amp;quot;,
        &amp;quot;network&amp;quot;: {
          &amp;quot;rabbit&amp;quot;: {
             &amp;quot;host&amp;quot;: &amp;quot;10.0.1.10&amp;quot;,
             &amp;quot;port&amp;quot;: &amp;quot;5672&amp;quot;
          }
        },
        &amp;quot;compute&amp;quot;: {
           &amp;quot;service_type&amp;quot;: &amp;quot;rabbitmq&amp;quot;,
          &amp;quot;rabbit&amp;quot;: {
            &amp;quot;host&amp;quot;: &amp;quot;10.0.1.10&amp;quot;,
            &amp;quot;port&amp;quot;: &amp;quot;5672&amp;quot;
          }
        },
        &amp;quot;block-storage&amp;quot;: {
          &amp;quot;service_type&amp;quot;: &amp;quot;rabbitmq&amp;quot;,
          &amp;quot;rabbit&amp;quot;: {
            &amp;quot;host&amp;quot;: &amp;quot;10.0.1.10&amp;quot;,
            &amp;quot;port&amp;quot;: &amp;quot;5672&amp;quot;
          }
        }
      }
    },
    &amp;quot;queue&amp;quot;: {
      &amp;quot;host&amp;quot;: &amp;quot;10.0.1.10&amp;quot;,
      &amp;quot;user&amp;quot;: &amp;quot;guest&amp;quot;,
      &amp;quot;vhost&amp;quot;: &amp;quot;/nova&amp;quot;
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;内容について全て説明するのは難しいですが、このファイルを作成するのが今回一番苦
労した点です。と言うのは、構成を作りつつそれぞれのコンポーネントのコンフィギュ
レーション、エンドポイントのアドレス、バインドアドレス、リスンポート等など、全
てが正常な値になるように Cookbooks を読みつつ作業するからです。この json ファ
イルが完成してしまえば、あとは簡単なのですが。&lt;/p&gt;

&lt;p&gt;前述しましたが KVM リソースが使えない環境の場合 Qemu で仮想マシンを稼働するこ
とができます。その場合、下記のように &amp;ldquo;libvirt&amp;rdquo; の項目に &amp;ldquo;virt_type&amp;rdquo; を追記して
ください。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;        &amp;quot;libvirt&amp;quot;: {
          &amp;quot;bind_interface&amp;quot;: &amp;quot;eth0&amp;quot;,
          &amp;quot;virt_type&amp;quot;: &amp;quot;qemu&amp;quot; # &amp;lt;------ 追記
        },
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;それではデプロイしていきます。&lt;/p&gt;

&lt;p&gt;ここで &amp;lsquo;allinone&amp;rsquo; はホスト名、&amp;rsquo;allinone-compute&amp;rsquo; は Role 名、neutron-allinone
は先ほど作成した json で指定している environment 名です。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# chef-client -z
# knife node -z run_list add allinone &#39;role[allinone-compute]&#39;
# chef-client -z -E neutron-allinone
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;環境にもよりますが、数分でオールインワンな OpenStack Icehouse が完成します。&lt;/p&gt;

&lt;p&gt;まとめ
+++&lt;/p&gt;

&lt;p&gt;Chef サーバを使わなくて良いのでお手軽に OpenStack が構築出来ました。この json
ファイルは実は他にも応用出来ると思っています。複数台構成の OpenStack も指定
Role を工夫すれば構築出来るでしょう。が、その場合は chef-zero は使えません。
Chef サーバ構成にする必要が出てきます。&lt;/p&gt;

&lt;p&gt;ちなみに OpenStack Paris Summit 2014 で「OpenStack のデプロイに何を使っている
か？」という調査結果が下記になります。Chef が2位ですが Pueppet に大きく離され
ている感があります。Juno 版の openstack-chef-repo も開発が進んでいますので、頑
張って広めていきたいです。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1位 Puppet&lt;/li&gt;
&lt;li&gt;2位 Chef&lt;/li&gt;
&lt;li&gt;3位 Ansible&lt;/li&gt;
&lt;li&gt;4位 DevStack&lt;/li&gt;
&lt;li&gt;5位 PackStack&lt;/li&gt;
&lt;li&gt;6位 Salt&lt;/li&gt;
&lt;li&gt;7位 Juju&lt;/li&gt;
&lt;li&gt;8位 Crowbar&lt;/li&gt;
&lt;li&gt;9位 CFEngine&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;参考 URL : &lt;a href=&#34;http://superuser.openstack.org/articles/openstack-user-survey-insights-november-2014&#34;&gt;http://superuser.openstack.org/articles/openstack-user-survey-insights-november-2014&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;ちなみに、Puppet を使った OpenStack デプロイも個人的に色々試しています。&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>