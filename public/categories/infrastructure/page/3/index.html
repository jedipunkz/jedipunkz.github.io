


    
        
    




<!DOCTYPE HTML>

<html>
    <head>
        
            <title>Infrastructure Posts - ジェダイさんのブログ</title>
        

        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1" />
        <meta name="generator" content="Hugo 0.53" />
        


        
        
            
        

        <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Infrastructure"/>
<meta name="twitter:description" content=""/>

        <meta property="og:title" content="Infrastructure" />
<meta property="og:description" content="" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://jedipunkz.github.io/categories/infrastructure/" />
<meta property="og:updated_time" content="2014-12-16T00:00:00&#43;00:00"/>

        
<meta itemprop="name" content="Infrastructure">
<meta itemprop="description" content="">


        

        

        
        
            
        

        
        
            <link rel="stylesheet" href="/css/google-font.css" />
            <link rel="stylesheet" href="/css/font-awesome.min.css" />
            <link rel="stylesheet" href="/css/main.css" />
            <link rel="stylesheet" href="/css/add-on.css" />
            <link rel="stylesheet" href="/css/monokai-sublime.css">
        

        

        
        
        
            
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-30563095-1', 'auto');
	
	ga('send', 'pageview');
}
</script>

        
    </head>
    <body>

        
        <div id="wrapper">

    
<header id="header">
    
        <h2><a href="/"></i></a></h2>
    

    <nav class="links">
        <ul>
            
                <li>
                    <a href="">
                        Blog
                    </a>
                </li>
            
                <li>
                    <a href="about/index.html">
                        About
                    </a>
                </li>
            
        </ul>
    </nav>
    <nav class="main">
        <ul>
            
            <li class="search">
                <a class="fa-search" href="#search">Search</a>
                <form id="search" method="get" action="//google.com/search">
                    <input type="text" name="q" placeholder="Search" />
                    <input type="hidden" name="q" value="site:https://jedipunkz.github.io">
                </form>
            </li>
            <li class="menu">
                <a class="fa-bars" href="#menu">Menu</a>
            </li>
        </ul>
    </nav>
</header>


<section id="menu">

    
        <section>
            <form class="search" method="get" action="//google.com/search">
                <input type="text" name="q" placeholder="Search" />
                <input type="hidden" name="q" value="site:https://jedipunkz.github.io">
            </form>
        </section>

    
        <section>
            <ul class="links">
                
                    <li>
                        <a href="">
                            <h3>
                                
                                Blog
                            </h3>
                        </a>
                    </li>
                
                    <li>
                        <a href="about/index.html">
                            <h3>
                                
                                About
                            </h3>
                        </a>
                    </li>
                
            </ul>
        </section>

    
        <section>
            <ul class="links">
                <header>
                    <h3>Recent Posts</h3>
                </header>
                
                    
                

                
                    <li>
                        <a href="https://jedipunkz.github.io/blog/2018/12/31/istio/"><p>Istio, Helm を使って Getting Started 的なアプリをデプロイ</p></a>
                    </li>
                
                    <li>
                        <a href="https://jedipunkz.github.io/blog/2017/07/02/test-kitchen-cluster/"><p>Docker,Test-Kitchen,Ansible でクラスタを構成する</p></a>
                    </li>
                
                    <li>
                        <a href="https://jedipunkz.github.io/blog/2017/04/13/gke-lb/"><p>GCP ロードバランサと GKE クラスタを Terraform を使って構築する</p></a>
                    </li>
                
                    <li>
                        <a href="https://jedipunkz.github.io/blog/2017/02/12/serverless-fission/"><p>Serverless on Kubernetes : Fission を使ってみた</p></a>
                    </li>
                
                    <li>
                        <a href="https://jedipunkz.github.io/blog/2017/01/13/kubernetes-deployments/"><p>Kubernetes Deployments を使ってみた！</p></a>
                    </li>
                
            </ul>
        </section>

    
        
</section>

    
    <div id="main">
        <h1>Infrastructure</h1>
        
        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2014/12/16/vyos-vxlan/">VyOS で VXLAN を使ってみる</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2014-12-16'>
            December 16, 2014</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは。@jedipunkz です。</p>

<p>VyOS に VXLAN が実装されたと聞いて少し触ってみました。この情報を知ったきっかけ
は @upaa さんの下記の資料です。</p>

<p>参考資料 : <a href="http://www.slideshare.net/upaa/vyos-users-meeting-2-vyosvxlan">http://www.slideshare.net/upaa/vyos-users-meeting-2-vyosvxlan</a></p>

<p>VyOS は御存知の通り実体は Debian Gnu/Linux 系の OS でその上に OSS なミドル
ウェアが搭載されていて CLI でミドルウェアのコンフィギュレーション等が行えるモ
ノになっています。Linux で VXLAN といえば OVS を使ったモノがよく知られています
が VyOS の VXLAN 機能は Linux Kernel の実装を使っているようです。</p>

<h2 id="要件">要件</h2>

<ul>
<li>トンネルを張るためのセグメントを用意</li>
<li>VyOS 1.1.1 (現在最新ステーブルバージョン) が必要</li>
<li>Ubuntu Server 14.04 LTS (同じく Linux VXLAN 搭載バージョン)</li>
</ul>

<h2 id="構成">構成</h2>

<p>特徴</p>

<ul>
<li>マネージメント用セグメント 10.0.1.0/24 を用意</li>
<li>GRE と同じくトンネル終端が必要なのでそのためのセグメント 10.0.2.0/24 を用意</li>
<li>各 eth1 は IP reachable である必要があるので予め IP アドレスの設定と疎通を確認</li>
<li>VXLAN を喋れる Ubuntu 14.04 LTS x 1 台と VyOS 1.1.1 x 2 台で相互に疎通確認</li>
</ul>

<pre><code>+-------------+-------------+------------ Management 10.0.1.0/24
|10.0.0.254   |10.0.0.253   |10.0.0.1
|eth0         |eth0         |eth0
+----------+  +----------+  +----------+ 
|  vyos01  |  |  vyos02  |  |  ubuntu  |
+-+--------+  +----------+  +----------+ 
| |eth1       | |eth1       | |eth1
| |10.0.2.254 | |10.0.2.253 | |10.0.2.1
| +-----------)-+-----------)-+---------- Tunneling 10.0.2.0/24
|             |             |
+-------------+-------------+------------ VXLAN(eth1にlink) 10.0.1.0/24
10.0.1.254     10.0.1.253    10.0.1.1
</code></pre>

<h2 id="設定を投入">設定を投入</h2>

<p>vyos01 の設定を行う。VXLAN の設定に必要なものは&hellip;</p>

<ul>
<li>VNI (VXLAN Network Ideintity)という識別子</li>
<li>Multicast Group Address</li>
<li>互いに IP reachable なトンネルを張るためのインターフェース</li>
</ul>

<p>です。これらを意識して下記の設定を vyos01 に投入します。</p>

<pre><code class="language-bash">$ configure
% set interfaces vxlan vxlan0
% set interfaces vxlan vxlan0 group 239.1.1.1
% set interfaces vxlan vxlan0 vni 42
% set interfaces vxlan vxlan0 address '10.0.1.254/24'
% set interfaces vxlan vxlan0 link eth1
</code></pre>

<p>設定を確認します</p>

<pre><code class="language-bash">% exit
$ show int
...&lt;省略&gt;...
    vxlan vxlan0 {
     address 10.0.1.254/24
     group 239.1.1.1
     link eth1
     vni 42
}
</code></pre>

<p>VyOS の CLI を介さず直 Linux の設定を iproute2 で確認してみましょう。
VNI, Multicast Group Address と共に &lsquo;link eth1&rsquo; で設定したトンネルを終端するための物理 NIC が確認できます。</p>

<pre><code class="language-bash">vyos@vyos01# ip -d link show vxlan0
5: vxlan0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN mode DEFAULT group default
    link/ether 86:24:26:b2:11:5c brd ff:ff:ff:ff:ff:ff promiscuity 0
    vxlan id 42 group 239.1.1.1 dev eth1 port 32768 61000 ttl 16 ageing 300
</code></pre>

<p>vyos02 の設定を同様に行います。</p>

<pre><code class="language-bash">$ congigure
% set interfaces vxlan vxlan0 address '10.0.1.253/24'
% set interfaces vxlan vxlan0 vni 42
% set interfaces vxlan vxlan0 group 239.1.1.1
% set interfaces vxlan vxlan0 link eth1
</code></pre>

<p>設定の確認を行います。</p>

<pre><code class="language-bash">... 省略 ...
vxlan vxlan0 {
     address 10.0.1.254/24
     group 239.1.1.1
     link eth1
     vni 42
}
</code></pre>

<p>同じく Linux の iproute2 で確認します。</p>

<pre><code class="language-bash">vyos@vyos01# ip -d link show vxlan0
5: vxlan0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN mode DEFAULT group default
    link/ether 86:24:26:b2:11:5c brd ff:ff:ff:ff:ff:ff promiscuity 0
    vxlan id 42 group 239.1.1.1 dev eth1 port 32768 61000 ttl 16 ageing 300
</code></pre>

<p>ubuntu ホストの設定を行っていきます。</p>

<p>Ubuntu Server 14.04 LTS であればパッチを当てること無く Linux Kernel の VXLAN 機能を使うことができます。
設定内容は VyOS と同等です。VyOS がこの Linux の実装を使っているのがよく分かります。</p>

<pre><code class="language-bash">sudo modprobe vxlan
sudo ip link add vxlan0 type vxlan id 42 group 239.1.1.1 dev eth1
sudo ip link set up vxlan0
sudo ip a add 10.0.1.1/24 dev vxlan0
</code></pre>

<p>同じく Linux iproute2 で確認を行います。</p>

<pre><code class="language-bash"> ip -d link show vxlan0
5: vxlan0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN mode DEFAULT group default
    link/ether d6:ff:c1:27:69:a0 brd ff:ff:ff:ff:ff:ff promiscuity 0
    vxlan id 42 group 239.1.1.1 dev eth1 port 32768 61000 ageing 300
</code></pre>

<h2 id="疎通確認">疎通確認</h2>

<p>疎通確認を行います。</p>

<p>ubuntu -&gt; vyos01 の疎通確認です。ICMP で疎通が取れることを確認できます。</p>

<pre><code class="language-bash">thirai@ubuntu:~$ ping 10.0.1.254 -c 3
PING 10.0.1.254 (10.0.1.254) 56(84) bytes of data.
64 bytes from 10.0.1.254: icmp_seq=1 ttl=64 time=0.272 ms
64 bytes from 10.0.1.254: icmp_seq=2 ttl=64 time=0.336 ms
64 bytes from 10.0.1.254: icmp_seq=3 ttl=64 time=0.490 ms

--- 10.0.1.254 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 1999ms
rtt min/avg/max/mdev = 0.272/0.366/0.490/0.091 ms
</code></pre>

<p>次に ubuntu -&gt; vyos02 の疎通確認です。</p>

<pre><code class="language-bash">thirai@ubuntu:~$ ping 10.0.1.253 -c 3
PING 10.0.1.253 (10.0.1.253) 56(84) bytes of data.
64 bytes from 10.0.1.253: icmp_seq=1 ttl=64 time=0.272 ms
64 bytes from 10.0.1.253: icmp_seq=2 ttl=64 time=0.418 ms
64 bytes from 10.0.1.253: icmp_seq=3 ttl=64 time=0.451 ms

--- 10.0.1.253 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 1998ms
rtt min/avg/max/mdev = 0.272/0.380/0.451/0.079 ms
</code></pre>

<p>この時点で ubuntu ホストの fdb (forwarding db) の内容を確認します。</p>

<pre><code class="language-bash">$ bridge fdb show dev vxlan0
00:00:00:00:00:00 dst 239.1.1.1 via eth1 self permanent
4e:69:a4:a7:ef:1c dst 10.0.2.253 self
86:24:26:b2:11:5c dst 10.0.2.254 self
</code></pre>

<p>vyos01, vyos02 のトンネル終端 IP アドレスと Mac アドレスが確認できます。ubuntu ホストから見ると
送信先は vyos0[12] の VXLAN インターフェースではなく、あくまでもトンネル終端を行っているインターフェース
になることがわかります。</p>

<h2 id="まとめ">まとめ</h2>

<p>VyOS ver 1.1.0 には VXLAN を物理インターフェースに link する機能に不具合がありそうなので今ら ver 1.1.1 を使うしか
なさそう。とは言え、ver 1.1.1 なら普通に動作しました。</p>

<p>VyOS は仮想ルータという位置付けなので今回紹介したようにインターフェースを VXLAN ネットワークに所属させる
機能があるのみです。VXLAN Trunk を行うような設定はありません。これはハイパーバイザ上で動作させることを前提
に設計されているので仕方ないです..というかスイッチで行うべき機能ですよね..。VM を接続して云々するには OVS
のようなソフトウェアスイッチを使えばできます。</p>

<p>また fdb は時間が経つと情報が消えます。これは VXLAN のメッシュ構造なトンネルがその都度張られているのかどうか
気になるところです。ICMP の送信で一発目のみマルチキャストでその後ユニキャストになることを確認しましたが、その
一発目のマルチキャストでトンネリングがされるものなのでしょうか&hellip;。あとで調べてみます。OVS のように CLI で
トンネルがどのように張られているか確認する手段があれば良いのですが。</p>

<p>以上です。</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2014/12/16/vyos-vxlan/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>


        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2014/12/13/aviator-openstack/">Aviator でモダンに OpenStack を操作する</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2014-12-13'>
            December 13, 2014</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは。@jedipunkz です。</p>

<p>自分は Ruby を普段使うのでいつも Fog というライブラリを使って OpenStack, AWS
を操作していました。Fog を使うとクラウドの操作が Ruby のネイティブコードで行え
るのでシステムコマンド打つよりミス無く済みます。</p>

<p>Fog より後発で Aviator というライブラリが登場してきたので少し使ってみたのです
がまだ未完成なところがあるものの便利な点もあって今後に期待だったので紹介します。</p>

<h2 id="認証情報を-yaml-ファイルに記す">認証情報を yaml ファイルに記す</h2>

<p>接続に必要な認証情報を yaml ファイルで記述します。名前を &lsquo;aviator.yml&rsquo; として
保存。この時に下記のように環境毎に認証情報を別けて書くことができます。こうする
ことでコードの中で開発用・サービス用等と使い分けられます。</p>

<pre><code class="language-yaml">production:
  provider: openstack
  auth_service:
    name: identity
    host_uri: &lt;Auth URL&gt;
    request: create_token
    validator: list_tenants
  auth_credentials:
    username: &lt;User Name&gt;
    password: &lt;Password&gt;
    tenant_name: &lt;Tenant Name&gt;

development:
  provider: openstack
  auth_service:
    name: identity
    host_uri: &lt;Auth URL&gt;
    request: create_token
    validator: list_tenants
  auth_credentials:
    username: &lt;User Name&gt;
    password: &lt;Password&gt;
    tenant_name: &lt;Tenant Name&gt;
</code></pre>

<p>シンタックス確認
+++</p>

<p>次に aviator のシンタックスを確認します。Fog に無い機能で、コマンドラインでシ
ンタックスを確認できてしかも指定可能はパラメータと必須なパラメータと共にサンプ
ルコードまで提供してくれます。公式サイトに&rsquo;サーバ作成&rsquo;のメソッドが掲載されてい
るので、ここでは仮想ディスクを作るシンタックスを確認してみます。</p>

<pre><code class="language-bash">% gem install aviator
% aviator describe openstack volume # &lt;-- 利用可能な機能を確認
Available requests for openstack volume_service:
v1 public list_volume_types
v1 public list_volumes
v1 public delete_volume
v1 public create_volume
v1 public get_volume
v1 public update_volume
  v1 public root
% aviator describe openstack volume v1 public create_volume # &lt;-- シンタックスを確認
:Request =&gt; create_volume

Parameters:
 +---------------------+-----------+
 | NAME                | REQUIRED? |
 +---------------------+-----------+
 | availability_zone   |     N     |
 | display_description |     Y     |
 | display_name        |     Y     |
 | metadata            |     N     |
 | size                |     Y     |
 | snapshot_id         |     N     |
 | volume_type         |     N     |
 +---------------------+-----------+

Sample Code:
  session.volume_service.request(:create_volume) do |params|
    params.volume_type = value
    params.availability_zone = value
    params.snapshot_id = value
    params.metadata = value
    params.display_name = value
    params.display_description = value
    params.size = value
  end
</code></pre>

<p>このように create_volume というメソッドが用意されていて、指定出来るパラメータ・
必須なパラメータが確認できます。必須なモノには &ldquo;Y&rdquo; が REQUIRED に付いています。
またサンプルコードが出力されるので、めちゃ便利です。</p>

<p>では create_volume のシンタックスがわかったので、コードを書いてみましょう。</p>

<p>コードを書いてみる
+++</p>

<pre><code class="language-ruby">#!/usr/bin/env ruby

require 'aviator'
require 'json'

volume_session = Aviator::Session.new(
              :config_file =&gt; '/home/thirai/aviator/aviator.yml',
              :environment =&gt; :production,
              :log_file    =&gt; '/home/thirai/aviator/aviator.log'
            )

volume_session.authenticate

volume_session.volume_service.request(:create_volume) do |params|
  params.display_description = 'testvol'
  params.display_name = 'testvol01'
  params.size = 1
end
puts volume_session.volume_service.request(:list_volumes).body
</code></pre>

<p>6行目で先ほど作成した認証情報ファイル aviator.yml とログ出力ファイル
aviator.log を指定します。12行目で実際に OpenStack にログインしています。</p>

<p>14-18行目はサンプルコードそのままです。必須パラメータの display_description,
display_name, size のみを指定し仮想ディスクを作成しました。最後の puts &hellip; は
実際に作成した仮想ディスク一覧を出力しています。</p>

<p>結果は下記のとおりです。</p>

<pre><code class="language-json">{ volumes: [{ status: 'available', display_name: 'testvol01', attachments: [],
availability_zone: 'az3', bootable: 'false', created_at:
description = 'testvol', volume_type:
'standard', snapshot_id: nil, source_volid: nil, metadata:  }, id:
'3a5f616e-a732-4442-a419-10369111bd4c', size: 1 }] }
</code></pre>

<p>まとめ
+++</p>

<p>サンプルコードやパラメータ一覧等がひと目でわかる aviator はとても便利です。ま
だ利用できるクラウドプラットフォームが OpenStack しかないのと、Neutron の機能
がスッポリ抜けているので、まだ利用するには早いかもです&hellip;。逆に言えばコントリ
ビューションするチャンスなので、もし気になった方がいたら開発に参加してみるのも
いいかもしれません。</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2014/12/13/aviator-openstack/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Categories
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
        <li><a href='/categories/tools'>tools</a></li>
    
</ul>

    </footer>
</article>


        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2014/11/15/chef-zero-openstack-icehouse/">Chef-Zero でお手軽に OpenStack Icehouse を作る</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2014-11-15'>
            November 15, 2014</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは。<a href="https://twitter.com/jedipunkz">@jedipunkz</a> です。</p>

<p>OpenStack Juno がリリースされましたが、今日は Icehouse ネタです。</p>

<p>icehouse 以降、自分の中で OpenStack を自動で作る仕組みが無くなりつつあり、気軽
に OpenStack を作って色々試したい！ッていう時に手段が無く困っていました。例え
ば仕事でちょっと OpenStack 弄りたい！って時に DevStack, RDO しかなく。DevStack
は御存知の通り動かない可能性が結構あるし RDO は Ubuntu/Debian Gnu Linux ベース
じゃないし。</p>

<p>ってことで、以前にも紹介した stackforge 管理の openstack-chef-repo と
Chef-Zero を使って OpenStack Icehouse (Neutron) のオールインワン構成を作る方法
を書きます。ちなみに最近 Chef-Solo が Chef-Zero に置き換わりつつあるらしいです。
Chef-Zero はオンメモリで Chef サーバを起動する仕組みです。Chef-Solo と違って Chef
サーバを扱う時と何も変更無く操作が出来るのでとても楽です。また、Chef サーバを
別途構、構築・管理しなくて良いので、気軽に OpenStack が作れます。</p>

<p>ちなみに stackforge/openstack-chef-repo の README.md に Chef-Zero での構築方法
が書いてありますが、沢山の問題があります。</p>

<ul>
<li>nova-network 構成</li>
<li>API の Endpoint が全て localhost に向いてしまうため外部から操作不可能</li>
<li>各コンポーネントの bind_address が localhost を向いてしまう</li>
<li>berkshelf がそのままでは入らない</li>
</ul>

<p>よって、今回はこれらの問題を解決しつつ &ldquo;オールインワンな Neutron 構成の
Icehouse OpenStack を作る方法&rdquo; を書いていきます。</p>

<h2 id="構成">構成</h2>

<pre><code>+----------------- 10.0.0.0/24 (api/management network)
|
+----------------+
| OpenStack Node |
|   Controller   |
|    Compute     |
+----------------+
|  |
+--(-------------- 10.0.1.0/24 (external network)
   |
   +-------------- 10.0.2.0/24 (guest vm network)
</code></pre>

<p>IP address 達</p>

<ul>
<li>10.0.0.10 (api/manageent network) : eth0</li>
<li>10.0.1.10 (external network) : eth1</li>
<li>10.0.2.10 (guest vm network) : eth2</li>
</ul>

<p>注意 : 操作は全て eth0 経由で行う</p>

<h2 id="前提の環境">前提の環境</h2>

<p>stackforge/openstack-chef-repo の依存している Cookbooks の関係上、upstart 周り
がうまく制御できていないので Ubuntu Server 12.04.x を使います。</p>

<h2 id="インストール方法">インストール方法</h2>

<p>上記のように3つのネットワークインターフェースが付いたサーバを1台用意します。
KVM が利用出来たほうがいいですが使えないくても構いません。KVM リソースが使えな
い場合の修正方法を後に記します。</p>

<p>サーバにログインし root ユーザになります。その後 Chef をオムニバスインストーラ
でインストールします。</p>

<pre><code class="language-bash">% sudo -i
# curl -L https://www.opscode.com/chef/install.sh | bash
</code></pre>

<p>次に stable/icehose ブランチを指定して openstack-chef-repo をクローンします。</p>

<pre><code class="language-bash"># cd ~
# git clone -b stable/icehouse https://github.com/stackforge/openstack-chef-repo
# 
</code></pre>

<p>berkshelf をインストールするのですが依存パッケージが足らないのでここでインストー
ルします。</p>

<pre><code class="language-bash"># apt-get -y install build-essential zlib1g-dev libssl-dev libreadline-dev \
  ruby-dev libxml2-dev libxslt-dev g++
</code></pre>

<p>berkshelf をインストールします。</p>

<pre><code class="language-bash"># /opt/chef/embedded/bin/gem install berkshelf --no-ri --no-rdoc
</code></pre>

<p>次に openstack-chef-repo に依存する Cookbooks を取得します。</p>

<pre><code class="language-bash"># cd ~/openstack-chef-repo
# /opt/chef/embedded/bin/berks vendor ./cookbooks
</code></pre>

<p>~/openstack-chef-repo/environments ディレクトリ配下に neutron-allinone.json と
いうファイル名で作成します。内容は下記の通りです。</p>

<pre><code class="language-json">{                                                                                                                                                      [0/215]
  &quot;name&quot;: &quot;neutron-allinone&quot;,
  &quot;description&quot;: &quot;test&quot;,
  &quot;cookbook_versions&quot;: {
  },
  &quot;json_class&quot;: &quot;Chef::Environment&quot;,
  &quot;chef_type&quot;: &quot;environment&quot;,
  &quot;default_attributes&quot;: {
  },
  &quot;override_attributes&quot;: {
    &quot;mysql&quot;: {
      &quot;bind_address&quot;: &quot;0.0.0.0&quot;,
      &quot;server_root_password&quot;: &quot;root&quot;,
      &quot;server_debian_password&quot;: &quot;root&quot;,
      &quot;server_repl_password&quot;: &quot;root&quot;,
      &quot;allow_remote_root&quot;: true,
      &quot;root_network_acl&quot;: [&quot;10.0.0.0/8&quot;]
    },
    &quot;rabbitmq&quot;: {
      &quot;address&quot;: &quot;10.0.1.10&quot;,
      &quot;port&quot;: &quot;5672&quot;
    },
    &quot;openstack&quot;: {
      &quot;auth&quot;: {
        &quot;validate_certs&quot;: false
      },
      &quot;dashboard&quot;: {
        &quot;session_backend&quot;: &quot;file&quot;
      },
      &quot;block-storage&quot;: {
        &quot;syslog&quot;: {
          &quot;use&quot;: false
        },
        &quot;api&quot;: {
          &quot;ratelimit&quot;: &quot;False&quot;
        },
        &quot;debug&quot;: true,
        &quot;image_api_chef_role&quot;: &quot;os-image&quot;,
        &quot;identity_service_chef_role&quot;: &quot;os-identity&quot;,
        &quot;rabbit_server_chef_role&quot;: &quot;os-ops-messaging&quot;
      },
      &quot;compute&quot;: {
        &quot;rabbit&quot;: {
          &quot;host&quot;: &quot;10.0.1.10&quot;
        },
        &quot;novnc_proxy&quot;: {
          &quot;bind_interface&quot;: &quot;eth0&quot;
        },
        &quot;libvirt&quot;: {
          &quot;bind_interface&quot;: &quot;eth0&quot;,
        },
        &quot;novnc_proxy&quot;: {
          &quot;bind_interface&quot;: &quot;eth0&quot;
        },
        &quot;xvpvnc_proxy&quot;: {
          &quot;bind_interface&quot;: &quot;eth0&quot;
        },
        &quot;image_api_chef_role&quot;: &quot;os-image&quot;,
        &quot;identity_service_chef_role&quot;: &quot;os-identity&quot;,
        &quot;nova_setup_chef_role&quot;: &quot;os-compute-api&quot;,
        &quot;rabbit_server_chef_role&quot;: &quot;os-ops-messaging&quot;,
        &quot;network&quot;: {
          &quot;public_interface&quot;: &quot;eth0&quot;,
          &quot;service_type&quot;: &quot;neutron&quot;
        }
      },
      &quot;network&quot;: {
        &quot;debug&quot;: &quot;True&quot;,
        &quot;dhcp&quot;: {
          &quot;enable_isolated_metadata&quot;: &quot;True&quot;
        },
        &quot;metadata&quot;: {
          &quot;nova_metadata_ip&quot;: &quot;10.0.1.10&quot;
        },
        &quot;openvswitch&quot;: {
          &quot;tunnel_id_ranges&quot;: &quot;1:1000&quot;,
          &quot;enable_tunneling&quot;: &quot;True&quot;,
          &quot;tenant_network_type&quot;: &quot;gre&quot;,
          &quot;local_ip_interface&quot;: &quot;eth2&quot;
        },
        &quot;api&quot;: {
          &quot;bind_interface&quot;: &quot;eth0&quot;
        },
        &quot;l3&quot;: {
          &quot;external_network_bridge_interface&quot;: &quot;eth1&quot;
        },
        &quot;service_plugins&quot;: [&quot;neutron.services.l3_router.l3_router_plugin.L3RouterPlugin&quot;]
      },
      &quot;db&quot;: {
        &quot;bind_interface&quot;: &quot;eth0&quot;,
        &quot;compute&quot;: {
          &quot;host&quot;: &quot;10.0.1.10&quot;
        },
        &quot;identity&quot;: {
          &quot;host&quot;: &quot;10.0.1.10&quot;
        },
        &quot;image&quot;: {
          &quot;host&quot;: &quot;10.0.1.10&quot;
        },
        &quot;network&quot;: {
          &quot;host&quot;: &quot;10.0.1.10&quot;
        },
        &quot;volume&quot;: {
          &quot;host&quot;: &quot;10.0.1.10&quot;
        },
        &quot;dashboard&quot;: {
          &quot;host&quot;: &quot;10.0.1.10&quot;
        },
        &quot;telemetry&quot;: {
          &quot;host&quot;: &quot;10.0.1.10&quot;
        },
        &quot;orchestration&quot;: {
          &quot;host&quot;: &quot;10.0.1.10&quot;
        }
      },
      &quot;developer_mode&quot;: true,
      &quot;endpoints&quot;: {
        &quot;compute-api-bind&quot;: {
          &quot;host&quot;: &quot;10.0.1.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;8774&quot;
        },
        &quot;compute-api&quot;: {
          &quot;host&quot;: &quot;10.0.1.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;8774&quot;
        },
        &quot;compute-ec2-admin-bind&quot;: {
          &quot;host&quot;: &quot;10.0.1.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;8773&quot;
        },
        &quot;compute-ec2-admin&quot;: {
          &quot;host&quot;: &quot;10.0.1.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;8773&quot;
        },
       &quot;compute-ec2-api-bind&quot;: {
          &quot;host&quot;: &quot;10.0.1.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;8773&quot;
        },
        &quot;compute-ec2-api&quot;: {
          &quot;host&quot;: &quot;10.0.1.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;8773&quot;
        },
        &quot;compute-xvpvnc&quot;: {
          &quot;host&quot;: &quot;10.0.1.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;6081&quot;
        },
        &quot;compute-novnc-bind&quot;: {
          &quot;host&quot;: &quot;10.0.1.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;6080&quot;
        },
        &quot;compute-novnc&quot;: {
          &quot;host&quot;: &quot;10.0.1.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;6080&quot;
        },
        &quot;compute-vnc&quot;: {
          &quot;host&quot;: &quot;0.0.0.0&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;6080&quot;
        },
        &quot;image-api&quot;: {
          &quot;host&quot;: &quot;10.0.1.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;9292&quot;
        },
        &quot;image-api-bind&quot;: {
          &quot;host&quot;: &quot;10.0.1.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;9292&quot;
        },
        &quot;image-registry&quot;: {
          &quot;host&quot;: &quot;10.0.1.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;9191&quot;
        },
        &quot;image-registry-bind&quot;: {
          &quot;host&quot;: &quot;10.0.1.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;9191&quot;
        },
        &quot;identity-api&quot;: {
          &quot;host&quot;: &quot;10.0.1.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;5000&quot;
        },
        &quot;identity-bind&quot;: {
          &quot;host&quot;: &quot;10.0.1.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;5000&quot;
        },
        &quot;identity-admin&quot;: {
          &quot;host&quot;: &quot;10.0.1.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;35357&quot;
        },
        &quot;volume-api-bind&quot;: {
          &quot;host&quot;: &quot;10.0.1.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;8776&quot;
        },
        &quot;volume-api&quot;: {
          &quot;host&quot;: &quot;10.0.1.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;8776&quot;
        },
        &quot;telemetry-api&quot;: {
          &quot;host&quot;: &quot;10.0.1.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;8777&quot;
        },
        &quot;network-api-bind&quot;: {
          &quot;host&quot;: &quot;10.0.1.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;9696&quot;
        },
        &quot;network-api&quot;: {
          &quot;host&quot;: &quot;10.0.1.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;9696&quot;
        },
        &quot;orchestration-api&quot;: {
          &quot;host&quot;: &quot;10.0.1.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;8004&quot;
        },
        &quot;orchestration-api-cfn&quot;: {
          &quot;host&quot;: &quot;10.0.1.10&quot;,
          &quot;scheme&quot;: &quot;http&quot;,
          &quot;port&quot;: &quot;8000&quot;
        }
      },
      &quot;identity&quot;: {
        &quot;admin_user&quot;: &quot;admin&quot;,
        &quot;bind_interface&quot;: &quot;eth0&quot;,
        &quot;debug&quot;: true
      },
      &quot;image&quot;: {
        &quot;api&quot;: {
          &quot;bind_interface&quot;: &quot;eth0&quot;
        },
        &quot;debug&quot;: true,
        &quot;identity_service_chef_role&quot;: &quot;os-identity&quot;,
        &quot;rabbit_server_chef_role&quot;: &quot;os-ops-messaging&quot;,
        &quot;registry&quot;: {
          &quot;bind_interface&quot;: &quot;eth0&quot;
        },
        &quot;syslog&quot;: {
          &quot;use&quot;: false
        },
        &quot;upload_images&quot;: [
          &quot;precise&quot;
        ]
      },
      &quot;mq&quot;: {
        &quot;bind_interface&quot;: &quot;eth0&quot;,
        &quot;host&quot;: &quot;10.0.1.10&quot;,
        &quot;user&quot;: &quot;guest&quot;,
        &quot;vhost&quot;: &quot;/nova&quot;,
        &quot;network&quot;: {
          &quot;rabbit&quot;: {
             &quot;host&quot;: &quot;10.0.1.10&quot;,
             &quot;port&quot;: &quot;5672&quot;
          }
        },
        &quot;compute&quot;: {
           &quot;service_type&quot;: &quot;rabbitmq&quot;,
          &quot;rabbit&quot;: {
            &quot;host&quot;: &quot;10.0.1.10&quot;,
            &quot;port&quot;: &quot;5672&quot;
          }
        },
        &quot;block-storage&quot;: {
          &quot;service_type&quot;: &quot;rabbitmq&quot;,
          &quot;rabbit&quot;: {
            &quot;host&quot;: &quot;10.0.1.10&quot;,
            &quot;port&quot;: &quot;5672&quot;
          }
        }
      }
    },
    &quot;queue&quot;: {
      &quot;host&quot;: &quot;10.0.1.10&quot;,
      &quot;user&quot;: &quot;guest&quot;,
      &quot;vhost&quot;: &quot;/nova&quot;
    }
  }
}
</code></pre>

<p>内容について全て説明するのは難しいですが、このファイルを作成するのが今回一番苦
労した点です。と言うのは、構成を作りつつそれぞれのコンポーネントのコンフィギュ
レーション、エンドポイントのアドレス、バインドアドレス、リスンポート等など、全
てが正常な値になるように Cookbooks を読みつつ作業するからです。この json ファ
イルが完成してしまえば、あとは簡単なのですが。</p>

<p>前述しましたが KVM リソースが使えない環境の場合 Qemu で仮想マシンを稼働するこ
とができます。その場合、下記のように &ldquo;libvirt&rdquo; の項目に &ldquo;virt_type&rdquo; を追記して
ください。</p>

<pre><code class="language-json">        &quot;libvirt&quot;: {
          &quot;bind_interface&quot;: &quot;eth0&quot;,
          &quot;virt_type&quot;: &quot;qemu&quot; # &lt;------ 追記
        },
</code></pre>

<p>それではデプロイしていきます。</p>

<p>ここで &lsquo;allinone&rsquo; はホスト名、&rsquo;allinone-compute&rsquo; は Role 名、neutron-allinone
は先ほど作成した json で指定している environment 名です。</p>

<pre><code class="language-bash"># chef-client -z
# knife node -z run_list add allinone 'role[allinone-compute]'
# chef-client -z -E neutron-allinone
</code></pre>

<p>環境にもよりますが、数分でオールインワンな OpenStack Icehouse が完成します。</p>

<p>まとめ
+++</p>

<p>Chef サーバを使わなくて良いのでお手軽に OpenStack が構築出来ました。この json
ファイルは実は他にも応用出来ると思っています。複数台構成の OpenStack も指定
Role を工夫すれば構築出来るでしょう。が、その場合は chef-zero は使えません。
Chef サーバ構成にする必要が出てきます。</p>

<p>ちなみに OpenStack Paris Summit 2014 で「OpenStack のデプロイに何を使っている
か？」という調査結果が下記になります。Chef が2位ですが Pueppet に大きく離され
ている感があります。Juno 版の openstack-chef-repo も開発が進んでいますので、頑
張って広めていきたいです。</p>

<ul>
<li>1位 Puppet</li>
<li>2位 Chef</li>
<li>3位 Ansible</li>
<li>4位 DevStack</li>
<li>5位 PackStack</li>
<li>6位 Salt</li>
<li>7位 Juju</li>
<li>8位 Crowbar</li>
<li>9位 CFEngine</li>
</ul>

<p>参考 URL : <a href="http://superuser.openstack.org/articles/openstack-user-survey-insights-november-2014">http://superuser.openstack.org/articles/openstack-user-survey-insights-november-2014</a></p>

<p>ちなみに、Puppet を使った OpenStack デプロイも個人的に色々試しています。</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2014/11/15/chef-zero-openstack-icehouse/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>


        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2014/11/04/midostack/">MidoStack を動かしてみる</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2014-11-04'>
            November 4, 2014</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは。@jedipunkz です。</p>

<p>昨晩 Midokura さんが Midonet を OSS 化したとニュースになりました。公式サイトは
下記の URL になっています。Midonet は OpenStack Neutron のプラグインとして動作
するソフトウェアです。</p>

<p><a href="http://www.midonet.org">http://www.midonet.org</a></p>

<p>下記のGithub 上でソースを公開しています。</p>

<p><a href="https://github.com/midonet">https://github.com/midonet</a></p>

<p>本体の midonet と共に midostack というレポジトリがあってどうやら公式サイトの
QuickStart を見ても devstack を交えての簡単な midonet の動作が確認できそう。</p>

<p><a href="https://github.com/midonet/midostack">https://github.com/midonet/midostack</a></p>

<h2 id="早速使ってみる">早速使ってみる</h2>

<p>早速 midostack を使って midonet を体験してみましょう。QuickStart には
Vagrant + VirtualBox を用いた使い方が改定ありますが手元の PC 端末だとリソース
が足らなくて CirrOS VM 一個すら立ち上がりませんでした。よって普通にリソースの
沢山あるサーバで稼働してみます。Vagrantfile 見ても</p>

<pre><code>config.vm.synced_folder &quot;./&quot;, &quot;/midostack&quot;
</code></pre>

<p>としているだけなので、Vagrant ではなくても大丈夫でしょう。</p>

<p>Ubuntu Server 14.04 をインストールしたマシンを用意して midostack を取得します。</p>

<pre><code class="language-bash">% git clone https://github.com/midonet/midostack.git
</code></pre>

<p>midonet_stack.sh を実行します。</p>

<pre><code class="language-bash">% cd midostack
% ./midonet_stack.sh
</code></pre>

<p>暫く待つと Neutron Middonet Plugin が有効になった OpenStack が立ち上がります。
Horizon にアクセスしましょう。ユーザ名 : admin, パスワード : gogomid0 (デフォ
ルト) です。</p>

<p>VM も普通に立ち上がりますし VM 同士の通信も良好です。</p>

<h2 id="neutron-プロセスを確認する">Neutron プロセスを確認する</h2>

<p>Neutron-Server は下記のように立ち上がっています。</p>

<pre><code class="language-bash">16229 pts/13   S+     0:06 python /usr/local/bin/neutron-server --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugins/midonet/midonet.ini
</code></pre>

<p>/etc/neutron/neutron.conf の midonet の指定はこんな感じ。</p>

<pre><code>core_plugin = midonet.neutron.plugin.MidonetPluginV2
api_extensions_path = /opt/stack/midonet/python-neutron-plugin-midonet/midonet/neutron/extensions
</code></pre>

<p>次に /etc/neutron/plugins/midonet/midonet.ini を確認してみましょう。</p>

<pre><code>[midonet]
# MidoNet API server URI
# midonet_uri = http://localhost:8080/midonet-api

# MidoNet admin username
# username = admin

# MidoNet admin password
# password = passw0rd

# ID of the project that MidoNet admin user belongs to
# project_id = 77777777-7777-7777-7777-777777777777

# Virtual provider router ID
# provider_router_id = 00112233-0011-0011-0011-001122334455

# Path to midonet host uuid file
# midonet_host_uuid_path = /etc/midolman/host_uuid.properties

[MIDONET]
project_id = admin
password = gogomid0
username = admin
midonet_uri = http://localhost:8081/midonet-api
</code></pre>

<h2 id="midonet-api-にアクセスする">Midonet API にアクセスする</h2>

<p>Midonet API のリファレンスが下記の URL で公開されていました。</p>

<p><a href="http://docs.midonet.org/docs/v1.8/rest-api/api/rest-api-specification.html">http://docs.midonet.org/docs/v1.8/rest-api/api/rest-api-specification.html</a></p>

<p>早速使ってみましょう。まず Token を得ます。</p>

<pre><code class="language-bash">curl -i 'http://127.0.0.1:5000/v2.0/tokens' -X POST -H &quot;Content-Type: application/json&quot; -H &quot;Accept: application/json&quot;  -d '{&quot;auth&quot;: {&quot;tenantName&quot;: &quot;admin&quot;, &quot;passwordCredentials&quot;: {&quot;username&quot;: &quot;admin&quot;, &quot;password&quot;: &quot;gogomid0&quot;}}}'
</code></pre>

<p>Token ID を取得したら &ldquo;/&rdquo; に対してアクセスしてみましょう。</p>

<pre><code class="language-bash">% curl -i -X GET http://localhost:8081/midonet-api/ -H &quot;User-Agent: python-keystoneclient&quot; -H &quot;X-Auth-Token: &lt;TokenID&gt;&quot;
</code></pre>

<p>レスポンス</p>

<pre><code class="language-json">{
&quot;routerTemplate&quot;: &quot;http://localhost:8081/midonet-api/routers/{id}&quot;,
&quot;portTemplate&quot;: &quot;http://localhost:8081/midonet-api/ports/{id}&quot;,
&quot;vipTemplate&quot;: &quot;http://localhost:8081/midonet-api/vips/{id}&quot;,
&quot;poolTemplate&quot;: &quot;http://localhost:8081/midonet-api/pools/{id}&quot;,
&quot;healthMonitorTemplate&quot;: &quot;http://localhost:8081/midonet-api/health_monitors/{id}&quot;,
&quot;healthMonitors&quot;: &quot;http://localhost:8081/midonet-api/health_monitors&quot;,
&quot;loadBalancers&quot;: &quot;http://localhost:8081/midonet-api/load_balancers&quot;,
&quot;ipAddrGroupTemplate&quot;: &quot;http://localhost:8081/midonet-api/ip_addr_groups/{id}&quot;,
&quot;tenants&quot;: &quot;http://localhost:8081/midonet-api/tenants&quot;,
&quot;tenantTemplate&quot;: &quot;http://localhost:8081/midonet-api/tenants/{id}&quot;,
&quot;portGroupTemplate&quot;: &quot;http://localhost:8081/midonet-api/port_groups/{id}&quot;,
&quot;loadBalancerTemplate&quot;: &quot;http://localhost:8081/midonet-api/load_balancers/{id}&quot;,
&quot;poolMemberTemplate&quot;: &quot;http://localhost:8081/midonet-api/pool_members/{id}&quot;,
&quot;hostVersions&quot;: &quot;http://localhost:8081/midonet-api/versions&quot;,
&quot;version&quot;: &quot;v1.7&quot;,
&quot;bridgeTemplate&quot;: &quot;http://localhost:8081/midonet-api/bridges/{id}&quot;,
&quot;hostTemplate&quot;: &quot;http://localhost:8081/midonet-api/hosts/{id}&quot;,
&quot;uri&quot;: &quot;http://localhost:8081/midonet-api/&quot;,
&quot;vteps&quot;: &quot;http://localhost:8081/midonet-api/vteps&quot;,
&quot;tunnelZoneTemplate&quot;: &quot;http://localhost:8081/midonet-api/tunnel_zones/{id}&quot;,
&quot;ipAddrGroups&quot;: &quot;http://localhost:8081/midonet-api/ip_addr_groups&quot;,
&quot;writeVersion&quot;: &quot;http://localhost:8081/midonet-api/write_version&quot;,
&quot;chainTemplate&quot;: &quot;http://localhost:8081/midonet-api/chains/{id}&quot;,
&quot;vtepTemplate&quot;: &quot;http://localhost:8081/midonet-api/vteps/{ipAddr}&quot;,
&quot;adRouteTemplate&quot;: &quot;http://localhost:8081/midonet-api/ad_routes/{id}&quot;,
&quot;bgpTemplate&quot;: &quot;http://localhost:8081/midonet-api/bgps/{id}&quot;,
&quot;hosts&quot;: &quot;http://localhost:8081/midonet-api/hosts&quot;,
&quot;routeTemplate&quot;: &quot;http://localhost:8081/midonet-api/routes/{id}&quot;,
&quot;ruleTemplate&quot;: &quot;http://localhost:8081/midonet-api/rules/{id}&quot;,
&quot;systemState&quot;: &quot;http://localhost:8081/midonet-api/system_state&quot;,
&quot;vips&quot;: &quot;http://localhost:8081/midonet-api/vips&quot;,
&quot;pools&quot;: &quot;http://localhost:8081/midonet-api/pools&quot;,
&quot;routers&quot;: &quot;http://localhost:8081/midonet-api/routers&quot;,
&quot;bridges&quot;: &quot;http://localhost:8081/midonet-api/bridges&quot;,
&quot;chains&quot;: &quot;http://localhost:8081/midonet-api/chains&quot;,
&quot;portGroups&quot;: &quot;http://localhost:8081/midonet-api/port_groups&quot;,
&quot;poolMembers&quot;: &quot;http://localhost:8081/midonet-api/pool_members&quot;,
&quot;tunnelZones&quot;: &quot;http://localhost:8081/midonet-api/tunnel_zones&quot;
}
</code></pre>

<p>なんとなく引数にこれらの文字列を渡せばいいのだなと分かります。</p>

<p>次に neutron の管理している subnets を確認してみましょう。</p>

<pre><code class="language-bash">% curl -i -X GET http://localhost:8081/midonet-api/neutron/subnets -H &quot;User-Agent: python-keystoneclient&quot; -H &quot;X-Auth-Token: &lt;TokenID&gt;&quot;
</code></pre>

<p>レスポンス</p>

<pre><code class="language-json">[
  {
    &quot;enable_dhcp&quot;: false,
    &quot;tenant_id&quot;: &quot;65f7012145d84ac5afc36572eabe5b09&quot;,
    &quot;host_routes&quot;: [],
    &quot;dns_nameservers&quot;: [],
    &quot;id&quot;: &quot;3dbe5cff-8a8c-4790-85b5-b789d8ede863&quot;,
    &quot;name&quot;: &quot;public-subnet&quot;,
    &quot;cidr&quot;: &quot;200.200.200.0/24&quot;,
    &quot;shared&quot;: false,
    &quot;ip_version&quot;: 4,
    &quot;network_id&quot;: &quot;45269fba-e32f-40b0-a542-f5cfe34ce1a1&quot;,
    &quot;gateway_ip&quot;: &quot;200.200.200.1&quot;,
    &quot;allocation_pools&quot;: [
      {
        &quot;last_ip&quot;: null,
        &quot;first_ip&quot;: null
      }
    ]
  },
  {
    &quot;enable_dhcp&quot;: true,
    &quot;tenant_id&quot;: &quot;f34b4398015546b8b84f50c731ed6c51&quot;,
    &quot;host_routes&quot;: [],
    &quot;dns_nameservers&quot;: [],
    &quot;id&quot;: &quot;3dbcf04a-9738-4b1f-b084-76f2a4b17cbc&quot;,
    &quot;name&quot;: &quot;private-subnet&quot;,
    &quot;cidr&quot;: &quot;10.0.0.0/24&quot;,
    &quot;shared&quot;: false,
    &quot;ip_version&quot;: 4,
    &quot;network_id&quot;: &quot;2edb78c3-0f23-4e29-a3e6-cc97f55baa6a&quot;,
    &quot;gateway_ip&quot;: &quot;10.0.0.1&quot;,
    &quot;allocation_pools&quot;: [
      {
        &quot;last_ip&quot;: null,
        &quot;first_ip&quot;: null
      }
    ]
  }
]
</code></pre>

<p>２つのサブネットが確認出来ました。</p>

<h2 id="まとめ">まとめ</h2>

<p>勉強不足でまだ全く midonet で出来る事がわからない..汗。でもとりあえず動かせた
し、API も引っ張れるのでこれから色々試せそうですね。OSS 化されたことで、コミュ
ニティの間でも使われていくことも想像出来ますし、自分たち技術者としてはとても有
り難いことでした。</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2014/11/04/midostack/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>


        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2014/07/16/chef-container/">Chef-Container Beta を使ってみる</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2014-07-16'>
            July 16, 2014</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは。<a href="https://twitter.com/jedipunkz">@jedipunkz</a> です。</p>

<p>昨晩 Chef が Chef-Container を発表しました。</p>

<ul>
<li><a href="http://www.getchef.com/blog/2014/07/15/release-chef-container-0-2-0-beta/">http://www.getchef.com/blog/2014/07/15/release-chef-container-0-2-0-beta/</a></li>
<li><a href="http://docs.opscode.com/containers.html">http://docs.opscode.com/containers.html</a></li>
</ul>

<p>まだ Beta リリースでバージョンは 0.2.0 です。(gem だと 0.1.1)</p>

<p>Docker を代表とするコンテナ周りの技術が最近、盛んにリリースされていますし、今
後クラウドプラットフォーム上でコンテナを使ってアプリを動かすケースも増えてくる
のではないでしょうか。Dockerfile を使っても Chef-Solo を使ってソフトウェアをデ
プロイ出来るのだけどそれだけだとしんどいので、コンテナに特化した Chef が出てき
たってことだと思います。特徴として SSH でログインしてブートストラップするので
はなくて Runit + Chef-init を用いてコンテナにデプロイすることが挙げられます。</p>

<p>では実際に使ってみたのでその時の手順をまとめてみます。</p>

<h2 id="事前に用意する環境">事前に用意する環境</h2>

<p>下記のソフトウェアを予めインストールしておきましょう。</p>

<ul>
<li>docker</li>
<li>chef</li>
<li>berkshelf</li>
</ul>

<p>ここで注意なのですが後に knife コマンドを使って Docker イメージをビルドします。
つまり root 権限が必要です。rbenv 等を使って ruby, chef をインストールすると、
辛いかもしれませんので OS のパッケージを使ってインストールすると良いと思います。
この辺りは今後改善策が出てくるかも&hellip;。</p>

<p>尚、インストール方法はここでは割愛します。</p>

<h2 id="chef-container-のインストール">Chef-Container のインストール</h2>

<p>下記の2つの Gems をインストールします。</p>

<ul>
<li>knife-container</li>
<li>chef-container</li>
</ul>

<pre><code class="language-bash">% sudo gem install knife-container
% sudo gem install chef-container
</code></pre>

<h2 id="使用方法">使用方法</h2>

<p>まず knife コマンドを使って操作に必要なディレクトリとファイルを生成します。</p>

<pre><code class="language-bash">% knife container docker init chef/ubuntu-12.04 -r 'recipe[apache2]' -z -b
</code></pre>

<p>ここで &lsquo;chef/ubuntu-12.04&rsquo; は Docker のイメージ名です。chef-init 等の環境が予
め入っていました。このイメージ以外では今のところ動作を確認していません..。これは後にまとめで触れます。</p>

<p>上記のコマンドの結果で得られるディレクトリとファイル達です。</p>

<pre><code>.
└── dockerfiles
    └── chef
        └── ubuntu-12.04
            ├── Berksfile
            ├── chef
            │   ├── first-boot.json
            │   └── zero.rb
            └── Dockerfile
</code></pre>

<p>また dockerfiles/chef/ubuntu-12.04/Dockerfile を確認すると&hellip;</p>

<pre><code># BASE chef/ubuntu-12.04:latest
FROM chef/ubuntu-12.04
ADD chef/ /etc/chef/
RUN chef-init --bootstrap
RUN rm -rf /etc/chef/secure/*
ENTRYPOINT [&quot;chef-init&quot;]
CMD [&quot;--onboot&quot;]
</code></pre>

<p>イメージを取得 -&gt; ディレクトリ同期 -&gt; chef-init 実行 -&gt; /etc/chef/secure 配下削除、と
実行しているようです。</p>

<p>次に first-boot.json という名前のファイルを生成します。chef-init が解釈するファ
イルです。</p>

<pre><code class="language-json">{
   &quot;run_list&quot;: [
      &quot;recipe[apache2]&quot;
   ],
   &quot;container_service&quot;: {
      &quot;apache2&quot;: {
         &quot;command&quot;: &quot;/usr/sbin/apache2 -k start&quot;
      }
   }
}
</code></pre>

<p>ではいよいよ knife コマンドで Docker イメージをビルドします。</p>

<pre><code class="language-bash">% sudo knife container docker build chef/ubuntu-12.04 -z
</code></pre>

<p>すると、下記のように Docker イメージが出来上がります。</p>

<pre><code class="language-bash">% sudo docker images
REPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE
chef/ubuntu-12.04   11                  03fd2357596f        4 days ago          397.7 MB
chef/ubuntu-12.04   11.12               03fd2357596f        4 days ago          397.7 MB
chef/ubuntu-12.04   11.12.8             03fd2357596f        4 days ago          397.7 MB
</code></pre>

<p>出来上がったイメージを利用してコンテナを稼働します。</p>

<pre><code class="language-bash">% sudo docker run chef/ubuntu-12.04
% sudo docker ps
CONTAINER ID        IMAGE               COMMAND              CREATED             STATUS              PORTS               NAMES
191cfdaf0bdb        650a89f73ed8        chef-init --onboot   39 minutes ago      Up 39 minutes                           agitated_almeida
</code></pre>

<h2 id="まとめ">まとめ</h2>

<p>コンテナと言っても今現在は Docker のみに対応しているようです。また init の際に指定する Docker イメージ
の中に chef-init が入っている必要がありそうです。Build する前に予めイメージを作っておく必要があるという
のはしんどいので、今後改善されるかもしれません。</p>

<p>そもそも Docker やコンテナ技術の登場で Puppet, Chef を代表とするツール類が不要になるのでは？という議論が
幾つかの場面であったように思います。つまりコンテナのイメージに予めソフトウェアを配布しそれを用いて稼働
することで、マシンが起動した後にデプロイすることが必要ないよね？という発想です。今回紹介したようにコンテナの
イメージを生成するのに Chef を用いるということであれば、また別の議論になりそうです。また稼働したコンテナに
ソフトウェアをデプロイすることも場合によっては必要なので、この辺りの技術の完成度が上がることを期待したいです。</p>

<h2 id="参考-url">参考 URL</h2>

<ul>
<li>CreationLine さんブログ <a href="http://www.creationline.com/lab/5346">http://www.creationline.com/lab/5346</a></li>
<li>公式サイト <a href="http://docs.opscode.com/containers.html">http://docs.opscode.com/containers.html</a></li>
</ul>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2014/07/16/chef-container/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>


        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2014/06/22/jtf2014-ceph/">JTF2014 で Ceph について話してきた！</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2014-06-22'>
            June 22, 2014</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは。<a href="https://twitter.com/jedipunkz">@jedipunkz</a> です。</p>

<p>今日、JTF2014 (July Tech Festa 2014) というイベントで Ceph のことを話してきま
した。Ceph ユーザ会の会員として話してきたのですが Ceph ユーザ会は実は最近発足
したばかりのユーザ会で、まだまだ活動が活発ではありません。もし興味がある方いらっ
しゃいましたら是非参加よろしくお願いしますー。下記の Google Groups になります。</p>

<p><a href="https://groups.google.com/forum/#!forum/ceph-jp">https://groups.google.com/forum/#!forum/ceph-jp</a></p>

<p>ユーザ会としての勉強会として初になるのですが、今回このイベントで自分は
Ceph-Deploy について話してきました。とりあえず皆さんに使ってもらいたかったので
この話をしてきました。が、予定時間がメチャ短かったので超絶早口で頑張った分、皆
さんに理解してもらえなかった気がしてちょっと反省&hellip;。なので、このブログを利用
して少し細くさせてもらいます。</p>

<p>今日の発表資料はこちらです！</p>

<script async class="speakerdeck-embed"
data-id="592a0b90ceb30131a5d25ae3f95c3a1a" data-ratio="1.33333333333333" src="//speakerdeck.com/assets/embed.js"></script>

<p>今日のテーマは 「Ceph-Deploy を使って Ceph を構築してみる」だったのですが、下
記のテーマを持って資料を作っています。</p>

<ul>
<li>単にミニマム構成ではなく運用を考慮した実用性のある構成</li>
<li>OSD, MON, MDS の各プロセスとノード・ディスクの数の関係を知ってもらう</li>
</ul>

<p>特に「実用性のある..」は意識したつもりでした。そのために前提とした構成に下記の
特徴を持たせています。(資料 6 ページ目に構成図があります。確認してみてください。)</p>

<ul>
<li>オブジェクト格納用ディスクは複数/ノードを前提</li>
<li>OSD レプリケーションのためのクラスタネットワークを用いる構成</li>
<li>OSD の扱うジャーナル格納用ディスクは高速な SSD を用いる</li>
<li>MDS は利用する HW リソースの特徴が異なるので別ノードへ配置</li>
</ul>

<p>ストレージ全体を拡張したければ</p>

<ul>
<li>図中 ceph01-03 の様なノードを増設する</li>
<li>ceph01-03 にディスクとそれに対する OSD を増設する</li>
</ul>

<p>ですが、前者がベストでしょう。ノード増設の場合 ceph-deploy を用いて</p>

<ul>
<li>ceph-deploy mon create &lt;新規ホスト名&gt; で MON を稼働</li>
<li>ceph-dploy disk zap, osd create で OSD を稼働</li>
</ul>

<p>で簡単に可能です。MDS の増設も負荷状況を見ながらするといいでしょう。自分はまだ
Ceph を運用していないので、各プロセスがどのようなリソースの消費の仕方をするの
か知りません。MDS がどのような数で運用していくべきなのか。早く運用から得たノウ
ハウが共有されないかなぁと期待しています。</p>

<p>また今回話すのを忘れたのですが SSD をジャーナル格納用ディスクとして用いたのは
ハードディスクに対して高速でアクセス出来ること・またメタデータはファイルオブジェ
クトに対して小容量で済む、といった理由からです。メタデータを扱うのに適している
と思います。また将来的には幾つかの KVS データベースソフトウェアをメタデータ管
理に使う実装がされるそうです。</p>

<p>以上です。皆さん、是非 Ceph を使ってみてください！ また興味のある方はユーザ会
への加入をご検討くださいー。</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2014/06/22/jtf2014-ceph/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Categories
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
        <li><a href='/categories/report'>report</a></li>
    
</ul>

    </footer>
</article>


        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2014/06/13/mesos-marathon-deimos-docker/">Mesos &#43; Marathon &#43; Deimos &#43; Docker を試してみた!</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2014-06-13'>
            June 13, 2014</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは。@jedipunkz です。</p>

<p>以前 Mesos, Docker について記事にしました。</p>

<p><a href="http://jedipunkz.github.io/blog/2013/09/28/mesos-architecture-number-1/">http://jedipunkz.github.io/blog/2013/09/28/mesos-architecture-number-1/</a>
<a href="http://jedipunkz.github.io/blog/2013/10/01/methos-architecture-number-2-docker-on-mesos/">http://jedipunkz.github.io/blog/2013/10/01/methos-architecture-number-2-docker-on-mesos/</a></p>

<p>Twitter で Docker 関連のオーケストレーションツールについて呟いていたら @everpeace さんから
こんな情報をもらいました。</p>

<p><blockquote class="twitter-tweet" lang="ja"><p><a href="https://twitter.com/jedipunkz">@jedipunkz</a> 元々meos-dockerっていうmesos executorがあったんですけど、mesosがcontainer部分をpluggableにしたので、それに合わせてdeimosっていうmesos用のexternal containerizer が作られました。</p>&mdash; Shingo Omura (@everpeace) <a href="https://twitter.com/everpeace/statuses/476998842383347712">2014, 6月 12</a></blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script></p>

<p>Deimos !!! 知らなかった。Mesos の Docker プラグインらしく下記の場所にありました。</p>

<p><a href="https://github.com/mesosphere/deimos">https://github.com/mesosphere/deimos</a></p>

<p>色々調べいたら、こんな資料が見つかりました。どうやらまだ公開されて4日しか経っていないようです。</p>

<p><a href="http://mesosphere.io/learn/run-docker-on-mesosphere/">http://mesosphere.io/learn/run-docker-on-mesosphere/</a></p>

<p>Mesos + Marathon + Deimos + Docker をオールインワン構成で構築する手順が書かれています。</p>

<p>内容はほぼ同じですが、一応自分がやってみて理解したことをまとめたいので下記に記していきます。</p>

<h2 id="構築してみる">構築してみる</h2>

<p>手順をまとめてスクリプトにしました。パッケージは Ubuntu 13.10 用のようですが 14.04 のホスト
で実行出来ました。14.04 のパッケージはまだ見つかっていません。</p>

<pre><code class="language-bash">#!/bin/bash
# disable ipv6
echo 'net.ipv6.conf.all.disable_ipv6 = 1' | sudo tee -a /etc/sysctl.conf
echo 'net.ipv6.conf.default.disable_ipv6 = 1' | sudo tee -a /etc/sysctl.conf
sudo sysctl -p

# install related tools
sudo apt-get update
sudo apt-get -y install curl python-setuptools python-pip python-dev python-protobuf

# install zookeeper
sudo apt-get -y install zookeeperd
echo 1 | sudo dd of=/var/lib/zookeeper/myid

# install docker
sudo apt-get -y install docker.io
sudo ln -sf /usr/bin/docker.io /usr/local/bin/docker
sudo sed -i '$acomplete -F _docker docker' /etc/bash_completion.d/docker.io
sudo docker pull libmesos/ubuntu

# install mesos
curl -fL http://downloads.mesosphere.io/master/ubuntu/13.10/mesos_0.19.0-xcon3_amd64.deb -o /tmp/mesos.deb
sudo dpkg -i /tmp/mesos.deb
sudo mkdir -p /etc/mesos-master
echo in_memory  | sudo dd of=/etc/mesos-master/registry
curl -fL http://downloads.mesosphere.io/master/ubuntu/13.10/mesos_0.19.0-xcon3_amd64.egg -o /tmp/mesos.egg
sudo easy_install /tmp/mesos.egg

# install marathon
curl -fL http://downloads.mesosphere.io/marathon/marathon_0.5.0-xcon2_noarch.deb -o /tmp/marathon.deb
sudo dpkg -i /tmp/marathon.deb

# restart each services
sudo service docker.io restart
sudo service zookeeper restart
sudo service mesos-master restart
sudo service mesos-slave restart

# install deimos
sudo pip install deimos
sudo mkdir -p /etc/mesos-slave

## Configure Deimos as a containerizer
echo /usr/bin/deimos  | sudo dd of=/etc/mesos-slave/containerizer_path
echo external     | sudo dd of=/etc/mesos-slave/isolation
</code></pre>

<h2 id="プロセスの確認">プロセスの確認</h2>

<p>実行が終わると各プロセスが確認出来ます。オプションでどのプロセスが何を見ているか大体
わかりますので見ていきます。</p>

<h4 id="mesos-master">mesos-master</h4>

<p>mesos-master は zookeeper を参照して 5050 番ポートで起動しているようです。</p>

<pre><code class="language-bash">% ps ax | grep mesos-master
 1224 ?        Ssl    0:30 /usr/local/sbin/mesos-master --zk=zk://localhost:2181/mesos --port=5050 --log_dir=/var/log/mesos --registry=in_memory
</code></pre>

<h4 id="mesos-slave">mesos-slave</h4>

<p>mesos-slave は同じく zookeeper を参照して containerizer を deimos として稼働していることが
わかります。</p>

<pre><code class="language-bash">% ps ax | grep mesos-slave
 1225 ?        Ssl    0:12 /usr/local/sbin/mesos-slave --master=zk://localhost:2181/mesos --log_dir=/var/log/mesos --containerizer_path=/usr/bin/deimos --isolation=external
</code></pre>

<h4 id="zookeeper">zookeeper</h4>

<p>zookeeper は OpenJDK7 で稼働している Java プロセスです。</p>

<pre><code class="language-bash">% ps ax | grep zookeeper
 1073 ?        Ssl    1:07 /usr/bin/java -cp /etc/zookeeper/conf:/usr/share/java/jline.jar:/usr/share/java/log4j-1.2.jar:/usr/share/java/xercesImpl.jar:/usr/share/java/xmlParserAPIs.jar:/usr/share/java/netty.jar:/usr/share/java/slf4j-api.jar:/usr/share/java/slf4j-log4j12.jar:/usr/share/java/zookeeper.jar -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.local.only=false -Dzookeeper.log.dir=/var/log/zookeeper -Dzookeeper.root.logger=INFO,ROLLINGFILE org.apache.zookeeper.server.quorum.QuorumPeerMain /etc/zookeeper/conf/zoo.cfg
</code></pre>

<h4 id="docker">docker</h4>

<p>docker が起動していることも確認できます。設定は特にしていないです。</p>

<pre><code class="language-bash">% ps axuw | grep docker
root       831  0.0  0.3 364776 14924 ?        Sl   01:30   0:01 /usr/bin/docker.io -d
</code></pre>

<h2 id="marathon-の-webui-にアクセス">Marathon の WebUI にアクセス</h2>

<p>Marathon の WebUI にアクセスしてみましょう。</p>

<p><img src="http://jedipunkz.github.com/pix/deimos_01.png"></p>

<p>まだ何も Tasks が実行されていないので一覧には何も表示されないと思います。</p>

<h2 id="tasks-の実行">Tasks の実行</h2>

<p>Marathon API に対してクエリを発行することで Mesos の Tasks として Docker コンテナを稼働させることが出来ます。
下記のファイルを ubuntu.json として保存。</p>

<pre><code class="language-json">{
    &quot;container&quot;: {
    &quot;image&quot;: &quot;docker:///libmesos/ubuntu&quot;
  },
  &quot;id&quot;: &quot;ubuntu&quot;,
  &quot;instances&quot;: &quot;1&quot;,
  &quot;cpus&quot;: &quot;.5&quot;,
  &quot;mem&quot;: &quot;512&quot;,
  &quot;uris&quot;: [ ]
}
</code></pre>

<p>下記の通り localhost:8080 が Marathon API の Endpoint になっているのでここに対して作成した JSON を POST します。</p>

<pre><code class="language-bash">% curl -X POST -H &quot;Content-Type: application/json&quot; localhost:8080/v2/apps -d@ubuntu.json
</code></pre>

<p>Tasks の一覧を取得してみます。</p>

<pre><code class="language-bash">% curl -X GET -H &quot;Content-Type: application/json&quot; localhost:8080/v2/apps
{&quot;apps&quot;:[{&quot;id&quot;:&quot;ubuntu&quot;,&quot;cmd&quot;:&quot;&quot;,&quot;env&quot;:{},&quot;instances&quot;:1,&quot;cpus&quot;:0.5,&quot;mem&quot;:512.0,&quot;executor&quot;:&quot;&quot;,&quot;constraints&quot;:[],&quot;uris&quot;:[],&quot;ports&quot;:[13049],&quot;taskRateLimit&quot;:1.0,&quot;container&quot;:{&quot;image&quot;:&quot;docker:///libmesos/ubuntu&quot;,&quot;options&quot;:[]},&quot;version&quot;:&quot;2014-06-13T01:45:58.693Z&quot;,&quot;tasksStaged&quot;:1,&quot;tasksRunning&quot;:0}]}
</code></pre>

<p>Tasks の一覧が JSON で返ってきます。id : ubuntu, インスタンス数 : 1, CPU 0.5, メモリー : 512MB で
Task が稼働していることが確認出来ます。</p>

<p>ここで WebUI 側も見てみましょう。</p>

<p><img src="http://jedipunkz.github.com/pix/deimos_05.png"></p>

<p>一つ Task が稼働していることが確認出来ると思います。</p>

<p><img src="http://jedipunkz.github.com/pix/deimos_04.png"></p>

<p>その Task をクリックすると詳細が表示されます。</p>

<p>次に Tasks のスケーリングを行ってみましょう。
下記の通り ubuntu.json を修正し instances : 2 とする。これによってインスタンス数が2に増えます。</p>

<pre><code class="language-json">{
    &quot;container&quot;: {
    &quot;image&quot;: &quot;docker:///libmesos/ubuntu&quot;
  },
  &quot;id&quot;: &quot;ubuntu&quot;,
  &quot;instances&quot;: &quot;2&quot;,
  &quot;cpus&quot;: &quot;.5&quot;,
  &quot;mem&quot;: &quot;512&quot;,
  &quot;uris&quot;: [ ]
}
</code></pre>

<p>修正した JSON を POST します。</p>

<pre><code class="language-bash">% curl -X PUT -H &quot;Content-Type: application/json&quot; localhost:8080/v2/apps/ubuntu -d@ubuntu.json
</code></pre>

<p>Tasks の一覧を取得し containers が 2 になっていることが確認できます。</p>

<pre><code class="language-bash">% curl -X GET -H &quot;Content-Type: application/json&quot; localhost:8080/v2/apps
{&quot;apps&quot;:[{&quot;id&quot;:&quot;ubuntu&quot;,&quot;cmd&quot;:&quot;&quot;,&quot;env&quot;:{},&quot;instances&quot;:2,&quot;cpus&quot;:0.5,&quot;mem&quot;:512.0,&quot;executor&quot;:&quot;&quot;,&quot;constraints&quot;:[],&quot;uris&quot;:[],&quot;ports&quot;:[17543],&quot;taskRateLimit&quot;:1.0,&quot;container&quot;:{&quot;image&quot;:&quot;docker:///libmesos/ubuntu&quot;,&quot;options&quot;:[]},&quot;version&quot;:&quot;2014-06-13T02:40:04.536Z&quot;,&quot;tasksStaged&quot;:3,&quot;tasksRunning&quot;:0}]}
</code></pre>

<p>最後に Tasks を削除してみましょう。</p>

<pre><code class="language-bash">% curl -X DELETE -H &quot;Content-Type: application/json&quot; localhost:8080/v2/apps/ubuntu
</code></pre>

<p>Tasks が削除されたことを確認します。</p>

<pre><code class="language-bash">% curl -X GET -H &quot;Content-Type: application/json&quot; localhost:8080/v2/apps
{&quot;apps&quot;:[]}
</code></pre>

<h2 id="marathon-api-v2">Marathon API v2</h2>

<p>Marathon API v2 について下記の URL に仕様が載っています。上記に記したクエリ以外にも色々載っているので
動作を確認してみるといいと思います。</p>

<p><a href="https://github.com/mesosphere/marathon/blob/master/docs/api/http/REST_template.md">https://github.com/mesosphere/marathon/blob/master/docs/api/http/REST_template.md</a></p>

<h2 id="まとめ">まとめ</h2>

<p>オールインワン構成が出来ました。また動作確認も無事出来ています。
以前試した時よりも大分、手順が簡潔になった印象があります。また参考資料中に</p>

<p>&ldquo;checkout our other multi-node tutorials on how to scale Docker in your data center.&rdquo;</p>

<p>とありますが、まだ見つかっていません(´・ω・｀)見つかった方教えてくださいー。</p>

<p>以前試した時は Mesos-Master の冗長化が出来なかったので今回こそ Multi Mesos-Masters,
Multi Mesos-Slaves の構成を作ってみたいと思います。</p>

<p>また今月？になって続々と Docker のオーケストレーションツールを各社が公開しています。</p>

<h5 id="centurion">centurion</h5>

<p>New Relic が開発したオーケストレーションツール。
<a href="https://github.com/newrelic/centurion">https://github.com/newrelic/centurion</a></p>

<h5 id="helios">helios</h5>

<p>Spotify が開発したオーケストレーションツール。
<a href="https://github.com/spotify/helios">https://github.com/spotify/helios</a></p>

<h5 id="fleet">fleet</h5>

<p>CoreOS 標準搭載。
<a href="https://github.com/coreos/fleet">https://github.com/coreos/fleet</a></p>

<h5 id="geard">geard</h5>

<p>RedHat が Red Hat Enterprise Linux Atomic Host に搭載しているツール。
<a href="http://openshift.github.io/geard/">http://openshift.github.io/geard/</a></p>

<h5 id="kubernetes">Kubernetes</h5>

<p>Google が開発したオーケストレーションツール。
<a href="https://github.com/GoogleCloudPlatform/kubernetes">https://github.com/GoogleCloudPlatform/kubernetes</a></p>

<h5 id="shipper">shipper</h5>

<p>Python のコードで Docker をオーケストレーション出来るツール。
<a href="https://github.com/mailgun/shipper">https://github.com/mailgun/shipper</a></p>

<p>幾つか試したのですが、まだまだ動く所までいかないツールがありました。github の README にも
&ldquo;絶賛開発中なのでプロダクトレディではない&rdquo; と書かれています。これからでしょう。</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2014/06/13/mesos-marathon-deimos-docker/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>


        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2014/05/29/fog-aws-ec2-elb/">クラウドライブラリ Fog で AWS を操作！..のサンプル</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2014-05-29'>
            May 29, 2014</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは。<a href="https://twitter.com/jedipunkz">@jedipunkz</a> です。</p>

<p>最近 OpenStack でサービスを開発！.. じゃなくて AWS でプロトタイプサービス作っ
ているのですが、Ruby で開発したかったので Fog を使っています。EC2 と ELB の
API を叩くコードになりつつあるのですが、サンプルコードって世の中に中々無いと気
がついたので、このブログ記事にサンプルコードを載せたいと思います。</p>

<h2 id="fog-とは">Fog とは ?</h2>

<p>Fog <a href="http://fog.io/">http://fog.io/</a> はクラウドライブラリソフトウェアです。AWS, Rackspace,
CloudStack, OpenStack .. と数ある世の中のクラウドプラットフォームを扱うために
用意されたソフトウェアです。対応しているプラットフォームの種別は下記を見ると参
考になります。</p>

<p><a href="http://fog.io/about/provider_documentation.html">http://fog.io/about/provider_documentation.html</a></p>

<p>ドキュメントがまだまだ揃っていなく、Fog のコードを覗きながら実装するしかない状
況です。なので「こう使えば良い！」というお手本があまりネット上にも無い気がしま
す。</p>

<p>ドキュメントは一応下記にあります。
が使い方がよくわからない・・！(´；ω；｀)ﾌﾞﾜｯ</p>

<p><a href="http://rubydoc.info/gems/fog/frames/index">http://rubydoc.info/gems/fog/frames/index</a></p>

<h2 id="ec2-インスタンスを使ってみる">EC2 インスタンスを使ってみる</h2>

<p>まずは AWS EC2 の API を叩いて t1.micro インスタンスを立ち上げてみましょう。</p>

<pre><code class="language-ruby">require 'fog'

compute = Fog::Compute.new({
  :provider =&gt; 'AWS',
  :aws_access_key_id =&gt; '....',
  :aws_secret_access_key =&gt; '....',
  :region =&gt; 'ap-northeast-1'
})

server = compute.servers.create(
  :image_id =&gt; 'ami-cedaa2bc',
  :flavor_id =&gt; 't1.micro',
  :key_name =&gt; 'test_key',
  :tags =&gt; {'Name' =&gt; 'test'},
  :groups =&gt; 'ssh-secgroup'
)

server.wait_for { print &quot;.&quot;; ready? }

puts &quot;created instance name :&quot;, server.dns_name
</code></pre>

<h4 id="解説">解説</h4>

<ul>
<li>compute = &hellip; とあるところで接続情報を記しています。</li>
</ul>

<p>&ldquo;ACCESS_KEY_ID&rdquo; や &ldquo;SECRET_ACCESS_KEY&rdquo; はみなさん接続する時にお持ちですよね。それ
とリージョン名やプロバイダ名 (ここでは AWS) を記して AWS の API に接続します。</p>

<ul>
<li>server = &hellip; とあるところで実際にインスタンスを作成しています。</li>
</ul>

<p>ここではインスタンス生成に必要な情報を盛り込んでいます。Flavor 名や AMI イメー
ジ名・SSH 鍵の名前・セキュリティグループ名等です。</p>

<h2 id="便利なメソッド">便利なメソッド</h2>

<p>server = &hellip; でインスタンスを生成すると便利なメソッドを扱って情報を読み込むこ
とが出来ます。</p>

<pre><code class="language-ruby">server.dns_name # =&gt; public な DNS 名を取得
server.private_dns_name # =&gt; private な DNS 名を取得
server.id # =&gt; インスタンス ID を取得
server.availability_zone # =&gt; Availability Zone を取得
server.public_ip_address # =&gt; public な IP アドレスを取得
server.private_ip_address # =&gt; private な IP アドレスを取得
</code></pre>

<p>これは便利&hellip;</p>

<p>モジュール化して利用
+++</p>

<p>毎回コードの中でこれらの接続情報を書くのはしんどいので、Ruby のモジュールを作
りましょう。</p>

<pre><code class="language-ruby">module AWSCompute
  def self.connect()
    conn = Fog::Compute.new({
      :provider =&gt; 'AWS',
      :aws_access_key_id =&gt; '...',
      :aws_secret_access_key =&gt; '...',
      :region =&gt; '...'
    })
    begin
      yield conn
    ensure
      # conn.close
    end
  rescue Errno::ECONNREFUSED
  end
end
</code></pre>

<p>こう書いておくと例えば&hellip;</p>

<h4 id="インスタンスのターミネイト">インスタンスのターミネイト</h4>

<pre><code class="language-ruby">AWSCompute.connect() do |sock|
  server = sock.servers.get(instance_id)
  server.destroy
  return server.id
end
</code></pre>

<h4 id="インスタンスの起動">インスタンスの起動</h4>

<pre><code class="language-ruby">AWSCompute.connect() do |sock|
  server = sock.servers.get(instance_id)
  server.start
  return server.id
end
</code></pre>

<h4 id="インスタンスの停止">インスタンスの停止</h4>

<pre><code class="language-ruby">AWSCompute.connect() do |sock|
  server = sock.servers.get(instance_id)
  server.stop
  return server.id
end
</code></pre>

<p>等と出来ます。</p>

<h2 id="elb-elastic-loadbalancer-を使ってみる">ELB (Elastic LoadBalancer) を使ってみる</h2>

<p>同様に ELB を扱うコードのサンプルも載せておきます。同じくモジュール化して書くと</p>

<pre><code class="language-ruby">module AWSELB
  def self.connect()
    conn = Fog::AWS::ELB.new(
      :aws_access_key_id =&gt; '...',
      :aws_secret_access_key =&gt; '...',
      :region =&gt; '...',
    )
    begin
      yield conn
    ensure
      # conn.close
    end
  rescue Errno::ECONNREFUSED
  end
end
</code></pre>

<p>としておいて&hellip;</p>

<h4 id="elb-の新規作成">ELB の新規作成</h4>

<p>下記のコードで ELB を新規作成出来ます。</p>

<pre><code class="language-ruby">AWSELB.connect() do |sock|
  availability_zone = '...'
  elb_name = '...'
  listeners = [{ &quot;Protocol&quot; =&gt; &quot;HTTP&quot;, &quot;LoadBalancerPort&quot; =&gt; 80, &quot;InstancePort&quot; =&gt; 80, &quot;InstanceProtocol&quot; =&gt; &quot;HTTP&quot; }]
  result = sock.create_load_balancer(availability_zone, elb_name, listeners)
  p result
end
</code></pre>

<p>この状態では ELB に対してインスタンスが紐付けられていないので使えません。下記の操作で
インスタンスを紐付けてみましょう。</p>

<pre><code class="language-ruby">AWSELB.connect() do |sock|
  insntance_id = '...'
  elb_name = '...'
  result = sock.register_instances_with_load_balancer(instance_id, elbname)
  p result
end
</code></pre>

<p>insntance_id には紐付けたいインスタンスの ID を、elb_name には先ほど作成した ELB の名前を
入力します。 この操作を繰り返せば AWS 上にクラスタが構成出来ます。</p>

<p>逆にクラスタからインスタンスの削除したい場合は下記の通り実行します。</p>

<pre><code class="language-ruby">AWSELB.connect() do |sock|
  result = sock.deregister_instances_from_load_balancer(instance_id, elbname)
  p result
end
</code></pre>

<h2 id="まとめ">まとめ</h2>

<p>今回は Fog を紹介しましたが Python 使いの方には libcloud をおすすめします。</p>

<p><a href="https://libcloud.apache.org/">https://libcloud.apache.org/</a></p>

<p>Apache ファウンデーションが管理しているクラウドライブラリです。こちらも複数の
クラウドプラットフォームに対応しているようです。</p>

<p>Fog で OpenStack も操作したことがあるのですが、AWS 用のコードの方が完成度が高
いのか、戻り値などが綺麗に整形されていて扱いやすかったり、メソッドも豊富に用意
されていたりという印象でした。これは&hellip; OpenStack 用の Fog コードにコントリビュー
トするチャンス・・！</p>

<p>皆さんもサンプルコードお持ちでしたら、ブログ等で公開していきましょうー。
ではでは。</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2014/05/29/fog-aws-ec2-elb/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>


        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2014/04/25/stackforge-openstack-chef-repo-icehouse-deploy/">stackforge/openstack-chef-repo で OpenStack Icehouse デプロイ</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2014-04-25'>
            April 25, 2014</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは。<a href="https://twitter.com/jedipunkz">@jedipunkz</a> です。</p>

<p>またまた OpenStack のデプロイをどうするか？についてです。</p>

<p>今まで自分の中では Rackspace Private Cloud で使われている Rackspace 管理の
rcbops/chef-cookbooks が今現在使うならベストの選択だと思っていました。これは内
部で Chef が使われていてしかも Cookbooks が Github 上で公開されています。
Apache ライセンスで使えるので、サービス構築にも使えちゃうというモノ。</p>

<p>先日、ある OpenStack コアデベロッパーの方から「jedipunkz さん、やっぱり rcbops
がいいですか？運営とかどうなっているんでしょう？マージの規準とかどうなのかな？」
と質問受けました。確かにマージの基準は Rackspace Private Cloud がベースになり
ますし、管理しているエンジニアの一覧を見ていると Rackspace 社のエンジニアがメ
インですし、今後どうなるのか分からない&hellip;。</p>

<p>逃げ道は用意したほうが良さそう。</p>

<p>ということで、以前自分も暑かったことのある StackForge の openstack-chef-repo
を久々に使ってみました。Icehouse 構成がこの時点で既に組めるようになっていて、
以前よりだい〜ぶ完成度増した感があります。今回は nova-network 構成を作ってみた
のですが、Neutron 構成ももちろん出来そうなので後に調べてまた公開したいです。</p>

<h2 id="stackforge-とは">StackForge とは</h2>

<p>StackForge は OpenStack のデプロイ・CI の仕組みとして公式に用いられているもの。
公式サイトは下記の場所にある。</p>

<p><a href="http://ci.openstack.org/stackforge.html">http://ci.openstack.org/stackforge.html</a></p>

<p>StackForge の openstack-chef-repo は下記の場所にある。</p>

<p><a href="https://github.com/stackforge/openstack-chef-repo">https://github.com/stackforge/openstack-chef-repo</a></p>

<p>openstack-chef-repo はまだ &lsquo;stable/icehouse&rsquo; ブランチが生成されていない。が直
ちに master からブランチが切られる様子。</p>

<h2 id="目的">目的</h2>

<p>StackForge の openstack-chef-repo を用いて Icehouse リリース版の OpenStack を
デプロイするための方法を記す。今回は未だ &lsquo;stable/icehouse&rsquo; ブランチが無いので
master ブランチを用いて Icehouse リリース版 OpenStack を構築する。</p>

<h2 id="構成">構成</h2>

<p>構成はこんな感じ。</p>

<pre><code>   +-----------------+
   |    GW Router    |
+--+-----------------+
|  |
|  +-------------------+-------------------+--------------------------------------
|  |eth0               |eth0               |eth0                      VM Network (fixed)
|  +-----------------+ +-----------------+ +-----------------+ +-----------------+ 
|  | Controller Node | |  Compute Node   | |  Compute Node   | | Chef Workstation|
|  +-----------------+ +-----------------+ +-----------------+ +-----------------+ 
|  |eth1               |eth1               |eth1               |  
+--+-------------------+-------------------+-------------------+------------------
                                                                 API/Management Network
</code></pre>

<ul>
<li>Nova-Network 構成</li>
<li>今回は fixed network 用の NIC を eth0(物理 NIC) にアサイン</li>
<li>fixed network 用のネットワークをパブリックにする</li>
<li>API/Management Network 側に全ての API を出す。またここから The Internet に迂回出来るようにする</li>
<li>VM Network も GW を介して The Internet へ迂回出来るようにする</li>
<li>全ての操作は &lsquo;Chef Workstaion&rsquo; から行う</li>
<li>Compute ノードはキャパシティの許す限り何台でも可</li>
</ul>

<p>IP 一覧 (この記事での例)</p>

<ul>
<li>Controller : 10.200.9.46 (eth0), 10.200.10.46 (eth1)</li>
<li>Compute    : 10.200.9.47 (eth0), 10.200.10.47 (eth1)</li>
<li>Compute    : 10.200.9.48 (eth0), 10.200.10.48 (eth1)</li>
</ul>

<h2 id="手順">手順</h2>

<p>openstack-chef-repo をワークステーションノード上で取得する。</p>

<pre><code class="language-bash">% git clone https://github.com/stackforge/openstack-chef-repo.git
% cd openstack-chef-repo
</code></pre>

<p>Berksfile があるのでこれを用いて Chef Cookbooks を取得する。</p>

<pre><code class="language-bash">% berks install --path=./cookbooks
</code></pre>

<p>Roles, Cookbooks を Chef サーバにアップロードする。</p>

<pre><code class="language-bash">% knife cookbook upload -o cookbooks -a
% knife role from file roles/*.rb
</code></pre>

<p>1 Environment に対して 1 OpenStack クラスタである。今回構築するクラスタのため
の Environment を作成する。</p>

<p>下記を environments/icehouse-nova-network.rb として生成する。</p>

<pre><code class="language-json">name &quot;icehouse-nova-network&quot;
description &quot;separated nodes environment&quot;

override_attributes(
    &quot;release&quot; =&gt; &quot;icehouse&quot;,
    &quot;osops_networks&quot; =&gt; {
      &quot;management&quot; =&gt; &quot;10.200.10.0/24&quot;,
      &quot;public&quot; =&gt; &quot;10.200.10.0/24&quot;,
      &quot;nova&quot; =&gt; &quot;10.200.10.0/24&quot;
    },
    &quot;mysql&quot; =&gt; {
      &quot;bind_address&quot; =&gt; &quot;0.0.0.0&quot;,
      &quot;root_network_acl&quot; =&gt; &quot;%&quot;,
      &quot;allow_remote_root&quot; =&gt; true,
      &quot;server_root_password&quot; =&gt; &quot;secrete&quot;,
      &quot;server_repl_password&quot; =&gt; &quot;secrete&quot;,
      &quot;server_debian_password&quot; =&gt; &quot;secrete&quot;
    },
    &quot;nova&quot; =&gt; {
      &quot;network&quot; =&gt; {
        &quot;fixed_range&quot; =&gt; &quot;172.18.0.0/24&quot;,
        &quot;public_interface&quot; =&gt; &quot;eth0&quot;
      }
    },
    &quot;rabbitmq&quot; =&gt; {
      &quot;address&quot; =&gt; &quot;10.200.10.46&quot;,
      &quot;port&quot; =&gt; &quot;5672&quot;
    },
    &quot;openstack&quot; =&gt; {
      &quot;developer_mode&quot; =&gt; true,
      &quot;compute&quot; =&gt; {
        &quot;rabbit&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;
        },
        &quot;novnc_proxy&quot; =&gt; {
          &quot;bind_interface&quot; =&gt; &quot;eth1&quot;
        },
        &quot;libvirt&quot; =&gt; {
          &quot;bind_interface&quot; =&gt; &quot;eth1&quot;
        },
        &quot;network&quot; =&gt; {
          &quot;fixed_range&quot; =&gt; &quot;10.200.9.0/24&quot;
        },
        &quot;rabbit_server_chef_role&quot; =&gt; &quot;os-ops-messaging&quot;,
        &quot;networks&quot; =&gt; [
        {
          &quot;label&quot; =&gt; &quot;private&quot;,
          &quot;ipv4_cidr&quot; =&gt; &quot;10.200.9.0/24&quot;,
          &quot;num_networks&quot; =&gt; &quot;1&quot;,
          &quot;network_size&quot; =&gt; &quot;255&quot;,
          &quot;bridge&quot; =&gt; &quot;br200&quot;,
          &quot;bridge_dev&quot; =&gt; &quot;eth0&quot;,
          &quot;dns1&quot; =&gt; &quot;8.8.8.8&quot;,
          &quot;dns2&quot; =&gt; &quot;8.8.4.4&quot;,
          &quot;multi_host&quot; =&gt; &quot;T&quot;
        }
        ]
      },
      &quot;identity&quot; =&gt; {
        &quot;bind_interface&quot; =&gt; &quot;eth1&quot;,
        &quot;users&quot; =&gt; {
          &quot;demo&quot; =&gt; {
            &quot;password&quot; =&gt; &quot;demo&quot;,
            &quot;default_tenant&quot; =&gt; &quot;service&quot;,
            &quot;roles&quot; =&gt; {
              &quot;Member&quot; =&gt; [ &quot;Member&quot; ]
            }
          }
        }
      },
      &quot;image&quot; =&gt; {
        &quot;api&quot; =&gt; {
          &quot;bind_interface&quot; =&gt; &quot;eth1&quot;
        },
        &quot;debug&quot; =&gt; true,
        &quot;identity_service_chef_role&quot; =&gt; &quot;os-identity&quot;,
        &quot;rabbit_server_chef_role&quot; =&gt; &quot;os-ops-messaging&quot;,
        &quot;registry&quot; =&gt; {
          &quot;bind_interface&quot; =&gt; &quot;eth1&quot;
        },
        &quot;syslog&quot; =&gt; {
          &quot;use&quot; =&gt; false
        },
        &quot;upload_image&quot; =&gt; {
          &quot;cirros&quot; =&gt; &quot;http://hypnotoad/cirros-0.3.0-x86_64-disk.img&quot;,
        },
        &quot;upload_images&quot; =&gt; [
          &quot;cirros&quot;
        ]
      },
      &quot;network&quot; =&gt; {
        &quot;api&quot; =&gt; {
          &quot;bind_interface&quot; =&gt; &quot;eth1&quot;,
        }
      },
      &quot;db&quot; =&gt; {
        &quot;bind_interface&quot; =&gt; &quot;eth1&quot;,
        &quot;compute&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;
        },
        &quot;identity&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;
        },
        &quot;image&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;
        },
        &quot;network&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;
        },
        &quot;volume&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;
        },
        &quot;dashboard&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;
        }
      },
      &quot;mq&quot; =&gt; {
        &quot;bind_interface&quot; =&gt; &quot;eth1&quot;,
        &quot;host&quot; =&gt; &quot;10.200.10.46&quot;,
        &quot;user&quot; =&gt; &quot;guest&quot;,
        &quot;vhost&quot; =&gt; &quot;/nova&quot;,
        &quot;servers&quot; =&gt; &quot;10.200.10.46&quot;,
        &quot;compute&quot; =&gt; {
          &quot;service_type&quot; =&gt; &quot;rabbitmq&quot;,
          &quot;rabbit&quot; =&gt; {
            &quot;host&quot; =&gt; &quot;10.200.10.46&quot;,
            &quot;port&quot; =&gt; &quot;5672&quot;
          }
        },
        &quot;block-storage&quot; =&gt; {
          &quot;service_type&quot; =&gt; &quot;rabbitmq&quot;,
          &quot;rabbit&quot; =&gt; {
            &quot;host&quot; =&gt; &quot;10.200.10.46&quot;,
            &quot;port&quot; =&gt; &quot;5672&quot;
          }
        }
      },
      &quot;endpoints&quot; =&gt; {
        &quot;compute-api-bind&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;,
          &quot;scheme&quot; =&gt; &quot;http&quot;,
          &quot;port&quot; =&gt; &quot;8774&quot;,
          &quot;path&quot; =&gt; &quot;/v2/%(tenant_id)s&quot;
        },
        &quot;compute-api&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;,
          &quot;scheme&quot; =&gt; &quot;http&quot;,
          &quot;port&quot; =&gt; &quot;8774&quot;,
          &quot;path&quot; =&gt; &quot;/v2/%(tenant_id)s&quot;
        },
        &quot;compute-ec2-admin-bind&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;,
          &quot;scheme&quot; =&gt; &quot;http&quot;,
          &quot;port&quot; =&gt; &quot;8773&quot;,
          &quot;path&quot; =&gt; &quot;/services/Admin&quot;
        },
        &quot;compute-ec2-admin&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;,
          &quot;scheme&quot; =&gt; &quot;http&quot;,
          &quot;port&quot; =&gt; &quot;8773&quot;,
          &quot;path&quot; =&gt; &quot;/services/Admin&quot;
        },
        &quot;compute-ec2-api-bind&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;,
          &quot;scheme&quot; =&gt; &quot;http&quot;,
          &quot;port&quot; =&gt; &quot;8773&quot;,
          &quot;path&quot; =&gt; &quot;/services/Cloud&quot;
        },
        &quot;compute-ec2-api&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;,
          &quot;scheme&quot; =&gt; &quot;http&quot;,
          &quot;port&quot; =&gt; &quot;8773&quot;,
          &quot;path&quot; =&gt; &quot;/services/Cloud&quot;
        },
        &quot;compute-xvpvnc-bind&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;,
          &quot;scheme&quot; =&gt; &quot;http&quot;,
          &quot;port&quot; =&gt; &quot;6081&quot;,
          &quot;path&quot; =&gt; &quot;/console&quot;
        },
        &quot;compute-xvpvnc&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;,
          &quot;scheme&quot; =&gt; &quot;http&quot;,
          &quot;port&quot; =&gt; &quot;6081&quot;,
          &quot;path&quot; =&gt; &quot;/console&quot;
        },
        &quot;compute-novnc-bind&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;,
          &quot;scheme&quot; =&gt; &quot;http&quot;,
          &quot;port&quot; =&gt; &quot;6080&quot;,
          &quot;path&quot; =&gt; &quot;/vnc_auto.html&quot;
        },
        &quot;compute-novnc&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;,
          &quot;scheme&quot; =&gt; &quot;http&quot;,
          &quot;port&quot; =&gt; &quot;6080&quot;,
          &quot;path&quot; =&gt; &quot;/vnc_auto.html&quot;
        },
        &quot;compute-vnc&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;0.0.0.0&quot;,
          &quot;scheme&quot; =&gt; &quot;http&quot;,
          &quot;port&quot; =&gt; &quot;6080&quot;,
          &quot;path&quot; =&gt; &quot;/vnc_auto.html&quot;
        },
        &quot;image-api-bind&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;,
          &quot;scheme&quot; =&gt; &quot;http&quot;,
          &quot;port&quot; =&gt; &quot;9292&quot;,
          &quot;path&quot; =&gt; &quot;/v2&quot;
        },
        &quot;image-api&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;,
          &quot;scheme&quot; =&gt; &quot;http&quot;,
          &quot;port&quot; =&gt; &quot;9292&quot;,
          &quot;path&quot; =&gt; &quot;/v2&quot;
        },
        &quot;image-registry-bind&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;,
          &quot;scheme&quot; =&gt; &quot;http&quot;,
          &quot;port&quot; =&gt; &quot;9191&quot;,
          &quot;path&quot; =&gt; &quot;/v2&quot;
        },
        &quot;image-registry&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;,
          &quot;scheme&quot; =&gt; &quot;http&quot;,
          &quot;port&quot; =&gt; &quot;9191&quot;,
          &quot;path&quot; =&gt; &quot;/v2&quot;
        },
        &quot;identity-bind&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;,
          &quot;scheme&quot; =&gt; &quot;http&quot;,
          &quot;port&quot; =&gt; &quot;5000&quot;,
          &quot;path&quot; =&gt; &quot;/v2.0&quot;
        },
        &quot;identity-api&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;,
          &quot;scheme&quot; =&gt; &quot;http&quot;,
          &quot;port&quot; =&gt; &quot;5000&quot;,
          &quot;path&quot; =&gt; &quot;/v2.0&quot;
        },
        &quot;identity-admin&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;,
          &quot;scheme&quot; =&gt; &quot;http&quot;,
          &quot;port&quot; =&gt; &quot;35357&quot;,
          &quot;path&quot; =&gt; &quot;/v2.0&quot;
        },
        &quot;volume-api-bind&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;,
          &quot;scheme&quot; =&gt; &quot;http&quot;,
          &quot;port&quot; =&gt; &quot;8776&quot;,
          &quot;path&quot; =&gt; &quot;/v1/%(tenant_id)s&quot;
        },
        &quot;block-storage-api&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;,
          &quot;scheme&quot; =&gt; &quot;http&quot;,
          &quot;port&quot; =&gt; &quot;8776&quot;,
          &quot;path&quot; =&gt; &quot;/v1/%(tenant_id)s&quot;
        },
        &quot;telemetry-api-bind&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;,
          &quot;scheme&quot; =&gt; &quot;http&quot;,
          &quot;port&quot; =&gt; &quot;8777&quot;,
          &quot;path&quot; =&gt; &quot;/v1&quot;
        },
        &quot;telemetry-api&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;,
          &quot;scheme&quot; =&gt; &quot;http&quot;,
          &quot;port&quot; =&gt; &quot;8777&quot;,
          &quot;path&quot; =&gt; &quot;/v1&quot;
        },
        &quot;network-api-bind&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;,
          &quot;scheme&quot; =&gt; &quot;http&quot;,
          &quot;port&quot; =&gt; &quot;9696&quot;,
          &quot;path&quot; =&gt; &quot;/v2&quot;
        },
        &quot;network-api&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;,
          &quot;scheme&quot; =&gt; &quot;http&quot;,
          &quot;port&quot; =&gt; &quot;9696&quot;,
          &quot;path&quot; =&gt; &quot;/v2&quot;
        },
        &quot;orchestration-api-bind&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;,
          &quot;scheme&quot; =&gt; &quot;http&quot;,
          &quot;port&quot; =&gt; &quot;8004&quot;,
          &quot;path&quot; =&gt; &quot;/v1/%(tenant_id)s&quot;
        },
        &quot;orchestration-api&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;,
          &quot;scheme&quot; =&gt; &quot;http&quot;,
          &quot;port&quot; =&gt; &quot;8004&quot;,
          &quot;path&quot; =&gt; &quot;/v1/%(tenant_id)s&quot;
        },
        &quot;orchestration-api-cfn-bind&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;,
          &quot;scheme&quot; =&gt; &quot;http&quot;,
          &quot;port&quot; =&gt; &quot;8000&quot;,
          &quot;path&quot; =&gt; &quot;/v1&quot;
        },
        &quot;orchestration-api-cfn&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;,
          &quot;scheme&quot; =&gt; &quot;http&quot;,
          &quot;port&quot; =&gt; &quot;8000&quot;,
          &quot;path&quot; =&gt; &quot;/v1&quot;
        },
        &quot;mq&quot; =&gt; {
          &quot;port&quot; =&gt; &quot;5672&quot;
        }
      }
    }
)
</code></pre>

<p>生成した environment を Chef サーバにアップロードする。</p>

<pre><code class="language-bash">% knife environment from file environments/icehouse-nova-network.rb
</code></pre>

<p>Spiceweasel をインストールする。Spiceweasel は yml ファイルを元に knife の操作
を書き出して、またそれを一気に実行することが出来るツールです。</p>

<pre><code class="language-bash">% gem install spiceweasel --no-ri --no-rdoc
% rbenv rehash
</code></pre>

<p>infrastructure.yml を下記の通り修正する。</p>

<pre><code class="language-bash">berksfile:
    options: '--no-freeze --halt-on-frozen'

cookbooks:
- apache2:
- apt:
- aws:
- build-essential:
- chef_handler:
- database:
- dmg:
- erlang:
- git:
- homebrew:
- iptables:
- logrotate:
- memcached:
- mysql:
- openssl:
- openstack-block-storage:
- openstack-common:
- openstack-compute:
- openstack-dashboard:
- openstack-identity:
- openstack-image:
- openstack-network:
- openstack-object-storage:
- openstack-ops-messaging:
- openstack-ops-database:
- openstack-orchestration:
- openstack-telemetry:
- pacman:
- postgresql:
- python:
- rabbitmq:
- runit:
- selinux:
- statsd:
- windows:
- xfs:
- yum:
- yum-epel:
- yum-erlang_solutions:

roles:
- allinone-compute:
- os-compute-single-controller:
- os-base:
- os-ops-caching:
- os-ops-messaging:
- os-ops-database:
- os-block-storage:
- os-block-storage-api:
- os-block-storage-scheduler:
- os-block-storage-volume:
- os-client:
- os-compute-api:
- os-compute-api-ec2:
- os-compute-api-metadata:
- os-compute-api-os-compute:
- os-compute-cert:
- os-compute-conductor:
- os-compute-scheduler:
- os-compute-setup:
- os-compute-vncproxy:
- os-compute-worker:
- os-dashboard:
- os-identity:
- os-image:
- os-image-api:
- os-image-registry:
- os-image-upload:
- os-telemetry-agent-central:
- os-telemetry-agent-compute:
- os-telemetry-api:
- os-telemetry-collector:
- os-network:
- os-network-server:
- os-network-l3-agent:
- os-network-dhcp-agent:
- os-network-metadata-agent:
- os-network-openvswitch:
- os-object-storage:
- os-object-storage-account:
- os-object-storage-container:
- os-object-storage-management:
- os-object-storage-object:
- os-object-storage-proxy:

environments:
- icehouse-nova-network:

nodes:
- 10.200.10.46:
    run_list: role[os-compute-single-controller]
    options: -N opstall01 -E icehouse-nova-network --sudo -x thirai
- 10.200.10.47:
    run_list: role[os-compute-worker]
    options: -N opstall02 -E icehouse-nova-network --sudo -x thirai
- 10.200.10.48:
    run_list: role[os-compute-worker]
    options: -N opstall03 -E icehouse-nova-network --sudo -x thirai
</code></pre>

<p>※ nodes: 項にはデプロイしたいノードと Roles を割り当て列挙する。</p>

<p>spiceweasel を実行する。この時点ではこれから実行されるコマンドの一覧が表示され
るのみである。</p>

<pre><code class="language-bash">% spiceweasel infrastructure.yml
berks upload --no-freeze --halt-on-frozen -b ./Berksfile
knife cookbook upload apache2 apt aws build-essential chef_handler database dmg erlang git homebrew iptables logrotate memcached mysql openssl openstack-block-storage openstack-common openstack-compute openstack-dashboard openstack-identity openstack-image openstack-network openstack-object-storage openstack-ops-messaging openstack-ops-database openstack-orchestration openstack-telemetry pacman postgresql python rabbitmq runit selinux statsd windows xfs yum yum-epel yum-erlang_solutions
knife environment from file separated.rb
knife role from file allinone-compute.rb os-base.rb os-block-storage-api.rb os-block-storage-scheduler.rb os-block-storage-volume.rb os-block-storage.rb os-client.rb os-compute-api-ec2.rb os-compute-api-metadata.rb os-compute-api-os-compute.rb os-compute-api.rb os-compute-cert.rb os-compute-conductor.rb os-compute-scheduler.rb os-compute-setup.rb os-compute-single-controller.rb os-compute-vncproxy.rb os-compute-worker.rb os-dashboard.rb os-identity.rb os-image-api.rb os-image-registry.rb os-image-upload.rb os-image.rb os-network-dhcp-agent.rb os-network-l3-agent.rb os-network-metadata-agent.rb os-network-openvswitch.rb os-network-server.rb os-network.rb os-object-storage-account.rb os-object-storage-container.rb os-object-storage-management.rb os-object-storage-object.rb os-object-storage-proxy.rb os-object-storage.rb os-ops-caching.rb os-ops-database.rb os-ops-messaging.rb os-telemetry-agent-central.rb os-telemetry-agent-compute.rb os-telemetry-api.rb os-telemetry-collector.rb
knife bootstrap 10.200.10.46 -N opstall01 -E separated --sudo -x thirai -r 'role[os-compute-single-controller]'
knife bootstrap 10.200.10.47 -N opstall02 -E separated --sudo -x thirai -r 'role[os-compute-worker]'
knife bootstrap 10.200.10.48 -N opstall03 -E separated --sudo -x thirai -r 'role[os-compute-worker]'
</code></pre>

<p>-e オプションを付与すると実際にこれらのコマンドが実行される。実行してデプロイを行う。</p>

<pre><code class="language-bash">% spiceweasel -e infrastructure.yml
</code></pre>

<p>この時点で、下記の操作も行われる。</p>

<ul>
<li>Nova-Network 上に仮想ネットワークの生成</li>
<li>OS イメージのダウンロードと登録 (Environment に記したモノ)</li>
</ul>

<h2 id="cinder-の設定">Cinder の設定</h2>

<p>デプロイ完了したところで cinder-volume プロセスは稼働しているが物理ディスクの
アサインが済んでいない。これは Chef では指定出来ないので手動で行う。</p>

<p>予め Cinder 用の物理ディスクを Controller ノードに付与する。(ここでは /dev/sdb1)</p>

<pre><code class="language-bash">controller% sudo -i
controller# pvcreate /dev/sdb1
controller# vgcreate cinder-volumes /dev/sdb1
controller# service cinder-volume restart
</code></pre>

<p>これで完了。</p>

<h2 id="使ってみる">使ってみる</h2>

<p>ではデプロイした OpenStack を使って仮想マシンを作ってみる。</p>

<pre><code class="language-bash">controller% sudo -i
controller# source openrc
controller# nova keypair-add novakey01 &gt; novakey01
controller# chmod 400 novakey01
controller# nova boot --image cirros --flavor 1 --key_name novakey01 cirros01
controller# nova list 
+--------------------------------------+----------+--------+------------+-------------+--------------------+
| ID                                   | Name     | Status | Task State | Power State | Networks           |
+--------------------------------------+----------+--------+------------+-------------+--------------------+
| e6687359-1aef-4105-a8db-894600001610 | cirros01 | ACTIVE | -          | Running     | private=10.200.9.2 |
+--------------------------------------+----------+--------+------------+-------------+--------------------+
controller# ssh -i novakey01 -l cirros 10.200.9.2
vm# ping www.goo.ne.jp
</code></pre>

<p>仮想マシンが生成され The Internet に対して通信が行えたことを確認。</p>

<p>次に仮想ディスクを生成して上記で作成した仮想マシンに付与する。</p>

<pre><code class="language-bash">controller# cinder create --display-name vol01 1
+--------------------------------------+-----------+--------------+------+-------------+----------+--------------------------------------+
|                  ID                  |  Status   | Display Name | Size | Volume Type | Bootable |             Attached to              |
+--------------------------------------+-----------+--------------+------+-------------+----------+--------------------------------------+
| 0f3726a0-c2f5-45bc-bcf2-1f6eb746f5c8 | available |    vol02     |  1   |     None    |  false   | b286387c-2311-4134-ab59-850fee3e4650 |
+--------------------------------------+-----------+--------------+------+-------------+----------+--------------------------------------+
controller# nova volume-attach e6687359-1aef-4105-a8db-894600001610 0f3726a0-c2f5-45bc-bcf2-1f6eb746f5c8 auto
controller# ssh -i novakey01 -l cirros 10.200.9.2
vm# mkfs.ext4 /dev/vdb
vm# mount -t ext4 /dev/vdb /mnt
vm# df -h | grep mnt
/dev/vdb        976M  1.3M  908M   1% /mnt
</code></pre>

<p>仮想ディスクを仮想マシンに付与しマウントできることを確認出来た。</p>

<h2 id="まとめ">まとめ</h2>

<p>今回は Nova-Network 構成の Icehouse を構築出来た。Nova-Network は Havana でサポート終了との話が延期
になっていることは聞いていたが Icehouse でもしっかり動いている。また後に Neutron 構成も調査を行う。
Neutron 構成は下記の Environments を参考にすると動作するかもしれない。</p>

<p><a href="https://github.com/stackforge/openstack-chef-repo/blob/master/environments/vagrant-multi-neutron.json">https://github.com/stackforge/openstack-chef-repo/blob/master/environments/vagrant-multi-neutron.json</a></p>

<p>キーとなるのは、</p>

<pre><code class="language-json">    &quot;network&quot;: {
      &quot;debug&quot;: &quot;True&quot;,
      &quot;dhcp&quot;: {
        &quot;enable_isolated_metadata&quot;: &quot;True&quot;
      },
      &quot;metadata&quot;: {
        &quot;nova_metadata_ip&quot;: &quot;192.168.3.60&quot;
      },
      &quot;openvswitch&quot;: {
        &quot;tunnel_id_ranges&quot;: &quot;1:1000&quot;,
        &quot;enable_tunneling&quot;: &quot;True&quot;,
        &quot;tenant_network_type&quot;: &quot;gre&quot;,
        &quot;local_ip_interface&quot;: &quot;eth2&quot;
      },
      &quot;api&quot;: {
        &quot;bind_interface&quot;: &quot;eth1&quot;
      }
    },
</code></pre>

<p>この辺り。</p>

<h2 id="残っている問題点">残っている問題点</h2>

<p>VNC コンソールにアクセス出来ない。調べたのですが、environment の修正で直せる問
題ではないように見えました。</p>

<p>cookbooks/openstack-compute/templates/default/nova.conf.rb を確認すると下記の
ようになっています。</p>

<pre><code class="language-ruby">##### VNCPROXY #####
novncproxy_base_url=&lt;%= @novncproxy_base_url %&gt;
xvpvncproxy_base_url=&lt;%= @xvpvncproxy_base_url %&gt;

# This is only required on the server running xvpvncproxy
xvpvncproxy_host=&lt;%= @xvpvncproxy_bind_host %&gt;
xvpvncproxy_port=&lt;%= @xvpvncproxy_bind_port %&gt;

# This is only required on the server running novncproxy
novncproxy_host=&lt;%= @novncproxy_bind_host %&gt;
novncproxy_port=&lt;%= @novncproxy_bind_port %&gt;

vncserver_listen=&lt;%= @vncserver_listen %&gt;
vncserver_proxyclient_address=&lt;%= @vncserver_proxyclient_address %&gt;
</code></pre>

<p>vncserver_listen, vncserver_proxyclient_address はそれぞれ</p>

<ul>
<li>@vncserver_listen</li>
<li>@vncserver_proxyclient_address</li>
</ul>

<p>という変数が格納されることになっている。</p>

<p>では cookbooks/openstack-compute/recipes/nova-common.rb を確認すると、</p>

<pre><code class="language-ruby">template '/etc/nova/nova.conf' do
  source 'nova.conf.erb'
  owner node['openstack']['compute']['user']
  group node['openstack']['compute']['group']
  mode 00644
  variables(
    sql_connection: sql_connection,
    novncproxy_base_url: novnc_endpoint.to_s,
    xvpvncproxy_base_url: xvpvnc_endpoint.to_s,
    xvpvncproxy_bind_host: xvpvnc_bind.host,
    xvpvncproxy_bind_port: xvpvnc_bind.port,
    novncproxy_bind_host: novnc_bind.host,
    novncproxy_bind_port: novnc_bind.port,
    vncserver_listen: vnc_endpoint.host,
    vncserver_proxyclient_address: vnc_endpoint.host,
    以下略
</code></pre>

<p>となっている。vncserver_listen, vncserver_proxyclient_address 共に
vnc_endpoint.host が格納されることになっている。vnc_endpont.host は</p>

<pre><code class="language-ruby">vnc_endpoint = endpoint 'compute-vnc' || {}
</code></pre>

<p>となっており、Attributes の</p>

<pre><code class="language-json">    &quot;compute-vnc&quot; =&gt; {
      &quot;host&quot; =&gt; &quot;0.0.0.0&quot;,
      &quot;scheme&quot; =&gt; &quot;http&quot;,
      &quot;port&quot; =&gt; &quot;6080&quot;,
      &quot;path&quot; =&gt; &quot;/vnc_auto.html&quot;
    },
</code></pre>

<p>の設定が入ることになる。つまり上記のような Attributes だと vncserver_listen, vncserver_proxyclient_address 共に
&lsquo;0.0.0.0&rsquo; のアドレスが controller, compute ノードの双方に入ってしまい、NoVNC が正しく格納しないことになる。</p>

<p>解決したらまたここに更新版を載せたいと思いまーす！</p>

<p>以上です。</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2014/04/25/stackforge-openstack-chef-repo-icehouse-deploy/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>


        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2014/04/23/mirantis-openstack/">Mirantis OpenStack (Neutron GRE)を組んでみた！</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2014-04-23'>
            April 23, 2014</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは。<a href="https://twitter.com/jedipunkz">@jedipunkz</a> です。</p>

<p>皆さん、Mirantis OpenStack はご存知ですか？ OpenStack ディストリビューションの
1つです。以下、公式サイトです。</p>

<p><a href="http://software.mirantis.com/main/">http://software.mirantis.com/main/</a></p>

<p>この Mirantis OpenStack を使って OpenStack Havana (Neutron GRE) 構成を作ってみ
ました。その時のメモを書いていきたいと思います。</p>

<h2 id="構成は">構成は?</h2>

<p>構成は下記の通り。</p>

<p><img src="http://jedipunkz.github.io/pix/mirantis_gre.jpg"></p>

<p>※ CreativeCommon</p>

<p>特徴は</p>

<ul>
<li>Administrative Network : Fuel Node, DHCP + PXE ブート用</li>
<li>Management Network : 各コンポーネント間 API 用</li>
<li>Public/Floating IP Network : パブリック API, VM Floating IP 用</li>
<li>Storage Network : Cinder 配下ストレージ &lt;-&gt; インスタンス間用</li>
<li>要インターネット接続 : Public/Floating Networks</li>
<li>Neutron(GRE) 構成</li>
</ul>

<p>です。タグ VLAN 使って物理ネットワークの本数を減らすことも出来るはずですが、僕
の環境では何故かダメだったので上記の4つの物理ネットワークを別々に用意しました。</p>

<h2 id="fuel-ノードの構築">Fuel ノードの構築</h2>

<p>Fuel ノードとは、OpenStack の各ノードをデプロイするための管理ノードのことです。
DHCP + PXE を管理する Cobbler やデプロイツールの Puppet が内部で稼働し、
Administrative Network 上で稼働したノードを管理・その後デプロイします。</p>

<p>構築方法は&hellip;</p>

<p>下記の URL より ISO ファイルをダウンロード。</p>

<p><a href="http://software.mirantis.com/main/">http://software.mirantis.com/main/</a></p>

<p>Administrative Network に NIC を出したノードを上記の ISO から起動。</p>

<p>Grub メニューが表示されたタイミングで &ldquo;Tab&rdquo; キーを押下。</p>

<p><img src="http://jedipunkz.github.io/pix/grub.png"></p>

<p>上記画面にてカーネルオプションにて hostname, ip, gw, dns を修正。下記は例。</p>

<pre><code class="language-bash">vmlinuz initrd=initrd.img biosdevname=0 ks=cdrom:/ks.cfg ip=10.200.10.76
gw=10.200.10.1 dns1=8.8.8.8 netmask=255.255.255.0 hostname=fuel.cpi.ad.jp
showmenu=no_
</code></pre>

<p>ブラウザで <a href="http://10.200.10.76:8080">http://10.200.10.76:8080</a> (上記例)にアクセスし、新しい &lsquo;OpenStack
Environment&rsquo; を作成する。</p>

<pre><code>Name : 任意
OpenStack Release : Havana on CentOS6.4
</code></pre>

<p>なお、Ubuntu 構成も組めるが、私の環境では途中でエラーが出力した。</p>

<p>Next を押下し、ネットワーク設定を行う。今回は &lsquo;Nuetron GRE&rsquo; を選択。</p>

<p><img src="http://jedipunkz.github.io/pix/mirantis_network.png"></p>

<p>&lsquo;Save Settings&rsquo; を押下し設定を保存。この時点では &lsquo;Verify Networks&rsquo; は行えない。
少なくとも 2 ノードが必要。次のステップで2ノードの追加を行う。</p>

<h2 id="ノードの追加">ノードの追加</h2>

<p>下記の4つのネットワークセグメントに NIC を出したノードを用意し、起動する。起動
すると Administrative Network 上で稼働している Cobbler によりノードが PXE 起動
しミニマムな OS がインストールされる。これは後に Fuel ノードよりログインがされ、
各インターフェースの Mac アドレス等の情報を知るためです。ネットワークベリファ
イ等もこのミニマムな OS 越しに実施されます。</p>

<ul>
<li>Administrative Network</li>
<li>Public/Floating IP Network</li>
<li>Storage Network</li>
<li>Management Network</li>
</ul>

<p>ノードが稼働した時点で Fuel によりノードが認識されるので、ここでは2つのノード
をそれぞれ</p>

<ul>
<li>Controller ノード</li>
<li>Compute ノード</li>
</ul>

<p>として画面上で割り当てます。</p>

<h2 id="インターフェースの設定">インターフェースの設定</h2>

<p><img src="http://jedipunkz.github.io/pix/mirantis_mac.png"></p>

<p><a href="http://10.200.10.76:8000/#cluster/1/nodes">http://10.200.10.76:8000/#cluster/1/nodes</a> にログインし作成した Environment
を選択。その後、&rsquo;Nodes&rsquo; タブを押下。ノードを選択し、&rsquo;Configure Interfaces&rsquo; を
選択。各ノードのインターフェースの Mac アドレスを確認し、各々のネットワークセ
グメントを紐付ける。尚、Fuel ノードから &lsquo;root&rsquo; ユーザで SSH(22番ポート) にノン
パスフレーズで公開鍵認証ログインが可能となっている。Fuel ノードに対しては SSH
(22番ポート) にて下記のユーザにてログインが可能です。</p>

<pre><code>username : root
password : r00tme
</code></pre>

<h2 id="ネットワークの確認">ネットワークの確認</h2>

<p><a href="http://10.200.10.76:8000/#cluster/1/network">http://10.200.10.76:8000/#cluster/1/network</a> にて &lsquo;Networks&rsquo; タブを開き、&rsquo;Verify
Networks&rsquo; を押下。ネットワーク設定が正しく行われているか否かを確認。</p>

<h2 id="デプロイ">デプロイ</h2>

<p><a href="http://10.200.10.76:8000/#cluster/1/nodes">http://10.200.10.76:8000/#cluster/1/nodes</a> にて &lsquo;Deploy Changes&rsquo; を押下しデプ
ロイ開始。kickstart にて OS が自動でインストールされ puppet にて fuel ノードか
ら自動で OpenStack がインストールされます。</p>

<h2 id="openstack-へのアクセス">OpenStack へのアクセス</h2>

<p>SSH では下記のステップで OpenStack コントローラノードにログイン。</p>

<p>fuel ノード (SSH) -&gt; node-1 (OpenStack コントローラノード)(SSH)</p>

<p>ブラウザで Horizon にアクセスするには</p>

<p><a href="http://10.200.10.2">http://10.200.10.2</a></p>

<p>にアクセス。これは例。Administrative Network に接続している NIC の IP アドレス
へ HTTP でログイン。</p>

<h2 id="まとめ">まとめ</h2>

<p>Mirantis OpenStack Neutron (GRE) 構成が組めた。上記構成図を見て疑問に思ってい
た &ldquo;VM 間通信のネットワークセグメント&rdquo; であるが、Administrative Network のセグ
メントを用いている事が判った。これは利用者が意図しているとは考えづらいので、正
直、あるべき姿では無いように思える。が、上記構成図に VM ネットワークが無い理由
はこれにて判った。</p>

<p>下記はノード上で ovs-vsctl show コマンドを打った結果の抜粋です。</p>

<pre><code class="language-bash">    Bridge &quot;br-eth1&quot;
        Port &quot;br-eth1&quot;
            Interface &quot;br-eth1&quot;
                type: internal
        Port &quot;br-eth1--br-fw-admin&quot;
            trunks: [0]
            Interface &quot;br-eth1--br-fw-admin&quot;
                type: patch
                options: {peer=&quot;br-fw-admin--br-eth1&quot;}
        Port &quot;eth1&quot;
            Interface &quot;eth1&quot;
</code></pre>

<p>今回の構成は eth1 は Administrative Network に割り当てていました。</p>

<p>一昔前は OS のディストリビュータが有料サポートをビジネスにしていました。Redhat
がその代表格だと思いますが、今は OS 上で何かを実現するにもソフトウェアの完成度
が高く、エンジニアが困るシチュエーションがそれほど無くなった気がします。そこで
その OS の上で稼働する OpenStack のサポートビジネスが出てきたか！という印象で
す。OpenStack はまだまだエンジニアにとって敷居の高いソフトウェアです。自らクラ
ウドプラットフォームを構築出来るのは魅力的ですが、サポート無しに構築・運用する
には、まだ難しい技術かもしれません。こういったディストリビューションが出てくる
辺りが時代だなぁと感じます。</p>

<p>尚、ISO をダウンロードして利用するだけでしたら無償で OK です。</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2014/04/23/mirantis-openstack/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>


        

        
<ul class="actions pagination">
    
        <li><a href="/categories/infrastructure/page/2/"
                class="button big previous">Previous Page</a></li>
    

    
        <li><a href="/categories/infrastructure/page/4/"
                class="button big next">Next Page</a></li>
    
</ul>

    </div>
    
<section id="sidebar">

    
        <section id="intro">
            
            
            
            <ul class="icons">
                
                    <li><a href="https://jedipunkz.github.io/categories/infrastructure/index.xml" type="application/rss+xml"
                        target="_blank" title="RSS" class="fa fa-rss"></a></li>
                
                
            </ul>
        </section>

    
        <section id="recent-posts">
            <ul class="posts">
                <header>
                    <h3>Recent Posts</h3>
                </header>
                
                    
                

                
                    
                

                
                    <li>
                        <article>
                            <header>
                                <h3><a href="https://jedipunkz.github.io/blog/2018/12/31/istio/">Istio, Helm を使って Getting Started 的なアプリをデプロイ</a></h3>
                                
                                    
                                
                                <time class="published" datetime=
                                    '2018-12-31'>
                                    December 31, 2018</time>
                            </header>
                        </article>
                    </li>
                
                    <li>
                        <article>
                            <header>
                                <h3><a href="https://jedipunkz.github.io/blog/2017/07/02/test-kitchen-cluster/">Docker,Test-Kitchen,Ansible でクラスタを構成する</a></h3>
                                
                                    
                                
                                <time class="published" datetime=
                                    '2017-07-02'>
                                    July 2, 2017</time>
                            </header>
                        </article>
                    </li>
                
                    <li>
                        <article>
                            <header>
                                <h3><a href="https://jedipunkz.github.io/blog/2017/04/13/gke-lb/">GCP ロードバランサと GKE クラスタを Terraform を使って構築する</a></h3>
                                
                                    
                                
                                <time class="published" datetime=
                                    '2017-04-13'>
                                    April 13, 2017</time>
                            </header>
                        </article>
                    </li>
                
                    <li>
                        <article>
                            <header>
                                <h3><a href="https://jedipunkz.github.io/blog/2017/02/12/serverless-fission/">Serverless on Kubernetes : Fission を使ってみた</a></h3>
                                
                                    
                                
                                <time class="published" datetime=
                                    '2017-02-12'>
                                    February 12, 2017</time>
                            </header>
                        </article>
                    </li>
                
                    <li>
                        <article>
                            <header>
                                <h3><a href="https://jedipunkz.github.io/blog/2017/01/13/kubernetes-deployments/">Kubernetes Deployments を使ってみた！</a></h3>
                                
                                    
                                
                                <time class="published" datetime=
                                    '2017-01-13'>
                                    January 13, 2017</time>
                            </header>
                        </article>
                    </li>
                

                
                    <li>
                        <ul class="actions">
                            <li><a href=
                            
                                "/post/"
                            
                            class="button">View more posts</a></li>
                        </ul>
                    </li>
                
            </ul>
        </section>

    
    
    
    
        <section id="categories">
            <ul class="posts">
                <header>
                    <h3><a href="/categories/">Categories</a></h3>
                </header>

                
                    
                

                
                    <li>
                        <article>
                            <header>
                                <a href="/categories/infrastructure/">infrastructure</a>
                                <span style="float:right;">110</span>
                            </header>
                        </article>
                    </li>
                
                    <li>
                        <article>
                            <header>
                                <a href="/categories/report/">report</a>
                                <span style="float:right;">9</span>
                            </header>
                        </article>
                    </li>
                
                    <li>
                        <article>
                            <header>
                                <a href="/categories/tools/">tools</a>
                                <span style="float:right;">11</span>
                            </header>
                        </article>
                    </li>
                
            </ul>
        </section>
    

    
        

    
        <section id="footer">
            <ul class="icons">
                
                    <li><a href="https://jedipunkz.github.io/categories/infrastructure/index.xml" type="application/rss+xml"
                        target="_blank" title="RSS" class="fa fa-rss"></a></li>
                
                
            </ul>

            <p class="copyright">&copy; ジェダイさんのブログ. テーマデザインは <a href="//github.com/jpescador" target="_blank">Julio Pescador</a>さんによるものです。 </p>
        </section>

</section>

            </div>
        <a id="back-to-top" href="#" class="fa fa-arrow-up fa-border fa-2x"></a>
        

        
        
            
        

        
        
            <script src="/js/jquery.min.js"></script>
            <script src="/js/skel.min.js"></script>
            <script src="/js/util.js"></script>
            <script src="/js/main.js"></script>
            <script src="/js/backToTop.js"></script>
            <script src="/js/highlight.pack.js"></script>
        

        

            
            <script>hljs.initHighlightingOnLoad();</script>
            
    </body>
</html>

