


    
        
    




<!DOCTYPE HTML>

<html>
    <head>
        
            <title>Infrastructure Posts - ジェダイさんのブログ</title>
        

        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1" />
        <meta name="generator" content="Hugo 0.53" />
        


        
        
            
        

        <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Infrastructure"/>
<meta name="twitter:description" content=""/>

        <meta property="og:title" content="Infrastructure" />
<meta property="og:description" content="" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://jedipunkz.github.io/categories/infrastructure/" />
<meta property="og:updated_time" content="2018-12-31T00:00:00&#43;00:00"/>

        
<meta itemprop="name" content="Infrastructure">
<meta itemprop="description" content="">


        

        

        
        
            
        

        
        
            <link rel="stylesheet" href="/css/google-font.css" />
            <link rel="stylesheet" href="/css/font-awesome.min.css" />
            <link rel="stylesheet" href="/css/main.css" />
            <link rel="stylesheet" href="/css/add-on.css" />
            <link rel="stylesheet" href="/css/monokai-sublime.css">
        

        

        
        
        
            
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-30563095-1', 'auto');
	
	ga('send', 'pageview');
}
</script>

        
    </head>
    <body>

        
        <div id="wrapper">

    
<header id="header">
    
        <h2><a href="/"></i></a></h2>
    

    <nav class="links">
        <ul>
            
                <li>
                    <a href="">
                        Blog
                    </a>
                </li>
            
                <li>
                    <a href="about/index.html">
                        About
                    </a>
                </li>
            
        </ul>
    </nav>
    <nav class="main">
        <ul>
            
            <li class="search">
                <a class="fa-search" href="#search">Search</a>
                <form id="search" method="get" action="//google.com/search">
                    <input type="text" name="q" placeholder="Search" />
                    <input type="hidden" name="q" value="site:https://jedipunkz.github.io">
                </form>
            </li>
            <li class="menu">
                <a class="fa-bars" href="#menu">Menu</a>
            </li>
        </ul>
    </nav>
</header>


<section id="menu">

    
        <section>
            <form class="search" method="get" action="//google.com/search">
                <input type="text" name="q" placeholder="Search" />
                <input type="hidden" name="q" value="site:https://jedipunkz.github.io">
            </form>
        </section>

    
        <section>
            <ul class="links">
                
                    <li>
                        <a href="">
                            <h3>
                                
                                Blog
                            </h3>
                        </a>
                    </li>
                
                    <li>
                        <a href="about/index.html">
                            <h3>
                                
                                About
                            </h3>
                        </a>
                    </li>
                
            </ul>
        </section>

    
        <section>
            <ul class="links">
                <header>
                    <h3>Recent Posts</h3>
                </header>
                
                    
                

                
                    <li>
                        <a href="https://jedipunkz.github.io/blog/2018/12/31/istio/"><p>Istio, Helm を使って Getting Started 的なアプリをデプロイ</p></a>
                    </li>
                
                    <li>
                        <a href="https://jedipunkz.github.io/blog/2017/07/02/test-kitchen-cluster/"><p>Docker,Test-Kitchen,Ansible でクラスタを構成する</p></a>
                    </li>
                
                    <li>
                        <a href="https://jedipunkz.github.io/blog/2017/04/13/gke-lb/"><p>GCP ロードバランサと GKE クラスタを Terraform を使って構築する</p></a>
                    </li>
                
                    <li>
                        <a href="https://jedipunkz.github.io/blog/2017/02/12/serverless-fission/"><p>Serverless on Kubernetes : Fission を使ってみた</p></a>
                    </li>
                
                    <li>
                        <a href="https://jedipunkz.github.io/blog/2017/01/13/kubernetes-deployments/"><p>Kubernetes Deployments を使ってみた！</p></a>
                    </li>
                
            </ul>
        </section>

    
        
</section>

    
    <div id="main">
        <h1>Infrastructure</h1>
        
        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2018/12/31/istio/">Istio, Helm を使って Getting Started 的なアプリをデプロイ</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2018-12-31'>
            December 31, 2018</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは。<a href="https://twitter.com/jedipunkz">@jedipunkz</a> です。</p>

<p>最近は kubernetes を触ってなかったのですが Istio や Envoy 等 CNCF 関連のソフトウェアの記事をよく見かけるようになって、少し理解しておいたほうがいいかなと思い Istio と Minikube を使って Getting Started 的な事をやってみました。Istio をダウンロードすると中にサンプルアプリケーションが入っているのでそれを利用してアプリのデプロイまでを行ってみます。</p>

<p>Istio をダウンロードするとお手軽に Istio 環境を作るための yaml ファイルがあり、それを kubectl apply することで Istio 環境を整えられるのですが、ドキュメントにプロダクション環境を想定した場合は Helm Template を用いた方がいいだろう、と記載あったので今回は Helm Template を用いて Istio 環境を作ります。</p>

<h2 id="前提の環境">前提の環境</h2>

<p>下記の環境でテストを行いました。</p>

<ul>
<li>macos Mojave</li>
<li>minikube v0.32.0</li>
<li>kubectl v1.10.3</li>
<li>helm v2.12.1</li>
<li>virtualbox</li>
</ul>

<h2 id="準備">準備</h2>

<h3 id="kubectl-と-helm-のインストール">kubectl と helm のインストール</h3>

<p>kubctl と helm をインストールします。両者共に homebrew でインストールします。</p>

<pre><code class="language-bash">brew install kubernetes-cli
brew install kubernetes-helm
</code></pre>

<h3 id="minikube-のインストールと起動">minikube のインストールと起動</h3>

<p>minikube をインストールして起動します。</p>

<pre><code class="language-bash">curl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.32.0/minikube-darwin-amd64 &amp;&amp; chmod +x minikube &amp;&amp; sudo cp minikube /usr/local/bin/ &amp;&amp; rm minikube
minikube start --memory 2048
</code></pre>

<h3 id="istio-のダウンロードとインストール">Istio のダウンロードとインストール</h3>

<p>Istio のダウンロードとインストールを行います。後術しますがこのディレクトリの中に Istio 環境を構築するためのファイルやサンプルアプリケーションが入っています。</p>

<pre><code class="language-bash">curl -L https://git.io/getLatestIstio | sh -
cd istio-1.0.5
sudo cp bin/istioctl /usr/local/bin/istioctl
</code></pre>

<h2 id="構築作業">構築作業</h2>

<h3 id="istio-の-custom-resource-definitions-をインストール">Istio の Custom Resource Definitions をインストール</h3>

<p>Istio の Custom Resource Definitions (以下 CRDs) をインストールします。Kubernetes の CRDs は独自のカスタムリソースを定義し追加するものです。Kubernets API Server を介して作成することで作成したリソースの CRUD の API が Kubernetes API に追加されます。</p>

<p>先程ダウンロードした Istio のディレクトリに crds.yaml があるのでそれを適用します。</p>

<pre><code class="language-bash">kubectl apply -f install/kubernetes/helm/istio/templates/crds.yaml
</code></pre>

<h3 id="helm-template-を使って-istio-の-core-components-をインストール">Helm Template を使って Istio の Core Components をインストール</h3>

<p>Helm Template の仕組みをつかって Istio の Core Components をインストールします。まず Helm Template を出力し istio-system というネームスペースを作成、その後生成した Template を用いて kubectl コマンドで適用します。</p>

<pre><code class="language-bash">helm template install/kubernetes/helm/istio --name istio --namespace istio-system &gt; ./istio.yaml
kubectl create namespace istio-system
kubectl apply -f ./istio.yaml
</code></pre>

<h3 id="状態の確認">状態の確認</h3>

<p>この状態で service と pods の状態を確認してみます。</p>

<pre><code class="language-bash">kubectl get svc -n istio-system
NAME                     TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)                                                                                                                   AGE
istio-citadel            ClusterIP      10.100.143.194   &lt;none&gt;        8060/TCP,9093/TCP                                                                                                         55s
istio-egressgateway      ClusterIP      10.108.243.97    &lt;none&gt;        80/TCP,443/TCP                                                                                                            55s
istio-galley             ClusterIP      10.98.99.11      &lt;none&gt;        443/TCP,9093/TCP                                                                                                          55s
istio-ingressgateway     LoadBalancer   10.97.17.220     &lt;pending&gt;     80:31380/TCP,443:31390/TCP,31400:31400/TCP,15011:30080/TCP,8060:31309/TCP,853:31151/TCP,15030:30455/TCP,15031:30836/TCP   55s
istio-pilot              ClusterIP      10.102.75.110    &lt;none&gt;        15010/TCP,15011/TCP,8080/TCP,9093/TCP                                                                                     55s
istio-policy             ClusterIP      10.101.145.62    &lt;none&gt;        9091/TCP,15004/TCP,9093/TCP                                                                                               55s
istio-sidecar-injector   ClusterIP      10.107.131.48    &lt;none&gt;        443/TCP                                                                                                                   55s
istio-telemetry          ClusterIP      10.96.248.64     &lt;none&gt;        9091/TCP,15004/TCP,9093/TCP,42422/TCP                                                                                     55s
prometheus               ClusterIP      10.98.228.190    &lt;none&gt;        9090/TCP                                                                                                                  55s
</code></pre>

<pre><code class="language-bash">kubectl get pods -n istio-system
NAME                                     READY     STATUS              RESTARTS   AGE
istio-citadel-55cdfdd57c-64tnn           0/1       ContainerCreating   0          1m
istio-cleanup-secrets-x6dmj              0/1       Completed           0          1m
istio-egressgateway-7798845f5d-qfx2k     1/1       Running             0          1m
istio-galley-76bbb946c8-5l626            0/1       ContainerCreating   0          1m
istio-ingressgateway-78c6d8b8d7-68r2z    1/1       Running             0          1m
istio-pilot-5fcb895bff-pmg8z             0/2       Pending             0          1m
istio-policy-7b6cc95d7b-w7ndg            2/2       Running             0          1m
istio-security-post-install-jcwg5        0/1       Completed           0          1m
istio-sidecar-injector-9c6698858-8lt92   0/1       ContainerCreating   0          1m
istio-telemetry-bfc9ff784-2mzzj          2/2       Running             0          1m
prometheus-65d6f6b6c-nttwz               0/1       ContainerCreating   0          1m
</code></pre>

<h2 id="サンプルアプリケーションのデプロイ">サンプルアプリケーションのデプロイ</h2>

<p>先程ダウンロードした Istio のディレクトリにサンプルアプリケーション &lsquo;bookinfo&rsquo; がありますのでそれをデプロイしてみます。
デプロイ方法は2パターンあります。</p>

<p>方法1. istioctl を使ってアプリの yaml に対して sidecar を記すよう変換しそれを kubctl apply します。</p>

<pre><code class="language-bash">kubectl apply -f &lt;(istioctl kube-inject -f samples/bookinfo/platform/kube/bookinfo.yaml)
</code></pre>

<p>方法2. Sidecar Injection をデフォルトの動きとして設定し kubectl apply します</p>

<pre><code class="language-bash">kubectl label namespace default istio-injection=enabled
kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yam
</code></pre>

<p>最後にアプリケーションの状態を確認します</p>

<pre><code class="language-bash">kubectl get svc
NAME          TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
details       ClusterIP   10.111.252.7     &lt;none&gt;        9080/TCP   17s
kubernetes    ClusterIP   10.96.0.1        &lt;none&gt;        443/TCP    6m
productpage   ClusterIP   10.108.205.216   &lt;none&gt;        9080/TCP   10s
ratings       ClusterIP   10.102.161.24    &lt;none&gt;        9080/TCP   14s
reviews       ClusterIP   10.106.18.105    &lt;none&gt;        9080/TCP   12s
</code></pre>

<pre><code class="language-bash">kubectl get pods
NAME                              READY     STATUS            RESTARTS   AGE
details-v1-5458f64c65-svrr7       0/2       PodInitializing   0          48s
productpage-v1-577c9594b7-4f49s   0/2       Init:0/1          0          43s
ratings-v1-79467df9b5-vrx4w       0/2       PodInitializing   0          47s
reviews-v1-5d46b744bd-686s9       0/2       Init:0/1          0          46s
reviews-v2-7f7d7f99f7-xsvm8       0/2       Init:0/1          0          46s
reviews-v3-7bc67f66-cmt4x         0/2       Init:0/1          0          45s
</code></pre>

<h2 id="まとめ">まとめ</h2>

<p>プロダクション環境を想定した Istio 構築と言っても Helm Template を用いて簡単に操作できることが分かりました。</p>

<p>またここで重要なのはサンプルアプリケーション &lsquo;bookinfo&rsquo; の Kubenetes Pods に Envoy プロキシを Sidecar 的に配置するための変換コマンドとして</p>

<pre><code class="language-bash">kubectl apply -f &lt;(istioctl kube-inject -f samples/bookinfo/platform/kube/bookinfo.yaml)
</code></pre>

<p>が用いられていることです。下記の操作を実行してみるとよくわかるでしょう。</p>

<pre><code class="language-bash">istioctl kube-inject -f samples/bookinfo/platform/kube/bookinfo.yaml &gt; ./bookinfo.sidecar.yaml
diff -u bookinfo.yaml bookinfo.sidecar.yaml
</code></pre>

<p>差分が長くなるためここでは省略しますが元の bookinfo.yaml には記されていなかった sidecar の文字が読み取れると思います。アプリに隣接した pods に Envoy が起動している pods が Sidecar 的に配置され下記のような構成になりました。</p>

<pre><code>              ingress          &quot;python&quot;            &quot;java&quot;            &quot;nodejs&quot;
           +-----------+    +-------------+    +-------------+    +-------------+
           | +-------+ |    |  +-------+  |    |  +-------+  |    |  +-------+  |
request -&gt; | | envoy | | -&gt; |  | envoy |  | -&gt; |  | envoy |  | -&gt; |  | envoy |  |
           | +-------+ |    |  +-------+  |    |  +-------+  |    |  +-------+  |
           +-----------+    | productpage |    |  review-v1  |    |   ratings   |
                            +-------------+    +-------------+    +-------------+
           
                                               +-------------+
                                               |  +-------+  |
                                               |  | envoy |  |
                                               |  +-------+  |
                                               |  review-v2  |
                                               +-------------+
           
                                               +-------------+
                                               |  +-------+  |
                                               |  | envoy |  |
                                               |  +-------+  |
                                               |  review-v3  |
                                               +-------------+
           
                                               +-------------+
                                               |  +-------+  |
                                               |  | envoy |  |
                                               |  +-------+  |
                                               |   details   |
                                               +-------------+
                                                    ruby
</code></pre>

<h2 id="参考-url">参考 URL</h2>

<ul>
<li><a href="https://istio.io/docs/setup/kubernetes/quick-start/">https://istio.io/docs/setup/kubernetes/quick-start/</a></li>
<li><a href="https://istio.io/docs/examples/bookinfo/">https://istio.io/docs/examples/bookinfo/</a></li>
<li><a href="https://istio.io/docs/setup/kubernetes/helm-install/">https://istio.io/docs/setup/kubernetes/helm-install/</a></li>
</ul>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2018/12/31/istio/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>


        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2017/07/02/test-kitchen-cluster/">Docker,Test-Kitchen,Ansible でクラスタを構成する</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2017-07-02'>
            July 2, 2017</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは。<a href="https://twitter.com/jedipunkz">@jedipunkz</a> です。</p>

<p>以前、&rdquo;Test-Kitchen, Docker で Ansible Role 開発サイクル高速化！&rdquo; ってタイトルで Ansible Role の開発を test-kitchen を使って行う方法について記事にしたのですが、やっぱりローカルで Docker コンテナ立ち上げてデプロしてテストして.. ってすごく楽というか速くて今の現場でも便利につかっています。前の記事の URL は下記です。</p>

<p><a href="https://jedipunkz.github.io/blog/2016/07/14/test-kitchen-with-ansible/">https://jedipunkz.github.io/blog/2016/07/14/test-kitchen-with-ansible/</a></p>

<p>最近？は ansible container って技術もあるけど、僕らが Docker 使う目的はコンテナでデプロイするのではなくて単に Ansible を実行するローカル環境が欲しいってこととか、Serverspec をローカル・実機に実行する環境が欲しいってことなので、今でも test-kitchen 使っています。</p>

<p>で、最近になって複数ノードの構成の Ansible Role を test-kitchen, Docker を使って開発できることに気がついたので記事にしようと思います。これができるとローカルで Redis Master + Redis Slave(s) + Sentinel って環境も容易にできると思います。</p>

<h2 id="使うソフトウェア">使うソフトウェア</h2>

<p>前提は macOS ですが Linux マシンでも問題なく動作するはずです。</p>

<p>ほぼ前回と同じです。</p>

<ul>
<li>Ansible</li>
<li>Docker</li>
<li>test-kitchen</li>
<li>kitchen-docker (test-kitchen ドライバ)</li>
<li>kitchen-ansible (test-kitchen ドライバ)</li>
<li>Serverspec</li>
</ul>

<h2 id="インストール">インストール</h2>

<p>ソフトウェアののインストール方法については前回の記事を見てもらうこととして割愛します。</p>

<h2 id="test-kitchen-の環境を作る">test-kitchen の環境を作る</h2>

<p>test-kitchen の環境を作ります。&rsquo;kitchen init&rsquo; を実行して基本的には生成された .kitchen.yml を弄るんじゃなくて .kitchen.local.yml を修正していきます。こちらの記述が必ず上書きされて優先されます。</p>

<p>.kitchen.local.yml の例を下記に記します。</p>

<pre><code class="language-yaml">---
driver:
  name: docker
  binary: /usr/local/bin/docker
  socker: unix:///var/run/docker.sock
  use_sudo: false

provisioner:
  name: ansible_playbook
  roles_path: ../../roles
  group_vars_path: ../../group_vars/local/
  hosts: kitchen-deploy
  require_ansible_omnibus: false
  ansible_platform: centos
  require_chef_for_busser: true

platforms:
  - name: centos
    driver_config:
      image: centos:7.2.1511 # (例)
      platform: centos
      require_chef_omnibus: false
      privileged: true                 # systemd 使うときの例
      run_command: /sbin/init; sleep 3 # 同上

suites:
  - name: master
    provisioner:
      name: ansible_playbook
      playbook: ./site_master.yml
    driver_config:
      run_options: --net=kitchen --ip=172.18.0.11
  - name: slave
    provisioner:
      name: ansible_playbook
      playbook: ./site_slave.yml
    driver_config:
      run_options: --net=kitchen --ip=172.18.0.12
</code></pre>

<p>各パラーメタの詳細については kitchen-ansibke, kitchen-docker のドキュメントを見ていただくとして&hellip;</p>

<ul>
<li><a href="https://github.com/neillturner/kitchen-ansible/blob/master/provisioner_options.md">https://github.com/neillturner/kitchen-ansible/blob/master/provisioner_options.md</a></li>
<li><a href="https://github.com/test-kitchen/kitchen-docker">https://github.com/test-kitchen/kitchen-docker</a></li>
</ul>

<p>特徴としては suites: の項目で mater, slave として Docker コンテナを2つ扱うことを宣言している部分です。</p>

<ul>
<li>name: master で site_master.yml という Playbook を実行することを宣言</li>
<li>name: master で 172.18.0.11 という IP アドレスを使うことを宣言</li>
<li>name: slave で site_slave.yml という Playbook を実行することを宣言</li>
<li>name: slave で 172.18.0.12 という IP アドレスを使うことを宣言</li>
</ul>

<p>勿論、同様の記述で 3, 4.. 個目のコンテナ環境を作ることもできます。</p>

<p>こんな感じです。ここで &ldquo;固定 IP アドレス&rdquo; を使うよう宣言したことはとても重要で、例えば NSD のクラスタ構成を作りたい時、お互いのコンテナ同士が相手のコンテナの IP アドレスを知り合う必要があります。</p>

<ul>
<li>NSD Master は Slave へ Xfer するため Slave コンテナの IP アドレスを知る必要あり</li>
<li>NSD Slave は Master からのみ Xfer 受信する設定が必要なので Master IP を知る必要あり</li>
</ul>

<p>って感じです。RabbitMQ や Redis, Consul などのクラスタの際にも同様に互い・もしくは一方の IP アドレスを知る必要が出てくる場合があります。</p>

<p>で、固定 IP アドレスなのですが Docker では動的 IP が基本なので、固定 IP アドレスを用いるために macOS + Docker 環境の中に一つネットワークを作る必要があります。下記を実行するだけで作成できます。</p>

<pre><code class="language-bash">docker network create --subnet 172.18.0.0/24 kitchen
</code></pre>

<h2 id="serverspec-のディレクトリ構成">Serverspec のディレクトリ構成</h2>

<p>前回の記事でも書いたように、Ansible でコンテナに対してデプロイした結果に対して Servrspec テストが流せます。で今回は、クラスタ構成で複数のコンテナを扱うのでそれぞれのコンテナに対して流すテストが必要になります。下記のようなディレクトリ構成があればそれを実現できます。</p>

<pre><code>.
├── README.md
├── chefignore
├── site_master.yml
├── site_slave.yml
└── test
    └── integration
        ├── master
        │   └── serverspec
        │       └── nsd_master_spec.rb
        └── slave
            └── serverspec
                └── nsd_slave_spec.rb
</code></pre>

<p>そして 上記の nsd_master_spec,rb, nsd_slave_spec.rb の記述の仕方ですが下記のようになります。</p>

<pre><code>require 'serverspec'

# Required by serverspec
set :backend, :exec

describe package('nsd') do
  it { should be_installed }
end

describe process('nsd') do
  it { should be_running }
end

...&lt;省略&gt;...

</code></pre>

<h2 id="コンテナデプロイ-テスト実行方法">コンテナデプロイ・テスト実行方法</h2>

<p>ここまでで環境が整ったと思うので (実際には site_master.yml 等の Playbook もしくは Ansible Role の指定が必要) 、コンテナに対して Ansible デプロイする方法・Serverspec テスト実行する方法を書いていきます。</p>

<pre><code class="language-bash">kitchen create   # 2 コンテナ作られます
kitchen converge # 2 コンテナに対して Ansible デプロイされます
kitchen verify   # 2 コンテナに対して Serverspec テストされます
kitchen test     # destroy, create, converge, veriy, destroy が一斉実行されます
</code></pre>

<p>各コンテナ毎に操作する場合</p>

<pre><code class="language-bash">kitchen create master-centos   # or slave-centos
kitchen converge master-centos # or slave-centos
kitchen verify master-centos   # or slave-centos
kitchen destroy master-centos  # or slave-centos
</code></pre>

<h2 id="まとめ">まとめ</h2>

<p>ローカルの Docker コンテナに対してデプロイテストできるのでとても手軽に、尚且つ高速に Ansible の Playbook や Role が開発出来ることは前回の記事でわかったと思うのですが、クラスタ構成についてもローカルで開発出来ることがわかりました。</p>

<p>ちなみに私達の環境だと .kitchen.local.yml に下記の記述をしているので&hellip;</p>

<pre><code>roles_path: ../../roles
</code></pre>

<p>site_master(slave).yml の中の記述としては下記のようにしています。&rsquo;nsd&rsquo; っていう Role を作る想定で Role 指定がしてあった、尚且つローカルでの Ansible Variable を上書き(一番優先される) することで、Role のローカルデプロイを実現しています。(variable 名は Role の作り方によります)</p>

<pre><code class="language-yaml">---
- hosts: kitchen-deploy
  sudo: yes
  roles:
    - { role: nsd, tags: nsd }
  vars:
    nsd_type: 'master'
    nsd_master_addr: '172.18.0.11'
    nsd_slave_addr: '172.18.0.12'
</code></pre>

<p>test-kitchen 自体はリリースされてだいぶ時間が経過している分、技術的に枯れてきていて未だに私達は Ansible の開発に役立てています。元々は Chef 向けの技術ではあるのだけどそこは今回紹介した .kitchen.local.yml の書き方で回避できるし、Serverspec のテストも流せるし..。</p>

<p>またローカル(ホストの macOS) へのポートマッピング等もコンテナ毎に記せますのでその辺りはドキュメントを読んでみてください。</p>

<p>最終的にはここで書いた Serverspec テストを実機へ流せれば文句なしだと思います。方法はあると思います。また気がついたら紹介しようと思います。</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2017/07/02/test-kitchen-cluster/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>


        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2017/04/13/gke-lb/">GCP ロードバランサと GKE クラスタを Terraform を使って構築する</a></h2>
        
        
            <p>GCP ロードバランサと GKE クラスタを Terraform を使って構築自動化</p>
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2017-04-13'>
            April 13, 2017</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは。<a href="https://twitter.com/jedipunkz">@jedipunkz</a> です。</p>

<p>少し前まで Google Cloud Platform (GCP) を使っていたのですが、今回はその時に得たノウハウを記事にしようと思います。</p>

<p>Google Container Engine (GKE) とロードバランサを組み合わせてサービスを立ち上げていました。手動でブラウザ上からポチポチして構築すると人的ミスや情報共有という観点でマズイと思ったので Terraform を使って GCP の各リソースを構築するよう仕掛けたのですが、まだまだ Terraform を使って GCP インフラを構築されている方が少ないため情報が無く情報収集や検証に時間が掛かりました。よって今回はまだネット上に情報が少ない GKE とロードバランサの構築を Terraform を使って行う方法を共有します。</p>

<h2 id="構築のシナリオ">構築のシナリオ</h2>

<p>構築するにあたって2パターンの構築の流れがあると思います。</p>

<ul>
<li>1) GKE クラスタとロードバランサを同時に構築する</li>
<li>2) GKE クラスタを予め構築しそのクラスタ向けにロードバランサを構築する</li>
</ul>

<p>両方の方法を記していきます。</p>

<h2 id="1-gke-クラスタとロードバランサを同時に構築する">1) GKE クラスタとロードバランサを同時に構築する</h2>

<p>GKE クラスタとロードバランサを同時に作る方法です。</p>

<p>早速ですが下記に terraform ファイルを記しました。それぞれのシンタックスの意味については Terraform の公式ドキュメントをご覧になってください。</p>

<p>公式ドキュメント : <a href="https://www.terraform.io/docs/providers/google/">https://www.terraform.io/docs/providers/google/</a></p>

<p>ここで特筆すべき点としては下記が挙げられます。</p>

<h3 id="ロードバランサに紐付けるバックエンドの-id-取得のため-replace-element-的なことをしている">ロードバランサに紐付けるバックエンドの ID 取得のため &ldquo;${replace(element&hellip;&rdquo; 的なことをしている</h3>

<p>ロードバランサのバックエンドサービスに対して GKE クラスタを作成した際に自動で作成されるインスタンスグループの URI を紐付ける必要があります。ユーザとしては URI ではなく &ldquo;インスタンスグループ名&rdquo; であると扱いやすいのですが、URI が必要になります。この情報は下記のサイトを参考にさせていただきました。</p>

<ul>
<li>参考サイト : GKEでkubernetesのnodesをロードバランサーのバックエンドとして使いたいとき with terraform</li>
<li>URL : <a href="http://qiita.com/techeten/items/b2ec5f11f4a70dd21d70">http://qiita.com/techeten/items/b2ec5f11f4a70dd21d70</a></li>
</ul>

<h3 id="ロードバランサ一つ作るために-6-個ものインフラリソースを作っている">ロードバランサ一つ作るために 6 個ものインフラリソースを作っている</h3>

<p>一つのロードバランサを作るために6つのインフラリソースが必要になるというのも驚きですが公式ドキュメントを読むとなかなかその感覚がつかめませんでした。それぞれの簡単な意味を下記に記しておきます。</p>

<ul>
<li>google_compute_http_health_check : ヘルスチェック</li>
<li>google_compute_backend_service : バックエンドサービス</li>
<li>google_compute_url_map : ロードバランサ名となるリソース</li>
<li>google_compute_target_http_proxy : プロキシ</li>
<li>google_compute_global_address : グローバル IP アドレス</li>
<li>google_compute_global_forwarding_rule : ポートマッピングによるフォワーディングルール</li>
</ul>

<p>それでは実際の Terraform コードです</p>

<pre><code># 共通変数
variable &quot;credentials&quot; {default = &quot;/path/to/credentials.json&quot;}
variable &quot;project&quot; {default = &quot;test01&quot;}
variable &quot;region&quot; {default = &quot;asia-northeast&quot;}
variable &quot;zone&quot; {default = &quot;asia-northeast1-b&quot;}
# GKE クラスタ用の変数
variable &quot;gke_name&quot; {default = &quot;gke-terraform-test&quot;}
variable &quot;machine_type&quot; {default = &quot;n1-standard-1&quot;}
variable &quot;disk_size_gb&quot; {default = &quot;50&quot;}
variable &quot;node_count&quot; {default = &quot;2&quot;}
variable &quot;network&quot; {default = &quot;default&quot;}
variable &quot;subnetwork&quot; {default = &quot;default&quot;}
variable &quot;cluster_ipv4_cidr&quot; {default = &quot;10.0.10.0/14&quot;}
variable &quot;username&quot; {default = &quot;username&quot;}
variable &quot;password&quot; {default = &quot;password&quot;}
# ロードバランサ用の変数
variable &quot;lb_name&quot; {default = &quot;lb-terraform-test&quot;}
variable &quot;healthcheck_name&quot; {default = &quot;healthcheck-terraform-test&quot;}
variable &quot;healthcheck_host&quot; {default = &quot;test.example.com&quot;}
variable &quot;healthcheck_port&quot; {default = &quot;30300&quot;}
variable &quot;backend_name&quot; {default = &quot;backend-terraform-test&quot;}
variable &quot;http_proxy_name&quot; {default = &quot;terraform-proxy&quot;}
variable &quot;global_address_name&quot; {default = &quot;terraform-global-address&quot;}
variable &quot;global_forwarding_rule_name&quot; {default = &quot;terraform-global-forwarding-rule&quot;}
variable &quot;global_forwarding_rule_port&quot; {default =&quot;80&quot;}
variable &quot;port_name&quot; {default = &quot;port-test&quot;}
variable &quot;enable_cdn&quot; {default = false}

provider &quot;google&quot; {
  credentials = &quot;${file(&quot;${var.credentials}&quot;)}&quot;
  project     = &quot;${var.project}&quot;
  region      = &quot;${var.region}&quot;
}

resource &quot;google_container_cluster&quot; &quot;cluster-terraform&quot; {
  name                = &quot;${var.gke_name}&quot;
  zone                = &quot;${var.zone}&quot;
  initial_node_count  = &quot;${var.node_count}&quot;
  network             = &quot;${var.network}&quot;
  subnetwork          = &quot;${var.subnetwork}&quot;
  cluster_ipv4_cidr   = &quot;${var.cluster_ipv4_cidr}&quot;

  master_auth {
    username = &quot;${var.username}&quot;
    password = &quot;${var.password}&quot;
  }

  node_config {
    machine_type = &quot;${var.machine_type}&quot;
    disk_size_gb = &quot;${var.disk_size_gb}&quot;
    oauth_scopes = [
      &quot;https://www.googleapis.com/auth/compute&quot;,
      &quot;https://www.googleapis.com/auth/devstorage.read_only&quot;,
      &quot;https://www.googleapis.com/auth/logging.write&quot;,
      &quot;https://www.googleapis.com/auth/monitoring&quot;
    ]
  }
  addons_config {
    http_load_balancing {
      disabled = true
    }
    horizontal_pod_autoscaling {
      disabled = true
    }
  }
}

resource &quot;google_compute_http_health_check&quot; &quot;healthcheck-terraform&quot; {
  name                = &quot;${var.healthcheck_name}&quot;
  project             = &quot;${var.project}&quot;
  request_path        = &quot;/&quot;
  host                = &quot;${var.healthcheck_host}&quot;
  check_interval_sec  = 5
  timeout_sec         = 5
  port                = &quot;${var.healthcheck_port}&quot;
}
resource &quot;google_compute_backend_service&quot; &quot;backend-terraform&quot; {
  name          = &quot;${var.backend_name}&quot;

  port_name     = &quot;${var.port_name}&quot;
  protocol      = &quot;HTTP&quot;
  timeout_sec   = 10
  enable_cdn    = &quot;${var.enable_cdn}&quot;
  region        = &quot;${var.region}&quot;
  project       = &quot;${var.project}&quot;
  backend {
    group = &quot;${replace(element(google_container_cluster.cluster-terraform.instance_group_urls, 1), &quot;Manager&quot;,&quot;&quot;)}&quot;
  }
  health_checks = [&quot;${google_compute_http_health_check.healthcheck-terraform.self_link}&quot;]
}
resource &quot;google_compute_url_map&quot; &quot;lb-terraform&quot; {
  name            = &quot;${var.lb_name}&quot;
  default_service = &quot;${google_compute_backend_service.backend-terraform.self_link}&quot;
  project         = &quot;${var.project}&quot;
}
resource &quot;google_compute_target_http_proxy&quot; &quot;http_proxy-terraform&quot; {
  name        = &quot;${var.http_proxy_name}&quot;
  url_map     = &quot;${google_compute_url_map.lb-terraform.self_link}&quot;
}
resource &quot;google_compute_global_address&quot; &quot;ip-terraform&quot; {
  name    = &quot;${var.global_address_name}&quot;
  project = &quot;${var.project}&quot;
}
resource &quot;google_compute_global_forwarding_rule&quot; &quot;forwarding_rule-terraform&quot; {
  name        = &quot;${var.global_forwarding_rule_name}&quot;
  target      = &quot;${google_compute_target_http_proxy.http_proxy-terraform.self_link}&quot;
  ip_address  = &quot;${google_compute_global_address.ip-terraform.address}&quot;
  port_range  = &quot;${var.global_forwarding_rule_port}&quot;
  project     = &quot;${var.project}&quot;
}
</code></pre>

<h2 id="2-gke-クラスタを予め構築しそのクラスタ向けにロードバランサを構築する">2) GKE クラスタを予め構築しそのクラスタ向けにロードバランサを構築する</h2>

<p>次に GKE クラスタとロードバランサを別々のタイミングに構築する方法です。
実際には GKE クラスタの上には多数のコンテナが起動されるのですでにクラスタが存在する状態でロードバランサを別サービスのために作成したいというケースが一般的なように思います。</p>

<h3 id="gke-クラスタのインスタンスグループ-uri-を取得し設定">GKE クラスタのインスタンスグループ URI を取得し設定</h3>

<p>既存 GKE クラスタを用いる場合、インスタンスグループ URI がいずれにせよ必要になります。
インスタンスグループ URI を取得するには gcloud CLI を使って下記のように知ることができます。</p>

<pre><code class="language-shell">$ gcloud compute instance-groups managed describe | grep -rin URI
</code></pre>

<p>ここで得たインスタンスグループ URI は下記のように variable &ldquo;backend_group&rdquo; に記します。</p>

<h3 id="port-マッピングを手動で設定">Port マッピングを手動で設定</h3>

<p>ここは残念なのですが <sup>2017</sup>&frasl;<sub>04</sub> 現在、Port マッピングの設定を WebUI 上から行う必要があります。UI から GKE クラスタのインスタンスグループを選択し、Port マッピングの設定を行い名前を記します。ここでは &ldquo;port-test&rdquo; として作成したとし説明します。</p>

<h3 id="terraform-のコードを記述">Terraform のコードを記述</h3>

<p>ここでロードバランサ単独で構築する際の Terraform コードを見てみます。</p>

<pre><code>variable &quot;credentials&quot; {default = &quot;/path/to/credentials.json&quot;}
variable &quot;project&quot; {default = &quot;test01&quot;}
variable &quot;region&quot; {default = &quot;asia-northeast1&quot;}
variable &quot;lb_name&quot; {default = &quot;lb-terraform-test&quot;}
variable &quot;healthcheck_name&quot; {default = &quot;healthcheck-terraform-test&quot;}
variable &quot;healthcheck_host&quot; {default = &quot;test.example.com&quot;}
variable &quot;healthcheck_port&quot; {default = &quot;30300&quot;}
variable &quot;backend_name&quot; {default = &quot;backend-terraform-test&quot;}
variable &quot;backend_group&quot; {default = &quot;https://www.googleapis.com/compute/v1/projects/test01/zones/asia-northeast1-b/instanceGroups/gke-gke-terraform-test-default-pool-c001020e-grp&quot;}
variable &quot;http_proxy_name&quot; {default = &quot;terraform-proxy&quot;}
variable &quot;global_address_name&quot; {default = &quot;terraform-global-address&quot;}
variable &quot;global_forwarding_rule_name&quot; {default = &quot;terraform-global-forwarding-rule&quot;}
variable &quot;global_forwarding_rule_port&quot; {default =&quot;80&quot;}
variable &quot;port_name&quot; {default = &quot;port-test&quot;}
variable &quot;enable_cdn&quot; {default = false}

provider &quot;google&quot; {
  credentials = &quot;${file(&quot;${var.credentials}&quot;)}&quot;
  project     = &quot;${var.project}&quot;
  region      = &quot;${var.region}&quot;
}
resource &quot;google_compute_http_health_check&quot; &quot;healthcheck-terraform-test&quot; {
  name                = &quot;${var.healthcheck_name}&quot;
  project             = &quot;${var.project}&quot;
  request_path        = &quot;/&quot;
  host                = &quot;${var.healthcheck_host}&quot;
  check_interval_sec  = 5
  timeout_sec         = 5
  port                = &quot;${var.healthcheck_port}&quot;
}
resource &quot;google_compute_backend_service&quot; &quot;backend-terraform-test&quot; {
  name          = &quot;${var.backend_name}&quot;
  port_name     = &quot;${var.port_name}&quot;
  protocol      = &quot;HTTP&quot;
  timeout_sec   = 10
  enable_cdn    = &quot;${var.enable_cdn}&quot;
  region        = &quot;${var.region}&quot;
  project       = &quot;${var.project}&quot;
  backend {
    group = &quot;${var.backend_group}&quot;
  }
  health_checks = [&quot;${google_compute_http_health_check.healthcheck-terraform-test.self_link}&quot;]
}
resource &quot;google_compute_url_map&quot; &quot;lb-terraform-test&quot; {
  name            = &quot;${var.lb_name}&quot;
  default_service = &quot;${google_compute_backend_service.backend-terraform-test.self_link}&quot;
  project         = &quot;${var.project}&quot;
}
resource &quot;google_compute_target_http_proxy&quot; &quot;http_proxy-terraform-test&quot; {
  name        = &quot;${var.http_proxy_name}&quot;
  url_map     = &quot;${google_compute_url_map.lb-terraform-test.self_link}&quot;
}
resource &quot;google_compute_global_address&quot; &quot;ip-terraform-test&quot; {
  name    = &quot;${var.global_address_name}&quot;
  project = &quot;${var.project}&quot;
}
resource &quot;google_compute_global_forwarding_rule&quot; &quot;forwarding_rule-terraform-test&quot; {
  name        = &quot;${var.global_forwarding_rule_name}&quot;
  target      = &quot;${google_compute_target_http_proxy.http_proxy-terraform-test.self_link}&quot;
  ip_address  = &quot;${google_compute_global_address.ip-terraform-test.address}&quot;
  port_range  = &quot;${var.global_forwarding_rule_port}&quot;
  project     = &quot;${var.project}&quot;
</code></pre>

<h2 id="まとめ">まとめ</h2>

<p>GCP のロードバランサが特徴的な構築方法が必要になることが分かったと思います。将来的に URI で記す箇所を名前で記せれば構築がもっと簡単になると思いますので GCP 側の API バージョンアップを期待します。また今回は記しませんでしたが SSL (HTTPS) 証明書をロードバランサに紐付けることも Terraform を使えば容易に出来ます。試してみてください。一旦 Terraform 化すればパラメータを変更するだけで各ロードバランサが一発で構築できるので自動化は是非しておきたいところです。私は以前、この構築を Terraform + Hubot により ChatOps 化していました。作業の見える化と、メンバ間のコードレビューが可能になるからです。</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2017/04/13/gke-lb/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>


        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2017/02/12/serverless-fission/">Serverless on Kubernetes : Fission を使ってみた</a></h2>
        
        
            <p>Kubernetes 上で Serverless を実現する Fission を使ってみた</p>
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2017-02-12'>
            February 12, 2017</time>
        <span class="author">jedipunkz</span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは。 @jedipunkz です。</p>

<p>今日は Kubernetes を使って Serverless を実現するソフトウェア Fission を紹介します。</p>

<p>AWS の Lambda とよく似た動きをします。Lambda も内部では各言語に特化したコンテナが起動してユーザが開発した Lambda Function を実行してくれるのですが、Fission も各言語がインストールされた Docker コンテナを起動しユーザが開発したコードを実行し応答を返してくれます。</p>

<p>それでは早速なのですが、Fission を動かしてみましょう。</p>

<h2 id="動作させるための環境">動作させるための環境</h2>

<p>macOS か Linux を前提として下記の環境を用意する必要があります。また Kubernetes 環境は minikube が手っ取り早いので用いますが、もちろん minikube 以外の kubernetes 環境でも動作します。</p>

<ul>
<li>macOS or Linux</li>
<li>minikube or kubernetes</li>
<li>kubectl</li>
<li>fission</li>
</ul>

<h2 id="ソフトウェアのインストール方法">ソフトウェアのインストール方法</h2>

<p>簡単にですが、ソフトウェアのインストール方法を書きます。</p>

<h4 id="os">OS</h4>

<p>私は Linux で動作させましたが筆者の方は macOS を使っている方が多数だと思いますので、この手順では macOS を使った利用方法を書いていきます。</p>

<h4 id="minikube">minikube</h4>

<p>ここでは簡単な手順で kubernetes 環境を構築できる minikube をインストールします。</p>

<pre><code>curl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.16.0/minikube-darwin-amd64 &amp;&amp; chmod +x minikube &amp;&amp; sudo mv minikube /usr/local/bin/fission
</code></pre>

<h4 id="kubectl">kubectl</h4>

<p>直接必要ではありませんが、kubectl があると minikube で構築した kubernetes 環境を操作できますのでインストールしておきます。</p>

<pre><code>curl -Lo kubectl https://storage.googleapis.com/kubernetes-release/release/v1.5.2/bin/linux/amd64/kubectl &amp;&amp; chmod +x kubectl &amp;&amp; sudo mv kubectl /usr/local/bin/
</code></pre>

<h4 id="fission">Fission</h4>

<p>Fission のインストールです。</p>

<pre><code>curl http://fission.io/linux/fission &gt; fission &amp;&amp; chmod +x fission &amp;&amp; sudo mv fission /usr/local/bin/fission
</code></pre>

<h2 id="kubernetes-の起動">kubernetes の起動</h2>

<p>ソフトウェアのインストールが完了したら minikube を使って kubernetes を起動します。</p>

<pre><code>$ minikube start
$ minikube status
minikubeVM: Running
localkube: Running
$ kubectl get nodes
NAME       STATUS    AGE
minikube   Ready     1h
</code></pre>

<h2 id="fission-の起動と環境変数の設定">Fission の起動と環境変数の設定</h2>

<p>Fission を起動します。</p>

<pre><code>$ kubectl create -f http://fission.io/fission.yaml
$ kubectl create -f http://fission.io/fission-nodeport.yaml
</code></pre>

<p>次に環境変数を設定します。</p>

<pre><code>$ export FISSION_URL=http://$(minikube ip):31313
$ export FISSION_ROUTER=$(minikube ip):31314
</code></pre>

<h2 id="fission-を使って-python-のコードを実行する">Fission を使って Python のコードを実行する</h2>

<p>例として Python の Hello World を用意します。hello.py として保存します。</p>

<pre><code>def main():
        return &quot;Hello, world!\n&quot;
</code></pre>

<p>ではいよいよ、kubernetes と Fission を使って上記の Hello World を実行させます。</p>

<p>まず Fission が用意してくれている Docker コンテナを扱うように &lsquo;env&rsquo; を作ります。</p>

<pre><code>$ fission env create --name python-env --image fission/python-env
</code></pre>

<p>次に Fission で Function を作ります。その際に上記の env と python コードを指定します。つまり、hello.py を fission/python-env という Docker コンテナで稼働する、という意味です。</p>

<pre><code>$ fission function create --name python-hello -env python-env --code ./hello.py
</code></pre>

<p>次に Router を作ります。クエリの Path に対して Fuction を関連付けることができます。</p>

<pre><code>$ fission route add --function python-hello --url /python-hello
</code></pre>

<p>Function を実行する環境ができました。実際に curl を使ってアクセスしてみましょう。</p>

<pre><code>$ curl http://$FISSION_ROUTER/python-hello
Hello, world!
</code></pre>

<p>hello.py の実行結果が得られました。</p>

<h2 id="まとめと考察">まとめと考察</h2>

<p>結果から Fission は &ldquo;各言語の実行環境として Docker コンテナを用いていて、ユーザが開発したコードをそのコンテナ上で起動し実行結果を得られる。また各コード毎に URL パスが指定することができ、それをルータとして関係性を持たせられる&rdquo; ということが分かりました。AWS の Lambda とほぼ同じことが実現出来ていることが分かると思います。</p>

<p>AWS には Lambda の実行結果を応答するための API Gateway があり、このマネージド HTTP サーバと併用することで API 環境を用意出来るのですが Fission の場合には HTTP サーバも込みで提供されていることも分かります。</p>

<p>あとは、この Fission を提供している元が &ldquo;Platform9&rdquo; という企業なのですが、この企業は OpenStack や kubernetes を使ったホスティングサービスを提供しているようです。開発元が一企業ということと、完全な OSS 開発体制になっていない可能性があって、万が一この企業に何かあった場合にこの Fission を使い続けられるのか問題がしばらくはありそうです。Fission 同等のソフトウェアを kubernetes が取り込むという話題は&hellip;あるのかなぁ？</p>

<p>kubernetes の Job Scheduler が同等の機能を提供してくれるかもしれませんが、まだ Job Scheduler は利用するには枯れていない印象があります。Fission と Job Scheduler 、どちらがいち早く完成度を上げられるのでしょうか。</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2017/02/12/serverless-fission/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>


        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2017/01/13/kubernetes-deployments/">Kubernetes Deployments を使ってみた！</a></h2>
        
        
            <p>Kubernetes Replication Controller の次世代版 Deployments を使ってみました</p>
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2017-01-13'>
            January 13, 2017</time>
        <span class="author">jedipunkz</span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは。 @jedipunkz です。</p>

<p>いま僕らは職場では GKE 上に Replication Controller と Services を使って Pod を起動しているのですが最近の Kubernetes 関連のドキュメントを拝見すると Deployments を使っている記事をよく見掛けます。Kubernetes 1.2 から実装されたようです。今回は Kubernetes の Replication Controller の次世代版と言われている Deployments について調べてみましたので理解したことを書いていこうかと思います。</p>

<h2 id="参考資料">参考資料</h2>

<p>今回は Kubernetes 公式の下記のドキュメントに記されているコマンドを一通り実行していきます。追加の情報もあります。</p>

<ul>
<li><a href="https://kubernetes.io/docs/user-guide/deployments/">https://kubernetes.io/docs/user-guide/deployments/</a></li>
</ul>

<h2 id="deployments-を使って-nginx-pod-を起動">Deployments を使って nginx Pod を起動</h2>

<p>nginx をデプロイするための Yaml ファイルを用意します。</p>

<pre><code>apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80
</code></pre>

<p>作成した yaml ファイルを指定して Pod を作ります。
下記の通りここで &ldquo;&ndash;record&rdquo; と記しているのは、後に Deployments の履歴を表示する際に &ldquo;何を行ったか&rdquo; を出力するためです。このオプションを指定しないと &ldquo;何を行ったか&rdquo; の出力が &ldquo;NONE&rdquo; となります。</p>

<pre><code>$ kubectl create -f nginx.yaml --record
</code></pre>

<p>ここで</p>

<ul>
<li>deployments</li>
<li>replica set</li>
<li>pod</li>
<li>rollout</li>
</ul>

<p>の状態をそれぞれ確認してみます。</p>

<pre><code>$ kubectl get deployments
NAME               DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   2         2         2            2           8s

$ kubectl get rs
NAME                          DESIRED   CURRENT   READY     AGE
nginx-deployment-4087004473   2         2         2         10s

$ kubectl get pods --show-labels
NAME                                READY     STATUS    RESTARTS   AGE       LABELS
nginx-deployment-4087004473-6csa7   1/1       Running   0          21s       app=nginx,pod-template-hash=4087004473
nginx-deployment-4087004473-teyzc   1/1       Running   0          21s       app=nginx,pod-template-hash=4087004473

$ kubectl rollout status deployment/nginx-deployment
deployment nginx-deployment successfully rolled out
</code></pre>

<p>結果から、下記の事が分かります。</p>

<ul>
<li>yaml に記した通り &ldquo;nginx-deployment&rdquo; という名前で deployment が生成された</li>
<li>&ldquo;nginx-deployment-4087004473&rdquo; という名前の rs (レプリカセット) が生成された</li>
<li>yaml に記した通り2つの Pod が起動した</li>
<li>&ldquo;nginx-deployment&rdquo; が正常に Rollout された</li>
</ul>

<p>Replication Controller でデプロイした際には作られない replica set, rollout というモノが出てきました。後に Deployments を使うメリットに繋がっていきます。</p>

<h2 id="nginx-イメージの-tag-を更新してみる">nginx イメージの Tag を更新してみる</h2>

<p>ここで yaml ファイル内で指定していた &ldquo;image: nginx:1.7.9&rdquo; を &ldquo;image: nginx:1.9.1&rdquo; と更新してみます。
Replication Controller で言う Rolling-Update になります。後に述べますが他にも更新方法があります。</p>

<pre><code>$ kubectl set image deployment/nginx-deployment nginx=nginx:1.9.1
</code></pre>

<p>ここで先ほどと同様に状態を確認してみます。</p>

<pre><code>$ kubectl get deployments
NAME               DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   2         2         2            2           2m

$ kubectl get rs
NAME                          DESIRED   CURRENT   READY     AGE
nginx-deployment-3599678771   2         2         2         39s
nginx-deployment-4087004473   0         0         0         2m

$ kubectl get pods
NAME                                READY     STATUS    RESTARTS   AGE
nginx-deployment-3599678771-0vj9m   1/1       Running   0          53s
nginx-deployment-3599678771-t1y62   1/1       Running   0          53s
</code></pre>

<p>ここで&hellip;</p>

<ul>
<li>新しい Replica Set &ldquo;nginx-deployment-3599678771&rdquo; が作成された</li>
<li>古い Replica Set &ldquo;nginx-deployment-4087004473&rdquo; の Pod は 0 個になった</li>
<li>Pod 内コンテナが更新された (NAME より判断)</li>
</ul>

<p>となったことが分かります。
Replication Controller と異なり、Deployments では以前の状態が Replica Set として保存されていて状態の履歴が追えるようになっています。</p>

<p>ここで Rollout の履歴を確認してみます。</p>

<pre><code>$ kubectl rollout history deployment/nginx-deployment
deployments &quot;nginx-deployment&quot;
REVISION        CHANGE-CAUSE
1               kubectl create -f nginx.yaml --record
2               kubectl set image deployment/nginx-deployment nginx=nginx:1.9.1
</code></pre>

<p>REVISION という名前で履歴番号が付き、どの様な作業を行ったか CHANGE-CAUSE という項目で記されていることがわかります。作業の履歴がリビジョン管理されています。</p>

<p>下記のように REVSION 番号を付与して履歴内容を表示することも可能です。</p>

<pre><code>$ kubectl rollout history deployment/nginx-deployment --revision=2
deployments &quot;nginx-deployment&quot; with revision #2
  Labels:       app=nginx
        pod-template-hash=3599678771
  Annotations:  kubernetes.io/change-cause=kubectl set image deployment/nginx-deployment nginx=nginx:1.9.1
  Containers:
   nginx:
    Image:      nginx:1.9.1
    Port:       80/TCP
    Volume Mounts:      &lt;none&gt;
    Environment Variables:      &lt;none&gt;
  No volumes.
</code></pre>

<h2 id="作業を切り戻してみる">作業を切り戻してみる</h2>

<p>先程 nginx の Image Tag を更新しましたが、ここで Deployments の機能を使って作業を切り戻してみます。下記の様に実行します。</p>

<pre><code>$ kubectl rollout undo deployment/nginx-deployment
</code></pre>

<p>状態を確認します。</p>

<pre><code>$ kubectl get deployments
NAME               DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   2         2         2            2           5m

$ kubectl rollout history deployment/nginx-deployment
deployments &quot;nginx-deployment&quot;
REVISION        CHANGE-CAUSE
2               kubectl set image deployment/nginx-deployment nginx=nginx:1.9.1
3               kubectl create -f nginx.yaml --record
</code></pre>

<p>ここでは&hellip;</p>

<ul>
<li>コンテナが2個、正常に起動した</li>
<li>REVISION 番号 3 として初期構築の状態 (kubectl create ..) が新たに保存</li>
</ul>

<p>ということが分かります。注意したいのは REVSION 番号 1 が削除され 3 が生成されたことです。1 と 3 は同じ作業ということと推測します。</p>

<p>念のため &lsquo;nginx&rsquo; コンテナの Image Tag が切り戻っているか確認してみます。</p>

<pre><code>$ kubectl describe pod nginx-deployment-4087004473-nq35u | grep &quot;Image:&quot;
    Image:              nginx:1.7.9
</code></pre>

<p>最初の Yaml ファイルに記した &lsquo;nginx&rsquo; イメージ Tag &ldquo;1.7.9&rdquo; となっていることが確認できました。set image &hellip; でイメージ更新をした作業が正常に切り戻ったことになります。</p>

<h2 id="レプリカ数を-2-3-へスケールしてみる">レプリカ数を 2-&gt;3 へスケールしてみる</h2>

<p>更に replicas の数値を 2 から 3 へスケールしてみます。</p>

<pre><code>$ kubectl scale deployment nginx-deployment --replicas 3
</code></pre>

<p>同様に状態を確認してみます。</p>

<pre><code>kubectl get pods
NAME                                READY     STATUS    RESTARTS   AGE
nginx-deployment-4087004473-esj5l   1/1       Running   0          6s
nginx-deployment-4087004473-nq35u   1/1       Running   0          4m
nginx-deployment-4087004473-tyibo   1/1       Running   0          4m

kubectl get rs
NAME                          DESIRED   CURRENT   READY     AGE
nginx-deployment-3599678771   0         0         0         9m
nginx-deployment-4087004473   3         3         3         11m

kubectl rollout history deployment/nginx-deployment
deployments &quot;nginx-deployment&quot;
REVISION        CHANGE-CAUSE
2               kubectl set image deployment/nginx-deployment nginx=nginx:1.9.1
3               kubectl scale deployment nginx-deployment --replicas 3
</code></pre>

<p>ここで気になるのは REVISION 3 が上書きされたことです。REVSION 番号 4 が新たに作成されると思っていたからです。先程 REVISION 番号 3 として保存されていた下記の履歴が消えてしまいました。この点については引き続き検証してみます。今の自分には理解できませんでした。ご存知の方いましたら、コメントお願いします！</p>

<pre><code>3               kubectl create -f nginx.yaml --record
</code></pre>

<h2 id="puase-resume-機能を使ってみる">Puase, Resume 機能を使ってみる</h2>

<p>次は deployments の機能を使って Image Tag を更に 1.9.1 へ変更し、その処理をポーズしてみます。</p>

<pre><code>kubectl set image deployment/nginx-deployment nginx=nginx:1.9.1; kubectl
</code></pre>

<p>同様に状態を確認してみます。</p>

<pre><code>$ kubectl get rs
NAME                          DESIRED   CURRENT   READY     AGE
nginx-deployment-3599678771   2         2         2         10m
nginx-deployment-4087004473   2         2         2         12m

$ kubectl rollout status deployment/nginx-deployment
Waiting for rollout to finish: 2 out of 3 new replicas have been updated...
Ctrl-C #&lt;--- キー入力
</code></pre>

<p>rollout status で deployment &ldquo;deployment/nginx-deployment&rdquo; を確認すると &ldquo;waiting for rollout to finish&rdquo; と表示され処理がポーズされていることが確認できました。また古い Deployment &ldquo;nginx-deployment-4087004473&rdquo; 上に未だコンテナが残り、新しい Deployment もコンテナが生成中であることが分かります。</p>

<p>では Resume します。</p>

<pre><code>$ kubectl rollout resume deployment/nginx-deployment
deployment &quot;nginx-deployment&quot; resumed
</code></pre>

<p>この時点の状態を確認しましょう。</p>

<pre><code>$ kubectl rollout status deployment/nginx-deployment
deployment nginx-deployment successfully rolled out

$ kubectl get rs
NAME                          DESIRED   CURRENT   READY     AGE
nginx-deployment-3599678771   3         3         3         11m
nginx-deployment-4087004473   0         0         0         14m
</code></pre>

<p>ここからは&hellip;</p>

<ul>
<li>正常に Deployment &ldquo;nginx-deployment&rdquo; が Rollout されたこと</li>
<li>古い Deployment 上のコンテナ数が 0 に、新しい Deployment 上のコンテナ数が 3 になった</li>
</ul>

<p>ということが分かります。</p>

<h2 id="rolling-update-相当の作業を行う方法">Rolling-Update 相当の作業を行う方法</h2>

<p>前述した通り、Replication Controller 時代にあった Rolling-Update 作業 (イメージタグ・レプリカ数等の更新) ですが、Deployments では下記の方法をとることが出来ます。</p>

<h4 id="set-オプションを付与する場合">set オプションを付与する場合</h4>

<p>set オプションを付与して Key 項目に対して新しい Value を渡します。</p>

<pre><code>$ kubectl set image deployment/nginx-deployment nginx=nginx:1.9.1
</code></pre>

<h4 id="yaml-ファイルを修正する場合">yaml ファイルを修正する場合</h4>

<p>yaml ファイルの内容を更新して適用したい場合、下記のように apply オプションを付与します。</p>

<pre><code>$ kubectl apply -f &lt;新しいYamlファイル&gt;
</code></pre>

<h2 id="まとめと考察">まとめと考察</h2>

<p>REVISION の履歴が上書きされる点など、まだ未完成な感が否めませんでしたが(自分の勘違いかもしれません！)、Replication Controller と比べると作業の切り戻しや、履歴が保存され履歴内容も確認できる点など機能が追加されていることが分かりました。公式ドキュメントを読んでいてもコマンド結果等怪しい点があって流石に API バージョンが &ldquo;v1beta1&rdquo; だなぁという感じではありますが、機能が整理されていて利便性が上がっているので Replication Controller を使っているユーザは自然と今後、Deployments に移行していくのではないかと感じました。</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2017/01/13/kubernetes-deployments/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>


        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2016/12/29/cloud-cdn/">Google Cloud CDN を使ってみた</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2016-12-29'>
            December 29, 2016</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは。@jedipunkz です。</p>

<p>今回は Google Cloud Platform の Google CloudCDN について調べてみたので記したいと思います。</p>

<p>CloudCDN は GCP のロードバランサのバックエンドサービスに紐付けられるサービスです。このバックエンドサービスで CloudCDN を有効にしていると CDN サービスを機能させることが出来ます。先に書いておくとこの CloudCDN はとてもシンプルで扱いやすいサービスだと判りました。高機能な他の CDN サービスと比べると機能が足らない感ありますが、必要最低限なところを抑えているのと、価格がとても安いです。(価格は下記の URL 参照)</p>

<p>価格表 : <a href="https://cloud.google.com/cdn/pricing">https://cloud.google.com/cdn/pricing</a></p>

<h2 id="構成">構成</h2>

<p>構成と構成の特徴です。</p>

<pre><code>+----------+                                    +---------+
| instance |--+                               +-| EndUser |
+----------+  |  +------------+  +----------+ | +---------+
              +--|LoadBalancer|--| CloudCDN |-+-| EndUser |
+----------+  |  +------------+  +----------+ | +---------+
| instance |--+                               +-| EndUser |
+----------+                                    +---------+
</code></pre>

<ul>
<li>コンテンツが初めてリクエストされた場合キャッシュミスする</li>
<li>キャッシュミスした際に近くにあるキャッシュからコンテンツを取得しようと試みる</li>
<li>近くのキャッシュがコンテンツがある場合、最初のキャッシュにコンテンツが送信される</li>
<li>近くのキャッシュにコンテンツがない場合、HTTP ロードバランサにリクエストが転送される</li>
<li>その後のリクエストはキャッシュが応答する(キャッシュヒット)</li>
<li>キャッシュ間のフィルは EndUser のリクエストに応じて実行される</li>
<li>キャッシュを事前に読み込むことできない</li>
<li>キャッシュは世界各地に配置されている</li>
</ul>

<h2 id="cloudcdn-を導入する方法">CloudCDN を導入する方法</h2>

<p>導入する方法は簡単で下記のとおりです。</p>

<ul>
<li>新規 LB を WebUI で作成</li>
<li>バックエンドサービスを作成</li>
<li>右パネルの下にある [Cloud CDN を有効にする] チェックボックスをオンに</li>
<li>作成ボタンを押下</li>
</ul>

<p>HTTPD サーバ側 (今回は Apache を使います) でキャッシュに関する設定を行います。
下記は例です。3600秒、キャッシュさせる設定になります。</p>

<pre><code>Header set Cache-control &quot;public, max-age=3600&quot;
</code></pre>

<p>キャッシュされる条件は RFC7324 に規定されているとおりです。</p>

<h2 id="特定のキャッシュを削除する方法">特定のキャッシュを削除する方法</h2>

<p>Google Cloud SDK <a href="https://cloud.google.com/sdk/">https://cloud.google.com/sdk/</a> を使うと CLI でコンテンツを指定してキャッシュのクリアが出来ます。また WebUI でもクリアは可能です。</p>

<pre><code>$ gcloud compute url-maps invalidate-cdn-cache &lt;url-map の名前&gt; --path &quot;コンテンツのパス&quot;
</code></pre>

<p>このときに &ndash;path で指定できるコンテンツの記し方は下記のように指定することも可能です。</p>

<pre><code>--path &quot;/cat.png # &lt;--- 1つのコンテンツキャッシュを削除
--path &quot;/*&quot;      # &lt;--- 全てのコンテンツキャッシュを削除
--path &quot;/pix/*&quot;  # &lt;--- ディレクトリ指定で削除
</code></pre>

<h2 id="まとめ">まとめ</h2>

<p>価格が低価格で必要最低限な機能に絞られている印象です。シンプルな分、理解し易いですしキャッシュクリアについても Hubot 化などすれば開発者の方に実行してもらいやすいのではないでしょうか。</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2016/12/29/cloud-cdn/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>


        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2016/11/27/etcd-operator/">coreos の etcd operator を触ってみた</a></h2>
        
        
            <p>coreos が最近アナウンスした etcd operator を触ってみた</p>
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2016-11-27'>
            November 27, 2016</time>
        <span class="author">jedipunkz</span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは @jedipunkz です。</p>

<p>今日は某アドベントカレンダーに参加していて記事を書いています。
記事だけ先出ししちゃいます..。</p>

<p>今日は最近 coreos がリリースした &lsquo;etcd-operator&rsquo; を触ってみようかと思います。ほぼ、README に書かれている手順通りに実施するのですが、所感を交えながら作業して記事にしてみたいと思います。</p>

<p>coreos が提供している etcd についてご存知ない方もいらっしゃると思いますが etcd は KVS ストレージでありながら Configuration Management Store として利用できる分散型ストレージです。Docker 等の環境を提供する coreos という軽量 OS 内でも etcd が起動していてクラスタで管理された情報をクラスタ内の各 OS が読み書きできる、といった機能を etcd が提供しています。
詳細については公式サイトを御覧ください。</p>

<p>etcd 公式サイト : <a href="https://coreos.com/etcd/docs/latest/">https://coreos.com/etcd/docs/latest/</a></p>

<p>etcd-operator はこの etcd クラスタを kubernetes 上でクラスタ管理するための簡単に運用管理するためのソフトウェアになります。</p>

<p>etcd-operator 公式アナウンス : <a href="https://coreos.com/blog/introducing-the-etcd-operator.html">https://coreos.com/blog/introducing-the-etcd-operator.html</a></p>

<p>後に実際に触れていきますが下記のような管理が可能になります。</p>

<ul>
<li>etcd クラスタの構築</li>
<li>etcd クラスタのスケールアップ・ダウン</li>
<li>etcd Pod の障害時自動復旧</li>
<li>etcd イメージをオンラインで最新のモノにアップグレード</li>
</ul>

<p>では早速利用してみたいと思います。</p>

<h2 id="必要な環境">必要な環境</h2>

<p>下記の環境が事前に用意されている必要があります。</p>

<ul>
<li>Docker</li>
<li>Kubernetes or minikube+kubernetes (<a href="https://github.com/kubernetes/minikube">https://github.com/kubernetes/minikube</a>)</li>
<li>etcdctl : <a href="https://github.com/coreos/etcd/tree/master/etcdctl">https://github.com/coreos/etcd/tree/master/etcdctl</a></li>
</ul>

<h2 id="作業準備">作業準備</h2>

<p>下記のレポジトリをクローンします。</p>

<pre><code>$ git clone https://github.com/coreos/etcd-operator.git
</code></pre>

<h2 id="operator-のデプロイ">Operator のデプロイ</h2>

<p>下記のような内容のファイルが記さているファイルを利用します。中身を確認しましょう。</p>

<pre><code>$ cat  example/deployment.yaml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: etcd-operator
spec:
  replicas: 1
  template:
    metadata:
      labels:
        name: etcd-operator
    spec:
      containers:
      - name: etcd-operator
        image: quay.io/coreos/etcd-operator
        env:
        - name: MY_POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
</code></pre>

<p>kind: Deployment で etcd-operator のイメージを使ってレプリカ数1のポッドを起動しているのが分かると思います。実際にデプロイします。</p>

<pre><code>$ kubectl create -f example/deployment.yaml
</code></pre>

<p>どんなポッドが起動したか確認します。</p>

<pre><code>kubectl get pods
NAME                                     READY     STATUS    RESTARTS   AGE
etcd-operator-217127005-futo0            1/1       Running   0          1m
</code></pre>

<p>あと、これは自分も知りませんでしたがサードパーティリソースという枠があるらしく下記のように確認することができます。</p>

<pre><code>kubectl get thirdpartyresources
NAME                      DESCRIPTION             VERSION(S)
etcd-cluster.coreos.com   Managed etcd clusters   v1
</code></pre>

<p>replica数1 ですが、ポッドなのでポッド内のプロセスに問題があれば kubernetes が自動で修復 (場合によってポッドの再構築) されます。また replica 数を増やす運用も考えられそうです。</p>

<h2 id="etcd-クラスタの構築">etcd クラスタの構築</h2>

<p>下記の内容のファイルで etcd をデプロイします。中身を確認しましょう。
API は coreos.com/v1 で kind: EtcdCluster という情報が記されています。また version でイメージバージョンが記されているのかなということが後に確認できます。また size でレプリカ数が記されているようです。</p>

<pre><code>$ cat example/example-etcd-cluster.yaml
apiVersion: &quot;coreos.com/v1&quot;
kind: &quot;EtcdCluster&quot;
metadata:
  name: &quot;etcd-cluster&quot;
spec:
  size: 3
  version: &quot;v3.1.0-alpha.1&quot;
</code></pre>

<p>デプロイをしてみます。</p>

<pre><code>$ kubectl create -f example/example-etcd-cluster.yaml
</code></pre>

<p>クラスタがデプロイされたか見てみます。3つのポッドが確認できます。やはりファイル中 size: 3 はレプリカ数だったようです。</p>

<pre><code>$ kubectl get pods --show-all
NAME                                     READY     STATUS      RESTARTS   AGE
etcd-cluster-0000                        1/1       Running     0          1m
etcd-cluster-0001                        1/1       Running     0          36s
etcd-cluster-0002                        1/1       Running     0          21s
</code></pre>

<p>同様に Service について確認します。etcd-cluster-000[012] の3つが今回作った etcd クラスタです。</p>

<pre><code>$ kubectl get svc
NAME                    CLUSTER-IP   EXTERNAL-IP   PORT(S)             AGE
etcd-cluster            10.0.0.80    &lt;none&gt;        2379/TCP            1m
etcd-cluster-0000       10.0.0.13    &lt;none&gt;        2380/TCP,2379/TCP   1m
etcd-cluster-0001       10.0.0.111   &lt;none&gt;        2380/TCP,2379/TCP   1m
etcd-cluster-0002       10.0.0.183   &lt;none&gt;        2380/TCP,2379/TCP   50s
kubernetes              10.0.0.1     &lt;none&gt;        443/TCP             5d
</code></pre>

<h2 id="etcd-クラスタをスケールアウト">etcd クラスタをスケールアウト</h2>

<p>では etcd クラスタのレプリカ数を増やすことでスケールアウトしてみます。が、今現在 (<sup>2016</sup>&frasl;<sub>12</sub>) 時点では一部不具合があるらしく回避策として下記の通り curl を用いてスケールアウトすることが可能なようです。</p>

<p>下記の内容で body.json ファイルを用意します。size: 5 になっていることが確認できて、レプリカ数5になるのだなと判断できます。</p>

<pre><code>$ cat body.json
{
  &quot;apiVersion&quot;: &quot;coreos.com/v1&quot;,
  &quot;kind&quot;: &quot;EtcdCluster&quot;,
  &quot;metadata&quot;: {
    &quot;name&quot;: &quot;etcd-cluster&quot;,
    &quot;namespace&quot;: &quot;default&quot;
  },
  &quot;spec&quot;: {
    &quot;size&quot;: 5
  }
}
</code></pre>

<p>ここで curl で実行するためプロキシを稼働します。</p>

<pre><code>$ kubectl proxy --port=8080
</code></pre>

<p>curl で先程作った body.json を PUT してみます。</p>

<pre><code>curl -H 'Content-Type: application/json' -X PUT --data @body.json http://127.0.0.1:8080/apis/coreos.com/v1/namespaces/default/etcdclusters/etcd-cluster
</code></pre>

<p>クラスタがスケールアウトされたか確認しましょう。</p>

<pre><code>kubectl get pods --show-all
NAME                                     READY     STATUS    RESTARTS   AGE
etcd-cluster-0000                        1/1       Running   0          5m
etcd-cluster-0001                        1/1       Running   0          4m
etcd-cluster-0002                        1/1       Running   0          4m
etcd-cluster-0003                        1/1       Running   0          32s
etcd-cluster-0004                        1/1       Running   0          17s
etcd-operator-217127005-futo0            1/1       Running   0          9m
</code></pre>

<p>5台構成のクラスタにスケールアウトしたことが確認できます。</p>

<h2 id="etcd-にアクセス">etcd にアクセス</h2>

<p>5台構成の etcd クラスタがデプロイできたので etcd に etcdctl を使ってアクセスしてみましょう。事前に etcdctl をインストールする必要があります。また私の環境もそうだったのですがローカルの Mac に minikube を使って kubernetes 環境を構築しているので下記のように nodePort を作るため作業が必要です。</p>

<pre><code>$ kubectl create -f example/example-etcd-cluster-nodeport-service.json
$ export ETCDCTL_API=3
$ export ETCDCTL_ENDPOINTS=$(minikube service etcd-cluster-client-service --url)
$ etcdctl put foo bar
$ etcdctl get foo
foo
bar
</code></pre>

<p>etcdctl でキー・値を入力・読み込みが可能であることが分かりました！</p>

<h2 id="ポッドの自動復旧">ポッドの自動復旧</h2>

<p>kubernetes を普段使っている方は分かると思うのですがポッドを落としても kubernetes が自動復旧してくれます。ここで一つポッドを削除してみようと思います。</p>

<pre><code>$kubectl get pods
NAME                                     READY     STATUS    RESTARTS   AGE
etcd-cluster-0000                        1/1       Running   0          11m
etcd-cluster-0001                        1/1       Running   0          11m
etcd-cluster-0002                        1/1       Running   0          11m
etcd-cluster-0003                        1/1       Running   0          6m
etcd-cluster-0004                        1/1       Running   0          6m
etcd-operator-217127005-futo0            1/1       Running   0          16m

$kubect delete pod etcd-cluster-0004
</code></pre>

<p>しばらくすると下記の通り etcd-cluster-0004 に代わり etcd-cluster-0005 が稼働していることが確認できると思います。</p>

<pre><code>$ kubectl get pods
NAME                                     READY     STATUS    RESTARTS   AGE
etcd-cluster-0000                        1/1       Running   0          12m
etcd-cluster-0001                        1/1       Running   0          12m
etcd-cluster-0002                        1/1       Running   0          11m
etcd-cluster-0003                        1/1       Running   0          7m
etcd-cluster-0005                        1/1       Running   0          3s
etcd-operator-217127005-futo0            1/1       Running   0          17m
</code></pre>

<h2 id="まとめ">まとめ</h2>

<p>kubernetes の上で構成することでうまく kubernetes のメリットを活かしつつ etcd クラスタを構成できていると言えると思います。記事執筆時ではまだ不具合が幾つかありました (上記の curl で実施した箇所や、イメージのアップグレード) が、etcd を手動で構築するより kubernetes で構成するほうがメリットが多いことは明らかです。また kubernetes のポッドから kubernetes dns を介してサービスネームに直接アクセスできるのでポッドから etcd に情報を読み書きすることも容易になりそうです。</p>

<p>ですが etcd に収めるデータの性質によっては運用面で厳しくなることも想定できます。coreos 内で etcd クラスタを介して互いのコンテナ間でコンフィグを共有し合う使い方は非常に意味があると思うのですが、coreos 外の独自のソフトウェアがいろんな種別のデータを etcd クラスタに外から入出力することの意味はそれほど無いように思えます。であれば高耐久性で運用のし易い軽量な KVS ソフトウェアを使うべきだからです。</p>

<p>また今回紹介した etcd-operator とは別に coreos が同時にアナウンスした(<a href="https://coreos.com/blog/the-prometheus-operator.html">https://coreos.com/blog/the-prometheus-operator.html</a>) に関しても興味を持っているので後に触ってみたいと思います。</p>

<p>何と言うか所感として最後に述べるとサーバレスが叫ばれている中でわざわざクラスタソフトウェアを自前で管理するのか？と疑問も確かに残ります。それこそクラウドプラットフォームが提供すべき機能だと思うからです..。</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2016/11/27/etcd-operator/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>


        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2016/11/20/helm/">Helm を使って Kubernetes を管理する</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2016-11-20'>
            November 20, 2016</time>
        <span class="author">jedipunkz</span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは。@jedipunkz です。</p>

<p>今回は Helm という kubernetes のパッケージマネージャ的なソフトウェアを使ってみたので記事にしたいと思います。</p>

<ul>
<li>公式サイト : <a href="https://helm.sh/">https://helm.sh/</a></li>
</ul>

<p>Kubernetes を仕事で使っているのですが &ldquo;レプリケーションコントローラ&rdquo; や &ldquo;サービス&rdquo; といった単位を使って Pod, Service を管理しています。Helm を使うことでこれらの管理方法が変わるのか調べたいと思います。</p>

<h2 id="依存するソフトウェア">依存するソフトウェア</h2>

<p>今回は MacOS を使って環境を整えます。</p>

<ul>
<li>virtualbox</li>
<li>minikube</li>
<li>kubectl</li>
</ul>

<p>これらのソフトウェアをインストールしていきます。</p>

<pre><code>$ brew cask install virtualbo
$ curl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.12.2/minikube-darwin-amd64 &amp;&amp; chmod +x minikube &amp;&amp; sudo mv minikube /usr/local/bin/
$ brew install kubectl
</code></pre>

<p>minikube を使って簡易的な kubernetes 環境を起動します。</p>

<pre><code>$ minikube start
$ eval $(minikube docker-env)
</code></pre>

<h2 id="helm-を使ってみる">Helm を使ってみる</h2>

<p>Helm は Charts という単位で Kubernetes をパッケージングします。Charts の一覧を見てみましょう。</p>

<pre><code>$ helm search
NAME                    VERSION         DESCRIPTION
stable/drupal           0.3.7           One of the most versatile open source content m...
stable/factorio         0.1.1           Factorio dedicated server.
stable/ghost            0.4.0           A simple, powerful publishing platform that all...
stable/jenkins          0.1.1           A Jenkins Helm chart for Kubernetes.
stable/joomla           0.4.0           PHP content management system (CMS) for publish...
stable/mariadb          0.5.3           Chart for MariaDB
stable/mediawiki        0.4.0           Extremely powerful, scalable software and a fea...
stable/memcached        0.4.0           Chart for Memcached
stable/minecraft        0.1.0           Minecraft server
stable/mysql            0.2.1           Chart for MySQL
stable/phpbb            0.4.0           Community forum that supports the notion of use...
stable/postgresql       0.2.0           Chart for PostgreSQL
stable/prometheus       1.3.1           A Prometheus Helm chart for Kubernetes. Prometh...
stable/redis            0.4.1           Chart for Redis
stable/redmine          0.3.5           A flexible project management web application.
stable/spark            0.1.1           A Apache Spark Helm chart for Kubernetes. Apach...
stable/spartakus        1.0.0           A Spartakus Helm chart for Kubernetes. Spartaku...
stable/testlink         0.4.0           Web-based test management system that facilitat...
stable/traefik          1.1.0-rc3-a     A Traefik based Kubernetes ingress controller w...
stable/uchiwa           0.1.0           Dashboard for the Sensu monitoring framework
stable/wordpress        0.3.2           Web publishing platform for building blogs and ...
</code></pre>

<p>各アプリケーションの名前で Charts が管理されていることが分かります。
ここでは stable/mysql を使って kubernetes の中に MySQL 環境を作ってみます。まず stable/mysql に設定できるパラメータ一覧を取得します。</p>

<pre><code>$ helm inspect stable/mysql
Fetched stable/mysql to mysql-0.2.1.tgz
description: Chart for MySQL
engine: gotpl
home: https://www.mysql.com/
keywords:
- mysql
- database
- sql
maintainers:
- email: viglesias@google.com
  name: Vic Iglesias
name: mysql
sources:
- https://github.com/kubernetes/charts
- https://github.com/docker-library/mysql
version: 0.2.1

---
## mysql image version
## ref: https://hub.docker.com/r/library/mysql/tags/
##
imageTag: &quot;5.7.14&quot;

## Specify password for root user
##
## Default: random 10 character string
# mysqlRootPassword: testing

## Create a database user
##
# mysqlUser:
# mysqlPassword:

## Allow unauthenticated access, uncomment to enable
##
# mysqlAllowEmptyPassword: true

## Create a database
##
# mysqlDatabase:

## Specify a imagePullPolicy
## 'Always' if imageTag is 'latest', else set to 'IfNotPresent'
## ref: http://kubernetes.io/docs/user-guide/images/#pre-pulling-images
##
# imagePullPolicy:

## Persist data to a persitent volume
persistence:
  enabled: true
  storageClass: generic
  accessMode: ReadWriteOnce
  size: 8Gi

## Configure resource requests and limits
## ref: http://kubernetes.io/docs/user-guide/compute-resources/
##
resources:
  requests:
    memory: 256Mi
    cpu: 100m
</code></pre>

<p>stable/mysql という Charts を構成するパラメータ一覧が取得できました。今回は DB 上のユーザを作ってパスワードを設定してみようと思います。上記のパラメータの中から &lsquo;mysqlUser&rsquo;, &lsquo;mysqlPasswword&rsquo; を編集してみます。下記の内容を config.yaml というファイル名で保存します。</p>

<pre><code>mysqlUser: user1
mysqlPassword: password1
</code></pre>

<p>この config.yaml を使って stable/mysql を起動してみます。</p>

<pre><code>$ helm install -f config.yaml stable/mysql
$ helm ls # &lt;--- 起動した環境を確認する
NAME            REVISION        UPDATED                         STATUS          CHART
dealing-ladybug 1               Sun Nov 20 10:44:00 2016        DEPLOYED        mysql-0.2.1
</code></pre>

<p>&lsquo;dealing-ladybug&rsquo; という名前で mysql が起動したことが分かります。</p>

<p>kubectl をつかって Services を確認してみます。</p>

<pre><code>$ kubectl get svc
NAME                    CLUSTER-IP   EXTERNAL-IP   PORT(S)    AGE
dealing-ladybug-mysql   10.0.0.24    &lt;none&gt;        3306/TCP   33m
</code></pre>

<p>&lsquo;dealing-ladybug-mysql&rsquo; という Service が kubernetes 環境に作られました。この名前は Kubernetes 内のコンテナから DNS 名前解決できるアドレスとなります。</p>

<p>ここで mysql-client 環境を作るために下記のようなコンテナを起動して mysql-client をインストール、config.yaml で作成したユーザ・パスワードで mysql サーバにアクセス確認します。</p>

<pre><code>$ kubectl run -i --tty ubuntu02 --image=ubuntu:16.04 --restart=Never -- bash -il
# apt-get update; apt-get install -y mysql-client
# mysql -h dealing-ladybug-mysql -u user1 -ppassword1
&lt;省略&gt;
mysql&gt;
</code></pre>

<p>config.yaml に記したユーザ情報で MySQL にアクセスできることを確認できました。</p>

<h2 id="まとめ">まとめ</h2>

<p>Helm を使うことでレプリケーションコントローラやサービスを直接 yaml で作らなくても MySQL 環境構築と設定が行えました。Helm の Charts は自分で開発することも可能です。(参考 URL)</p>

<p><a href="https://github.com/kubernetes/helm/blob/master/docs/charts.md">https://github.com/kubernetes/helm/blob/master/docs/charts.md</a></p>

<p>パッケージングすることで他人の作った資源を有用に活用することもできるため、こういった何かを抽象化する技術を採用することにはとても意味があると思います。自動化を考える上でも抽象化できる技術は有用だと思います。</p>

<p>ですがレプリケーションコントローラを使っても Helm でも Yaml で管理することに変わりはなく、またレプリケーションコントローラで指定できる詳細なパラメータ (replicas や command, image など) を指定できないためすぐに実用するというわけにはいかないなぁと感じました。</p>

<h2 id="参考-url">参考 URL</h2>

<ul>
<li><a href="https://github.com/kubernetes/helm/blob/master/docs/charts.md">https://github.com/kubernetes/helm/blob/master/docs/charts.md</a></li>
<li><a href="https://github.com/kubernetes/helm/blob/master/docs/quickstart.md">https://github.com/kubernetes/helm/blob/master/docs/quickstart.md</a></li>
</ul>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2016/11/20/helm/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>


        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2016/07/25/minikube/">Minikube で簡易 kubernetes 環境構築</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2016-07-25'>
            July 25, 2016</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは。@jedipunkz です。</p>

<p>kubernetes の環境を簡易的に作れる Minikube (<a href="https://github.com/kubernetes/minikube">https://github.com/kubernetes/minikube</a>) が2ヶ月前ほどにリリースになっていました。簡単ですが少し触ってみたのでその際のメモを記したいと思います。VirtualBox もしくは VMware Fusion がインストールされていればすぐにでも稼働可能です。私は Kubernetes 初心者ですが何も考えずに kubernetes を動かすことが出来ました。</p>

<h2 id="前提">前提</h2>

<p>前提として下記の環境が必要になります。</p>

<ul>
<li>Mac OSX がインストールされていること</li>
<li>VirtualBox もしくは VMware Fusion がインストールされていること</li>
</ul>

<h2 id="minikube-をインストール">minikube をインストール</h2>

<p>minikube をインストールします。</p>

<pre><code>$ curl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.6.0/minikube-darwin-amd64 &amp;&amp; chmod +x minikube &amp;&amp; sudo mv minikube /usr/local/bin/
</code></pre>

<h2 id="kubetl-をインストール">kubetl をインストール</h2>

<p>次に kubectl をインストールします。</p>

<pre><code>$ curl -k -o kubectl https://kuar.io/darwin/kubectl &amp;&amp; chmod +x kubectl &amp;&amp; sudo mv kubectl /usr/local/bin/
</code></pre>

<h2 id="minikube-で-kurbernates-を稼働">Minikube で Kurbernates を稼働</h2>

<p>Minikube を使って Kubernetes を稼働してみます。下記のコマンドを実行すると Virtualbox 上で仮想マシンが稼働し Kubernetes 一式も立ち上がります。</p>

<pre><code>$ minikube start
</code></pre>

<h2 id="kurbernates-を使ってみる">Kurbernates を使ってみる</h2>

<p>Pods を立ち上げてみましょう。下記の内容を redis-django.yaml ファイルに保存します。</p>

<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: redis-django
  labels:
    app: web
spec:
  containers:
    - name: key-value-store
      image: redis
      ports:
        - containerPort: 6379
    - name: frontend
      image: django
      ports:
        - containerPort: 8000
</code></pre>

<p>kubectl コマンドで Pod を立ち上げます。</p>

<pre><code>$ kubectl create -f ./redis-django.yaml
</code></pre>

<p>Pod の様子を確認します。</p>

<pre><code>$ kubectl get pods
NAME           READY     STATUS             RESTARTS   AGE
redis-django   1/2       CrashLoopBackOff   7          15m
</code></pre>

<p>Minikube はクラスタといってもノードが1つなので READY <sup>1</sup>&frasl;<sub>2</sub> となるようです。Nodes の様子を見てみます。</p>

<pre><code>$ kubectl get nodes
NAME         LABELS                                                                                        STATUS
minikubevm   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=minikubevm   Ready
</code></pre>

<p>Docker ホスト上の様子を見てみましょう。Kubernetes を形成するコンテナと共に redis のコンテナが稼働していることが確認できます。</p>

<pre><code>$ eval $(minikube docker-env)
$ docker ps
CONTAINER ID        IMAGE                                                  COMMAND                  CREATED             STATUS              PORTS               NAMES
550285614e33        redis                                                  &quot;docker-entrypoint.sh&quot;   20 minutes ago      Up 20 minutes                           k8s_key-value-store.a3b8356e_redis-django_default_4440a1d8-5272-11e6-9f19-6e0006e7fb51_90c3fec8
aba3a8c040d4        gcr.io/google_containers/pause-amd64:3.0               &quot;/pause&quot;                 20 minutes ago      Up 20 minutes                           k8s_POD.822b267d_redis-django_default_4440a1d8-5272-11e6-9f19-6e0006e7fb51_5bef1d2a
9ea96a3f3e10        gcr.io/google-containers/kube-addon-manager-amd64:v2   &quot;/opt/kube-addons.sh&quot;    48 minutes ago      Up 48 minutes                           k8s_kube-addon-manager.a1c58ca2_kube-addon-manager-minikubevm_kube-system_48abed82af93bb0b941173334110923f_84f4fd38
192e886a5795        gcr.io/google_containers/pause-amd64:3.0               &quot;/pause&quot;                 48 minutes ago      Up 48 minutes                           k8s_POD.d8dbe16c_kube-addon-manager-minikubevm_kube-system_48abed82af93bb0b941173334110923f_6c65b482
7b005c68d9d4        gcr.io/google_containers/pause-amd64:3.0               &quot;/pause&quot;                 48 minutes ago      Up 48 minutes                           k8s_POD.2225036b_kubernetes-dashboard-pzdxy_kube-system_7005dce1-479a-11e6-a0ce-86b669e45864_c08bd009
</code></pre>

<h2 id="まとめ">まとめ</h2>

<p>Kubernetes のことを殆ど知らない私でもなんとなくですが稼働させて基本的な操作が出来ました。2016/5/31 にリリースされたツールなのでまだ安定しないところもありますが、より容易に Kubernetes が稼働できるようになったのでエンジニアの敷居が下がったのではないでしょうか。</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2016/07/25/minikube/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>


        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2016/07/23/influxdb-go/">Go言語とInfluxDBで監視のコード化</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2016-07-23'>
            July 23, 2016</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは。<a href="https://twitter.com/jedipunkz">@jedipunkz</a> です。</p>

<p>今日は Go 言語でサーバのメトリクスデータを InfluxDB に入れてリソース監視を行う方法について書きます。</p>

<p>Ansible, Terraform, Chef などのソフトウェアを使ってインフラを定義するのが当たり前になった現在ですが、本当の意味でのソフトウェアによるインフラの定義ってなんだろと最近考えています。aws-sdk や fog などを使ったネイティブな言語でインフラを定義することの意味もあるように感じているからです。某サービスプロバイダのエンジニアはこうした言語によるインフラの定義の一番大きなメリットとして &ldquo;再利用性&rdquo; をあげていました。こうしたソフトウェアによるインフラの定義や構成を行う上で監視についてもコード化できるのでは？と考えて今回の記事に至りました。</p>

<h2 id="使うモノ">使うモノ</h2>

<ul>
<li><a href="https://github.com/influxdata/influxdb/tree/master/client">https://github.com/influxdata/influxdb/tree/master/client</a></li>
</ul>

<p>公式の InfluxDB Go Client です。InfluxDB 自体が Go 言語で書かれていますがクライアントも Go 言語で記述することができます。ここにあるサンプルコードをすこしいじって、今回の記事を書こうと思います。</p>

<ul>
<li><a href="https://github.com/shirou/gopsutil">https://github.com/shirou/gopsutil</a></li>
</ul>

<p>@shirou さんが作られた psutil の Go 言語版です。CPU, Mem などリソースをモニタするのに便利なので利用します。</p>

<h2 id="環境構築">環境構築</h2>

<p>環境を作っていきます。InfluxDB と Chronograf を構築するのですが Docker で構築するのが簡単なのでこれを利用します。Chronograf は InfluxDB 内のデータを可視化するためのソフトウェアです。</p>

<ul>
<li>InfluxDB の起動</li>
</ul>

<p>InfluxDB のコンテナを起動します。</p>

<pre><code>docker run -p 8083:8083 -p 8086:8086 \
      -v $PWD:/var/lib/influxdb \
      influxdb
</code></pre>

<ul>
<li>Chronograf の起動</li>
</ul>

<p>Chronograf のコンテナを起動します。</p>

<pre><code>docker run -p 10000:10000 chronograf
</code></pre>

<p>この時点で http://${DOCKER_HOST}:10000/ にアクセスすると Chronograf の UI を確認できます。</p>

<h2 id="influxdb-にユーザ-データベースを作成する">InfluxDB にユーザ・データベースを作成する</h2>

<p>InfluxDB 上にユーザとデータベースを作成します。言語の中でも作ることが出来ますが、今回は手動で。
Mac OSX を使っている場合 homebrew で influxdb をインストールすることが簡単にできます。</p>

<pre><code>brew install influxdb
</code></pre>

<p>ユーザを作ります。</p>

<pre><code>influx -host 192.168.99.100 -port 8086
&gt; create user foo with password 'foo'
&gt; grant all privileges to foo
</code></pre>

<p>データベースを作ります。</p>

<pre><code>influx -host 192.168.99.100 -port 8086
&gt; CREATE DATABASE IF NOT EXISTS square_holes;
</code></pre>

<h2 id="go言語で-cpu-時間を取得し-influxdb-にメトリクスデータを挿入">Go言語で CPU 時間を取得し InfluxDB にメトリクスデータを挿入</h2>

<p>Go 言語でメモリー使用率を取得し得られたメトリクスデータを InfluxDB に挿入するコードを書きます。</p>

<pre><code class="language-golang">package main

import (
    &quot;log&quot;
    &quot;time&quot;

    &quot;github.com/influxdata/influxdb/client/v2&quot;
    &quot;github.com/shirou/gopsutil/cpu&quot;
)

const (
    MyDB = &quot;square_holes&quot;
    username = &quot;foo&quot;
    password = &quot;foo&quot;
)

func main() {
    for {
        // Make client
        c, err := client.NewHTTPClient(client.HTTPConfig{
            Addr: &quot;http://192.168.99.100:8086&quot;,
            Username: username,
            Password: password,
        })
    
        if err != nil {
            log.Fatalln(&quot;Error: &quot;, err)
        }
    
        // Create a new point batch
        bp, err := client.NewBatchPoints(client.BatchPointsConfig{
            Database:  MyDB,
            Precision: &quot;s&quot;,
        })
    
        if err != nil {
            log.Fatalln(&quot;Error: &quot;, err)
        }
    
        // get CPU info
        cp, _ := cpu.Times(true)

        // get CPU status info for each core
        var user, system, idle float64 = 0, 0, 0
        for _, sub_cpu := range cp {
            user = user + sub_cpu.User
            system = system + sub_cpu.System
            idle = idle + sub_cpu.Idle
        }
    
        // Create a point and add to batch
        tags := map[string]string{&quot;cpu&quot;: &quot;cpu&quot;}
        fields := map[string]interface{}{
            &quot;User&quot;:     user / float64(len(cp)),
            &quot;System&quot;:   system / float64(len(cp)),
            &quot;Idle&quot;:     idle / float64(len(cp)),
        }
        pt, err := client.NewPoint(&quot;cpu&quot;, tags, fields, time.Now())
    
        if err != nil {
            log.Fatalln(&quot;Error: &quot;, err)
        }
    
        bp.AddPoint(pt)
    
        // Write the batch
        c.Write(bp)
        time.Sleep(1 * time.Second)
    }
}
</code></pre>

<p>ビルドして実行すると下記のように influxdb 上のデータベースにメトリクスデータが挿入されていることを確認できます。</p>

<pre><code>influx -host 192.168.99.100 -port 8086 -execute 'SELECT * FROM cpu' -database=square_holes -precision=s | head -8
name: cpu
---------
time            Idle            System          User            cpu
1469342272      20831.04296875  3700.185546875  3544.90234375   cpu
1469342273      20831.666015625 3700.302734375  3544.966796875  cpu
1469342274      20832.2109375   3700.447265625  3545.068359375  cpu
1469342275      20832.828125    3700.546875     3545.13671875   cpu
1469342291      20841.728515625 3702.482421875  3546.806640625  cpu
</code></pre>

<p>Chronograf の UI で確認してみましょう。</p>

<p><img src="http://jedipunkz.github.io/pix/influx-go.png" width="80%"></p>

<p>得られた CPU に関するデータが可視化されていることが確認できます。変化に乏しいグラフですが&hellip;。
この辺りは CPU 時間から CPU 使用率を得るコードに書き換えるといいかもしれません。</p>

<h2 id="まとめと考察">まとめと考察</h2>

<p>InfluxDB の提供元が出している Telegraf というメトリクスデータの送信エージェントがありますが、同じような動きを Go 言語で簡単に開発できることが分かりました。ネイティブな言語で開発するとより柔軟にデータの送信ができることも期待できます。また冒頭に述べた通り再利用も用意になるのではと思います。インフラの状態をメトリクスデータとして時系列 DB に挿入して可視化するということは監視のコード化とも言えると思います。ただし、フレームワークが出てきてもっと簡単に書ける仕組みが出てこないと厳しい気もしますが。果たしてこれらインフラを言語で記述していくことがどれだけ有用なのかまだわかりませんが、いつか現場で実践してみたいと思います。</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2016/07/23/influxdb-go/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>


        

        
<ul class="actions pagination">
    
        <li><a href="#" class="disabled button big previous">Previous Page</a></li>
    

    
        <li><a href="/categories/infrastructure/page/2/"
                class="button big next">Next Page</a></li>
    
</ul>

    </div>
    
<section id="sidebar">

    
        <section id="intro">
            
            
            
            <ul class="icons">
                
                    <li><a href="https://jedipunkz.github.io/categories/infrastructure/index.xml" type="application/rss+xml"
                        target="_blank" title="RSS" class="fa fa-rss"></a></li>
                
                
            </ul>
        </section>

    
        <section id="recent-posts">
            <ul class="posts">
                <header>
                    <h3>Recent Posts</h3>
                </header>
                
                    
                

                
                    
                

                
                    <li>
                        <article>
                            <header>
                                <h3><a href="https://jedipunkz.github.io/blog/2018/12/31/istio/">Istio, Helm を使って Getting Started 的なアプリをデプロイ</a></h3>
                                
                                    
                                
                                <time class="published" datetime=
                                    '2018-12-31'>
                                    December 31, 2018</time>
                            </header>
                        </article>
                    </li>
                
                    <li>
                        <article>
                            <header>
                                <h3><a href="https://jedipunkz.github.io/blog/2017/07/02/test-kitchen-cluster/">Docker,Test-Kitchen,Ansible でクラスタを構成する</a></h3>
                                
                                    
                                
                                <time class="published" datetime=
                                    '2017-07-02'>
                                    July 2, 2017</time>
                            </header>
                        </article>
                    </li>
                
                    <li>
                        <article>
                            <header>
                                <h3><a href="https://jedipunkz.github.io/blog/2017/04/13/gke-lb/">GCP ロードバランサと GKE クラスタを Terraform を使って構築する</a></h3>
                                
                                    
                                
                                <time class="published" datetime=
                                    '2017-04-13'>
                                    April 13, 2017</time>
                            </header>
                        </article>
                    </li>
                
                    <li>
                        <article>
                            <header>
                                <h3><a href="https://jedipunkz.github.io/blog/2017/02/12/serverless-fission/">Serverless on Kubernetes : Fission を使ってみた</a></h3>
                                
                                    
                                
                                <time class="published" datetime=
                                    '2017-02-12'>
                                    February 12, 2017</time>
                            </header>
                        </article>
                    </li>
                
                    <li>
                        <article>
                            <header>
                                <h3><a href="https://jedipunkz.github.io/blog/2017/01/13/kubernetes-deployments/">Kubernetes Deployments を使ってみた！</a></h3>
                                
                                    
                                
                                <time class="published" datetime=
                                    '2017-01-13'>
                                    January 13, 2017</time>
                            </header>
                        </article>
                    </li>
                

                
                    <li>
                        <ul class="actions">
                            <li><a href=
                            
                                "/post/"
                            
                            class="button">View more posts</a></li>
                        </ul>
                    </li>
                
            </ul>
        </section>

    
    
    
    
        <section id="categories">
            <ul class="posts">
                <header>
                    <h3><a href="/categories/">Categories</a></h3>
                </header>

                
                    
                

                
                    <li>
                        <article>
                            <header>
                                <a href="/categories/infrastructure/">infrastructure</a>
                                <span style="float:right;">110</span>
                            </header>
                        </article>
                    </li>
                
                    <li>
                        <article>
                            <header>
                                <a href="/categories/report/">report</a>
                                <span style="float:right;">9</span>
                            </header>
                        </article>
                    </li>
                
                    <li>
                        <article>
                            <header>
                                <a href="/categories/tools/">tools</a>
                                <span style="float:right;">11</span>
                            </header>
                        </article>
                    </li>
                
            </ul>
        </section>
    

    
        

    
        <section id="footer">
            <ul class="icons">
                
                    <li><a href="https://jedipunkz.github.io/categories/infrastructure/index.xml" type="application/rss+xml"
                        target="_blank" title="RSS" class="fa fa-rss"></a></li>
                
                
            </ul>

            <p class="copyright">&copy; ジェダイさんのブログ. テーマデザインは <a href="//github.com/jpescador" target="_blank">Julio Pescador</a>さんによるものです。 </p>
        </section>

</section>

            </div>
        <a id="back-to-top" href="#" class="fa fa-arrow-up fa-border fa-2x"></a>
        

        
        
            
        

        
        
            <script src="/js/jquery.min.js"></script>
            <script src="/js/skel.min.js"></script>
            <script src="/js/util.js"></script>
            <script src="/js/main.js"></script>
            <script src="/js/backToTop.js"></script>
            <script src="/js/highlight.pack.js"></script>
        

        

            
            <script>hljs.initHighlightingOnLoad();</script>
            
    </body>
</html>

