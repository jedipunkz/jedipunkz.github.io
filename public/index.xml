<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>jedipunkz 🚀 のブログ</title>
    <link>https://jedipunkz.github.io/</link>
    <description>Recent content on jedipunkz 🚀 のブログ</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Fri, 16 Jun 2023 23:28:09 +0900</lastBuildDate><atom:link href="https://jedipunkz.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>k8s コンテナをインクリメンタルサーチ&amp;ログインする kubectl プラグインの開発</title>
      <link>https://jedipunkz.github.io/post/kubectl-plugin/</link>
      <pubDate>Fri, 16 Jun 2023 23:28:09 +0900</pubDate>
      
      <guid>https://jedipunkz.github.io/post/kubectl-plugin/</guid>
      <description>こんにちは。jedipunkz です。
今回は、kubectl プラグインを開発したことがなかったので、Go の学習と合わせてためしに1つ作ってみたのでその内容を記したいと思います。
開発した kubectl plugin: kubectl-fuzzy-login 下記が今回開発した kubectl プラグインです。
https://github.com/jedipunkz/kubectl-fuzzy-login
何が出来るか 下記のキャプチャをご覧頂くと一目瞭然だと思います。
Kubernetes のポッドとコンテナをインクリメンタルサーチしつつ選択し、最終的にコンテナにログイン出来るプラグインになっています。コンテナがサイドカー構成になっていた場合は、そのうちのどのコンテナにログインするかもインクリメンタルサーチ出来ます。なお、このプラグインは Go で開発しました。
インストール方法 Krew を利用している場合は下記の操作でインストールできます。Krew が事前にインストールされている必要があります。
git clone https://github.com/jedipunkz/kubectl-fuzzy-login.git kubectl krew install --manifest=./kubectl-fuzzy-login/krew/fuzzy-login.yaml マニュアル操作でインストールする場合は下記です。
git clone https://github.com/jedipunkz/kubectl-fuzzy-login.git cd kubectl-fuzzy-login go build cp kubectl-fuzzy-login /your/bin/path 使用方法 オプション無しで、全 Namespaces を対象に検索・ログインする オプションを使用しない場合は下記のように実行します。
kubectl fuzzy login まず Pod を選択します。Pod 名の一部を入力することでインクリメンタル・ファジー検索出来ます。その Pod に複数のコンテナ (サイドカー) がある場合、更にコンテナをインクリメンタルサーチ出来ます。最終的にコンテナを選択し Enter ボタンを押すことでコンテナにログイン出来ます。ただしコンテナイメージにシェルが入っていない場合は入ることが出来ません。
シェル指定 また下記のように -s オプションでデフォルトのシェルを指定することもできます。
kubectl fuzzy login -s /bin/bash Namespace 指定 Namespace を -n オプションで指定することもできます。</description>
    </item>
    
    <item>
      <title>手軽にローカルで Argo Rollouts, Istio, Prometheus で Progressive Delivery を試す</title>
      <link>https://jedipunkz.github.io/post/argo-rollout-progressive-delivery/</link>
      <pubDate>Sat, 03 Jun 2023 05:55:09 +0900</pubDate>
      
      <guid>https://jedipunkz.github.io/post/argo-rollout-progressive-delivery/</guid>
      <description>こんにちは。jedipunkz🚀 です。
以前こちらの PipeCD 検証の記事 で Progressive Deliver について調査したのですが、Kubernetes でこの Progressive Delivery を実現する方法を調べておきたいなと思って手元の Macbook 上で検証してみたのでその際の手順を記そうかと思います。
Progressive Delivery の概要 ここで概要だけ記しておきます。Canary リリースは新しいデプロイメントをある程度の割合だけリリースし、徐々にリリースを進行させるデプロイ方式ということはご存知だと思いますが、Progressive Delivery はその過程で
新しいデプロイメントの統計情報を得る 予め定義したデプロイ成功定義に対して条件満たしているかを過程毎にチェックする チェック OK であれば次の過程にデプロイを進める 予め定義した幾つかのデプロイ過程を全て終えるとデプロイ完了となる というステップを経ます。
用いるソフトウェア Kubernetes で Progressive Delivery を実現するには下記のソフトウェアを用いる事が可能です。 また今回の手順は MacOS を前提に記します。
Argo Rollouts Prometheus Istio Kubernetes (今回は Minikube を使いました) 事前の準備 Istio Istio をダウンロードします。
curl -L https://istio.io/downloadIstio | ISTIO_VERSION=17.2 sh - Istio を Minikube にデプロイします。
cd istio-17.2 istioctl install --set profile=demo -y Kubernetes Namespace default で起動した Pod が自動的に Envoy サイドカーを取得するように設定します。</description>
    </item>
    
    <item>
      <title>自前ツールと Cloudwatch 高解像度メトリクスを使ったより高速な ECS オートスケールの実現</title>
      <link>https://jedipunkz.github.io/post/esp/</link>
      <pubDate>Fri, 24 Mar 2023 14:54:11 +0900</pubDate>
      
      <guid>https://jedipunkz.github.io/post/esp/</guid>
      <description>こんにちは @jedipunkz 🚀 です。
普段仕事で AWS ECS を使っていて Autoscallng Group によってアプリケーションを据えケールさせて運用していますが、運用している中でより高速にオートスケール出来ないものだろうか？と思うシチュエーションが何回か発生し、対応方法について模索していました。
実際に発生したシチュエーション 下記はコンテナ毎の CPU 使用率です。1分未満の間に急激にアクセスが増えコンテナの CPU 使用率が 100% に達し (実際には vCPU に基づいて 200% となっている)、ECS Service のヘルスチェックに Fail して、コンテナが落ち、新しいコンテナは起動するものの、アクセス不可に耐えられず、コンテナ停止と起動を繰り返すといった状況でした。
Autoscaling Policy, Cloudwatch Metrics Alarm の調整 まず最初に考えたのが下記の値の調整です。
aws_app_autoscaling_policy の cooldown 値 aws_cloudwatch_metric_alarm の period 値 具体的には 60sec となっていた値を 10sec などに変更しました。これによって 60sec のインターバルでしきい値計算してスケールさせていたところを 10sec にインターバルを縮めつつスケールさせる。つまりより迅速にスケールさせることで上記のシチュエーションに耐えられるのではと考えました。
ですが、結果は NG でした。
下記は Cloudwatch Metrics の様子です。データはプロットされているものの、データ不足 という状態に陥っている事がわかります。
実際に ECS はこの設定をした Metrics Alarm ではスケールしてくれませんでした。
高解像度メトリクスの利用について であれば高解像度メトリクス を利用すれば良いのではと考えました。
歴史的に</description>
    </item>
    
    <item>
      <title>Sysdig&#43;ECS Fargate でコンテナランタイムセキュリティ実践</title>
      <link>https://jedipunkz.github.io/post/sysdig-ecs-fargate/</link>
      <pubDate>Sat, 24 Sep 2022 22:00:00 +0900</pubDate>
      
      <guid>https://jedipunkz.github.io/post/sysdig-ecs-fargate/</guid>
      <description>こんにちは @jedipunkz 🚀 です。
ECS 構成をもう少しセキュアに保てる構成はないものだろうかと模索しているなかで Sysdig を見つけました。まだ導入できる目処は立っていないのですがある程度ノウハウ蓄積出来てきたのでここで検証内容等を記事にしようかと思っています。
Sysdig は幾つかのサービスが存在するのですが今回検証したのは Sysdig Serverless Security と呼ばれるモノで ECS Fargate 上のコンテナランタイムセキュリティを実践することができるサービスです。
Sysdig とは AWS のサービスにも脅威検知を行うことができるサービスが揃っているのはご存知と思います
対象 目的 技術・サービス AWS リソース 驚異検知 AWS GuardDuty また予防の観点で脆弱性診断が出来るサービスもありあす
対象 目的 技術・サービス AWS リソース セキュリティ診断 AWS Trusted Advisor ECS コンテナ 脆弱性診断 ECR Image Scan EC2 上のソフトウェア 脆弱性診断 AWS Inspector ここで気がつくと思うのですがコンテナ上の驚異検知を行うサービスが AWS には無いと思っています。 (2022/09 時点)
Sysdig Serverless Security は ECS Fargate コンテナ上の脅威検知を行うサービスです。ECS Fargate 利用時にコンテナ上の脅威検知を行うサービスは他にも幾つかありますが、Sysdig はシステムコールを利用したコンテナランタイムセキュリティを実践して脅威検知・通知が行えるものになります。自分も詳しくないのですがこれを CWPP (Cloud Workload Protection Platform) と言うらしいです。ワークロードというのはクラウド上の仮想マシン・稼働中のソフトウェアを指して、CWPP はマルウェア保護、脆弱性スキャン、アクセス制御、異常検知の機能を使用してそれぞれのワークロードを保護する、ということらしいです。</description>
    </item>
    
    <item>
      <title>ECS &#43; PipeCD &#43; Datadog でプログレッシブデリバリーを実現</title>
      <link>https://jedipunkz.github.io/post/ecs-pipecd/</link>
      <pubDate>Wed, 10 Aug 2022 09:11:04 +0900</pubDate>
      
      <guid>https://jedipunkz.github.io/post/ecs-pipecd/</guid>
      <description>こんにちは @jedipunkz 🚀 です。
今回は CNCF にジョインした PipeCD と Datadog を用いて ECS 環境にてプログレッシブデリバリーを実現する方法について調査したので、その内容を記したいと思います。
そもそもプログレッシブデリバリーとは アプリケーションのデリバリー方法はカナリーリリースやブルーグリーンデプロイメント等がよく知られていると思います。プログレッシブデリバリーはその一歩先を行くデリバリー方式で、Prometheus や Datadog 等のメトリクスを用いて SLO (SRE の SLO と言うよりはデプロイのための指標という意味での) を元にカナリーリリースしたアプリケーションが期待した動作をしているかを確認し (プログレッシブデリバリー的にはこのフェーズを ANALYSIS という様です)、その上でカナリーリリースを完了するというフローになります。
構成 Pipecd, Piped 共に Kubernetes (EKS) クラスタ上に起動する構成 この検証ではこちらの構成を選択しました。この構成の特徴は
piped は pipecd の API エンドポイントを指し示す pipecd は UI を提供 pipecd は Filestore (S3, GCS, Minio など), Datastore (MySQL, Firestore など) を利用可 (今回は Minio, MySQL を選択) piped は Target Group, ECS タスク定義等の操作を行うため ECS API へのアクセス権限が必要 piped の pipeline 上のステージで ANALYSIS という Datadog 等のメトリクスを解析する機能を有している アプリケーションレポジトリには app.</description>
    </item>
    
    <item>
      <title>ECR 脆弱性スキャン結果表示 CLI の開発と Datadog プロット</title>
      <link>https://jedipunkz.github.io/post/ecr-scan-datadog-go/</link>
      <pubDate>Sat, 30 Apr 2022 13:56:56 +0900</pubDate>
      
      <guid>https://jedipunkz.github.io/post/ecr-scan-datadog-go/</guid>
      <description>こんにちは。jedipunkz🚀 です。
引き続き Go を学習しています。前回の記事 ECS コンテナにログインする CLI を Go 言語で作った話 のまとめにも記したのですが Go のコードを書くアイデアとして下記をぼんやり考えていました。
ECR 脆弱性スキャンのパッケージを開発 そのパッケージを利用して Datadog のカスタムメトリクスとして送信 同様にそのパッケージを利用して ECR スキャンの CLI を作成 その紹介を軽くしたいと思います。
開発した ECR 脆弱性スキャンの Go パッケージ 開発したパッケージは https://github.com/jedipunkz/ecrscan になります。
下記のように Ecr 構造体を初期化します。
e := myecr.Ecr{} e.Repositories = [][]string{ {&amp;#34;image-to-scan&amp;#34;, &amp;#34;latest&amp;#34;}, } e.Resion = &amp;#34;ap-northeast-1&amp;#34; finding, vulFindings, _ := e.ListFindings() その後 ListFindings() メソッドでスキャンします。結果、finding.FindingSeverityCounts には下記の深刻度毎のイメージに含まれている脆弱性の数が入ります。
INFORMATIONAL LOW MEDIUM HIGH CRITICAL UNDEFINED また、vulFindings には含まれている脆弱性の
CVE 名 深刻度レベル (INFORMATIONAL, LOW, MEDIUM, HIGH, CRITICAL, UNDEFINED) CVE URI 説明 が入ります。</description>
    </item>
    
    <item>
      <title>ECS コンテナにログインする CLI を Go 言語で作った話</title>
      <link>https://jedipunkz.github.io/post/ecs-login-cli/</link>
      <pubDate>Sat, 05 Feb 2022 00:00:27 +0900</pubDate>
      
      <guid>https://jedipunkz.github.io/post/ecs-login-cli/</guid>
      <description>こんにちは @jedipunkz 🚀 です。
今回は Go 言語で ECS コンテナにログインする CLI を作った話を書きます。
開発の経緯 自分はまだ Go 言語の初学者で学習のために開発するアイデアを探していた状態でした。そこで自分の勤めている会社で ECS Execute 機能を使ったコンテナログインの機能を開発者に提供していた事を思い出し色々調べていて「もう少し手間が省けないか？」と思い立った、という経緯で開発をはじめました。
awscli を使った ECS Execute 機能によるコンテナログイン 手間が多いと書きましたが実際に awscli を使う場合どの程度の手間があるのか簡単に記します。まず下記のコマンドを実行して
$ aws ecs list-tasks --cluster &amp;lt;クラスタ名&amp;gt; --service &amp;lt;サービス名&amp;gt; taskArn が得られるので Arn から task ID を拾って、その task ID を使って
$ aws ecs execute --cluster &amp;lt;クラスタ名&amp;gt; \ --task &amp;lt;task ID&amp;gt; \ -- container &amp;lt;コンテナ名&amp;gt; \ --interfactive \ --command &amp;#34;sh&amp;#34; とコマンドを実行することでコンテナにログイン出来ます。が手間が少し多いのと task ID を拾い出す作業も辛いので改善したい&amp;hellip;。
操作画面 ということで miniecs という CLI を作ったのですが、 まずは操作している様子を貼り付けます。😷 Fuzzy Finder なインクリメンタルサーチが出来る CLI になっていて、ECS クラスタ名・ECS サービス名・コンテナ名を一部入力するとログインしたい環境が選択出来るツールになっています。</description>
    </item>
    
    <item>
      <title>App Mesh と ECS によるカナリーリリース構成を検証してみた</title>
      <link>https://jedipunkz.github.io/post/app-mesh-ecs-canary/</link>
      <pubDate>Fri, 10 Dec 2021 13:56:56 +0900</pubDate>
      
      <guid>https://jedipunkz.github.io/post/app-mesh-ecs-canary/</guid>
      <description>こんにちは。jedipunkz🚀 です。
今回も READYFOR Advent Calendar 2021 の記事として執筆します。
今回のテーマ 前回の記事 では ECS 移行後の構成について検討する内容を記しました。Progressive Delivery を実践する上でもその一歩手前の構成と言っていいカナリーリリース構成について、今回は記していきたいと思います。
デグレしてしまっていたカナリーリリース READYFOR では AWS ECS 移行を行い ECS + CodeDeploy による Blue/Green デプロイメントを導入しました。逆に移行前までに出来ていたカナリーリリースが実施できなくなりました。とは言ってもそれまで開発者がカナリーリリースに対して求めていた主な機能はロールバックだったため、ひとまずは Blue/Green デプロイメントで事足りている状況なのですが、今後大きな機能をリリースする際にはトラヒックを徐々に寄せ影響を把握した上でリリース進めるという作業は必要になってくる可能性があります。
よって、
Progressive Delivery の一歩手前の構成を実践する 大きなリリースのための環境整備 という意味でも、一回カナリーリリース構成について検討しておこうと考えました。
環境構築 今回用意した Terraform コード 検証で作成した AWS 環境をデプロイするための Terraform コードを下記の場所に置いてあります。参考にしてください。
https://github.com/jedipunkz/tf-appmesh-ecs-canary-release
(今回検証で作成したコードは業務上作成したものですが、READYFOR の OSS ポリシーに則り著作権譲渡をうけており、自らのGitHubリポジトリで公開しています。)
App Mesh ECS NLB Service Discovery Envoy X-Ray といった技術要素で構成されています。
Terraform コードによるデプロイ実施 上記に記した Terraform コードを使った構成のデプロイ手順を記します。
前提として Terraform バージョン 1.0.x 系以上をローカルにインストールする必要があります。
$ # AWS クレデンシャル情報を設定 $ git clone https://github.</description>
    </item>
    
    <item>
      <title>ECS 以後の構成と Progressive Delivery の調査</title>
      <link>https://jedipunkz.github.io/post/progressive_delivery/</link>
      <pubDate>Thu, 11 Nov 2021 17:28:46 +0900</pubDate>
      
      <guid>https://jedipunkz.github.io/post/progressive_delivery/</guid>
      <description>こんにちは。jedipunkz です。
今回は READYFOR Advent Calendar 2021 の記事として執筆します。
READYFOR では 2021 年の夏に AWS ECS へプラットフォーム移行をしました。ECS は自分達の要件を全て満たしてくれ運用コストも極小化できて更に周辺の技術も AWS 公式のものが揃っているので、とても満足している状況です。
移行を終えたばかりなので「では次のアーキテクチャは？」という話にはまだなっていないのですが、今は準備期間として余裕を持ってスケジューリングできる状態にして頂いているので、SRE チームとしては色々な技術をリサーチしている段階になります。
今現在は ECS + CodeDeploy を使って Blue/Green デプロイメントを実現しているのですが、よりモダンなデプロイ方式 Progressive Delivery について去年あたりから興味を持っていました。ただ、今までは実際に技術を触るまでには至っていなかったのでこの機会に色々と触ってみたという次第です。
今までも Blue/Green デプロイメント, Canary リリースとデプロイ方式が複数ありましたが、これらを含む継続的デリバリの次のステップと言われているのが Progressive Delivery です。2020年に Hashicorp 社の Mitchell Hashimoto 氏 が来日した際に「今一番気になっているワード」としてあげていましたのが印象的でした。
ECS を使った Canary リリース Progressive Delivery の話をする前に ECS を使った Canary リリースについて少し触れておきます。 (具体的な話についても、どこかのタイミングで記事にできればと思っています)
AWS App Mesh と ECS, X-Ray を使って下記のような構成を作りました。この構成中の App Mesh の Virtual Router のルーティング情報を修正する形で Canary リリースのトラヒック操作が行えます。ECS 以前は Canary リリースを実現できていて ECS 導入によってそれがデグレした状態だったので、この構成の検証は一つの成果だったと思っていますし、今回話をする Progressive Delivery のひとつ前のステップとも考えています。</description>
    </item>
    
    <item>
      <title>Go 言語と awscli を使って ECS/Fargate 上でコマンド実行してみた</title>
      <link>https://jedipunkz.github.io/post/ecs-execute-command/</link>
      <pubDate>Tue, 13 Apr 2021 18:35:36 +0900</pubDate>
      
      <guid>https://jedipunkz.github.io/post/ecs-execute-command/</guid>
      <description>こんにちは @jedipunkz 🚀 です。
最近、職場では ECS/Fargate でサービスを運用しています。そこで ECS Task 上でコマンドを実行する必要に迫られて幾つか調べたのですが、複数の方法があり検証をしてみました。これには 2021/03 にリリースされたばかりの ECS 上のコンテナでコマンドを実行する機能も含まれています。
自分たちは自動化する必要があったので Go 言語 (aws-sdk-go) を中心に検証実施しましたが同時に awscli でも動作検証しましたので、その方法をこの記事に記そうかと思います。
下記の2つの ECS の機能についてそれぞれ Go 言語, awscli で動作検証実施しました。
(1) ECS Execute Command (2021/03 リリース) (2) ECS Run Task 用いるツール類 下記のツールを前提に記事を記します。
aws-sdk-go Terraform awscli 共通で必要な taskRoleArn まず Task Definition に対して executeRoleArn とは別に TaskRoleArn の付与が必要になります。
resource &amp;#34;aws_ecs_task_definition&amp;#34; &amp;#34;sample&amp;#34; { family = &amp;#34;sample&amp;#34; cpu = &amp;#34;256&amp;#34; memory = &amp;#34;512&amp;#34; network_mode = &amp;#34;awsvpc&amp;#34; requires_compatibilities = [&amp;#34;FARGATE&amp;#34;] execution_role_arn = module.</description>
    </item>
    
    <item>
      <title>自己紹介</title>
      <link>https://jedipunkz.github.io/about/profile/</link>
      <pubDate>Sun, 27 Dec 2020 17:21:23 +0900</pubDate>
      
      <guid>https://jedipunkz.github.io/about/profile/</guid>
      <description>こんにちは @jedipunkz 🚀 です。 元インフラエンジニア・クラウドエンジニアで最近は AWS, GCP を扱う SRE として働いています。ソフトウェアでインフラの課題を解決すべく勉強していきます。学んだことをこのブログに記せたらいいなと思っています。</description>
    </item>
    
    <item>
      <title>興味のある技術/テーマたち</title>
      <link>https://jedipunkz.github.io/about/interested/</link>
      <pubDate>Sun, 27 Dec 2020 17:21:16 +0900</pubDate>
      
      <guid>https://jedipunkz.github.io/about/interested/</guid>
      <description>こんな技術に興味を持っています。
クラウド・コンテナ基盤, CNCF 技術 AWS ECS EKS GCP GKE BigQuery OpenStack Kubernetes Ceph Prometheus ArgoCD (Rollouts) Flux PipeCD プログラミング言語 Golang Rust Python Infrastracture as a Code Tools Terraform Ansible AWS CDK テーマ AWS, GCP を使った SRE 活動 プログレッシブデリバリー サービスメッシュ データ基盤 ログ基盤 Datadog Hashicorp の各技術・ソフトウェア </description>
    </item>
    
    <item>
      <title>EKS/Fargate &#43; ArgoCD でボット環境 GitOps 化</title>
      <link>https://jedipunkz.github.io/post/eks-fargate-bot-env/</link>
      <pubDate>Sun, 27 Dec 2020 00:49:51 +0900</pubDate>
      
      <guid>https://jedipunkz.github.io/post/eks-fargate-bot-env/</guid>
      <description>こんにちは。jedipunkz です。
仕事ではこれから会社のサービス環境として AWS ECS の導入を始めていくところなのですが、最近の SRE/インフラ界隈のトレンドを自分たちが追うために自分たち SRE が管理しているボット環境を EKS を使って GitOps 化しようということになり色々と準備を進めています。導入までもう一歩のところまで来たので、構成や技術要素についてここに記したいと思います。
どんなボットが動いているの？ まずボット開発に用いてる言語は Go 言語です。主に aws-sdk-go を使ってボットを開発しています。私達はコミュニケーションツールとして Slack を使っているので Slack ボット化するためには slack-go を使っています。 ただまだボットの数が少なく主に利用されてるのは Ansible を実行するモノです。開発環境へアプリをデプロイするのに Ansible を使っています。もうすぐ ECS 化するので役割はそろそろ終えるのですが&amp;hellip; 利点は開発者だけでなく非エンジニアの方が GitHub のブランチ上のアプリの動作をしたい際に Slack を使って簡単にアプリの動作ができるところです。今後は自動化目的でもっと沢山のボットを開発していきたいです。
EKS/Fargate vs EKS/EC2 EKS の利用を検討する際に Fargate タイプと EC2 タイプがあります。2020年の今年頭に評価した際には ALB Ingress Controller と HPA のための Metrics Server が正常に動作しない状態だったので、まだ EC2 タイプを選択すべきなのかな&amp;hellip;と考えたのですが AWS 的にも Fargate を推してる気もするし再度評価実施しました。結果ドキュメントもソフトウェアもだいぶ更新されていて ALB Ingress Controller も Metrics Server もあっけなく動作し、今回のボット環境も EKS/Fargte を選択しました。</description>
    </item>
    
    <item>
      <title>マルチクラウド対応 SDK の Pulumi を使って Kubernetes を操作</title>
      <link>https://jedipunkz.github.io/post/pulumi/</link>
      <pubDate>Tue, 26 Nov 2019 01:27:22 +0900</pubDate>
      
      <guid>https://jedipunkz.github.io/post/pulumi/</guid>
      <description>こんにちは。@jedipunkz です。
今日は Pulumi (https://www.pulumi.com/) について紹介したいと思います。
最近ではすっかり Terraform がマルチクラウドな IaC ツールとして定着しましたが、巷では AWS CDK を使う現場も増えてきている印象です。AWS CDK は Typescript, Python などのプログラミング言語の中でインフラを定義・操作することができる AWS が提供しているツールです。ただ AWS CDK は名前の通り AWS にのみ対応していて内部的には Cloudformation Template がエキスポートされています。AWS オンリーという点と Cloudformation という点、また 2019 年時点で進化が激しく後方互換性を全く失っているので AWS CDK のアップデートに追従するのに苦労する点、色々ありまだ利用するには早いのかなぁという印象を個人的には持っています。
そこで今回紹介する Pulumi なのですが CDK 同様にプログラミング言語の中でインフラを定義できて尚且つマルチクラウド対応しています。どちらかというと旧来の SDK の分類だと思うのですが、Terraform 同様にマルチクラウドという点で個人的には以前よりウォッチしていました。
今回は公式の Getting Started 的なドキュメントに従って作業を進め Kubernetes の上に Pod を起動、その後コードを修正して再デプロイを実施して理解を深めてみたいと思います。
作業に必要なソフトウェア 下記のソフトウェア・ツールが事前にインストールされていることを前提としたいと思います。また macOS を前提に手順を記します。
Python3, Pip Minikube 参考 https://www.pulumi.com/docs/get-started/kubernetes/ 事前準備 まず macOS を使っている場合 Pulumi は下記の通り brew を使ってインストールできます。</description>
    </item>
    
    <item>
      <title>ECS の構成と Terraform コード化する際の構造化について</title>
      <link>https://jedipunkz.github.io/post/ecs/</link>
      <pubDate>Thu, 17 Oct 2019 18:37:36 +0900</pubDate>
      
      <guid>https://jedipunkz.github.io/post/ecs/</guid>
      <description>こんにちは。@jedipunkz です。
今回は AWS ECS についてです。直近の仕事で ECS の Terraform コード開発をしていたのですがコードの構造化について考えていました。一枚岩のコードを書いても運用に耐えられるとは考えられません。また ECS を構成するにあたって ECS のネットワークモードとコンテナのロギングについて考えているうちに、どの構成が一番適しているのか？について時間を掛けて考えました。ここではそれらについてまとめたいと思います。
Terraform コードの構造化 運用の精神的な負担を軽減するという観点で Terraform のコード開発をする上で一番重要なのはコードの構造化だと思います。前回のブログ記事に書いたのですがコードの構造化をする上で下記に留意して考えると良いと思います。
影響範囲 ステートレスかステートフルか 安定度 ライフサイクル 結果、具体的に Terraform のコードはどのような構造になるでしょうか。自分は下記のようにコンポーネント化して Terraform の実行単位を別けました。ここは人それぞれだと思いますが、ECS 本体と ECS の周辺 AWS サービスの一般的な物を考慮しつつ、いかにシンプルに構造化するかを考えると自然と下記の区分けになる気がします。
コンポーネント 具体的なリソース ネットワーク vpc, route table, igw, endpoint, subnet ECS 本体 ecs, alb, autoscaling, cloudwatch, iam CI/CD codebuild, codepipeline, ecr, iam パラメータ ssm, kms データストア s3, rds, elasticache &amp;hellip; vpc や subnet に関して頻繁に更新を掛ける人は少ないのではないでしょうか。よってネットワークは &amp;ldquo;影響範囲&amp;rdquo; を考慮しつつコンポーネントを別けました。また、同じ理由でパラメータ・CI/CD も ECS 本体とは実行単位を別けた方が好ましいと思います。また &amp;ldquo;ステートフルかステートレスか&amp;rdquo; という観点でデータベースやストレージは頻繁に更新する ECS 本体とは別けるべきでしょう。</description>
    </item>
    
    <item>
      <title>Pragmatic Terraform on AWS の内容が素晴らしかったので感想を述べる</title>
      <link>https://jedipunkz.github.io/post/2019/07/27/pragmatic-terraform/</link>
      <pubDate>Sat, 27 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2019/07/27/pragmatic-terraform/</guid>
      <description>こんにちは。@jedipunkz です。
今回は電子書籍 &amp;lsquo;Pragmatic Terraform on AWS&amp;rsquo; を読んでとても内容が素晴らしかったので紹介させて頂きます。書籍の購入は下記の URL から可能です。
https://booth.pm/ja/items/1318735
ブログ記事では書籍の細かい内容については述べません。これから購入される方々が買う意欲を無くすようなブログ記事にしたくないからです。なのでこのブログでは自分が Terraform 運用について感じていた問題点とこの電子書籍がどう解決してくれたのかについて記したいと思います。
自分が Terraform 運用で感じていた問題点 Terraform を使ったインフラコード化と構築は自分の結構経験があるのですが、その構築に使ったコードで構成を運用する難しさはいつも感じていました。Terraform を使った継続的なインフラの運用についてです。具体的には下記のような疑問と言いますか問題点です。
(1) どのような実行単位で .tf コードを書くか (2) 削除系・修正系の操作も Terraform で行うのか (3) ステートフルなインフラとステートレスなインフラの管理方法 (1) は terraform plan/apply を実行するディレクトリの構造についてです。Terraform は同じディレクトリ上にある .tf ファイル全てを読み込んでくれますし一斉にインフラをデプロイすることも可能です。ですが、何かインフラを修正・削除したい場合、削除してはいけないリソースも同ディレクトリ上の .tf ファイルで管理しているわけですから何かしらのミスで大事なインフラに影響を与えてしまう事になります。影響範囲が大きすぎるのです。
(2) は、&amp;lsquo;初期の構成のみを Terraform で構築自動化する&amp;rsquo; のかどうか、ということになります。構築に使ったコードで継続的に削除系・修正系の操作も行うのか。これも (1) と同様に管理するインフラの規模が大きくなると影響範囲が大きくなり、運用者の精神的負担が増します。
(3) は RDS, S3 等のステートフルなインフラと、それ以外のステートレスなインフラを同じ .tf コードで管理していいのか、という疑問です。修正・削除が多発するインフラは .tf コードで継続的に運用出来たとしても、RDS, S3 の様な状態が重要になるインフラは滅多に削除・修正操作は通常行いません。これら二種類のインフラを同じように管理してしまっていいのか？という疑問です。
これらの疑問や思っていた問題点について、この &amp;lsquo;Pragmatic Terraform on AWS&amp;rsquo; は全て解決してくれました。
Pragmatic Terraform on AWS の構成 章ごとの説明は詳細には書きませんが、大体の流れは下記のようになっています。</description>
    </item>
    
    <item>
      <title>期待のツール Terrafomer の基本動作方法と問題点</title>
      <link>https://jedipunkz.github.io/post/2019/07/26/terraformer/</link>
      <pubDate>Fri, 26 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2019/07/26/terraformer/</guid>
      <description>こんにちは。@jedipunkz です。
少し前の話なのですが Google Cloud Platform が Terraformer というツールを出しました。正確には数年前に Google が買収した Waze というサービスのエンジニア達が開発したようです。このツールは GCP, AWS, OpenStack, Kubernetes 等、各クラウド・プラットフォームに対応したリバース Terraform と言えるツールになっていて、構築されたクラウド上の状態を元に terraform の .tf コードと .tfstate ファイルをインポートしてくれます。terraform import は tfstate のインポートのみに対応してたのでこれは夢のツールじゃないか！ということで当初期待して使い始めたのですが、使ってみる中で幾つかの問題点も見えてきました。今回はその気になった問題点を中心に Terraformer の基本的な動作を説明していきたいと思います。
公式サイト 下記の Github アカウントで公開されています。
https://github.com/GoogleCloudPlatform/terraformer
Requrements Terraformer を動作させるには下記のソフトウェアが必要です。今回は macos を想定して情報を記していますが Linux でも動作します。適宜読み替えてください。インストール方法と設定方法はここでは割愛させて頂きます。
macos homebrew terraform awscli 今回の前提のクラウドプラットフォーム 自分がいつも使っているプラットフォームということで今回は aws を前提に記事を書いていきます。ここが結構肝心なところで、Google Cloud Platform が開発したこともあり GCP 向けの機能が一番 Feature されているように読み取れます。つまり aws を対象とした Terraformer の機能が一部追いついていない点も後に述べたいと思います。
動作させるまでの準備 Terraform と同様に Terraformer でも動作せせるディレクトリが必要になります。
mkdir working_dir cd working_dir Terraformer を動作させるために Terraform の plugin が必要です。先に述べたようにここでは &amp;lsquo;aws&amp;rsquo; Plugin をダウンロードします。そのために init.</description>
    </item>
    
    <item>
      <title>Consul Helm Chart で Kubernetes 上に Consul をデプロイ</title>
      <link>https://jedipunkz.github.io/post/consul-helm-chart/</link>
      <pubDate>Fri, 26 Apr 2019 17:08:02 +0900</pubDate>
      
      <guid>https://jedipunkz.github.io/post/consul-helm-chart/</guid>
      <description>こんにちは。@jedipunkz です。
今回は Hashicorp の Consul クラスタを Kubernetes 上で稼働させる方法について調べてみました。
Hashicorp Consul はサービスディスカバリが行えるソフトウェアで、私も以前居た職場で利用していました。アプリケーション間で互いに接続先を確認し合う事が出来ます。以前構築した Consul クラスタはインスタンス上に直に起動していたのですが最近だとどうやってデプロイするのか興味を持ち Kubernetes 上にデプロイする方法を調べた所 Helm を使って簡単にデプロイ出来る事が分かりました。
また今回は minikube を使って複数のレプリカを起動するようにしていますが、Helm Chart を用いると Kubernetes のノード毎に Consul Pod が1つずつ起動するようになっていて、ノードの障害を考慮した可用性という点でも優れているなぁと感じました。また Kubernetes の Pod ですのでプロセスが落ちた際に即座に再起動が行われるという点でも優れています。勿論 Consul クラスタの Leader が落ちた場合には Leader Election (リーダ昇格のための選挙) が行われ、直ちに隣接した Kubernetes ノード上の Consul Pod がリーダーに昇格します。といった意味でも Kubernetes 上に Consul をデプロイするという考えは優れているのではないでしょうか。
Requirements 下記のソフトウェアが事前に必要です。この手順では予めこれらがインストールされていることとして記していきます。
minikube kubectl helm Consul クラスタ起動までの手順 早速ですが手順を記していきます。
Hashicorp の Github にて Consul の Helm Chart が公開されています。helm search しても出てきますが、今回は Github のものを用いました。</description>
    </item>
    
    <item>
      <title>Istio Sidecar Injection を理解する</title>
      <link>https://jedipunkz.github.io/post/istio-sidecar-injection/</link>
      <pubDate>Wed, 24 Apr 2019 22:55:45 +0900</pubDate>
      
      <guid>https://jedipunkz.github.io/post/istio-sidecar-injection/</guid>
      <description>こんにちは。@jedipunkz です。
前回の記事 「Istio, Helm を使って Getting Started 的なアプリをデプロイ」で kubernetes 上で istio をインストールし sidecar injection を有効化しサンプルアプリケーションを起動しました。その結果、sidecar 的に envoy コンテナが起動するところまで確認しました。今回はもう少し単純な pod を用いて &amp;lsquo;sidecar injection&amp;rsquo; の中身をもう少しだけ深掘りして見ていきたいと思います。
Rquirements 記事と同等の動きを確認するために下記のソフトウェアが必要になります。 それぞれのソフトウェアは事前にインストールされた前提で記事を記していきます。
macos or linux os kubectl istioctl minikube 参考 URL 下記の istio 公式ドキュメントを参考に動作確認しました。
https://istio.io/docs/setup/kubernetes/additional-setup/sidecar-injection/ https://istio.io/docs/setup/kubernetes/additional-setup/sidecar-injection/ minikube で kubenetes をデプロイ 前回同様に minikube 上で動作を確認していきます。パラメータは適宜、自分の環境に読み替えてください。
minikube start --memory=8192 --cpus=4 --kubernetes-version=v1.10.0 \ --extra-config=controller-manager.cluster-signing-cert-file=&amp;#34;/var/lib/minikube/certs/ca.crt&amp;#34; \ --extra-config=controller-manager.cluster-signing-key-file=&amp;#34;/var/lib/minikube/certs/ca.key&amp;#34; \ --vm-driver=virtualbox istio を稼働させる 下記のコマンドを用いてカレントディレクトリに istio のサンプル yaml が入ったフォルダを展開します。
curl -L https://git.io/getLatestIstio | sh - 次に下記のコマンドで kubernetes 上に istio をインストールします。 istio コンテナ間の通信をプレインテキスト or TLS で行うよう istio-demo.</description>
    </item>
    
    <item>
      <title>Istio, Helm を使って Getting Started 的なアプリをデプロイ</title>
      <link>https://jedipunkz.github.io/post/2018/12/31/istio/</link>
      <pubDate>Mon, 31 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2018/12/31/istio/</guid>
      <description>こんにちは。@jedipunkz です。
最近は kubernetes を触ってなかったのですが Istio や Envoy 等 CNCF 関連のソフトウェアの記事をよく見かけるようになって、少し理解しておいたほうがいいかなと思い Istio と Minikube を使って Getting Started 的な事をやってみました。Istio をダウンロードすると中にサンプルアプリケーションが入っているのでそれを利用してアプリのデプロイまでを行ってみます。
Istio をダウンロードするとお手軽に Istio 環境を作るための yaml ファイルがあり、それを kubectl apply することで Istio 環境を整えられるのですが、ドキュメントにプロダクション環境を想定した場合は Helm Template を用いた方がいいだろう、と記載あったので今回は Helm Template を用いて Istio 環境を作ります。
前提の環境 下記の環境でテストを行いました。
macos Mojave minikube v0.32.0 kubectl v1.10.3 helm v2.12.1 virtualbox 準備 kubectl と helm のインストール kubctl と helm をインストールします。両者共に homebrew でインストールします。
brew install kubernetes-cli brew install kubernetes-helm minikube のインストールと起動 minikube をインストールして起動します。</description>
    </item>
    
    <item>
      <title>Docker,Test-Kitchen,Ansible でクラスタを構成する</title>
      <link>https://jedipunkz.github.io/post/test-kitchen-cluster/</link>
      <pubDate>Sun, 02 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/test-kitchen-cluster/</guid>
      <description>こんにちは。@jedipunkz です。
以前、&amp;ldquo;Test-Kitchen, Docker で Ansible Role 開発サイクル高速化！&amp;rdquo; ってタイトルで Ansible Role の開発を test-kitchen を使って行う方法について記事にしたのですが、やっぱりローカルで Docker コンテナ立ち上げてデプロしてテストして.. ってすごく楽というか速くて今の現場でも便利につかっています。前の記事の URL は下記です。
https://jedipunkz.github.io/blog/2016/07/14/test-kitchen-with-ansible/
最近？は ansible container って技術もあるけど、僕らが Docker 使う目的はコンテナでデプロイするのではなくて単に Ansible を実行するローカル環境が欲しいってこととか、Serverspec をローカル・実機に実行する環境が欲しいってことなので、今でも test-kitchen 使っています。
で、最近になって複数ノードの構成の Ansible Role を test-kitchen, Docker を使って開発できることに気がついたので記事にしようと思います。これができるとローカルで Redis Master + Redis Slave(s) + Sentinel って環境も容易にできると思います。
使うソフトウェア 前提は macOS ですが Linux マシンでも問題なく動作するはずです。
ほぼ前回と同じです。
Ansible Docker test-kitchen kitchen-docker (test-kitchen ドライバ) kitchen-ansible (test-kitchen ドライバ) Serverspec インストール ソフトウェアののインストール方法については前回の記事を見てもらうこととして割愛します。
test-kitchen の環境を作る test-kitchen の環境を作ります。&amp;lsquo;kitchen init&amp;rsquo; を実行して基本的には生成された .</description>
    </item>
    
    <item>
      <title>GCP ロードバランサと GKE クラスタを Terraform を使って構築する</title>
      <link>https://jedipunkz.github.io/post/gke-lb/</link>
      <pubDate>Thu, 13 Apr 2017 14:53:37 +0900</pubDate>
      
      <guid>https://jedipunkz.github.io/post/gke-lb/</guid>
      <description>GCP ロードバランサと GKE クラスタを Terraform を使って構築自動化</description>
    </item>
    
    <item>
      <title>Serverless on Kubernetes : Fission を使ってみた</title>
      <link>https://jedipunkz.github.io/post/serverless-fission/</link>
      <pubDate>Sun, 12 Feb 2017 14:55:01 +0900</pubDate>
      
      <guid>https://jedipunkz.github.io/post/serverless-fission/</guid>
      <description>Kubernetes 上で Serverless を実現する Fission を使ってみた</description>
    </item>
    
    <item>
      <title>Kubernetes Deployments を使ってみた！</title>
      <link>https://jedipunkz.github.io/post/kubernetes-deployments/</link>
      <pubDate>Fri, 13 Jan 2017 20:29:07 +0900</pubDate>
      
      <guid>https://jedipunkz.github.io/post/kubernetes-deployments/</guid>
      <description>Kubernetes Replication Controller の次世代版 Deployments を使ってみました</description>
    </item>
    
    <item>
      <title>fluentd-sidecar-gcp と Kubernetes Volumes で Cloud Logging ログ転送</title>
      <link>https://jedipunkz.github.io/post/fluentd-sidecar-gcp/</link>
      <pubDate>Thu, 29 Dec 2016 09:43:18 +0900</pubDate>
      
      <guid>https://jedipunkz.github.io/post/fluentd-sidecar-gcp/</guid>
      <description>fluentd-sidecar-gcp と Kubernetes Volumes で Cloud Logging へログ転送</description>
    </item>
    
    <item>
      <title>Google Cloud CDN を使ってみた</title>
      <link>https://jedipunkz.github.io/post/cloud-cdn/</link>
      <pubDate>Thu, 29 Dec 2016 09:32:49 +0900</pubDate>
      
      <guid>https://jedipunkz.github.io/post/cloud-cdn/</guid>
      <description>こんにちは。@jedipunkz です。
今回は Google Cloud Platform の Google CloudCDN について調べてみたので記したいと思います。
CloudCDN は GCP のロードバランサのバックエンドサービスに紐付けられるサービスです。このバックエンドサービスで CloudCDN を有効にしていると CDN サービスを機能させることが出来ます。先に書いておくとこの CloudCDN はとてもシンプルで扱いやすいサービスだと判りました。高機能な他の CDN サービスと比べると機能が足らない感ありますが、必要最低限なところを抑えているのと、価格がとても安いです。(価格は下記の URL 参照)
価格表 : https://cloud.google.com/cdn/pricing
構成 構成と構成の特徴です。
+----------+ +---------+ | instance |--+ +-| EndUser | +----------+ | +------------+ +----------+ | +---------+ +--|LoadBalancer|--| CloudCDN |-+-| EndUser | +----------+ | +------------+ +----------+ | +---------+ | instance |--+ +-| EndUser | +----------+ +---------+ コンテンツが初めてリクエストされた場合キャッシュミスする キャッシュミスした際に近くにあるキャッシュからコンテンツを取得しようと試みる 近くのキャッシュがコンテンツがある場合、最初のキャッシュにコンテンツが送信される 近くのキャッシュにコンテンツがない場合、HTTP ロードバランサにリクエストが転送される その後のリクエストはキャッシュが応答する(キャッシュヒット) キャッシュ間のフィルは EndUser のリクエストに応じて実行される キャッシュを事前に読み込むことできない キャッシュは世界各地に配置されている CloudCDN を導入する方法 導入する方法は簡単で下記のとおりです。</description>
    </item>
    
    <item>
      <title>coreos の etcd operator を触ってみた</title>
      <link>https://jedipunkz.github.io/post/etcd-operator/</link>
      <pubDate>Sun, 27 Nov 2016 21:00:45 +0900</pubDate>
      
      <guid>https://jedipunkz.github.io/post/etcd-operator/</guid>
      <description>coreos が最近アナウンスした etcd operator を触ってみた</description>
    </item>
    
    <item>
      <title>Helm を使って Kubernetes を管理する</title>
      <link>https://jedipunkz.github.io/post/helm/</link>
      <pubDate>Sun, 20 Nov 2016 11:27:00 +0900</pubDate>
      
      <guid>https://jedipunkz.github.io/post/helm/</guid>
      <description>こんにちは。@jedipunkz です。
今回は Helm という kubernetes のパッケージマネージャ的なソフトウェアを使ってみたので記事にしたいと思います。
公式サイト : https://helm.sh/ Kubernetes を仕事で使っているのですが &amp;ldquo;レプリケーションコントローラ&amp;rdquo; や &amp;ldquo;サービス&amp;rdquo; といった単位を使って Pod, Service を管理しています。Helm を使うことでこれらの管理方法が変わるのか調べたいと思います。
依存するソフトウェア 今回は MacOS を使って環境を整えます。
virtualbox minikube kubectl これらのソフトウェアをインストールしていきます。
$ brew cask install virtualbo $ curl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.12.2/minikube-darwin-amd64 &amp;amp;&amp;amp; chmod +x minikube &amp;amp;&amp;amp; sudo mv minikube /usr/local/bin/ $ brew install kubectl minikube を使って簡易的な kubernetes 環境を起動します。
$ minikube start $ eval $(minikube docker-env) Helm を使ってみる Helm は Charts という単位で Kubernetes をパッケージングします。Charts の一覧を見てみましょう。</description>
    </item>
    
    <item>
      <title>kubernetes1.4 で実装された ScheduledJob を試してみた！</title>
      <link>https://jedipunkz.github.io/post/kubernetes-scheduledjob/</link>
      <pubDate>Thu, 03 Nov 2016 09:08:48 +0900</pubDate>
      
      <guid>https://jedipunkz.github.io/post/kubernetes-scheduledjob/</guid>
      <description>こんにちは、@jedipunkz です。今回は Kubernetes1.4 から実装された ScheduledJob を試してみたのでその内容を記したいと思います。
ScheduledJob はバッチ的な処理を Kubernetes の pod を使って実行するための仕組みです。現在は alpha バージョンとして実装されています。 kubernetes の pod, service は通常、永続的に立ち上げておくサーバなどを稼働させるものですが、それに対してこの scheduledJob は cron 感覚でバッチ処理を pod に任せることができます。
Alpha バージョンということで今回の環境構築は minikube を使って簡易的に Mac 上に構築しました。Docker がインストールされていれば Linux でも環境を作れます。
参考 URL 今回利用する yaml ファイルなどは下記のサイトを参考にしています。
http://kubernetes.io/docs/user-guide/scheduled-jobs/ https://github.com/kubernetes/minikube 前提の環境 私の環境では下記の環境を利用しました。
Mac OSX Docker-machine or Docker for Mac minikube kubernetes 1.4 以降の構成を minikube で構築する まず minikube のインストールを行います。
$ curl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.12.0/minikube-darwin-amd64 &amp;amp;&amp;amp; chmod +x minikube &amp;amp;&amp;amp; sudo mv minikube /usr/local/bin/ 早速 minikube を起動します。</description>
    </item>
    
    <item>
      <title>Minikube で簡易 kubernetes 環境構築</title>
      <link>https://jedipunkz.github.io/post/minikube/</link>
      <pubDate>Mon, 25 Jul 2016 23:18:16 +0900</pubDate>
      
      <guid>https://jedipunkz.github.io/post/minikube/</guid>
      <description>こんにちは。@jedipunkz です。
kubernetes の環境を簡易的に作れる Minikube (https://github.com/kubernetes/minikube) が2ヶ月前ほどにリリースになっていました。簡単ですが少し触ってみたのでその際のメモを記したいと思います。VirtualBox もしくは VMware Fusion がインストールされていればすぐにでも稼働可能です。私は Kubernetes 初心者ですが何も考えずに kubernetes を動かすことが出来ました。
前提 前提として下記の環境が必要になります。
Mac OSX がインストールされていること VirtualBox もしくは VMware Fusion がインストールされていること minikube をインストール minikube をインストールします。
$ curl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.6.0/minikube-darwin-amd64 &amp;amp;&amp;amp; chmod +x minikube &amp;amp;&amp;amp; sudo mv minikube /usr/local/bin/ kubetl をインストール 次に kubectl をインストールします。
$ curl -k -o kubectl https://kuar.io/darwin/kubectl &amp;amp;&amp;amp; chmod +x kubectl &amp;amp;&amp;amp; sudo mv kubectl /usr/local/bin/ Minikube で Kurbernates を稼働 Minikube を使って Kubernetes を稼働してみます。下記のコマンドを実行すると Virtualbox 上で仮想マシンが稼働し Kubernetes 一式も立ち上がります。</description>
    </item>
    
    <item>
      <title>Go言語とInfluxDBで監視のコード化</title>
      <link>https://jedipunkz.github.io/post/influxdb-go/</link>
      <pubDate>Sat, 23 Jul 2016 02:40:11 +0900</pubDate>
      
      <guid>https://jedipunkz.github.io/post/influxdb-go/</guid>
      <description>こんにちは。@jedipunkz です。
今日は Go 言語でサーバのメトリクスデータを InfluxDB に入れてリソース監視を行う方法について書きます。
Ansible, Terraform, Chef などのソフトウェアを使ってインフラを定義するのが当たり前になった現在ですが、本当の意味でのソフトウェアによるインフラの定義ってなんだろと最近考えています。aws-sdk や fog などを使ったネイティブな言語でインフラを定義することの意味もあるように感じているからです。某サービスプロバイダのエンジニアはこうした言語によるインフラの定義の一番大きなメリットとして &amp;ldquo;再利用性&amp;rdquo; をあげていました。こうしたソフトウェアによるインフラの定義や構成を行う上で監視についてもコード化できるのでは？と考えて今回の記事に至りました。
使うモノ https://github.com/influxdata/influxdb/tree/master/client 公式の InfluxDB Go Client です。InfluxDB 自体が Go 言語で書かれていますがクライアントも Go 言語で記述することができます。ここにあるサンプルコードをすこしいじって、今回の記事を書こうと思います。
https://github.com/shirou/gopsutil @shirou さんが作られた psutil の Go 言語版です。CPU, Mem などリソースをモニタするのに便利なので利用します。
環境構築 環境を作っていきます。InfluxDB と Chronograf を構築するのですが Docker で構築するのが簡単なのでこれを利用します。Chronograf は InfluxDB 内のデータを可視化するためのソフトウェアです。
InfluxDB の起動 InfluxDB のコンテナを起動します。
docker run -p 8083:8083 -p 8086:8086 \ -v $PWD:/var/lib/influxdb \ influxdb Chronograf の起動 Chronograf のコンテナを起動します。
docker run -p 10000:10000 chronograf この時点で http://${DOCKER_HOST}:10000/ にアクセスすると Chronograf の UI を確認できます。</description>
    </item>
    
    <item>
      <title>Test-Kitchen, Docker で Ansible Role 開発サイクル高速化！</title>
      <link>https://jedipunkz.github.io/post/test-kitchen-with-ansible/</link>
      <pubDate>Thu, 14 Jul 2016 09:10:57 +0900</pubDate>
      
      <guid>https://jedipunkz.github.io/post/test-kitchen-with-ansible/</guid>
      <description>test-kitchen with ansible, docker</description>
    </item>
    
    <item>
      <title>イベントドリブンな StackStorm で運用自動化</title>
      <link>https://jedipunkz.github.io/post/stackstorm/</link>
      <pubDate>Sat, 02 Jul 2016 23:37:17 +0900</pubDate>
      
      <guid>https://jedipunkz.github.io/post/stackstorm/</guid>
      <description>こんにちは。@jedipunkz です。
今回は StackStorm (https://stackstorm.com/) というイベントドリブンオートメーションツールを使ってみましたので 紹介したいと思います。
クラウドとプロビジョニングツールの登場で昨今はエンジニアがほぼ全ての操作を自動化出来るようになりました。 ですが監視についてはどうでしょうか？監視システムを自動で構築することが出来ても障害発生時に対応を行う のは手動になっていませんでしょうか。もちろんクラスタ組んでいれば大抵のアラートは放置出来ますが、クラスタ を組むことが出来ないような箇所はクラウドを使ってもどうしても出てきます。
そこで登場するのが今回紹介する StackStorm のようなツールかなぁと考えるようになりました。
インストール インストール手順は下記の URL にあります。
https://docs.stackstorm.com/install/index.html
自分は CentOS7 を使ったので下記のワンライナーでインストールできました。 password は任意のものを入れてください。
curl -sSL https://stackstorm.com/packages/install.sh | bash -s -- --user=st2admin --password=foo MongoDB, postgreSQL が依存してインストールされます。
80番ポートで下記のような WEB UI も起動します。
StackStorm の基本 基本を知るために幾つかの要素について説明していきます。
まず CLI で操作するために TOKEN を取得して環境変数にセットする必要があります。 上記で設定したユーザ名・パスワードを入力してください。
export ST2_AUTH_TOKEN=`st2 auth -t -p foo st2admin` Action Action はイベントが発生した際に実行できるアクションになります。早速アクションの一覧を取得してみましょう。
$ st2 action list +---------------------------------+---------+-------------------------------------------------------------+ | ref | pack | description | +---------------------------------+---------+-------------------------------------------------------------+ | chatops.</description>
    </item>
    
    <item>
      <title>Vagrant で Mesosphere DC/OS を構築</title>
      <link>https://jedipunkz.github.io/post/mesos-dcos-vagrant/</link>
      <pubDate>Tue, 21 Jun 2016 17:05:25 +0900</pubDate>
      
      <guid>https://jedipunkz.github.io/post/mesos-dcos-vagrant/</guid>
      <description>こんにちは。@jedipunkz です。
今回は DC/OS (https://dcos.io/) を Vagrant を使って構築し評価してみようと思います。 DC/OS はその名の通りデータセンタ OS として利用されることを期待され開発された OS で内部で Docker と Mesos が稼働しています。
一昔前に Mesos のマルチノード構成は構築したことあったのですが、DC/OS はデプロイ方法が全く変わっていました。 はじめに想定する構成から説明していきます。
想定する構成 本来 DC/OS は public, private ネットワーク構成ですが利用するレポジトリではこのような構成が想定されていました。
+----+ +----+ +----+ +------+ | m1 | | a1 | | p1 | | boot | +----+ +----+ +----+ +------+ | | | | +------+------+------+--------- 192.168.65/24 各ノードは下記の通り動作します。
m1 : Mesos マスタ, Marathon a1 : Mesos スレーブ(Private Agent) p1 : Mesos スレーブ(Public Agent) boot : DC/OS インストレーションノード 前提の環境 Vagrant が動作するマシンであれば問題ないと思いますが私は下記の構成で利用しました。 比較的たくさんのマシンリソースを使うのでメモリ 8GB はほしいところです。</description>
    </item>
    
    <item>
      <title>Chronograf, Telegraf, Influxdbでサーバとコンテナ情報を可視化する</title>
      <link>https://jedipunkz.github.io/post/2015/12/28/chronograf-telegraf-influxdb/</link>
      <pubDate>Mon, 28 Dec 2015 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2015/12/28/chronograf-telegraf-influxdb/</guid>
      <description>こんにちは。@jedipunkz です。
Influxdb が Influxdata (https://influxdata.com/) として生まれ変わり公式の メトリクス送信エージェント Telegraf と可視化ツール Chronograf をリリースしたので 使ってみました。
3つのツールの役割は下記のとおりです。
Chronograf : 可視化ツール, Grafana 相当のソフトウェアです Telegraf : メトリクス情報を Influxdb に送信するエージェント Influxdb : メトリクス情報を格納する時系列データベース 以前に cAdvisor, influxdb, grafana を使って Docker コンテナのメトリクスを可視 化する記事を書きましたが telegraf を使うとサーバ情報と合わせて Docker コンテナ のメトリクスも influxdb に送信することが出来ます。個人的にはそのコンテナ情報の 扱いもサーバ情報と同様に扱ってくれる点に期待しつつ、評価してみました。
今回の環境 今回は Ubuntu 15.04 vivid64 を使ってテストしています。
influxdb をインストールして起動 最新リリース版の deb パッケージが用意されていたのでこれを使いました。
wget http://influxdb.s3.amazonaws.com/influxdb_0.9.5.1_amd64.deb sudo dpkg -i influxdb_0.9.5.1_amd64.deb sudo service influxdb start telegraf のインストールと起動 こちらも deb パッケージで。
wget http://get.</description>
    </item>
    
    <item>
      <title>Weave を使った Docker ネットワーク</title>
      <link>https://jedipunkz.github.io/post/2015/12/22/weave-docker-network/</link>
      <pubDate>Tue, 22 Dec 2015 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2015/12/22/weave-docker-network/</guid>
      <description>こんにちは。@jedipunkz です。
今回は Weave というコンテナ間のネットワークを提供してくれる Docker のネットワークプラ グインを使ってみました。下記のような沢山の機能があるようです。
Fast Data Path Docker Network Plugin Security Dynamic Netwrok Attachment Service Binding Fault Tolerance etc &amp;hellip; この記事では上から幾つか抜粋して、Weave ってどのように動かせるのか？を解説します。 そこから Weave が一体ナニモノなのか理解できればなぁと思います。
Vagrant を使った構成 この記事では下記の構成を作って色々と試していきます。使う技術は
Vagrant Docker Weave です。
+---------------------+ +---------------------+ +---------------------+ | docker container a1 | | docker container a2 | | docker container a3 | +---------------------+ +---------------------+ +---------------------+ | vagrant host 1 | | vagrant host 2 | | vagrant host 3 | +---------------------+-+---------------------+-+---------------------+ | Mac or Windows | +---------------------------------------------------------------------+ 特徴としては</description>
    </item>
    
    <item>
      <title>CodeDeploy, S3 を併用して CircleCI により VPC にデプロイ</title>
      <link>https://jedipunkz.github.io/post/2015/11/15/circleci-codedeploy/</link>
      <pubDate>Sun, 15 Nov 2015 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2015/11/15/circleci-codedeploy/</guid>
      <description>こんにちは。@jedipunkz です。
最近、業務で CircleCI を扱っていて、だいぶ &amp;ldquo;出来ること・出来ないこと&amp;rdquo; や &amp;ldquo;出来ないこと に対する回避方法&amp;rdquo; 等のノウハウが若干溜まってきたので共有したいなと思います。
この記事の前提&amp;hellip; ここでは CodeDeploy の設定方法や、CircleCIの設定方法等に関しては記述しませ ん。あくまで、Tips 的な内容にしています。また運用する上で想定できる問題点と、 それの回避方法等&amp;hellip;についてまとめています。
CirlceCI と併用するサービスについて CircleCI は Github と連携してレポジトリ内の制御ファイル circle.yml に従ってテ スト・ビルド・デプロイを実施してくれる CI サービスです。ただ CircleCI は自分た ちの管理しているシステム外にあるので、AWS VPC を用いていると VPC 内のプライベー トインスタンスにデプロイするのが難しいです。プロキシ挟んで・・ってことは出来そ うですがプロキシの運用もしたくない、AWS のインフラリソースに任せることが出来た らインスタンス・インスタンス上のミドルウェアを運用しなくて済むので運用コストが 省けそう。ってことで
AWS S3 (https://aws.amazon.com/jp/s3/) AWS CodeDeploy (https://aws.amazon.com/jp/codedeploy/) を併用することを考えました。
S3 は皆さんご存知のオブジェクトストレージです。CircleCI 用のバケットを作って、 ビルドした結果を格納します。私の務めている会社ではプログラミング言語として Scala を用いているので SBT というツールを使ってビルドします。その結果もバージョ ニングしつつ S3 バケットに格納できれば、万が一問題が発生した時にバイナリ毎切り 戻すことが出来そうです。
また CodeDeploy は EC2 インスタンス・またオンプレのインスタンスへコードのデプ ロイが可能になるサービスです。東京リージョンでは 2015/08 から利用が可能になり ました。これの便利なところは CircleCI 等の CI サービスから簡単に叩けて、VPC 内 のインスタンスに対してもデプロイが可能なところです。</description>
    </item>
    
    <item>
      <title>cAdvisor/influxDB/GrafanaでDockerリソース監視</title>
      <link>https://jedipunkz.github.io/post/2015/09/12/cadvisor-influxdb-grafana-docker/</link>
      <pubDate>Sat, 12 Sep 2015 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2015/09/12/cadvisor-influxdb-grafana-docker/</guid>
      <description>こんにちは。@jedipunkz です。
今回は Docker ネタです。Docker 導入するにしても監視はどうする？という話になる と思うのですが、各 Monitoring as a Service を使うにしてもエージェント入れない といけないしお金掛かることもあるし..で、調べていたら cAdvisor というキーワード が出てきました。今回は cAdvisor を使ってコンテナの監視が出来ないか、について書 いていきたいと想います。
cAdvisor とは ? cAdvisor は Kubernates で用いられているコンポーネントで単体でも利用可能とのこ と。Google が開発しています。また Docker コンテナの監視においてこの cAdvisor は一般化しつつあるようです。
https://github.com/google/cadvisor
収集したメトリクスの保存 cAdvisor 自体も Docker で起動して、同ホスト上に起動している Docker コンテナの リソースをモニタリングしてくれます。そのメトリクスデータは幾つかの DB に保存出 来るのですが、そのうちの一つが influxDB です。influxDB は時系列データベースで す。システムのメトリクスデータを収めるのにちょうどいいデータベースになります。
https://influxdb.com/
DB に収めたメトリクスの可視化 influxDB に収めたメトリクスデータを可視化するのが Grafana です。Grafana のデー タソースは influxDB の他にも幾つかあり Elasticsearch, KairosDB, Graphite 等が それです。
http://grafana.org/
では早速試してみましょう。
前提の環境 今回は Vagrant を使います。また Vagrant 上で上記の3つのソフトウェアを Docker で稼働します。またどうせなので docker-compose を使って3つのコンテナを一斉に立 ち上げてみましょう。</description>
    </item>
    
    <item>
      <title>Knife-ZeroでOpenStack Kiloデプロイ(複数台編)</title>
      <link>https://jedipunkz.github.io/post/2015/07/20/knife-zero-openstack-kilo/</link>
      <pubDate>Mon, 20 Jul 2015 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2015/07/20/knife-zero-openstack-kilo/</guid>
      <description>こんにちは。@jedipunkz です。
前回 OpenStack Kilo のオールインワン構成を Chef-Zero を使ってデプロイする方法 を書きましたが、複数台構成についても調べたので結果をまとめていきます。
使うのは openstack/openstack-chef-repo です。下記の URL にあります。
https://github.com/openstack/openstack-chef-repo
この中に Vagrant を使ったファイルが存在しますが、実機でのデプロイには全く役に 立ちません。自分で Environment ファイルを作成する必要があります。今回は前提の 構成を作って、それに合わせた Environment ファイルを記します。ほぼスタンダード な構成にしましたので、自分の環境に合わせて修正するのも比較的簡単だと想います。 参考にしてください。
今回は knife-zero を使ってデプロイします。Chef サーバが必要なく、knife-zero を 使うホスト上のオンメモリで Chef サーバが稼働するので準備がほとんど必要ありません。
早速ですが、構成と準備・そしてデプロイ作業を記していきます。
前提の構成 +------------+ | GW Router | +--+------------+ | | | +--------------+--------------+---------------------------- public network | | eth0 | eth0 | +------------+ +------------+ +------------+ +------------+ | | Controller | | Network | | Compute | | Knife-Zero | | +------------+ +-------+----+ +------+-----+ +------------+ | | eth1 | eth1 | | eth1 | | eth1 +--+--------------+-------)------+------)-------+------------- api/management network | eth2 | eth2 +-------------+--------------------- guest network 特徴としては&amp;hellip;</description>
    </item>
    
    <item>
      <title>Chef-ZeroでOpenStack Kiloデプロイ(オールインワン編)</title>
      <link>https://jedipunkz.github.io/post/2015/07/16/chef-zero-openstack-allinone/</link>
      <pubDate>Thu, 16 Jul 2015 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2015/07/16/chef-zero-openstack-allinone/</guid>
      <description>こんにちは。@jedipunkz です。
久々に openstack-chef-repo を覗いてみたら &amp;lsquo;openstack/openstack-chef-repo&amp;rsquo; とし て公開されていました。今まで stackforge 側で管理されていましたが &amp;lsquo;openstack&amp;rsquo; の方に移動したようです。
https://github.com/openstack/openstack-chef-repo
結構安定してきているのかな？と想い、ちらっと試したのですが案の定、簡単に動作さ せることが出来ました。
今回はこのレポジトリを使ってオールインワン構成の OpenStack Kilo を作る方法をま とめていきます。
前提の構成 このレポジトリは Vagrant で OpenStack を作るための環境一式が最初から用意されて いますが、Vagrant では本番環境を作ることは出来ないため、Ubuntu ホストを前提と した記述に差し替えて説明していきます。前提にする構成は下記のとおりです。
Uuntu Linux 14.04 x 1 台 ネットワークインターフェース x 3 つ eth0 : External ネットワーク用 eth1 : Internal (API, Manage) ネットワーク用 eth2 : Guest ネットワーク用 特徴としては上記なのですが、eth2 に関してはオールインワンなので必ずしも必要と いうわけではありません。複数台構成を考慮した設定になっています。
前提のIP アドレス この記事では下記の IP アドレスを前提にします。お手持ちの環境の IP アドレスが違 い場合はそれに合わせて後に示す json ファイルを変更してください。
10.0.1.10 (eth0) : external ネットワーク 10.</description>
    </item>
    
    <item>
      <title>オブジェクトストレージ minio を使ってみる</title>
      <link>https://jedipunkz.github.io/post/2015/06/25/minio/</link>
      <pubDate>Thu, 25 Jun 2015 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2015/06/25/minio/</guid>
      <description>こんにちは、@jedipunkz です。
久々にブログ更新になりましたが、ウォーミングアップで minio というオブジェクト ストレージを使ってみたメモを記事にしたいと想います。
minio は Minimal Object Storage の名の通り、最小限の小さなオブジェクトストレー ジになります。公式サイトは下記のとおりです。
http://minio.io/
Golang で記述されていて Apache License v2 の元に公開されています。
最近、資金調達の話も挙がっていたので、これから一般的になってくるのかもしれません。
早速ですが、minio を動かしてみます。
Minio を起動する 方法は mithub.com/minio/minio の README に書かれていますが、バイナリを持ってき て実行権限を与えるだけのシンプルな手順になります。
Linux でも Mac でも動作しますが、今回私は Mac 上で動作させました。
% wget https://dl.minio.io:9000/updates/2015/Jun/darwin-amd64/minio % chmod +x minio % ./minio mode memory limit 512MB Starting minio server on: http://127.0.0.1:9000 Starting minio server on: http://192.168.1.123:9000 起動すると Listening Port と共に EndPoint の URL が表示されます。
次に mc という minio client を使って動作確認します。</description>
    </item>
    
    <item>
      <title>VyOS で VXLAN を使ってみる</title>
      <link>https://jedipunkz.github.io/post/2014/12/16/vyos-vxlan/</link>
      <pubDate>Tue, 16 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2014/12/16/vyos-vxlan/</guid>
      <description>こんにちは。@jedipunkz です。
VyOS に VXLAN が実装されたと聞いて少し触ってみました。この情報を知ったきっかけ は @upaa さんの下記の資料です。
参考資料 : http://www.slideshare.net/upaa/vyos-users-meeting-2-vyosvxlan
VyOS は御存知の通り実体は Debian Gnu/Linux 系の OS でその上に OSS なミドル ウェアが搭載されていて CLI でミドルウェアのコンフィギュレーション等が行えるモ ノになっています。Linux で VXLAN といえば OVS を使ったモノがよく知られています が VyOS の VXLAN 機能は Linux Kernel の実装を使っているようです。
要件 トンネルを張るためのセグメントを用意 VyOS 1.1.1 (現在最新ステーブルバージョン) が必要 Ubuntu Server 14.04 LTS (同じく Linux VXLAN 搭載バージョン) 構成 特徴
マネージメント用セグメント 10.0.1.0/24 を用意 GRE と同じくトンネル終端が必要なのでそのためのセグメント 10.0.2.0/24 を用意 各 eth1 は IP reachable である必要があるので予め IP アドレスの設定と疎通を確認 VXLAN を喋れる Ubuntu 14.</description>
    </item>
    
    <item>
      <title>Aviator でモダンに OpenStack を操作する</title>
      <link>https://jedipunkz.github.io/post/2014/12/13/aviator-openstack/</link>
      <pubDate>Sat, 13 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2014/12/13/aviator-openstack/</guid>
      <description>こんにちは。@jedipunkz です。
自分は Ruby を普段使うのでいつも Fog というライブラリを使って OpenStack, AWS を操作していました。Fog を使うとクラウドの操作が Ruby のネイティブコードで行え るのでシステムコマンド打つよりミス無く済みます。
Fog より後発で Aviator というライブラリが登場してきたので少し使ってみたのです がまだ未完成なところがあるものの便利な点もあって今後に期待だったので紹介します。
認証情報を yaml ファイルに記す 接続に必要な認証情報を yaml ファイルで記述します。名前を &amp;lsquo;aviator.yml&amp;rsquo; として 保存。この時に下記のように環境毎に認証情報を別けて書くことができます。こうする ことでコードの中で開発用・サービス用等と使い分けられます。
production: provider: openstack auth_service: name: identity host_uri: &amp;lt;Auth URL&amp;gt; request: create_token validator: list_tenants auth_credentials: username: &amp;lt;User Name&amp;gt; password: &amp;lt;Password&amp;gt; tenant_name: &amp;lt;Tenant Name&amp;gt; development: provider: openstack auth_service: name: identity host_uri: &amp;lt;Auth URL&amp;gt; request: create_token validator: list_tenants auth_credentials: username: &amp;lt;User Name&amp;gt; password: &amp;lt;Password&amp;gt; tenant_name: &amp;lt;Tenant Name&amp;gt; シンタックス確認 +++</description>
    </item>
    
    <item>
      <title>Chef-Zero でお手軽に OpenStack Icehouse を作る</title>
      <link>https://jedipunkz.github.io/post/2014/11/15/chef-zero-openstack-icehouse/</link>
      <pubDate>Sat, 15 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2014/11/15/chef-zero-openstack-icehouse/</guid>
      <description>こんにちは。@jedipunkz です。
OpenStack Juno がリリースされましたが、今日は Icehouse ネタです。
icehouse 以降、自分の中で OpenStack を自動で作る仕組みが無くなりつつあり、気軽 に OpenStack を作って色々試したい！ッていう時に手段が無く困っていました。例え ば仕事でちょっと OpenStack 弄りたい！って時に DevStack, RDO しかなく。DevStack は御存知の通り動かない可能性が結構あるし RDO は Ubuntu/Debian Gnu Linux ベース じゃないし。
ってことで、以前にも紹介した stackforge 管理の openstack-chef-repo と Chef-Zero を使って OpenStack Icehouse (Neutron) のオールインワン構成を作る方法 を書きます。ちなみに最近 Chef-Solo が Chef-Zero に置き換わりつつあるらしいです。 Chef-Zero はオンメモリで Chef サーバを起動する仕組みです。Chef-Solo と違って Chef サーバを扱う時と何も変更無く操作が出来るのでとても楽です。また、Chef サーバを 別途構、構築・管理しなくて良いので、気軽に OpenStack が作れます。
ちなみに stackforge/openstack-chef-repo の README.md に Chef-Zero での構築方法 が書いてありますが、沢山の問題があります。
nova-network 構成 API の Endpoint が全て localhost に向いてしまうため外部から操作不可能 各コンポーネントの bind_address が localhost を向いてしまう berkshelf がそのままでは入らない よって、今回はこれらの問題を解決しつつ &amp;ldquo;オールインワンな Neutron 構成の Icehouse OpenStack を作る方法&amp;rdquo; を書いていきます。</description>
    </item>
    
    <item>
      <title>MidoStack を動かしてみる</title>
      <link>https://jedipunkz.github.io/post/2014/11/04/midostack/</link>
      <pubDate>Tue, 04 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2014/11/04/midostack/</guid>
      <description>こんにちは。@jedipunkz です。
昨晩 Midokura さんが Midonet を OSS 化したとニュースになりました。公式サイトは 下記の URL になっています。Midonet は OpenStack Neutron のプラグインとして動作 するソフトウェアです。
http://www.midonet.org
下記のGithub 上でソースを公開しています。
https://github.com/midonet
本体の midonet と共に midostack というレポジトリがあってどうやら公式サイトの QuickStart を見ても devstack を交えての簡単な midonet の動作が確認できそう。
https://github.com/midonet/midostack
早速使ってみる 早速 midostack を使って midonet を体験してみましょう。QuickStart には Vagrant + VirtualBox を用いた使い方が改定ありますが手元の PC 端末だとリソース が足らなくて CirrOS VM 一個すら立ち上がりませんでした。よって普通にリソースの 沢山あるサーバで稼働してみます。Vagrantfile 見ても
config.vm.synced_folder &amp;#34;./&amp;#34;, &amp;#34;/midostack&amp;#34; としているだけなので、Vagrant ではなくても大丈夫でしょう。
Ubuntu Server 14.04 をインストールしたマシンを用意して midostack を取得します。
% git clone https://github.com/midonet/midostack.git midonet_stack.sh を実行します。
% cd midostack % .</description>
    </item>
    
    <item>
      <title>Chef-Container Beta を使ってみる</title>
      <link>https://jedipunkz.github.io/post/2014/07/16/chef-container/</link>
      <pubDate>Wed, 16 Jul 2014 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2014/07/16/chef-container/</guid>
      <description>こんにちは。@jedipunkz です。
昨晩 Chef が Chef-Container を発表しました。
http://www.getchef.com/blog/2014/07/15/release-chef-container-0-2-0-beta/ http://docs.opscode.com/containers.html まだ Beta リリースでバージョンは 0.2.0 です。(gem だと 0.1.1)
Docker を代表とするコンテナ周りの技術が最近、盛んにリリースされていますし、今 後クラウドプラットフォーム上でコンテナを使ってアプリを動かすケースも増えてくる のではないでしょうか。Dockerfile を使っても Chef-Solo を使ってソフトウェアをデ プロイ出来るのだけどそれだけだとしんどいので、コンテナに特化した Chef が出てき たってことだと思います。特徴として SSH でログインしてブートストラップするので はなくて Runit + Chef-init を用いてコンテナにデプロイすることが挙げられます。
では実際に使ってみたのでその時の手順をまとめてみます。
事前に用意する環境 下記のソフトウェアを予めインストールしておきましょう。
docker chef berkshelf ここで注意なのですが後に knife コマンドを使って Docker イメージをビルドします。 つまり root 権限が必要です。rbenv 等を使って ruby, chef をインストールすると、 辛いかもしれませんので OS のパッケージを使ってインストールすると良いと思います。 この辺りは今後改善策が出てくるかも&amp;hellip;。
尚、インストール方法はここでは割愛します。
Chef-Container のインストール 下記の2つの Gems をインストールします。
knife-container chef-container % sudo gem install knife-container % sudo gem install chef-container 使用方法 まず knife コマンドを使って操作に必要なディレクトリとファイルを生成します。</description>
    </item>
    
    <item>
      <title>JTF2014 で Ceph について話してきた！</title>
      <link>https://jedipunkz.github.io/post/2014/06/22/jtf2014-ceph/</link>
      <pubDate>Sun, 22 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2014/06/22/jtf2014-ceph/</guid>
      <description>こんにちは。@jedipunkz です。
今日、JTF2014 (July Tech Festa 2014) というイベントで Ceph のことを話してきま した。Ceph ユーザ会の会員として話してきたのですが Ceph ユーザ会は実は最近発足 したばかりのユーザ会で、まだまだ活動が活発ではありません。もし興味がある方いらっ しゃいましたら是非参加よろしくお願いしますー。下記の Google Groups になります。
https://groups.google.com/forum/#!forum/ceph-jp
ユーザ会としての勉強会として初になるのですが、今回このイベントで自分は Ceph-Deploy について話してきました。とりあえず皆さんに使ってもらいたかったので この話をしてきました。が、予定時間がメチャ短かったので超絶早口で頑張った分、皆 さんに理解してもらえなかった気がしてちょっと反省&amp;hellip;。なので、このブログを利用 して少し細くさせてもらいます。
今日の発表資料はこちらです！
今日のテーマは 「Ceph-Deploy を使って Ceph を構築してみる」だったのですが、下 記のテーマを持って資料を作っています。
単にミニマム構成ではなく運用を考慮した実用性のある構成 OSD, MON, MDS の各プロセスとノード・ディスクの数の関係を知ってもらう 特に「実用性のある..」は意識したつもりでした。そのために前提とした構成に下記の 特徴を持たせています。(資料 6 ページ目に構成図があります。確認してみてください。)
オブジェクト格納用ディスクは複数/ノードを前提 OSD レプリケーションのためのクラスタネットワークを用いる構成 OSD の扱うジャーナル格納用ディスクは高速な SSD を用いる MDS は利用する HW リソースの特徴が異なるので別ノードへ配置 ストレージ全体を拡張したければ
図中 ceph01-03 の様なノードを増設する ceph01-03 にディスクとそれに対する OSD を増設する ですが、前者がベストでしょう。ノード増設の場合 ceph-deploy を用いて
ceph-deploy mon create &amp;lt;新規ホスト名&amp;gt; で MON を稼働 ceph-dploy disk zap, osd create で OSD を稼働 で簡単に可能です。MDS の増設も負荷状況を見ながらするといいでしょう。自分はまだ Ceph を運用していないので、各プロセスがどのようなリソースの消費の仕方をするの か知りません。MDS がどのような数で運用していくべきなのか。早く運用から得たノウ ハウが共有されないかなぁと期待しています。</description>
    </item>
    
    <item>
      <title>Mesos &#43; Marathon &#43; Deimos &#43; Docker を試してみた!</title>
      <link>https://jedipunkz.github.io/post/2014/06/13/mesos-marathon-deimos-docker/</link>
      <pubDate>Fri, 13 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2014/06/13/mesos-marathon-deimos-docker/</guid>
      <description>こんにちは。@jedipunkz です。
以前 Mesos, Docker について記事にしました。
http://jedipunkz.github.io/blog/2013/09/28/mesos-architecture-number-1/ http://jedipunkz.github.io/blog/2013/10/01/methos-architecture-number-2-docker-on-mesos/
Twitter で Docker 関連のオーケストレーションツールについて呟いていたら @everpeace さんから こんな情報をもらいました。
@jedipunkz 元々meos-dockerっていうmesos executorがあったんですけど、mesosがcontainer部分をpluggableにしたので、それに合わせてdeimosっていうmesos用のexternal containerizer が作られました。
&amp;mdash; Shingo Omura (@everpeace) 2014, 6月 12 Deimos !!! 知らなかった。Mesos の Docker プラグインらしく下記の場所にありました。
https://github.com/mesosphere/deimos
色々調べいたら、こんな資料が見つかりました。どうやらまだ公開されて4日しか経っていないようです。
http://mesosphere.io/learn/run-docker-on-mesosphere/
Mesos + Marathon + Deimos + Docker をオールインワン構成で構築する手順が書かれています。
内容はほぼ同じですが、一応自分がやってみて理解したことをまとめたいので下記に記していきます。
構築してみる 手順をまとめてスクリプトにしました。パッケージは Ubuntu 13.10 用のようですが 14.04 のホスト で実行出来ました。14.04 のパッケージはまだ見つかっていません。
#!/bin/bash # disable ipv6 echo &amp;#39;net.ipv6.conf.all.disable_ipv6 = 1&amp;#39; | sudo tee -a /etc/sysctl.conf echo &amp;#39;net.ipv6.conf.default.disable_ipv6 = 1&amp;#39; | sudo tee -a /etc/sysctl.</description>
    </item>
    
    <item>
      <title>クラウドライブラリ Fog で AWS を操作！..のサンプル</title>
      <link>https://jedipunkz.github.io/post/2014/05/29/fog-aws-ec2-elb/</link>
      <pubDate>Thu, 29 May 2014 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2014/05/29/fog-aws-ec2-elb/</guid>
      <description>こんにちは。@jedipunkz です。
最近 OpenStack でサービスを開発！.. じゃなくて AWS でプロトタイプサービス作っ ているのですが、Ruby で開発したかったので Fog を使っています。EC2 と ELB の API を叩くコードになりつつあるのですが、サンプルコードって世の中に中々無いと気 がついたので、このブログ記事にサンプルコードを載せたいと思います。
Fog とは ? Fog http://fog.io/ はクラウドライブラリソフトウェアです。AWS, Rackspace, CloudStack, OpenStack .. と数ある世の中のクラウドプラットフォームを扱うために 用意されたソフトウェアです。対応しているプラットフォームの種別は下記を見ると参 考になります。
http://fog.io/about/provider_documentation.html
ドキュメントがまだまだ揃っていなく、Fog のコードを覗きながら実装するしかない状 況です。なので「こう使えば良い！」というお手本があまりネット上にも無い気がしま す。
ドキュメントは一応下記にあります。 が使い方がよくわからない・・！(´；ω；｀)ﾌﾞﾜｯ
http://rubydoc.info/gems/fog/frames/index
EC2 インスタンスを使ってみる まずは AWS EC2 の API を叩いて t1.micro インスタンスを立ち上げてみましょう。
require &amp;#39;fog&amp;#39; compute = Fog::Compute.new({ :provider =&amp;gt; &amp;#39;AWS&amp;#39;, :aws_access_key_id =&amp;gt; &amp;#39;....&amp;#39;, :aws_secret_access_key =&amp;gt; &amp;#39;....&amp;#39;, :region =&amp;gt; &amp;#39;ap-northeast-1&amp;#39; }) server = compute.servers.create( :image_id =&amp;gt; &amp;#39;ami-cedaa2bc&amp;#39;, :flavor_id =&amp;gt; &amp;#39;t1.</description>
    </item>
    
    <item>
      <title>stackforge/openstack-chef-repo で OpenStack Icehouse デプロイ</title>
      <link>https://jedipunkz.github.io/post/2014/04/25/stackforge-openstack-chef-repo-icehouse-deploy/</link>
      <pubDate>Fri, 25 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2014/04/25/stackforge-openstack-chef-repo-icehouse-deploy/</guid>
      <description>こんにちは。@jedipunkz です。
またまた OpenStack のデプロイをどうするか？についてです。
今まで自分の中では Rackspace Private Cloud で使われている Rackspace 管理の rcbops/chef-cookbooks が今現在使うならベストの選択だと思っていました。これは内 部で Chef が使われていてしかも Cookbooks が Github 上で公開されています。 Apache ライセンスで使えるので、サービス構築にも使えちゃうというモノ。
先日、ある OpenStack コアデベロッパーの方から「jedipunkz さん、やっぱり rcbops がいいですか？運営とかどうなっているんでしょう？マージの規準とかどうなのかな？」 と質問受けました。確かにマージの基準は Rackspace Private Cloud がベースになり ますし、管理しているエンジニアの一覧を見ていると Rackspace 社のエンジニアがメ インですし、今後どうなるのか分からない&amp;hellip;。
逃げ道は用意したほうが良さそう。
ということで、以前自分も暑かったことのある StackForge の openstack-chef-repo を久々に使ってみました。Icehouse 構成がこの時点で既に組めるようになっていて、 以前よりだい〜ぶ完成度増した感があります。今回は nova-network 構成を作ってみた のですが、Neutron 構成ももちろん出来そうなので後に調べてまた公開したいです。
StackForge とは StackForge は OpenStack のデプロイ・CI の仕組みとして公式に用いられているもの。 公式サイトは下記の場所にある。
http://ci.openstack.org/stackforge.html
StackForge の openstack-chef-repo は下記の場所にある。
https://github.com/stackforge/openstack-chef-repo
openstack-chef-repo はまだ &amp;lsquo;stable/icehouse&amp;rsquo; ブランチが生成されていない。が直 ちに master からブランチが切られる様子。</description>
    </item>
    
    <item>
      <title>Mirantis OpenStack (Neutron GRE)を組んでみた！</title>
      <link>https://jedipunkz.github.io/post/2014/04/23/mirantis-openstack/</link>
      <pubDate>Wed, 23 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2014/04/23/mirantis-openstack/</guid>
      <description>こんにちは。@jedipunkz です。
皆さん、Mirantis OpenStack はご存知ですか？ OpenStack ディストリビューションの 1つです。以下、公式サイトです。
http://software.mirantis.com/main/
この Mirantis OpenStack を使って OpenStack Havana (Neutron GRE) 構成を作ってみ ました。その時のメモを書いていきたいと思います。
構成は? 構成は下記の通り。
※ CreativeCommon
特徴は
Administrative Network : Fuel Node, DHCP + PXE ブート用 Management Network : 各コンポーネント間 API 用 Public/Floating IP Network : パブリック API, VM Floating IP 用 Storage Network : Cinder 配下ストレージ &amp;lt;-&amp;gt; インスタンス間用 要インターネット接続 : Public/Floating Networks Neutron(GRE) 構成 です。タグ VLAN 使って物理ネットワークの本数を減らすことも出来るはずですが、僕 の環境では何故かダメだったので上記の4つの物理ネットワークを別々に用意しました。
Fuel ノードの構築 Fuel ノードとは、OpenStack の各ノードをデプロイするための管理ノードのことです。 DHCP + PXE を管理する Cobbler やデプロイツールの Puppet が内部で稼働し、 Administrative Network 上で稼働したノードを管理・その後デプロイします。</description>
    </item>
    
    <item>
      <title>Geard のポートマッピングについて調べてみた</title>
      <link>https://jedipunkz.github.io/post/2014/04/22/geard-port-mapping/</link>
      <pubDate>Tue, 22 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2014/04/22/geard-port-mapping/</guid>
      <description>こんにちは。@jedipunkz です。
今週 Redhat が &amp;lsquo;Redhat Enterprise Linux Atomic Host&amp;rsquo; しましたよね。Docker を特 徴としたミニマムな OS だとのこと。その内部で用いられている技術 Geard について 少し調べてみました。複数コンテナの関連付けが可能なようです。ここでは調べた結果 について簡単にまとめていきます。
参考資料 http://openshift.github.io/geard/deploy_with_geard.html
利用方法 ここではホスト OS に Fedora20 を用意します。
まず Geard をインストール
% sudo yum install --enablerepo=updates-testing geard 下記の json ファイルを作成します。ここにはコンテナをデプロイするための情報と関 連付けのための情報を記します。
$ ${EDITOR} rockmongo_mongodb.json { &amp;#34;containers&amp;#34;:[ { &amp;#34;name&amp;#34;:&amp;#34;rockmongo&amp;#34;, &amp;#34;count&amp;#34;:1, &amp;#34;image&amp;#34;:&amp;#34;derekwaynecarr/rockmongo&amp;#34;, &amp;#34;publicports&amp;#34;:[{&amp;#34;internal&amp;#34;:80,&amp;#34;external&amp;#34;:6060}], &amp;#34;links&amp;#34;:[{&amp;#34;to&amp;#34;:&amp;#34;mongodb&amp;#34;}] }, { &amp;#34;name&amp;#34;:&amp;#34;mongodb&amp;#34;, &amp;#34;count&amp;#34;:1, &amp;#34;image&amp;#34;:&amp;#34;ccoleman/ubuntu-mongodb&amp;#34;, &amp;#34;publicports&amp;#34;:[{&amp;#34;internal&amp;#34;:27017}] } ] } 上記のファイルの解説。
コンテナ &amp;lsquo;rockmongo&amp;rsquo; と &amp;lsquo;mongodb&amp;rsquo; を作成 それぞれ1個ずつコンテナを作成 &amp;lsquo;image&amp;rsquo; パラメータにて docker イメージの指定 &amp;lsquo;publicports&amp;rsquo; パラメータにてコンテナ内部とホスト側のポートマッピングを行う &amp;rsquo;links&amp;rsquo; パラメータで &amp;lsquo;rockmongo&amp;rsquo; を &amp;lsquo;mongodb&amp;rsquo; に関連付け では、デプロイ開始します。</description>
    </item>
    
    <item>
      <title>OpenStack Havana Cinder,Glance の分散ストレージ Ceph 連携</title>
      <link>https://jedipunkz.github.io/post/2014/04/04/openstack-havana-cinder-glance-ceph/</link>
      <pubDate>Fri, 04 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2014/04/04/openstack-havana-cinder-glance-ceph/</guid>
      <description>こんにちは！@jedipunkz です。
今回は Havana 版の OpenStack Glance, Cinder と分散ストレージの Ceph を連携させ る手順を書いていきます。元ネタはこちら。下記の Ceph の公式サイトに手順です。
https://ceph.com/docs/master/rbd/rbd-openstack/
この手順から下記の変更を行って、ここでは記していきます。
Nova + Ceph 連携させない cinder-backup は今のところ動作確認出来ていないので省く 諸々の手順がそのままでは実施出来ないので補足を入れていく。 cinder-backup は Cinder で作成した仮想ディスクのバックアップを Ceph ストレージ 上に取ることが出来るのですが、そもそも Ceph 上にある仮想ディスクを Ceph にバッ クアップ取っても意味が薄いですし、まだ動いていないので今回は省きます。LVM やそ の他ストレージを使った Cinder 連携をされている方にとっては cinder-backup の Ceph 連携は意味が大きくなってくると思います。
構成 下記の通りの物理ネットワーク6つの構成です。 OpenStack, Ceph 共に最小構成を前提にします。
+--------------------------------------------------------------------- external | +--------------+--(-----------+--------------+------------------------------------------ public | | | | | +------------+ +------------+ +------------+ +------------+ +------------+ +------------+ | controller | | network | | compute | | ceph01 | | ceph02 | | ceph03 | +------------+ +------------+ +------------+ +------------+ +------------+ +------------+ | | | | | | | | | | | | | | | | +--------------+--(-----------+--(-----------+--(--(--------+--(--(--------+--(--(------- management | | | | | | | | | | | +--------------+--(-----------(--(-----------(--(-----------(--(------- guest | | | | | | | | +--------------------------------+-----------+--(-----------+--(-----------+--(------- storage | | | +--------------+--------------+------- cluster 特徴</description>
    </item>
    
    <item>
      <title>rcbops/chef-cookbooks で Keystone 2013.2.2(Havana) &#43; Swift 1.10.0 を構築</title>
      <link>https://jedipunkz.github.io/post/2014/03/16/rcbops-chef-cookbooks-keystone-havana-swift-1-10-0.deploy/</link>
      <pubDate>Sun, 16 Mar 2014 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2014/03/16/rcbops-chef-cookbooks-keystone-havana-swift-1-10-0.deploy/</guid>
      <description>こんにちは。@jedipunkz です。
追記 2014/03/20 : 一旦削除していた記事なのですが、無事動作が確認出来たので再度アッ プします！
第17回 OpenStack 勉強会で rcbops/chef-cookbooks の話をしてきたのですが会場から 質問がありました。「Havana の Swift 構成を作る Cookbooks はどこにありますか？」 と。私が試したのが Grizzly 時代のモノで、よく rcbops/chef-cookbooks を見てみる と Havana ブランチ又は Havana に対応したリリースタグのファイル構成に Swift が 綺麗サッパリ消えているではありませんか・・！下記の Swift の Cookbooks は幸い github 上に残っていました。
https://github.com/rcbops-cookbooks/swift
が rcbops/chef-cookbooks との関連付けが切れています・・。ぐあぁ。
ということで Havana 構成の Keystone 2013.2.2 と Swift 1.10.0 の構成を Chef で 作らねば！と思い色々試していたら結構あっさりと出来ました。今回はその方法を書い ていきたいと思います！
構成 構成は&amp;hellip;以前の記事 http://jedipunkz.github.io/blog/2013/10/27/swift-chef/ と同じです。
+-----------------+ | load balancer | +-----------------+ | +-------------------+-------------------+-------------------+-------------------+---------------------- proxy network | | | | | +-----------------+ +-----------------+ +-----------------+ +-----------------+ +-----------------+ | chef server | | chef workstation| | swift-mange | | swift-proxy01 | | swift-proxy02 | +-----------------+ +-----------------+ +-----------------+ +-----------------+ +-----------------+ .</description>
    </item>
    
    <item>
      <title>Sensu,Chef,OpenStack,Fog を使ったオレオレオートスケーラを作ってみた！</title>
      <link>https://jedipunkz.github.io/post/2014/03/05/sensu-chef-openstack-fog-autoscaler/</link>
      <pubDate>Wed, 05 Mar 2014 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2014/03/05/sensu-chef-openstack-fog-autoscaler/</guid>
      <description>こんにちは。@jedipunkz です。
今まで監視システムの Sensu やクラウドプラットフォームの OpenStack、コンフィギュ レーションマネージメントツールの Chef やクラウドライブラリの Fog 等使ってきま したが、これらを組み合わせるとオレオレオートスケーラ作れるんじゃないか？と思い、 ちょろっと作ってみました。
ちなみに自分はインフラエンジニアでしかも運用の出身なので Ruby に関しては初心者 レベルです。Chef で扱っているのと Rails アプリを作った経験はありますが、その程 度。Fog というクラウドライブラリにコントリビュートしたことはアリますが..。ちな みに Fog のコントリビュート内容は OpenStack Neutron(当時 Quantum) の仮想ルータ の操作を行う実装です。
そんな自分ですが&amp;hellip;設計1周間・実装1周間でマネージャと CLI が出来ました。 また暫く放置していたマネージャと CLI に WebUI くっつけようかなぁ？と思って sinatra の学習を始めたのですが、学習を初めて 1.5 日で WebUI が動くところまで行 きました。何故か？Ruby には有用な技術が揃っているから・・！(´；ω；｀)ﾌﾞﾜｯ
オレオレオートスケーラ &amp;lsquo;sclman&amp;rsquo; の置き場所 https://github.com/jedipunkz/sclman
スクリーンショット +++
構成は？ +-------------- public network +-------------+ | |sclman-api.rb| +----+----+---+ | sclman.rb | | vm | vm |.. | |sclman-cli.rb| +-------------+ +-------------+ +-------------+ +-------------+ | openstack | | chef server | | sensu server| | workstation | +-------------+ +-------------+ +-------------+ +-------------+ | | | | +---------------+---------------+---------------+--------------- management network &amp;lsquo;sclman&amp;rsquo; っていう名前です。上図の workstation ノードで稼働します。処理の流れは</description>
    </item>
    
    <item>
      <title>Journal 用 SSD を用いた Ceph 構成の構築</title>
      <link>https://jedipunkz.github.io/post/2014/02/27/journal-ssd-ceph-deploy/</link>
      <pubDate>Thu, 27 Feb 2014 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2014/02/27/journal-ssd-ceph-deploy/</guid>
      <description>こんにちは、@jedipunkz です。
前回、&amp;lsquo;Ceph のプロセス配置ベストプラクティス&amp;rsquo; というタイトルで記事を書きました。
http://jedipunkz.github.io/blog/2014/01/29/ceph-process-best-practice/
今回はこの記事にあるポリシに従って下記のような特徴を持った構成を作る手順を具体 的に書いていきたいと思います。
ceph01 - ceph04 の4台構成 ノードに HDD 2台搭載されていることを前提 (/dev/sdb, /dev/sdc) ノードに Journal 用 SSD 1台搭載されていることを前提 (/dev/ssd) ceph04 は mds サービス稼働 ceph05 は ceph-deploy を実行するためのワークステーション 最終的に ceph04 から Ceph をマウントする mon は ノード単位で稼働 osd は HDD 単位で稼働 mds は ceph04 に稼働 構成 : ハードウェアとノードとネットワークの関係 public network +-------------------+-------------------+-------------------+-------------------+--------- | | | | | +--------+--------+ +--------+--------+ +--------+--------+ +--------+--------+ +--------+--------+ | ceph01 | | ceph02 | | ceph03 | | ceph04 | | ceph05 | | +-----+ +-----+ | | +-----+ +-----+ | | +-----+ +-----+ | | | | | | | sdb | | sdc | | | | sdb | | sdc | | | | sdb | | sdc | | | | | | | +-----+-+-----+ | | +-----+-+-----+ | | +-----+-+-----+ | | | | | | | ssd | | | | ssd | | | | ssd | | | | | | | +-------------+ | | +-------------+ | | +-------------+ | | | | | +--------+--------+ +--------+--------+ +--------+--------+ +-----------------+ +-----------------+ | | | cluster network +-------------------+-------------------+------------------------------------------------- 構成 : プロセスとノードとネットワークの関係 public network +-------------------+-------------------+-------------------+-------------------+--------- | | | | | +--------+--------+ +--------+--------+ +--------+--------+ +--------+--------+ +--------+--------+ | ceph01 | | ceph02 | | ceph03 | | ceph04 | | ceph05 | | +-----+ +-----+ | | +-----+ +-----+ | | +-----+ +-----+ | | +-------------+ | | | | | osd | | osd | | | | osd | | osd | | | | osd | | osd | | | | mds | | | | | +-----+-+-----+ | | +-----+-+-----+ | | +-----+-+-----+ | | +-------------+ | | | | | mon | | | | mon | | | | mon | | | | | | | +-------------+ | | +-------------+ | | +-------------+ | | | | | +--------+--------+ +--------+--------+ +--------+--------+ +-----------------+ +-----------------+ | | | cluster network +-------------------+-------------------+------------------------------------------------- 注意 : 上記の図だと ssd : mon が対に見えますが、そうではありません。</description>
    </item>
    
    <item>
      <title>Ceph のプロセス配置ベストプラクティス</title>
      <link>https://jedipunkz.github.io/post/2014/01/29/ceph-process-best-practice/</link>
      <pubDate>Wed, 29 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2014/01/29/ceph-process-best-practice/</guid>
      <description>Ceph はブロック型の分散ストレージファイルシステムです。POSIX のファイルシステ ムとしてマウント出来ます。Linux の Kernel ドライバや FUSE ドライバを用いてマウ ントします。またブロックデバイスとしてマウントする方法もあります。
だいぶ前ですが、Ceph に関する記事を以前下記の通り書きました。
http://jedipunkz.github.io/blog/2013/05/25/ceph-cluster-network/ http://jedipunkz.github.io/blog/2013/05/11/ceph-deploy/ Ceph の構築方法について記したブログだったのですが、今まで mon, osd, mds の各プ ロセスをそれぞれ何台のノードに対して配置し、またそれぞれのプロセス幾つを何に対 して配置するのか？という疑問が付きまとわっていました。node, disk, process のそ れぞれの数の関係について知りたいなぁと思っていました。幾つかのドキュメントを読 んでいて、ぼんやり見えてきた事があるのでそれを今回はまとめたいと思います。
また、皆さん気になるトコロだと思われる容量設計についても軽く触れたいと思います。
参考資料 http://ceph.com/docs/master/rados/configuration/mon-config-ref/ http://www.sebastien-han.fr/blog/2013/12/02/ceph-performance-interesting-things-going-on/ 各要素の数の関係 ハードウェア要素である node, disk(hdd), ssd そしてソフトウェア要素である mon, osd, mds の数の関係はどのようにするべきか？基本となる関係は
1 mds process / node 1 mon process / node 1 osd process / disk n jornal ssd device / disk / node だと考えられます。僕が今のところ理想かなぁと思っている構成をまとめたいと思いま す。
下記の図がそれです。
+------------------------+ | client | +------------------------+ | +--------------------------+--------------------------+-------------------------------+------------------------- | | | | public network +------------------------+ +------------------------+ +------------------------+ +------------------------+ | mon | | mon | | mon | | mds | +------+ +------+ +------+ +------+ +------+ +------+ +------+ +------+ +------+ +------------------------+ | osd | | osd | | osd | | osd | | osd | | osd | | osd | | osd | | osd | | | +------+ +------+ +------+ +------+ +------+ +------+ +------+ +------+ +------+ | | | disk | | disk | | disk | | disk | | disk | | disk | | disk | | disk | | disk |.</description>
    </item>
    
    <item>
      <title>第17回 OpenStack 勉強会で話してきました</title>
      <link>https://jedipunkz.github.io/post/2014/01/21/17th-openstack-study/</link>
      <pubDate>Tue, 21 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2014/01/21/17th-openstack-study/</guid>
      <description>こんにちは。@jedipunkz です。
昨晩、第17回 OpenStack 勉強会が開催されました
http://connpass.com/event/4545/
ここで発表をしてきましたぁ！発表タイトルは &amp;ldquo;rcbops/chef-cookbooks&amp;rdquo; です。
何を発表したかと言うと詳しくは上記のスライドを見ていただくとして、簡単に言うと &amp;ldquo;RackSpace 社のエンジニアが管理している Chef Cookbooks でOpenStack 構成を作ろ う&amp;rdquo; ってことです。
今日知ったのですがどうも昨晩は初心者向けの勉強会という位置付けだったらしく..少 しだけディープな話題を話してしまったかもしれません！すいません！＞＜
でもとても楽しく発表出来ましたし、逆に質問のコーナーで最新の情報も教えてもらえ たり！なんと Havana 対応の v4.2.0 以降では Swift の Cookbooks が消えてしまった とか！&amp;hellip; 皆 Swift 好きくないの？&amp;hellip;; ;
rcbops/chef-cookbooks はずっと追っていますが、ものすごいスピードで開発進んでい るので、今後ぜひみなさん使ってみて下さいー。
最後に詳しい利用方法を記した僕のブログの URL を貼り付けておきます。
OpenStack Havana を Chef でデプロイ http://jedipunkz.github.io/blog/2013/11/17/openstack-havana-chef-deploy/
Swift HA 構成を Chef でデプロイ http://jedipunkz.github.io/blog/2013/07/26/swift-ha-chef-deploy/
実用的な Swift 構成を Chef でデプロイ http://jedipunkz.github.io/blog/2013/10/27/swift-chef/</description>
    </item>
    
    <item>
      <title>Chef で自律的クラスタを考える</title>
      <link>https://jedipunkz.github.io/post/2013/12/09/chef-autonoumous-cluster/</link>
      <pubDate>Mon, 09 Dec 2013 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2013/12/09/chef-autonoumous-cluster/</guid>
      <description>こんにちは。@jedipunkz です。
Serf の登場があったり、ここ最近オーケストレーションについて考える人が増えた気 がします。システムをデプロイしてその後各ノード間の連結だったりも同じ Chef, Puppet 等のコンフィギュレーションツールで行うのか？全く別のツールで？..
最近 Serf というツールの登場がありました。
僕も Serf を触ってつい先日ブログに書きました。有用なツールだと思います。シ ンプルだからこそ応用が効きますし、リアルタイム性もあり、将来的に異なるネットワー クセグメント上のノードとも連結出来るようになりそうですし、とても期待です。
話が少し飛びますが..
いつも Rebuild.fm を楽しく聞いているのですが Immutable Infrastructure の話題の 時にオーケストレーションの話題になって、どうも &amp;lsquo;Chef でも自律的なクラスタを組 むことが認知されていないのでは？&amp;rsquo; と思うようになりました。もちろん Chef でやる べき！とは言い切りませんし、今後どうなるかわかりません。Opscode の中の人も &amp;lsquo;オー ケストレーションは自分でやってね&amp;rsquo; というスタンスだったとずいぶん前ですが聞きま した。Serf を等のオーケストレーションツールを使う使わないの話は今回はしないの ですが Chef でも自律的クラスタを組むことは出来ますよ〜というのが今回の話題。
まえがきが長くなりました。
今回は Chef で自律的クラスタを構成する方法を記したいと思います。
haproxy 等を利用すれば尚良いと思いますが、よりクラスタを組むのが簡単な nginx を今回は利用したいと思います。
https://github.com/opscode-cookbooks/nginx
構成 &amp;lsquo;web&amp;rsquo; という Role 名と &amp;rsquo;lb&amp;rsquo; という Role 名で単純な HTTP サーバとしての nginx ノードを複数台と、ロードバランサとしての nginx ノードを1台でクラスタを構成しま す。また共に environment 名は同じものを利用します。別の environment 名の場合は 別クラスタという区切りです。</description>
    </item>
    
    <item>
      <title>CoreOS etcd のクラスタとその応用性</title>
      <link>https://jedipunkz.github.io/post/2013/12/09/coreos-etcd-cluster/</link>
      <pubDate>Mon, 09 Dec 2013 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2013/12/09/coreos-etcd-cluster/</guid>
      <description>こんにちは。@jedipunkz です。
皆さん CoreOS は利用されたことありますか？CoreOS は軽量な docker との相性の良 い OS です。下記が公式サイト。
http://coreos.com/
特徴としては下記の3つがあります。
etcd systemd docker ここではこの中の etcd について注目していきたいと思います。etcd はクラスタエイ ブルな KVS データベースです。コンフィギュレーションをクラスタ間で共有すること がなので、オーケストレーションの分野でも期待出来るのでは？と個人的に感じていま す。今回は etcd のクラスタ構成構築の手順とその基本動作の確認、またどう応用出来 るのか？について記していきたいと思います。
参考 URL http://coreos.com/using-coreos/etcd/ https://github.com/coreos/etcd ビルド go 1.1 or later をインストールして etcd のコンパイル準備を行います。Ubuntu Saucy のパッケージを用いると容易に行えます。
% apt-get -y install golang coreos/etcd を取得しビルド
% git clone https://github.com/coreos/etcd % cd coreos % ./build % ./etcd --version v0.2.0-rc1-60-g73f04d5 CoreOS の用意 ここではたまたま手元にあった OpenStack を用いて CoreOS のイメージを登録してい みます。ベアメタルでも可能ですのでその場合は手順を読み替えて作業してみてくださ い。OpenStack 等クラウドプラットフォームを利用する場合は metadata サービスが必 須となるので注意してください。</description>
    </item>
    
    <item>
      <title>Ironic でベアメタル OpenStack ！..の一歩手前</title>
      <link>https://jedipunkz.github.io/post/2013/12/05/ironic-openstack-beremetal/</link>
      <pubDate>Thu, 05 Dec 2013 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2013/12/05/ironic-openstack-beremetal/</guid>
      <description>こんにちは。@jedipunkz です。
アドベントカレンダーの季節がやって参りました。
Ironic を使って OpenStack でベアメタルサーバを扱いたい！ということで色々とやっ ている最中 (今週から始めました..) なのですが、まだまだ incubator プロジェクト ということもあって実装が追い付いていなかったりドキュメントも揃っていなかったり とシンドい状況ｗ ここ2日程で集めた情報を整理するためにも 2013年 OpenStack アド ベントカレンダーに参加させてもらいますー。
参考資料のまとめ まずは公式 wiki ページ。逆に言うとここに記されている以上の情報は無いんじゃ？あ とはコード読め！の世界かも..。
https://wiki.openstack.org/wiki/Ironic
devtest_undercloud です。上の資料の中でも手順の中で度々こちらにジャンプしている。 同じく incubator プロジェクトの TrippleO のデベロッパ用ドキュメントになっている。 上記の公式 wiki の情報を合わせ読むことで Ironic を使ったデプロイの手順に仕上がります。
http://docs.openstack.org/developer/tripleo-incubator/devtest_undercloud.html
ソースコードとドキュメント。あとでドキュメント作成方法を記しますが、こちらを取 得して作成します。
https://github.com/openstack/ironic
ドキュメントサイト。まだ情報が揃っていません。よって上の github から取得したモ ノからドキュメントを作る方法を後で書きます。
http://docs.openstack.org/developer/ironic/
launchpad サイト。全てのバグ情報やブループリント等が閲覧出来ます。まだ絶賛開発 中なので読む必要があると思います。
https://launchpad.net/ironic
ドキュメントを作る +++
公式 ドキュメントサイトは一応、上記の通りあるのですが、ドキュメントも絶賛執筆 中ということで所々抜けがあります。また公式ドキュメントサイトがどのスパンで更新 されているか分からないので、いち早く情報をゲットしたい場合ドキュメントを作る必 要があると思います。ということで、その作り方を記していきます。尚、公式 wiki サ イトにも手順が載っていますが Vagrant と Apache を用いた方法になっているので、 普通に Ubuntu サーバが手元にある環境を想定して読み替えながら説明していきます。
必要なパッケージのインストールを行います。
% sudo apt-get update % sudo apt-get install -y git python-dev swig libssl-dev python-pip \ libmysqlclient-dev libxml2-dev libxslt-dev libxslt1-dev python-mysqldb \ libpq-dev % sudo pip install virtualenv setuptools-git flake8 tox % sudo easy_install nose ソースコード・ドキュメントを取得します。</description>
    </item>
    
    <item>
      <title>sensu-chef で監視システム Sensu を管理 #2</title>
      <link>https://jedipunkz.github.io/post/2013/11/27/sensu-chef-deploy-2/</link>
      <pubDate>Wed, 27 Nov 2013 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2013/11/27/sensu-chef-deploy-2/</guid>
      <description>こんにちは。@jedipunkz です。
以前、Sensu を Chef で管理する方法について書きました。
http://jedipunkz.github.io/blog/2013/06/20/sensu-chef-controll/
これは今年(2013)の6月頃の記事ですが、この時はまだ sensu-chef を include して使う別の Chef Cookbook が必要でした。また Redis 周りの Cookbooks が完成度あまく、またこれも 公式とは別の Cookbooks を改修して再利用する形でした。この作業は結構しんどかっ た記憶があるのですが、最近 GlideNote さんのブログを読んで( ﾟдﾟ)ﾊｯ!と思い、 sensu-chef を再確認したのですが、だいぶ更新されていました。
下記が sensu-chef です。
https://github.com/sensu/sensu-chef
この Chef Cookbook 単体で利用できる形に更新されていて、plugins, checks 等は Recipe に追記することで対応可能になっていました。早速利用してみたので簡単に使 い方を書いていきます。
下記が Sensu の管理画面です。最終的にこの画面に監視対象のアラートが上がってきます。
{% img /pix/sensu.png %}
使い方 sensu-chef を取得する。chef-repo になっています。
% git clone https://github.com/sensu/sensu-chef.git ~/chef-repo-sensu bundle にて Gemfile に記述の在る gem パッケージをインストールします。
% cd ~/chef-repo-sensu % bundle install .chef/ 配下の設定は割愛します。chef サーバの情報に合わせて設定します。</description>
    </item>
    
    <item>
      <title>Vyatta で Mac 用 TimeMachine サーバ兼ファイルサーバを構築！</title>
      <link>https://jedipunkz.github.io/post/2013/11/26/vyatta-timemachine-netatalk/</link>
      <pubDate>Tue, 26 Nov 2013 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2013/11/26/vyatta-timemachine-netatalk/</guid>
      <description>こんにちは。@jedipunkz です。
自宅ルータを Vyatta で運用しているのですが、諸電力な筐体に交換した際に HDD ス ロットが余っていたので HDD を一本さしてみました。もったいないので Netatalk を インストールして Mac 用の TimeMachine サーバにするか！そんでもってファイルサー バ兼務にしよう！と思い立って作業したら簡単に出来たので共有します。
Vyatta はご存知の通り Debian Gnu/Linux がベースになっているのでパッケージレポ ジトリを追加してちょちょいで設定出来ます。
手順 電源を通して Disk を追加します。その後起動。私は 3TB Disk が余っていたのでそれ を挿しました。
debian wheezy のパッケージレポジトリを Vyatta に追記します。
% configure # set system package repository debian url http://ftp.jp.debian.org/debian # set system package repository debian distribution wheezy # set system package repository debian components &amp;#34;main contrib&amp;#34; # commit # save # exit netatalk, avahi をインストールする。その際に libgcrypt11 のバージョン 1.</description>
    </item>
    
    <item>
      <title>OpenStack Havana を Chef でデプロイ</title>
      <link>https://jedipunkz.github.io/post/2013/11/17/openstack-havana-chef-deploy/</link>
      <pubDate>Sun, 17 Nov 2013 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2013/11/17/openstack-havana-chef-deploy/</guid>
      <description>こんにちは。@jedipunkz です。
毎度お馴染みになった OpenStack の Chef によるデプロイですが、今回は OpenStack Havana 構成を Chef でデプロイする方法についてです。使用するのは今回も rcbops/chef-cookbooks です。ブランチは &amp;lsquo;havana&amp;rsquo; を用います。
早速ですが構成について。4.1.2 辺りからだと思うのですが構成の前提が物理ネットワー ク4つを前提にし始めました。public, external (VM) を別ける必要が出てきました。 通信の特性も異なるので (public は public API を。external は VM 用) 、別けるの が得策かもしれません。
構成 +--------------+------------------------------------------------------- external | | +--------------+--(-----------+--(-----------+--------------+---------------------------- public | | | | | | | +------------+ +------------+ +------------+ +------------+ +------------+ +------------+ | controller | | network | | network | | compute | | compute | | workstation| +------------+ +------------+ +------------+ +------------+ +------------+ +------------+ | | | | | | | | | | +--------------+--(-----------+--(-----------+--(-----------+--(-----------+------------- management | | | | +--------------+--------------+--------------+------------------------- guest 上記の構成の特徴 4つの物理ネットワークを前提 public ネットワーク : 外部 API 用ネットワーク external ネットワーク : インスタンス外部接続用ネットワーク guest ネットワーク : インスタンス内部用ネットワーク management ネットワーク : 各コンポーネント接続用ネットワーク public, external のみグローバルネットワーク controller : 2 nics, network : 4 nics, compute : 3nics の構成 controller はシングル構成 network ノードは台数拡張可能, agent 単位でノード間移動可能 compute ノードも台数拡張可能 workstation は chef-repo の所在地, management ネットワークに所属 各ノードの準備 OS インストール後、各ノードのネットワークインターフェースの設定を下記の通り行っ てください。また LVM を使うのであれば cinder ボリュームの設定も必要になってきます。</description>
    </item>
    
    <item>
      <title>第2回 Elasticsearch 勉強会参加レポート</title>
      <link>https://jedipunkz.github.io/post/2013/11/13/elasticsearch-second-study-report/</link>
      <pubDate>Wed, 13 Nov 2013 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2013/11/13/elasticsearch-second-study-report/</guid>
      <description>こんにちは。@jedipunkz です。
第2回 Elasticsearch 勉強会に参加してきました。箇条書きですが参加レポートを記し ておきます。
開催日 : 2013/11/12 場所 : 東京駅 グラントウキョウサウスタワー リクルートテクノロジーズさま URL : http://elasticsearch.doorkeeper.jp/events/6532 Routing 周りの話 株式会社シーマーク　大谷純さん (@johtani) Index 構成 cluster の中に index -&amp;gt; type が作成される index は shard という部分的な index の集まり shard 数は生成時のみ指定可能 node ごとに replica, primary を別ける replica 数は後に変えられる doc -&amp;gt; hash 値を shard 数で割って replica, primary に登録 doc の id の ハッシュ値を利用 type も含める場合はかの設定を true に クライアントはどのノードに対してクエリを投げても OK routing id の代わりに routing (URL パラメータ) で登録 url リクエストパラメータとして登録時にルーティングパラメータを登録 id の代わりにパラメータで指定された値のハッシュ値を計算して利用 検索時 routing 指定で関係のある shard のみを指定出来る スケールアウト sharding によるスケールアウト数 = インデックス作成時に指定 shard によるインデックスの分割以外にインデックス自体を複数持つことによるスケール 複数のドキュメントをエイリアス書けることが可能 所感 個人的には非常に興味のあるところでした。mongodb のような sharding をイメージし てよいのか？そうでないのか？すら理解出来ていなかったので。sharding を理解する 前提知識の話もあって非常に参考になりました。</description>
    </item>
    
    <item>
      <title>Serf を使ってみた</title>
      <link>https://jedipunkz.github.io/post/2013/11/10/serf/</link>
      <pubDate>Sun, 10 Nov 2013 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2013/11/10/serf/</guid>
      <description>こんにちは。@jedipunkz です。
僕は Chef 使いなのですが、Chef はオーケストレーションまで踏み込んだツールでは ないように思います。せいぜいインテグレーションが出来る程度なのかなぁと。 しかもインテグレーションするにも Cookbooks の工夫が必要です。以前聞いたことの ある話ですが Opscode 社のエンジニア曰く「オーケストレーション等へのアプローチ はそれぞれ好きにやってね」だそうです。
個人的にオーケストレーションをテーマに色々調べようかと考えているのですが、 Serf という面白いツールが出てきました。&amp;lsquo;Serf&amp;rsquo; はオーケストレーションを手助けし てくれるシンプルなツールになっています。
もう既にいろんな方が Serf について調べていますが、どのような動きをするのかを自 分なりに理解した点を記しておこうと思います。
参考にしたサイト 公式サイト http://www.serfdom.io/ クラスメソッド開発者ブログ http://dev.classmethod.jp/cloud/aws/serf_on_ec2/ Glidenote さん http://blog.glidenote.com/blog/2013/10/30/serf-haproxy/ Serf とは Serf は gossip protocol をクラスタにブロードキャストする。gossip protocol は SWIM : Scalable Weakly-consistent Infecton-style process Group Membership Protocol” をベースとして形成されている。
SWIM Protocol 概略 serf は新しいクラスタとして稼働するか、既存のクラスタに ‘join’ する形で稼働 するかのどちらかで起動する。
新しいメンバは TCP で状態を &amp;lsquo;full state sync&amp;rsquo; され既存のクラスタ内にて ‘gossipin (噂)される。この ’gosiping’ は UDP で通信されこれはネットワーク使 用量はノード数に比例することになる。</description>
    </item>
    
    <item>
      <title>実用的な Swift 構成を Chef でデプロイ</title>
      <link>https://jedipunkz.github.io/post/2013/10/27/swift-chef/</link>
      <pubDate>Sun, 27 Oct 2013 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2013/10/27/swift-chef/</guid>
      <description>こんにちは。@jedipunkz です。
以前、&amp;ldquo;Swift HA 構成を Chef でデプロイ&amp;rdquo; というタイトルで記事を書きました。
http://jedipunkz.github.io/blog/2013/07/26/swift-ha-chef-deploy/
こちらですが、Swift-Proxy, MySQL, Keystone をそれぞれ haproxy, keepalived で HA 組みました。ですが、これは実用的なのかどうか自分でずっと考えていました。
MySQL と KeepAlived はできればシングル構成にしたいのと、Swift-Proxy は HA で組 みたい。MySQL は Master/Master レプリケーション構成になり、どちらかのノードが 障害を起こし万が一復旧が難しくなった時、構築し直しがしんどくなります。かと言っ て Swift-Proxy をシングル構成にすると今度はノード追加・削除の作業時にサービス 断が発生します。Swift-Proxy を再起動書ける必要があるからです。なので Swift-Proxy は引き続き HA 構成にしたい。
もう一点、見直したいと思っていました。
日経コンピュータから出版されている &amp;ldquo;仮想化大全 2014&amp;rdquo; の記事を読んでいて 気がついたのですが。Swift には下記の通りそれぞれのサーバがあります。
swift-proxy-server swift-account-server swift-container-server swift-object-server Swift には下記のような特徴がある事がわかりました。
swift-object swift-object は swift-accout, swift-container とは物理リソースの扱いに全く異な る特性を持っています。swift-account, swift-container はクライアントからのリクエ ストに対して &amp;ldquo;アカウントの存在を確認&amp;rdquo;, &amp;ldquo;ACL 情報の確認&amp;rdquo; 等を行うサーバであるの に対して swift-object はストレージ上のオブジェクトをクライアントに提供、または 逆に格納するサーバです。よって、Disk I/O の利用特性として swift-account, container は SSD 等、高スループットの Disk を利用するケースが推奨されるのに対 して swift-object はオブジェクトの実体を格納する必要があるため Disk 容量の大き なストレージを要する。</description>
    </item>
    
    <item>
      <title>test-kitchen と OpenStack で Chef Cookbooks テスト (後篇)</title>
      <link>https://jedipunkz.github.io/post/2013/10/20/test-kitchen-openstack-chef-cookbooks-test-2/</link>
      <pubDate>Sun, 20 Oct 2013 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2013/10/20/test-kitchen-openstack-chef-cookbooks-test-2/</guid>
      <description>こんにちは。@jedipunkz です。
前回、OpenStack と test-kitchen を使った環境構築方法を書きました。下記の記事で す。
http://jedipunkz.github.io/blog/2013/10/13/test-kitchn-openstack-chef-cookbooks-test/
今回は実際にテストを書く方法を記していたい思います。
今回使用するテストツールは下記の2つです。
rspec と serverspec busser-bats 参考資料 Creationline lab さんの資料を参考にさせて頂きました。
http://www.creationline.com/lab/2933
用意するモノ達 OpenStack にアクセスするためのユーザ・パスワード Keystone の AUTH_URL テストに用いる OS イメージの Image ID テナント ID nova 管理のキーペアの作成 これらは OpenStack を普段から利用されている方なら馴染みのモノかと思います。
.kitchen.yml ファイルの作成 下記の通り .kitchen.yml ファイルを test-kitchen のルートディレクトリで作成しま す。今後の操作は全てこのディレクトリで作業行います。
&amp;ldquo;&amp;lt;&amp;gt;&amp;rdquo; で括った箇所が環境に合わせた設定になります。
また、ここでは前回同様に &amp;rsquo;ntp&amp;rsquo; の Cookbook をテストする前提で記します。
+++ driver_plugin: openstack suites: - name: default run_list: - recipe[ntp::default] attributes: {} platforms: - name: ubuntu-12.04 driver_config: openstack_username: &amp;lt;openstack_username&amp;gt; openstack_api_key: &amp;lt;openstack_password&amp;gt; openstack_auth_url: http://&amp;lt;openstack_ip_addr&amp;gt;:5000/v2.</description>
    </item>
    
    <item>
      <title>test-kitchen と OpenStack で Chef Cookbooks テスト(前篇)</title>
      <link>https://jedipunkz.github.io/post/2013/10/13/test-kitchn-openstack-chef-cookbooks-test/</link>
      <pubDate>Sun, 13 Oct 2013 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2013/10/13/test-kitchn-openstack-chef-cookbooks-test/</guid>
      <description>こんにちは。@jedipunkz です。
test-kitchen + Vagrant を利用して複数環境で Chef Cookbooks のテストを行う方法は 結構皆さん利用されていると思うのですが Vagrant だと手元のマシンに仮想マシンが バシバシ立ち上げるので僕はあまり好きではないです。そこで、OpenStack のインスタ ンスをその代替で使えればいいなぁと結構前から思っていたのですが、今回うまくいっ たのでその方法を記します。
用意するモノ OpenStack 環境一式 Chef がインストールされた OS イメージとその ID test-kitchen を実行するワークステーション (お手持ちの Macbook 等) OS イメージの作成ですが Veewee などで自動構築できますし、インスタンス上で Chef のインストールを行った後にスナップショットを作成してそれを利用しても構いません。
test-kitchen のインストール test-kitchen をインストールします。versoin 1.0.0 はまだリリースされていないの で github から master ブランチを取得してビルドします。直近で OpenStack に関連 する不具合の修正等が入っているのでこの方法を取ります。
% git clone https://github.com/opscode/test-kitchen.git % cd test-kitchen % bundle install % rake build # &amp;lt;--- gem をビルド % gen install ./pkg/test-kitchen-1.0.0.dev.gem 現時点 (2013/10/13) で berkshelf の利用しているソフトウェアと衝突を起こす問題 があるので bundle で解決します。下記のように Gemfile に gem &amp;lsquo;kitchen-openstack&amp;rsquo; と記述します。</description>
    </item>
    
    <item>
      <title>GlusterFS の各クラスタタイプ構築</title>
      <link>https://jedipunkz.github.io/post/2013/10/12/glusterfs-install/</link>
      <pubDate>Sat, 12 Oct 2013 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2013/10/12/glusterfs-install/</guid>
      <description>こんにちは。@jedipunkz です。
GlusterFS をちょっと前に調べてました。何故かと言うと OpenStack Havana がもうす ぐリリースされるのですが、Havana から GlusterFS がサポートされる予定だからです。
この辺りに色々情報が載っています。
http://www.gluster.org/category/openstack/
その前に GlusterFS を構築出来ないといけないので、今回はその方法を書いていきま す。各クラスタタイプ毎に特徴や構築方法が異なるのでその辺りを重点的に。
環境 Ubuntu Server 12.04.3 LTS PPA レポジトリ利用 /dev/sdb を OS 領域とは別の disk としてサーバに追加する 用いる PPA レポジトリ Ubuntu 12.04.3 LTS の GlusterFS バージョンは 3.2 です。3.4 系が今回使いたかっ たので下記の PPA レポジトリを利用させてもらいます。ちゃんと構築するなら自分で パッケージを作成することをオススメします。
https://launchpad.net/~semiosis/+archive/ubuntu-glusterfs-3.4
準備 ここからの手順は全てのサーバで操作します。
レポジトリの利用方法 % sudo aptitude install python-software-properties % sudo add-apt-repository ppa:semiosis/ubuntu-glusterfs-3.4 % sudo apt-get update GlusterFS3.4 のインストール % sudo apt-get install glusterfs-server gluserfs-client xfsprogs のインストール glusterfs は xfs を扱うため xfsprogs をインストールする。</description>
    </item>
    
    <item>
      <title>Methos アーキテクチャ #2 (Docker on Mesos)</title>
      <link>https://jedipunkz.github.io/post/2013/10/01/methos-architecture-number-2-docker-on-mesos/</link>
      <pubDate>Tue, 01 Oct 2013 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2013/10/01/methos-architecture-number-2-docker-on-mesos/</guid>
      <description>こんにちは。@jedipunkz です。
Mesos アーキテクチャについて2つめの記事です。
http://jedipunkz.github.io/blog/2013/09/28/mesos-architecture-number-1/
上記の前回の記事で Mesos 自体のアーキテクチャについて触れましたが、今回は Mesos + Marathon + Docker の構成について理解したことを書いていこうと思います。
mesos クラスタは 幾つかの mesos masters と沢山の mesos slaves から成っており、 mesos slaves の上では docker を操作する executor が稼働している。marathon は mesos master の上で稼働する mesos framework である。init や upstart の様な存在 であることが言え、REST API を持ち container の動作を制御する。marathon には ruby の client 等も存在する。下記がそれ。
https://github.com/mesosphere/marathon_client
構成 +-----------------+ | docker registry | index.docker.io (もしくは local registry) +-----------------+ | +----------------+ | | +--------------+ +--------------+ | mesos master | | mesos master | +--------------+ +--------------+ | | |----------------+-----------------------------------| +--------------+ +--------------+ +--------------+ | mesos slave | | mesos slave | .</description>
    </item>
    
    <item>
      <title>DevOps Day Tokyo 2013 参加レポート</title>
      <link>https://jedipunkz.github.io/post/2013/09/29/devops-day-tokyo-2013-report/</link>
      <pubDate>Sun, 29 Sep 2013 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2013/09/29/devops-day-tokyo-2013-report/</guid>
      <description>こんにちは。@jedipunkz です。
DevOps Day Tokyo 2013 に参加してきました。たくさんの刺激を受けたのでレポート書 いておきます。
開催日 : 2013年09月28日 場所 : 東京六本木ミッドタウン Yahoo! Japan さま URL : http://www.devopsdays.org/events/2013-tokyo/ Making Operation Visible Nick Galbreath (@ngalbreath) さん DevOps の拠点 Etsy に努めた経緯のある DevOps リーダ Galbreath さん。DevOps の 概略から何が必要でありどう行動に起こせばよいか説明してくださいました。
Making operations visible - devopsdays tokyo 2013 from Nick Galbreath こちら、Galbreath さんの当日の資料。
DevOps が実行出来ない理由 Tool が足りない 社風の影響 見えないモノが価値がないと事業から考えられている 出来る事は、価値があるモノの社内への説明と、Tool を使った可視化。データの可視 化が重要。Ops の人は結構「データをどこそこの部署に見せても理解してもらえない」 だとか「データを閲覧させると万が一の時にシステムが破損する」等と考えがち。が、 ビジネス寄りの人にとって重要なグラフが含まれていたり、アカウント担当の人に役立 つものも含まれている。ましてシステムが破損することなど決して無い。
重要なのは &amp;ldquo;運用のメトリクスを公開する&amp;rdquo; こと！
Graphite グラフ描画ツール まず完成度が高いわけではない 同類のソフトウェアでは行えないクエリが発行出来る REST API Flexible Input &amp;amp; Output Simple UI &amp;amp; Dashboard 3rd Party Custom Client Side Dashboard あり URL 型なので Dashboard 開発が楽ちん 稼働させるための物理インフラリソースは結構必要 apt-get install graphite できるよ statd UDP 使ってる Event Data を Application から statd へ 下記は例。ログイン情報を送るためのコードはこれだけ。</description>
    </item>
    
    <item>
      <title>Mesos アーキテクチャ #1</title>
      <link>https://jedipunkz.github.io/post/2013/09/28/mesos-architecture-number-1/</link>
      <pubDate>Sat, 28 Sep 2013 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2013/09/28/mesos-architecture-number-1/</guid>
      <description>こんにちは。@jedipunkz です。
今回はクラスタマネージャである Mesos について書こうと思います。
Mesos は Apache Software Foundation によって管理されるソフトウェアで分散アプリ ケーションをクラスタ化することが出来るマネージャです。Twitter が採用しているこ とで有名だそうで、開発にも積極的に参加しているそうです。
http://mesos.apache.org/
@riywo さんが既に Mesos + Marathon + Zookeper + Docker な構成を組む手順をブロ グで紹介されていますので是非試してみると面白いと思います。
http://tech.riywo.com/blog/2013/09/27/mesos-introduction-1/
私は理解した Mesos のアーキテクチャについて少し書いていきたいと思います。
全体の構造 +-----------+ | zookeeper | | quorum | +-----------+ | +----------------+----------------+ | | | +--------------+ +--------------+ +--------------+ | mesos-master | | mesos-master | | mesos-master | | active | | hot standby | | hot standby | +--------------+ +--------------+ +--------------+ .</description>
    </item>
    
    <item>
      <title>Chef で kibana &#43; elasticsearch &#43; fluentd デプロイしてみた</title>
      <link>https://jedipunkz.github.io/post/2013/09/13/chef-kibana-elasticsearch-fluentd/</link>
      <pubDate>Fri, 13 Sep 2013 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2013/09/13/chef-kibana-elasticsearch-fluentd/</guid>
      <description>こんにちは。@jedipunkz です。
前回 kibana + elasticsearch + fluentd を構築する方法を載せたのだけど手動で構築 したので格好悪いなぁと思っていました。いうことで！ Chef でデプロイする方法を調 べてみました。
意外と簡単に出来たし、スッキリした形でデプロイ出来たのでオススメです。
前提の環境は&amp;hellip; Ubuntu 12.04 LTS precise Chef サーバ構成 入力するログは nginx (例) オールインワン構成 Cookbook が他の OS に対応しているか確認していないので Ubuntu を前提にしていま す。Chef サーバのデプロイや knife の設定は済んでいるものとして説明していきます。 例で nginx のログを入力します。なので nginx も Chef でデプロイします。ここは他 のものに置き換えてもらっても大丈夫です。手順を省略化するためオールインワン構成 で説明します。nginx, fluentd は複数のノードに配置することも手順を読み替えれば もちろん可能です。
Cookbook の準備 お決まり。Cookbooks の取得に Berkshelf を用いる。
% cd chef-repo % gem install berkshelf % ${EDITOR} Berksfile cookbook &amp;#39;elasticsearch&amp;#39;, git: &amp;#39;https://github.com/elasticsearch/cookbook-elasticsearch.git&amp;#39; cookbook &amp;#39;td-agent&amp;#39;, git: &amp;#39;https://github.</description>
    </item>
    
    <item>
      <title>第14回 OpenStack 勉強会参加ログ</title>
      <link>https://jedipunkz.github.io/post/2013/09/09/14th-openstack-study-hackathon/</link>
      <pubDate>Mon, 09 Sep 2013 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2013/09/09/14th-openstack-study-hackathon/</guid>
      <description>こんにちは。@jedipunkz です。
OpenStack 第14回勉強会 ハッカソンに参加してきました。その時の自分の作業ログを 記しておきます。自分の作業内容は &amp;lsquo;OpenStack + Docker 構築&amp;rsquo; です。
場所 : CreationLine さま 日時 : 2013年9月8日(土) 当日の atnd。
http://atnd.org/events/42891
当日発表のあった内容
Ansible で OpenStack を実際に皆の前でデプロイ！ Yoshiyama さん開発 LogCas お披露目 Havana の機能改善・機能追加内容確認 その他 Horizon の機能についてだったり openstack.jp の運用についてなど 自分が話を聞きながら黙々とやったことは
OpenStack + Docker 構築 結果&amp;hellip; NG 動かず。時間切れ。公式の wiki の手順がだいぶ変なので手順を修正しながら進めました。
公式の wiki はこちらにあります。
https://wiki.openstack.org/wiki/Docker
その修正しながらメモった手順を下記に貼り付けておきます。
作業環境 ホスト : Ubuntu 12.04.3 Precise OpenStack バージョン : devstack (2013/09/08 master ブランチ) 構成 : オールインワン (with heat, ceilometer, neutron) 普通に動かすとエラーが出力される これは devstack (2013/09/08 時点) での不具合なので直ちに修正されるかも。</description>
    </item>
    
    <item>
      <title>Kibana3 &#43; elasticsearch &#43; fluentd を試した</title>
      <link>https://jedipunkz.github.io/post/2013/09/08/kibana3-plus-elasticsearch-plus-fluentd/</link>
      <pubDate>Sun, 08 Sep 2013 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2013/09/08/kibana3-plus-elasticsearch-plus-fluentd/</guid>
      <description>こんにちは。@jedipunkz です。
{% img /pix/kibana3.png %}
前回の記事で Kibana + elasticsearch + fluentd を試しましたが、ツイッターで @nora96o さんに &amp;ldquo;Kibana3 使うと、幸せになれますよ！&amp;rdquo; と教えてもらいました。早 速試してみましたので、メモしておきます。
前回の記事。
http://jedipunkz.github.io/blog/2013/09/07/kibana-plus-elasticsearch-plus-fluentd/
前半の手順は前回と同様ですが、念のため書いておきます。
前提の環境 OS : Ubuntu 12.04 Precise (同じ方法で 13.04 Raring でも出来ました) 必要なパッケージのインストール 下記のパッケージを事前にインストールします。
% sudo apt-get install git-core build-essential ruby1.9.3 openjdk-7-jdk 手順を省くために Ruby はパッケージで入れました。また Java は他の物を利用しても 構いません。Ruby は fluentd が、Java は elasticsearch が必要とします。
elasticsearch のインストール 下記のサイトより elasticsearch をダウンロードします。
http://www.elasticsearch.org/download/
現時点 (2013/09/08) で最新のバージョンは 0.90.3 でした。
% wget https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-0.90.3.deb % sudo dpkg -i elasticsearch-0.</description>
    </item>
    
    <item>
      <title>Kibana &#43; ElasticSearch &#43; fluentd を試してみた</title>
      <link>https://jedipunkz.github.io/post/2013/09/07/kibana-plus-elasticsearch-plus-fluentd/</link>
      <pubDate>Sat, 07 Sep 2013 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2013/09/07/kibana-plus-elasticsearch-plus-fluentd/</guid>
      <description>こんにちは。@jedipunkz です。
自動化の流れを検討する中でログ解析も忘れてはいけないということで ElasticSearch を使いたいなぁとぼんやり考えていて Logstash とか Kibana とかいうキーワードも目 に止まるようになってきました。
ElasticSaerch は API で情報を検索出来たりするので自動化にもってこい。バックエ ンドに Logstash を使って&amp;hellip; と思ってたのですが最近よく聞くようになった fluentd をそろそろ真面目に使いたい！ということで、今回は Kibana + ElasticSearch + fluentd の組み合わせでログ解析システムを組む方法をメモしておきます。
参考にさせて頂いた URL http://memocra.blogspot.jp/2013/04/kibanakibanaelasticsearchfluentd.html
前提の環境 OS : Ubuntu 12.04 Precise (同じ方法で 13.04 Raring でも出来ました) 必要なパッケージインストール 下記のパッケージを事前にインストールします。
% sudo apt-get install git-core build-essential ruby1.9.3 openjdk-7-jdk 手順を省くために Ruby はパッケージで入れました。また Java は他の物を利用しても 構いません。Ruby は Kibana, fluentd が、Java は ElasticSearch が必要とします。
ElasticSearch のインストール 下記のサイトより ElasticSearch をダウンロードします。
http://www.elasticsearch.org/download/
現時点 (2013/09/07) で最新のバージョンは 0.</description>
    </item>
    
    <item>
      <title>Hurricane Electric &#43; Vyatta で宅内 IPv6 化</title>
      <link>https://jedipunkz.github.io/post/2013/09/01/hurricane-electric-vyatta-ipv6/</link>
      <pubDate>Sun, 01 Sep 2013 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2013/09/01/hurricane-electric-vyatta-ipv6/</guid>
      <description>こんにちは。@jedipunkz です。
自宅の IPv6 化、したいなぁとぼんやり考えていたのですが、Hurricane Electric Internet Services を見つけました。IPv4 の固定グローバル IP を持っていれば誰で も IPv6 のトンネルサービスを無料で受けられるサービスです。
1つのユーザで5アカウントまで取得でき (5 エンドポイント)、1アカウントで /64 の アドレスがもらえます。また申請さえすれば (クリックするだけ) /48 も1アカウント 毎にもらえます。つまり /48 x 5 + /64 x 5 &amp;hellip; でか！
私の宅内は Vyatta で PPPOE してるのですが、各種 OS (Debian, NetBSD&amp;hellip;) や機器 (Cisco, JunOS..)のコンフィギュレーションを自動生成してくれるので、接続するだけ であればそれをターミナルに貼り付けるだけ！です。
サービス URL Hurricane Electric は下記の URL です。アカウントもここで作成出来ます。
http://tunnelbroker.net
IPv6 接続性を確保する方法 Vyatta が IPv6 のアドレスを持ち接続性を確保するだけであれば、上に記したように コピペで出来ます。上記の URL でアカウントを作成しログインします。左メニューの &amp;ldquo;Create Regular Tunnel&amp;rdquo; を押して、自分の情報 (IPv4 のエンドポイントアドレス等) を入力します。その後、取得した IPv6 のレンジのリンクをクリックし上記メニュー &amp;ldquo;Example Configuration&amp;rdquo; を選択します。プルダウンメニューが現れるので、自宅の OS や機器に合った名前を選択します。</description>
    </item>
    
    <item>
      <title>Vyatta で L2TP over IPsec による VPN 構築</title>
      <link>https://jedipunkz.github.io/post/2013/08/24/vyatta-l2tp-ipsec-vpn/</link>
      <pubDate>Sat, 24 Aug 2013 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2013/08/24/vyatta-l2tp-ipsec-vpn/</guid>
      <description>こんにちは。@jedipunkz です。
以前、こんな記事をブログに記しました。2012/06 の記事です。
http://jedipunkz.github.io/blog/2012/06/13/vyatta-vpn/
その後、PPTP で保護されたネットワークの VPN パスワードを奪取出来るツールが公開 されました。2012/07 のことです。よって今では VPN に PPTP を用いることが推奨さ れていません。
ということで L2TP over IPsec による VPN 構築を Vyatta で行う方法を記します。
fig.1 : home lan と vyatta のアドレス +--------+ +-----+ home lan ---| vyatta | --- the internet --- | CPE | +--------+ +-----+ X.X.X.X/X(NAT) pppoe0 Y.Y.Y.Y この様に X.X.X.X/X と Y.Y.Y.Y/Y が関係しているとします。CPE は VPN により X.X.X.X/X に接続することが出来ます。
手順 : IPsec 下記の操作で IPsec を待ち受けるインターフェースの設定します。
% configure # edit vpn ipsec # set ipsec-interface interface pppoe0 インターフェース名は環境に合わせて設定してください。私の環境では pppoe0 です。</description>
    </item>
    
    <item>
      <title>OpenStack nova-network IPv6 対応</title>
      <link>https://jedipunkz.github.io/post/2013/08/18/openstack-nova-network-ipv6/</link>
      <pubDate>Sun, 18 Aug 2013 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2013/08/18/openstack-nova-network-ipv6/</guid>
      <description>こんにちは。@jedipunkz です
今更なのかもしれませんが、OpenStack の nova-network を IPv6 対応する方法を調べ てみました。何故 nova-network なのか? 自宅の構成が nova-network だからです..。 以前は Quantum (現 Neutron) 構成で使っていましたが、ノードをコントローラとコン ピュートに別けた時に NIC が足らなくなり&amp;hellip;。
さて本題です。下記のサイトを参考にしました。ほぼそのままの手順ですが、自分のた めにもメモです。
参考 URL http://docs.openstack.org/grizzly/openstack-compute/admin/content/configuring-compute-to-use-ipv6-addresses.html
前提 OpenStack の構成は予め構築されている nova-network を用いている 構成はオールインワンでも複数台構成でも可能 手順 nova がインストールされているすべてのノードで python-netaddr をインストールし ます。私の場合は rcbops の chef cookbooks で構築したのですが、既にインストール されていました。
% sudo apt-get install python-netaddr nova-network が稼働しているノードで radvd をインストールします。これは IPv6 を Advertise しているルータ等が予め備わっている環境であっても、インストー ルする必要があります。また /etc/radvd.conf が初め無いので radvd 単体では稼働し ませんが、問題ありません。OpenStack の場合 /var/lib/nova 配下のコンフィギュレー ションファイルを読み込んでくれます。
% sudo apt-get install radvd /etc/sysctl.</description>
    </item>
    
    <item>
      <title>rcbops Cookbooks で Neutron 構成 OpenStack</title>
      <link>https://jedipunkz.github.io/post/2013/08/16/rcbops-cookbooks-neutron-openstack/</link>
      <pubDate>Fri, 16 Aug 2013 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2013/08/16/rcbops-cookbooks-neutron-openstack/</guid>
      <description>こんにちは。@jedipunkz です。
rcbops Cookbooks で Neutron 構成の OpenStack をデプロイする方法を書きたいと思 います。先日紹介した openstack-chef-repo にも Neutron のレシピが含まれているの ですが、まだまだ未完成で手作業をおりまぜながらのデプロイになっていまうので、今 現在のところ Neutron 構成を組みたいのであればこの rcbops の Cookbooks を用いる しかないと思います。
今回は VLAN モードの構築を紹介します。GRE モードも少し手順を修正すれば構成可能 です。最後のまとめに GRE モードの構築について少し触れています。
構成 public network +----------------+----------------+----------------+----------------+---------------- | | | +--------------+ +--------------+ +--------------+ +--------------+ +--------------+ | controller01 | | network01 | | network02 | | compute01 | | compute02 | +--------------+ +-------+------+ +-------+------+ +-------+------+ +-------+------+ | | | | | | | | | management network +----------------+-------o--------+-------o--------+-------o--------+-------o-------- | | | | vm network +----------------+----------------+----------------+-------- 特徴は&amp;hellip;</description>
    </item>
    
    <item>
      <title>Emacs &#43; Mew で Gmail を読み書きする</title>
      <link>https://jedipunkz.github.io/post/2013/08/12/emacs-mew-gmail/</link>
      <pubDate>Mon, 12 Aug 2013 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2013/08/12/emacs-mew-gmail/</guid>
      <description>こんにちは。@jedipunkz です。
今日も軽めの話題を。
Gmail を Emacs + Mew で読み書きする方法を何故かいつも忘れてしまうので自分のた めにもメモしておきます。Gmail はブラウザで読み書き出来るのに！と思われるかもし れませんが、Emacs で文章が書けるのは重要なことです。:D
対象 OS 比較的新しい&amp;hellip;
Debian Gnu/Linux Ubuntu を使います。
手順 Emacs, Mew, stunnel4 をインストールします。Emacs は好きな物を入れてください。
% sudo apt-get install emacs24-nox stunnel4 mew mew-bin ca-certificates openssl コマンドで mail.pem を生成します。生成したものを /etc/stunnel 配下に設 置します。
% openssl req -new -out mail.pem -keyout mail.pem -nodes -x509 -days 365 % sudo cp mail.pem /etc/stunnel/ stunnel はインストール直後、起動してくれないので ENABLE=1 に修正します。
% sudo ${EDITOR} /etc/default/stunnel4 ENABLE=1 # 0 -&amp;gt; 1 へ変更 stunenl.</description>
    </item>
    
    <item>
      <title>Debian Unstable で stumpwm</title>
      <link>https://jedipunkz.github.io/post/2013/08/09/debian-unstable-stumpwm/</link>
      <pubDate>Fri, 09 Aug 2013 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2013/08/09/debian-unstable-stumpwm/</guid>
      <description>こんにちは。@jedipunkz です。
Linux のウィンドウマネージャは使い続けて長いのですが、既に1周半しました。twm -&amp;gt; gnome -&amp;gt; enlightenment -&amp;gt; OpenBox -&amp;gt; .. 忘れた .. -&amp;gt; twm -&amp;gt; vtwm -&amp;gt; awesome -&amp;gt; kde -&amp;gt; gnome -&amp;gt; enligtenment &amp;hellip;
巷では Linux のデスクトップ環境は死んだとか言われているらしいですが、stumpwm というウィンドウマネージャは結構いいなと思いました。タイル型のウィンドウマネー ジャで Emacs 好きの人が開発したらしいです。設定は lisp で書けます。
見た目は派手では無いのですが、
グルーピング機能 すべての操作がキーボードで出来る タイル型であるので煩わしいマウスでのウィンドウ操作が不要 という点に惹かれました。
Linux を使う時、私の場合 Debian Gnu/Linux Unstalble をいつも使うのですが、 Unstable だと apt-get install stumpwm したバイナリがコケる&amp;hellip;ということでビル ドしてあげました。普段慣れないビルド方法だったので、その時の手順を自分のために もメモしておきます。
前提環境 Debian Gnu/Linux unstable 利用 X の環境は揃っている ビルド手順 clisp をインストール clisp をインストールします。
% sudo apt-get install clisp-dev lisp.</description>
    </item>
    
    <item>
      <title>openstack-chef-repo で OpenStack 複数台構成をデプロイ</title>
      <link>https://jedipunkz.github.io/post/2013/08/06/opscode-cookbooks-openstack-deploy/</link>
      <pubDate>Tue, 06 Aug 2013 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2013/08/06/opscode-cookbooks-openstack-deploy/</guid>
      <description>こんにちは。@jedipunkz です。
前回、github.com/rcbops の Cookbooks を利用した OpenStack デプロイ方法を紹介し ました。これは RackSpace 社の Private Cloud Service で使われている Cookbooks で Apache ライセンスの元、誰でも利用できるようになっているものです。HA 構成を 組めたり Swift の操作 (Rings 情報管理など) も Chef で出来る優れた Cookbooks な わけですが、運用するにあたり幾つか考えなくてはならないこともありそうです。
chef-client の実行順番と実行回数が密接に関わっていること HA 構成の手動切替等、運用上必要な操作について考慮する必要性 ※ 後者については OpenStack ユーザ会の方々に意見もらいました。
特に前項は Chef を利用する一番の意義である &amp;ldquo;冪等性&amp;rdquo; をある程度 (全くという意味 ではありませんが) 犠牲にしていると言えます。また chef-client の実行回数、タイ ミング等 Cookbooks を完全に理解していないと運用は難しいでしょう。自ら管理して いる Cookbooks なら問題ないですが、rcbops が管理しているので常に更新状況を追っ ていく必要もありそうです。
一方、Opscode, RackSpace, AT&amp;amp;T 等のエンジニアが管理している Cookbooks がありま す。これは以前、日本の OpenStack 勉強会で私が話した &amp;lsquo;openstack-chef-repo&amp;rsquo; を利 用したモノです。github.com/stackforge の元に管理されています。 openstack-chef-repo は Berksfile, Roles, Environments のみの構成で各 Cookbooks は Berksfile を元に取得する形になります。取得先は同じく github.</description>
    </item>
    
    <item>
      <title>Cobbler で OS 自動インストール</title>
      <link>https://jedipunkz.github.io/post/2013/07/28/cobbler-os-automation-install/</link>
      <pubDate>Sun, 28 Jul 2013 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2013/07/28/cobbler-os-automation-install/</guid>
      <description>こんにちは。@jedipunkz です。
最近、自動化は正義だと最近思うのですが、その手助けをしてくれるツール Cobbler を試してみました。Cobbler と複数 OS, ディストリビューションを CLI, GUI で管理出 来るツールです。PXE, TFTP, DHCPを組分せれば OS の自動構築が出来るのは古くから ありますが、TFTP サーバに配置するカーネルイメージやマックアドレスの管理を一元 して管理してくれるのがこの Cobbler です。
今回は Cobbler の構築方法をお伝えします。本当は Chef Cookbooks で構築したかっ たのですが Opscode Community にある Cookbooks はイマイチだったので、今回は手動 で。
前提環境 OS は CentOSを。Ubuntu を利用すると DHCP のコンフィギュレーションを自動で出 来ません 利用するネットワークの DHCP はオフにします 構築手順 SELINUX を無効にします。石◯さん、ごめんなさい。
# ${EDITOR} /etc/sysconfig/selinux SELINUX=disabled # setenforce 0 EPEL のレポジトリを追加します。
# rpm -Uvh http://ftp.iij.ad.jp/pub/linux/fedora/epel/6/x86_64/epel-release-6-8.noarch.rpm cobbler をインストールします。またその他必要なパッケージもここでインストールし ます。
# yum install cobbler debmirror pykickstart 自分の設定したいパスワードを生成して /etc/cobbler/settings 内の default_password_crypted: に設定します。パスワードの生成は下記のように openssl コマンドを利用します。</description>
    </item>
    
    <item>
      <title>Swift HA 構成を Chef でデプロイ</title>
      <link>https://jedipunkz.github.io/post/2013/07/26/swift-ha-chef-deploy/</link>
      <pubDate>Fri, 26 Jul 2013 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2013/07/26/swift-ha-chef-deploy/</guid>
      <description>こんにちは。@jedipunkz です。
最近 Chef で OpenStack をデプロイすることばかりに興味持っちゃって、他のことも やらんとなぁと思っているのですが、せっかくなので Swift HA 構成を Chef でデプロ イする方法を書きます。
Swift って分散ストレージなのに HA ってなんよ！と思われるかもしれませんが、ご存 知の様に Swift はストレージノード (accout, object, container) とプロキシノード に別れます。今回紹介する方法だとプロキシノードを Keepalived と Haproxy で HA、 また MySQL も KeepAlived で HA の構成に出来ました。いつものように RackSpace 管 理の Cookbooks を使っています。
参考資料 http://www.rackspace.com/knowledge_center/article/openstack-object-storage-configuration
構成 構成は簡単に記すと下記のようになります。特徴としては&amp;hellip;
swift-proxy01, swift-proxy02 で HA。VRRP + LB な構成。 swift-proxy01 で git サーバ稼働。Rings 情報を管理。 swift-storageNN がストレージノード OS は Ubuntu server 12.04 です。
|--------- VRRP + Load Balancer ------| +-----------------+ +-----------------+ +-----------------+ +-----------------+ +-----------------+ | swift-proxy01 | | swift-proxy02 | | swift-storage01 | | swift-storage02 | | swift-storage03 | +-----------------+ +-----------------+ +-----------------+ +-----------------+ +-----------------+ | | | | | +-------------------+-------------------+-------------------+-------------------+------------------ | | +-----------------+ +-----------------+ | chef workstation| | chef server | +-----------------+ +-----------------+ 絵は書く意味なかったか&amp;hellip;。</description>
    </item>
    
    <item>
      <title>OpenStack HA 構成を Chef でデプロイ</title>
      <link>https://jedipunkz.github.io/post/2013/07/17/openstach-ha-chef-deploy/</link>
      <pubDate>Wed, 17 Jul 2013 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2013/07/17/openstach-ha-chef-deploy/</guid>
      <description>こんにちは。@jedipunkz です。
OpenStack を運用する中でコントローラは重要です。コントローラノードが落ちると、 仮想マシンの操作等が利用出来ません。コントローラの冗長構成を取るポイントは公式 wiki サイトに記述あるのですが PaceMaker を使った構成でしんどいです。何より運用 する人が混乱する仕組みは避けたいです。
RackSpace 社の管理している Chef Cookbooks の Roles に &amp;lsquo;ha-controller1&amp;rsquo;, &amp;lsquo;ha-controller2&amp;rsquo; というモノがあります。今回はこれを使った HA 構成の構築方法に ついて書いていこうかと思います。
構成 最小構成を作りたいと思います。HA のためのコントローラノード2台, コンピュートノー ド1台, Chef ワークショテーション1台, Chef サーバノード1台。
+----------------+----------------+----------------+----------------+--------------- public network | | | eth0 | | 10.0.0.0/24 +--------------+ +--------------+ +--------------+ +--------------+ +--------------+ | controller01 | | controller01 | | compute01 | | chef server | | workstation | +--------------+ +--------------+ +--------------+ +--------------+ +--------------+ | eth1 +------------------------------------------------- fixed range network 172.</description>
    </item>
    
    <item>
      <title>Chef で OpenStack デプロイ</title>
      <link>https://jedipunkz.github.io/post/2013/07/08/chef-openstack-deploy/</link>
      <pubDate>Mon, 08 Jul 2013 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2013/07/08/chef-openstack-deploy/</guid>
      <description>こんにちは。@jedipunkz です。
前回の記事で OpenCenter を使った OpenStack デプロイを行いましたが、デプロイの 仕組みの実体は Opscode Chef です。慣れている人であれば Chef を単独で使った方が よさそうです。僕もこの方法を今後取ろうと思っています。
幾つかの構成を試している最中ですが、今回 nova-network を使ったオールインワン構 成を作ってみたいと思います。NIC の数は1つです。ノート PC や VPS サービス上にも 構築できると思いますので試してみてください。
今回は Chef サーバの構築や Knife の環境構築に関しては割愛します。
また全ての操作は workstation ノードで行います。皆さんお手持ちの Macbook 等です。 デプロイする先は OpenStack をデプロイするサーバです。
手順 Chef Cookbook を取得 RackSpace 社のエンジニアがメンテナンスしている Chef Cookbook を使います。各 Cookbook が git submodule 化されているので &amp;ndash;recursive オプションを付けます。
% git clone https://github.com/rcbops/chef-cookbooks.git ~/openstack-chef-repo % cd openstack-chef-repo &amp;lsquo;v4.0.0&amp;rsquo; ブランチをチェックアウト master ブランチは今現在 (2013/07/08) folsom ベースの構成になっているので &amp;lsquo;grizzly&amp;rsquo; のためのブランチ &amp;lsquo;v4.</description>
    </item>
    
    <item>
      <title>内部で Chef を使っている OpenCenter で OpenStack 構築</title>
      <link>https://jedipunkz.github.io/post/2013/07/02/chef-opencenter-openstack/</link>
      <pubDate>Tue, 02 Jul 2013 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2013/07/02/chef-opencenter-openstack/</guid>
      <description>こんにちは。@jedipunkz です。
第13回 OpenStack 勉強会に参加してきました。内容の濃い収穫のある勉強会でした。 参加してよかった。特にえぐちさんの OpenCenter に関するプレゼン (下記のスライド 参照のこと) には驚きました。ちょうど当日 RackSpace のエンジニアが管理している github 上の Chef Cookbooks を使って OpenStack 構築できたぁ！と感動していたのに、 その晩のうちに Chef を使って GUI で！ OpenStack が自動構築出来るだなんて&amp;hellip;。
えぐちさんのスライド資料はこちら。
http://www.slideshare.net/guchi_hiro/open-centeropenstack
早速、私も手元で OpenCenter 使ってみました。えぐちさん、情報ありがとうございましたー。
実は私 としては Chef 単体で OpenStack を構築したいのですが、OpenCenter がどう Cookbook や Roles, Environment を割り当てているのか知りたかったので、 OpenCenter を使って構築してみました。今回はその準備。今日は OpenCenter で皆さ んも OpenStack を構築できるようキャプチャ付きで方法を紹介しますが、次の機会に Chef 単体での OpenStack の構築方法を紹介出来ればいいなぁと思っています。
構成 +---------------+---------------+---------------+---------------+-------------- public network |eth0 |eth0 |eth1 |eth1 | eth1 |10.200.10.11 |10.200.10.12 |10.200.10.13 |10.200.10.14 |10.200.10.15 +-------------+ +-------------+ +-------------+ +-------------+ +-------------+ | opencenter | | oc-chef | |oc-controller| | oc-compute01| | oc-compute02| +-------------+ +-------------+ +-------------+ +-------------+ +-------------+ |10.</description>
    </item>
    
    <item>
      <title>Sensu 監視システムを Chef で制御</title>
      <link>https://jedipunkz.github.io/post/2013/06/20/sensu-chef-controll/</link>
      <pubDate>Thu, 20 Jun 2013 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2013/06/20/sensu-chef-controll/</guid>
      <description>こんにちは。@jedipunkz です。
自動化の基盤を導入するために色々調べているのですが、監視も自動化しなくちゃ！と いうことで Sensu を調べてたのですが Chef との相性バッチリな感じで、自分的にイ ケてるなと思いました。
公式サイト http://www.sonian.com/cloud-monitoring-sensu/ ドキュメント http://docs.sensuapp.org/0.9/index.html 開発元が予め Chef の Cookbook (正確にはラッパー Cookbook 開発のための Cookbook で Include して使う) を用意してくれていたり、インストールを容易にする ための Omnibus 形式のパッケージの提供だったり。Omnibus なのでインストールと共 に Sensu が推奨する Ruby 一式も一緒にインストールされます。Chef と同じですね。
今回紹介したいのは、Chef で Sensu を構築・制御する方法です。
+--------------+ +--------------+ | chef-server | | workstation | +--------------+ +--------------+ | | +----------------+ | +--------------+ | sensu-server | +--------------+ | +----------------+----------------+----------------+ | | | | +--------------+ +--------------+ +--------------+ +--------------+ | sensu-client | | sensu-client | | sensu-client | | sensu-client | .</description>
    </item>
    
    <item>
      <title>Chef を Ruby コード内で利用する</title>
      <link>https://jedipunkz.github.io/post/2013/06/12/chef-ruby-code/</link>
      <pubDate>Wed, 12 Jun 2013 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2013/06/12/chef-ruby-code/</guid>
      <description>こんにちは。@jedipunkz です。
require &amp;lsquo;chef&amp;rsquo; して Ruby コードの中で chef を利用したいと思って色々調べていた のですが、そもそもリファレンスが無くサンプルコードもごくわずかしかネット上に見 つけられない状態でした。結局ソースコードを読んで理解していく世界なわけですが、 サンプルコードが幾つかあると他の人に役立つかなぁと思い、ブログに載せていこうか なぁと。
まず Chef サーバへアクセスするためには下記の情報が必要です。
ユーザ名 ユーザ用のクライアント鍵 Chef サーバの URL これらは Chef::Config で記していきます。
では早速サンプルコードです。まずは data bags 内データの一覧を取得するコードで す。data bags 内のデータを全で取得し配列で表示します。
#!/usr/bin/env ruby require &amp;#39;rubygems&amp;#39; require &amp;#39;chef/rest&amp;#39; require &amp;#39;chef/search/query&amp;#39; Chef::Config[:node_name]=&amp;#39;user01&amp;#39; Chef::Config[:client_key]=&amp;#39;/home/user01/user01.pem&amp;#39; Chef::Config[:chef_server_url]=&amp;#34;https://10.200.9.22&amp;#34; Chef::DataBag::list.each do |bag_name, url| Chef::DataBag::load(bag_name).each do |item_name, url| item = Chef::DataBagItem.load(bag_name, item_name).to_hash puts item end end 次は data bags にデータを入力するコードです。json_data という JSON 形式のデー タを test_data という data bag に放り込んでいます。</description>
    </item>
    
    <item>
      <title>Ceph クラスターネットワーク構成</title>
      <link>https://jedipunkz.github.io/post/2013/05/25/ceph-cluster-network/</link>
      <pubDate>Sat, 25 May 2013 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2013/05/25/ceph-cluster-network/</guid>
      <description>こんにちは。@jedipunkz です。
Ceph を運用する上で考慮しなければいけないのがトラフィックの負荷です。特に OSD 同士のレプリケーション・ハートビートには相当トラフィックの負荷が掛かることが想 像出来ます。
このため MDS, MON の通信に影響を与えないよう、OSD レプリケーション・ハートビー トのためのネットワークを別に設けるのがベストプラクティスな構成の様です。このネッ トワークのことをクラスターネットワークと Ceph 的に言うそうです。
こんな接続になります。
+------+ |Client| +------+ | +-------+-------+-------+-------+------ public network | | | | | +-----+ +-----+ +-----+ +-----+ +-----+ | MON | | MDS | | OSD | | OSD | | OSD | +-----+ +-----+ +-----+ +-----+ +-----+ | | | ----------------+-------+-------+------ cluster network 上図の様に MON, MDS は public ネットワークを介し OSD のレプリケーション・ハー トビートのみ cluster ネットワークを介します。Client と MDS との通信に影響を与 えない構成になります。</description>
    </item>
    
    <item>
      <title>OpenStack &#43; Ceph 連携</title>
      <link>https://jedipunkz.github.io/post/2013/05/19/openstack-ceph/</link>
      <pubDate>Sun, 19 May 2013 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2013/05/19/openstack-ceph/</guid>
      <description>こんにちは。最近 OpenStack の導入に向けて保守性や可用性について調査している @jedipunkz です。
OpenStack は MySQL のダンプや OS イメージ・スナップショットのバックアップをとっ ておけばコントローラの復旧も出来ますし、Grizzly 版の Quantum では冗長や分散が 取れるので障害時に耐えられます。また Quantum の復旧は手動もで可能です。最後の 悩みだった Cinder の接続先ストレージですが、OpenStack のスタンスとしては高価な ストレージの機能を使ってバックアップ取るか、Ceph, SheepDog のようなオープンソー スを使うか、でした。で、今回は Ceph を OpenStack に連携させようと思いました。
この作業により Cinder の接続先ストレージが Ceph になるのと Glance の OS イメー ジ・スナップショットの保管先が Ceph になります。
下記の参考資料が完成度高く、ほぼ内容はそのままです。若干付け足していますが。
参考資料 http://ceph.com/docs/master/rbd/rbd-openstack/
前提の構成 +-------------+-------------+--------------------------------------------- Public/API Network | | | +-----------+ +-----------+ +-----------+ +-----------+ +-----------+ +-----------+ | | | | |vm|vm|.. | | | | | | | | controller| | network | +-----------+ | ceph01 | | ceph01 | | ceph01 | | | | | | compute | | | | | | | | | | | | | | | | | | | +-----------+ +-----------+ +-----------+ +-----------+ +-----------+ +-----------+ | | | | | | | | +-------------+-----)-------+-----)-------+-------------+-------------+-- Management/API Network | | +-------------+-----------------------------------+-- Data Network Ceph は OpenStack の Management Network 上に配置 Ceph は3台構成 (何台でも可) OpenStack も3台構成 (何台でも可) 連携処理するのは controller, compute ノード では早速手順ですが、OpenStack と Ceph の構築手順は割愛します。私の他の記事を参 考にしていただければと思います。</description>
    </item>
    
    <item>
      <title>Chef Cookbook でユーザ・グループ追加</title>
      <link>https://jedipunkz.github.io/post/2013/05/18/chef-cookbook-adding-users/</link>
      <pubDate>Sat, 18 May 2013 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2013/05/18/chef-cookbook-adding-users/</guid>
      <description>こんにちは。@jedipunkz です。 今回は Opscode Chef でユーザ・グループを作成する方法をまとめます。
&amp;lsquo;users&amp;rsquo; Cookbook を使います。
% cd ${YOUR_CHEF_REPO} % ${EDITOR} Berksfile cookbook &#39;users&#39; % berks install --path ./cookbooks data_bag を使ってユーザ・グループの管理をしたいので管理ディレクトリを作成しま す。
% mkdir -p data_bags/users data_bags/users/jedipunkz.json ファイルを作成します。必要に応じて内容を書き換えてください。
{ &amp;quot;id&amp;quot;: &amp;quot;jedipunkz&amp;quot;, &amp;quot;ssh_keys&amp;quot;: &amp;quot;ssh-rsa AAAABx92tstses jedipunkz@somewhere&amp;quot;, &amp;quot;groups&amp;quot;: [ &amp;quot;sysadmin&amp;quot;, &amp;quot;sudo&amp;quot; ], &amp;quot;uid&amp;quot;: 2001, &amp;quot;shell&amp;quot;: &amp;quot;\/usr\/bin\/zsh&amp;quot;, &amp;quot;comment&amp;quot;: &amp;quot;jedipunkz sysadmin&amp;quot;, &amp;quot;password&amp;quot;: &amp;quot;$1$s%H8BMHlB$7s3h30y9IB1SklftZXYhvssJ&amp;quot; } json ファイルの説明です。
id : ユーザ名 ssh_keys : SSH 公開鍵 groups : 所属させるグループ uid : unix id sheell : ログインシェル comment : コメント passwd : ハッシュ化したパスワード 特にハッシュ化したパスワードは下記のコマンドで生成出来ます。</description>
    </item>
    
    <item>
      <title>Ceph-Deploy で Ceph 分散ストレージ構築</title>
      <link>https://jedipunkz.github.io/post/2013/05/11/ceph-deploy/</link>
      <pubDate>Sat, 11 May 2013 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2013/05/11/ceph-deploy/</guid>
      <description>今回は ceph-deploy というツールを使って Ceph ストレージを簡単に構築することが 出来るので紹介します。Ceph は分散ストレージでオブジェクトストレージとしてもブ ロックストレージとしても動作します。今回の構築ではブロックストレージとしてのみ の動作です。
Ceph が公開しているのが ceph-deploy なわけですが、マニュアル操作に代わる構築方 法として公開しているようです。その他にも Chef Cookbook も公開されているようで す。
それでは早速。
今回の構成 +--------+ +--------+ +--------+ | ceph01 | | ceph02 | | ceph03 | | osd | | osd | | osd | | mon | | mon | | mon | | mds | | mds | | mds | +--------+ +--------+ +--------+ | 10.0.0.1 | 10.0.0.2 | 10.0.0.3 | | | +----------+----------+ | | 10.</description>
    </item>
    
    <item>
      <title>Quantum Network ノードの分散・冗長</title>
      <link>https://jedipunkz.github.io/post/2013/04/26/quantum-network-distributing/</link>
      <pubDate>Fri, 26 Apr 2013 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2013/04/26/quantum-network-distributing/</guid>
      <description>こんにちは。Grizzly がリリースされてから暫く経ちました。今回は Folsom リリース まであった Quantum ノードのボトルネックと単一障害点を解決する新しい機能につい て評価した結果をお伝えします。
Folsom までは
Quantum L3-agent が落ちると、その OpenStack 一式の上にある仮想マシン全ての通 信が途絶える Quantum L3-agent に仮想マシンの全てのトラフィックが集まりボトルネックとなる。 という問題がありました。Folsom リリース時代にもし僕が職場で OpenStack を導入す るのであればこれらを理由に nova-network を選択していたかもしれません。 nova-network は compute ノードが落ちればその上の仮想マシンも同時に落ちるが、他 の compute ノード上の仮想マシンの通信には影響を与えないからです。もちろん仮想 ルータ・仮想ネットワークの生成等を API でユーザに提供したいなどの要望があれば Quantum を選択するしかありませんが。これに対して Grizzly リリースの Quantum は 改善に向けて大きな機能を提供してくれています。L3-agent, DHCP-agent の分散・冗 長機能です。
下記の構成が想定出来ます。ここでは Network ノードを2台用意しました。それ以上の 台数に増やすことも出来ます。
+-------------+-------------+-------------------------- Public/API Network | | | +-----------+ +-----------+ +-----------+ +-----------+ | | | | | | |vm|vm|.. | | controller| | network | | network | +-----------+ | | | | | | | compute | +-----------+ +-----------+ +-----------+ +-----------+ | | | | | | | +-------------+-----)-------+-----)-------+-----)------ Management/API Network | | | +-------------+-------------+------ Data Network L3-agent の分散は仮想ルータ単位で行います。それに対し DHCP-agent は仮想 ネットワーク単位で行います。</description>
    </item>
    
    <item>
      <title>Chef for OpenStack</title>
      <link>https://jedipunkz.github.io/post/2013/04/21/chef-for-openstack-grizzly-roadmap/</link>
      <pubDate>Sun, 21 Apr 2013 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2013/04/21/chef-for-openstack-grizzly-roadmap/</guid>
      <description>以前にも話題にしたことがある Chef For OpenStack ですが今週新しい情報が入って来 ました。#ChefConf 2013 というイベントがあったのですがここで Opscode の Matt Ray さんらが集まり OpenStack を Chef で構築する &amp;lsquo;Chef for OpenStack&amp;rsquo; について 語られた模様です。その時の資料が SlideShare に上がっていたので見てみました。
Chef for OpenStack: Grizzly Roadmap from Matt Ray 気にあった点を幾つか挙げていきます。
https://github.com/osops で管理される 各コンポーネントの cookbook の名前には &amp;lsquo;-cookbook&amp;rsquo; を最後に付ける quantum, cinder, ceilometer, heat 等、比較的新しいコンポーネントも加わる gerrit でコードレビューされ CI も提供される Chef11 が用いられる Ruby 1.9.x に対応した chef-client が用いられる Foodcritic で可能な限りテストされる chef-solo はサポートされない 5月に &amp;lsquo;2013.1.0&amp;rsquo; がリリースされる (openstack 2013.1 対応と思われる) chef-repo の形で提供される Ubuntu 12.04 が前提 HyperVisor は KVM, LXC がサポートされる 以上です。恐らく chef-repo で提供されるということは spiceweasel を使った構成構 築が出来るような形になるでしょう。楽しみです。またコントリビュートする方法も掲 載されているので興味が有る方は協力してみるのも楽しいかもしれません。</description>
    </item>
    
    <item>
      <title>OpenStack Grizzy で非 Virtio OS 稼働</title>
      <link>https://jedipunkz.github.io/post/2013/04/21/openstack-non-virtio/</link>
      <pubDate>Sun, 21 Apr 2013 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2013/04/21/openstack-non-virtio/</guid>
      <description>こんにちは jedipunkz です。
Virtio に対応していない OS を OpenStack で稼働させることが今まで出来なかったの ですが Grizzly から非 Virtio な OS イメージが扱えるようになった。今まで NetBSD やら古い FreeBSD やら virtio ドライバを OS イメージに入れることに苦労していたの だけど、これで問題無くなった。
最初、この機能のこと調べるのに「どうせ libvirt が生成する xml を書き換えるのだ から nova 周りの設定なんだろうー」と思っていたら全く方法が見つからず&amp;hellip;。結局 OS イメージを格納している Glance の設定にありました。
ここでは FreeBSD7.4 Release を例に挙げて説明していきます。
前提とする環境 OpenStack Grizzly が稼働していること ホスト OS に Ubuntu 12.04.2 LTS が稼働していること ゲスト OS に FreeBSD 7.4 Release を用いる とします。OS のバージョンはホスト・ゲスト共に、上記以外でも構いません。Grizzly さえ動いていれば OK です。
OS イメージ作成 KVM で OS イメージを作成します。もちろん virtio なインターフェースは指定せず</description>
    </item>
    
    <item>
      <title>OpenStack Grizzly 構築スクリプト</title>
      <link>https://jedipunkz.github.io/post/2013/04/20/openstack-grizzly-installation-script/</link>
      <pubDate>Sat, 20 Apr 2013 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2013/04/20/openstack-grizzly-installation-script/</guid>
      <description>OpenStack Grizzly がリリースされて2週間ほど経過しました。皆さん動かしてみまし たか？今回、毎度の構築 Bash スクリプトを開発したので公開します。
下記のサイトで公開しています。
https://github.com/jedipunkz/openstack_grizzly_install
このスクリプト、複数台構成とオールインワン構成の両方が構成出来るようなっていま すが、今回は簡単なオールインワン構成の組み方をを簡単に説明したいと思います。
前提の環境 Ubuntu 12.04 LTS が稼働している Cinder のためのディスクを OS 領域と別に用意 (/dev/sdb1 など) オールインワン構成の場合は 2 NICs 準備 Ubuntu 13.04 の daily build も完成度上がっている時期ですが OVS 側の対応が OpenStack 構成に問題を生じさせるため 12.04 LTS + Ubuntu Cloud Archive の組み合 わせで構築するのが主流になっているようです。また、Cinder 用のディスクは OS 領 域を保持しているディスクとは別 (もしくはパーティションを切ってディスクデバイス を別けても可) が必要です。オールインワン構成の場合は NIC を2つ用意する必要があ ります。通常 OpenStack を複数台構成する場合は
コントローラノード x 1 台 ネットワークノード x 1 台 コンピュートノード x n 台 で組み、VM はコンピュートノードからネットワークノードを介してインターネットに 接続します。よってそのため更に NIC が必要になるのですが、オールインワン構成の 場合は</description>
    </item>
    
    <item>
      <title>Chef 11 サーバのローカルネットワーク上構築</title>
      <link>https://jedipunkz.github.io/post/2013/04/06/chef-11-private-network/</link>
      <pubDate>Sat, 06 Apr 2013 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2013/04/06/chef-11-private-network/</guid>
      <description>chef-solo を使うの？Chef サーバを使うの？という議論は結構前からあるし、答えは 「それぞれの環境次第」だと思うのだが、僕は個人的に Chef サーバを使ってます。ホ ステッド Chef を使いたいけどお金ないし。会社で導入する時はホステッド Chef を契 約してもらうことを企んでます。(・∀・) 何故なら cookbooks を開発することがエン ジニアの仕事の本質であって Chef サーバを運用管理することは本質ではないから。そ れこそクラウド使えという話だと思う。
でも！Chef に慣れるには無料で使いたいし、継続的に Cookbooks をターゲットノード で実行したい。ということで Chef サーバを構築して使っています。
Chef 10 の時代は Chef サーバの構築方法は下記の通り3つありました。
手作業！ Bootstrap 構築 Opscode レポジトリの Debian, Ubuntu, CentOS パッケージ構築 それが Chef 11 では
Ubuntu, RHEL のパッケージ (パッケージインストールですべて環境が揃う) http://www.opscode.com/chef/install/
この方法1つだけ。でも簡単になりました。
&amp;lsquo;Chef Server&amp;rsquo; タブを選択するとダウンロード出来る。じっくりは deb ファイルの中 身を見たことがないけど、チラ見した時に chef を deb の中で実行しているように見 えた。徹底してるｗ
Chef 10 時代のパッケージと違って行う操作は下記の2つのコマンドだけ。
% sudo dpkg -i chef-server_11.0.6-1.ubuntu.12.04_amd64.deb # ダウンロードしたもの % sudo chef-server-ctl reconfigure 簡単。でも&amp;hellip; この状態だと https://&amp;lt;サーバの FQDN&amp;gt; でサーバが Listen している。 IP アドレスでアクセスしてもリダイレクトされる。つまり、ローカルネットワーク上 に構築することが出来ない。安易に hosts で解決も出来ない。何故ならターゲットノー ドは通常まっさらな状態なので bootstrap するたびに hosts を書くなんてアホらしい しやってはいけない。</description>
    </item>
    
    <item>
      <title>クラウドマネジメント勉強会レポ</title>
      <link>https://jedipunkz.github.io/post/2013/04/06/cloudmanagement/</link>
      <pubDate>Sat, 06 Apr 2013 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2013/04/06/cloudmanagement/</guid>
      <description>クラウドマネジメント勉強会に参加してきた。今が旬なのか定員140名が埋まっていま した。クラウドフェデレーションサービス各種の話が聞ける貴重な勉強会の場でした。
場所 : スクエアエニックスさん 日程 : 2013年4月5日 19:00 - 少し長くなるので、早速。
クラウド運用管理研究会 クラウド利用推進機構が運営するクラウド運用管理研究会は下記の3つに分別されるそ うです。今回は一項目の &amp;lsquo;クラウドマネジメントツール研究会&amp;rsquo; にあたるそう。別の研 究会も既に勉強会を実施しているそうです。
クラウドマネジメントツール研究会 デザインパターン研究会 運用管理・監視研究会 AWS OpsWorks アマゾンデータサービスジャパン AWS 片山さん, 船崎さん OpsWorks は最近話題になった AWS 利用者に無料で提供されるクラウドフェデレーショ ンサービス。Web UI で操作し簡単デプロイを実現するサービスです。
OpsWorks が自動化するモノ サーバ設定 ミドルウェア構築 特徴 Chef フレームワークを利用 (chef-solo を内部的に利用) 任意の cookbooks を利用可能 LB, AP, DB などをレイヤ化, 任意のレイヤも作成可能 OpsWorks の流れ Stack 作成 レイヤ作成 (LB, AP, DB, 任意) レシピの作成 レイヤにインスタンス作成 下記をレイヤ化で区別する Package インストール OS 設定 アプリデプロイ 所感 AWS OpsWorks の登場で他のクラウドフェデレーションサービスがどうなるの？とさえ 思った。AWS はインターネット・ホスティング業界のあらゆるサービスを押さえようと している感がある。もう隙間がない！ｗ OpsWorks に関してまだ問題は残っているそう だ。VPC, micro 現在未対応など。が解決に向けて作業しているそう。</description>
    </item>
    
    <item>
      <title>NetBSD on OpenStack</title>
      <link>https://jedipunkz.github.io/post/2013/03/28/netbsd-on-openstack/</link>
      <pubDate>Thu, 28 Mar 2013 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2013/03/28/netbsd-on-openstack/</guid>
      <description>もう数日で OpenStack の次期バージョン版 Grizzly がリリースされるタイミングだが 現行バージョン Folsom の OpenStack の上に NetBSD を載せてみた。完全にお遊び だけど&amp;hellip;。
結局、ほとんど何も特別な対応取ることなく NetBSD が動いた。もちろんハイパーバイ ザは KVM です。だけど少し条件がある。
qemu の不具合があり Ubuntu 12.04 LTS + Ubuntu Cloud Archives の組み合わせでは NetBSD が動作しなかった。下記のようなカーネルパニックが発生。
panic: pci_make_tag: bad request この不具合に相当するんじゃないかと思ってる。
https://bugs.launchpad.net/qemu/+bug/897771
よって下記の組み合わせで動作を確認した。
Ubuntu 12.10 + OpenStack (Native Packages) qemu, kvm : 1.2.0+noroms-0ubuntu2.12.10.3 NetBSD 6.1 RC2 amd64 前提条件 OpenStack Folsom が動作していること。
NetBSD OS イメージ作成 nova-compute が動作しているホストの qemu-kvm を利用する。OpenStack 上に何でも 良いので OS を動作させこの kvm プロセスのパラメータを参考に kvm コマンドを実行 し NetBSD をインストールさせた。一番確実な方法。</description>
    </item>
    
    <item>
      <title>Berkshelf で Chef Cookbook の管理</title>
      <link>https://jedipunkz.github.io/post/2013/03/17/berkshelf-chef-cookbook-manage/</link>
      <pubDate>Sun, 17 Mar 2013 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2013/03/17/berkshelf-chef-cookbook-manage/</guid>
      <description>こんにちは。@jedipunkz です。
今日は Chef Cookbook の管理をしてくれる Berkshelf について。
Berkshelf は Librarian-Chef と同じく Cookbook の管理をしてくれるツールです。依 存関係のクリアもしてくれます。Opscode の中の人 @someara さんにこんなこと言われ て、
@jedipunkz berkshelf &amp;gt; librarian-chef
&amp;mdash; somearaさん (@someara) 2013年2月5日 Librarian-chef じゃなくて Berkshelf 使えってことだろうなぁと思ったので僕は Bekshelf を使うようにしてます。先日ブログ記事にした openstack-chef-repo も以前 は Librarian-chef を使っていたのですが最近 Berkshelf に置き換わりました。 openstack-chef-repo は Opscode の中の人の @mattray さん達が管理しています。
では早速。
インストール インストールは簡単。gem install するだけです。
% gem install berkshelf 使い方 chef-repo 配下で Berksfile を下記のように書きます。
site :opscode cookbook &#39;chef-client&#39; cookbook &#39;nginx&#39;, &#39;= 0.101.2&#39; berks コマンドを実行して Cookbooks をダウンロードします。</description>
    </item>
    
    <item>
      <title>chef-client の継続的デリバリ</title>
      <link>https://jedipunkz.github.io/post/2013/03/15/chef-contenuously-deploy/</link>
      <pubDate>Fri, 15 Mar 2013 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2013/03/15/chef-contenuously-deploy/</guid>
      <description>こんにちは。@jedipunkz です。
久々に Chef の話題。
有名な方々が最近 Chef について記事書いたりで Chef が再び盛り上がってきましたね。 僕のブログにも chef-solo で検索してアクセスしてくる方が増えているようです。
ちょうど今、仕事で試験的なサービスを立ち上げてそこで Chef を使っているのですが Server, Workstation, Target Node(s) の構成で使っていて、僕らは最初から chef-solo と capistrano でってことは考えていませんでした。もちろん chef-solo + capistrano の環境も調査しましたが、今の Server 構成が便利なのでもう戻れない。
今日は Chef サーバ構成の良さについての記事ではないですが、それについては次回、 時間見つけて書こうかと思ってます。
今日は &amp;lsquo;chef-client をどうアップデートするか&amp;rsquo; について。せっかく Chef でサーバ構成を継続的にデプロイ出来ても Chef 自身をアップデート出来ないと悲しい ですよね。chef-client が稼働しているインスタンスなんて起動して利用してすぐに破 棄だって時代ですが、なかなかそうもいなかい気がしています。
「ほら、だから chef-solo 使えばいいんだよ！」って思ってるあなた！違うんですよー。 そのデメリットを上回るメリットが Chef サーバ構成にあるんです。次回書きますｗ
Chef10 から Chef11 と試験してみるにはちょうど良い時期でした。今回の構成は&amp;hellip;
旧環境 +------------------+ | chef server | | version 10.18 | +------------------+ ^ | +--------------------+ | | | | +------------------+ +------------------+ | chef workstation | | target node | | version 10.</description>
    </item>
    
    <item>
      <title>pry のススメ</title>
      <link>https://jedipunkz.github.io/post/2013/03/06/pry/</link>
      <pubDate>Wed, 06 Mar 2013 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2013/03/06/pry/</guid>
      <description>OpenStack をコードで管理するためのフレームワークは幾つか存在するのだけど Ruby で記述出来る Fog が良い！と隣に座ってるアプリエンジニアが言うので僕も最近少し 触ってます。
Fog を使った OpenStack を管理するコードを書くことも大事なのだけど、Fog のコン トリビュートってことで幾つかの機能を付け足して (Quantum Router 周り) ってこと をやってました。まだ取り込まれてないけど。
その開発の中で pry の存在を教えてもらいその便利さに驚いたので少し説明します。 バリバリ開発系の人は既に知っているだろうけど、インフラ系エンジニアの僕にとって は感激モノでした。
pry は irb 代替な Ruby のインタラクティブシェルです。下記の URL から持ってこれ ます。
https://github.com/pry/pry
シンタックスハイライトされたり json のレスポンスが綺麗に成形されたり irb 的に 使うだけでも便利なのだけど &amp;lsquo;?&amp;rsquo; や &amp;lsquo;$&amp;rsquo; でコードのシンタックスを確認したりコード 内容を確認したり出来るのがアツい！
ちょうど今回追加した Fog の機能を使って説明していみます。
Fog のコードを require して OpenStack に接続するための情報を設定し OpenStack Quantum に接続します。これで準備完了。
[38] pry(main)&amp;gt; require &amp;#39;/home/jedipunkz/fog/lib/fog.rb&amp;#39; [49] pry(main)&amp;gt; @connection_hash = { [49] pry(main)* :openstack_username =&amp;gt; &amp;#39;demo&amp;#39;, [49] pry(main)* :openstack_api_key =&amp;gt; &amp;#39;demo&amp;#39;, [49] pry(main)* :openstack_tenant =&amp;gt; &amp;#39;service&amp;#39;, [49] pry(main)* :openstack_auth_url =&amp;gt; &amp;#39;http://172.</description>
    </item>
    
    <item>
      <title>第11回OpenStack勉強会で話してきた</title>
      <link>https://jedipunkz.github.io/post/2013/02/10/di-11hui-openstack-study11-openstack-chef-repo/</link>
      <pubDate>Sun, 10 Feb 2013 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2013/02/10/di-11hui-openstack-study11-openstack-chef-repo/</guid>
      <description>2013年2月9日に行われた OpenStack 勉強会第11回で話してきました。
openstack-chef-repo と言う、Opscode Chef で OpenStack を構築する内容を話して きました。その時に説明出来なかった詳細についてブログに書いておきます。
Openstack chef-repo from Tomokazu Hirai 説明で使ったスライドです。
まえがき Essex ベースで構築することしか今は出来ません。Folsom に関しては &amp;lsquo;直ちに開発が スタートする&amp;rsquo; と記されていました。今回は Opscode と RackSpace のエンジニアが共 同で開発を進めているので期待しています。今まで個人で OpenStack の各コンポーネ ントの cookbook を開発されていた方がいらっしゃるのだけど汎用性を持たせるという 意味で非常に難しく、またどの方の開発に追従していけばよいか判断困っていました。 よって今回こそ期待。
前提の構成 +-------------+ | chef-server | +-------------+ 10.0.0.10 | +---------------+ 10.0.0.0/24 | | +-------------+ +-------------+ | workstation | | node | +-------------+ +-------------+ 10.0.0.11 10.0.0.12 chef-server : chef API を持つ chef-server 。cookbook, role..などのデータを持つ workstation : openstack-chef-repo を使うノード。knife が使える必要がある。 node : OpenStack を構築するターゲットノード 目次 chef-server の構築 (BootStrap 使う) openstack-chef-repo を使用する準備 openstack-chef-repo 実行 chef-server の構築 Opscode の wiki に記されている通りなのですが、簡単に書いておきます。今回は bootstrap 方式で用意します。</description>
    </item>
    
    <item>
      <title>Spiceweasel で knife バッチ処理</title>
      <link>https://jedipunkz.github.io/post/2013/02/01/spiceweasel-knife-bootstrap/</link>
      <pubDate>Fri, 01 Feb 2013 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2013/02/01/spiceweasel-knife-bootstrap/</guid>
      <description>Spiceweasel https://github.com/mattray/spiceweasel#cookbooks を使ってみた。
Spiceweasel は Chef の cookbook のダウンロード, role/cookbook の chef server へのアップロード, ブートストラップ等をバッチ処理的に行なってくれる(もしくはコ マンドラインを出力してくれる)ツールで、自分的にイケてるなと感じたのでブログに 書いておきます。
クラウドフェデレーション的サービスというかフロントエンドサービスというか、複数 のクラウドを扱えるサービスは増えてきているけど、chef を扱えるエンジニアであれ ば、この Spiceweasel で簡単・一括デプロイ出来るので良いのではないかと。
早速だけど chef-repo にこんな yamp ファイルを用意します。
cookbooks: - apt: - nginx: roles: - base: nodes: - 172.24.17.3: run_list: role[base] options: -i ~/.ssh/testkey01 -x root -N webset01 - 172.24.17.4: run_list: role[base] options: -i ~/.ssh/testkey01 -x root -N webset02 上から説明すると&amp;hellip;
&amp;lsquo;apt&amp;rsquo;, &amp;rsquo;nginx&amp;rsquo; の cookbook を opscode レポジトリからダウンロード &amp;lsquo;apt&amp;rsquo;, &amp;rsquo;nginx&amp;rsquo; の cookbook を chef-server へアップロード roles/base.</description>
    </item>
    
    <item>
      <title>Arch Linux セットアップまとめ</title>
      <link>https://jedipunkz.github.io/post/2013/01/14/arch-linux-setup/</link>
      <pubDate>Mon, 14 Jan 2013 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2013/01/14/arch-linux-setup/</guid>
      <description>(2013/08/31 修正しました)
自宅のノート PC にいつも Debian Gnu/Linux unstable を入れて作業してたのだけど、 Arch Linux が試したくなって入れてみた。すごくイイ。ミニマル思考で常に最新。端 末に入れる OS としては最適かも!と思えてきた。Ubuntu はデスクトップ環境で扱う にはチト大きすぎるし。FreeBSD のコンパイル待ち時間が最近耐えられないし&amp;hellip;。
前リリースの Arch Linux には /arch/setup という簡易インストーラがあったのだけ ど、それすら最近無くなった。環境作る方法を自分のためにもメモしておきます。
OS イメージ iso 取得とインストール用 USB スティック作成 Linux, Windows, Mac で作り方が変わるようだけど、自分は Mac OSX を使ってインス トール USB スティックを作成した。
diskutil で USB スティック装着前後の disk デバイス番号を覚える
% diskutil list (ここでは /dev/rdisk4 として進める。)
アンマウントする。
% sudo diskutil unmountDisk /dev/rdisk4 ダウンロードした iso を USB スティックに書き込む。
% sudo dd if=/path/to/downloaded/iso of=/dev/rdisk4 bs=8192 % sudo diskutil eject /dev/rdisk4 USB スティック装着しインストール開始 起動するとメニューが表示されるので x86_64 を選んで起動。プロンプトが表示される。</description>
    </item>
    
    <item>
      <title>OpenStack Folsom on Thinkpad</title>
      <link>https://jedipunkz.github.io/post/2013/01/12/openstack-on-thinkpad/</link>
      <pubDate>Sat, 12 Jan 2013 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2013/01/12/openstack-on-thinkpad/</guid>
      <description>以前紹介した OpenStack Folsom 構築 bash スクリプトなのだけど quantum の代わり に nova-network も使えるようにしておいた。
構築 bash スクリプトは、
https://github.com/jedipunkz/openstack_folsom_deploy/blob/master/README_jp.md
に詳しい使い方を書いておきました。またパラメータを修正して実行するのだけどパラ メータについては、
https://github.com/jedipunkz/openstack_folsom_deploy/blob/master/README_parameters_jp.md
に書いておきました。
手持ちの Thinkpad に OpenStack folsom 入れた写真。この写真の OpenStack Folsom を構築した時の手順を書いておくよ。
OS をインストール OS をインストールします。12.10 を使いました。(12.04 LTS でも可)。/dev/sda6 等、 Cinder 用に一つパーティションを作ってマウントしないでおきます。また固定 IP ア ドレスを NIC に付与しておきます。
スクリプト取得 スクリプトを取得する。
% sudo apt-get update; sudo apt-get install git-core % git clone https://github.com/jedipunkz/openstack_folsom_deploy.git % cd openstack_folsom_deploy パラメータ修正 deploy_with_nova-network.conf 内のパラメータを修正します。オールインワン構成な ので、ほぼほぼ修正せずに実行しますが
HOST_IP=&#39;&amp;lt;Thinkpad の IP アドレス&amp;gt;&#39; だけ修正。
実行&amp;hellip; 実行する。</description>
    </item>
    
    <item>
      <title>FreeBSD on OpenStack</title>
      <link>https://jedipunkz.github.io/post/2013/01/01/freebsd-on-openstack/</link>
      <pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2013/01/01/freebsd-on-openstack/</guid>
      <description>FreeBSD を OpenStack で管理したいなぁと思って自宅に OpenStack 環境作ってました。
お正月なのに&amp;hellip;
使ったのは folsom ベースの OpenStack (nova-network) と FreeBSD 9.1 です。8 系 の FreeBSD でも大体同じ作業で実現出来るぽいです。あと nova-network でって書い たのは自宅に quantum だと少し厳しいからです。FlatDHCPManager が調度良かった。
今回のポイントは FreeBSD の HDD, NIC のドライバに virtio を使うように修正する ところです。OpenStack (KVM) は virtio 前提なので、そうせざるを得なかったです。
今回使ったソフトウェア OpenStack Folsom (nova-network) Ubuntu Server 12.10 FreeBSD 9.1 amd64 作業方法 準備としてこれらが必要になります。事前に行なってください。
FreeBSD-9.1-RELEASE-amd64-disc1.iso ダウンロード 作業ホスト (Ubuntu Server 12.10) に qemu-kvm をインストール freebsd9.img として qcow2 イメージを作成します。
% kvm-img create -f qcow2 freebsd9.img 8G 作成したイメージファイルに FreeBSD 9.</description>
    </item>
    
    <item>
      <title>OpenStack API を理解しインフラエンジニアの仕事の変化を感じる</title>
      <link>https://jedipunkz.github.io/post/2012/12/08/knife-fog-openstack-api/</link>
      <pubDate>Sat, 08 Dec 2012 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2012/12/08/knife-fog-openstack-api/</guid>
      <description>今日は &amp;ldquo;OpenStack Advent Calendar 2012 JP&amp;rdquo; というイベントのために記事を書きた いと思います。Advent Calendar とはキリスト生誕を祝うため 12/25 まで毎日誰かがブログ 等で特定の話題について述べるもの、らしいです。CloudStack さん, Eucalyptus さん も今年はやっているそうですね。
イベントサイト : http://atnd.org/events/34389
では早速！(ただ..CloudStack の Advent Calendar とネタがかぶり気味です..。)
御存知の通り OpenStack は API を提供していてユーザがコードを書くことで OpenStack のコマンド・Horizon で出来ることは全て可能です。API を叩くのに幾つか フレームワークが存在します。
fog libcloud deltacloud などです。
ここでは内部で fog を使っている knife-openstack を利用して API に触れてみよう かと思います。API を叩くことを想像してもらって、インフラエンジニアの仕事内容の 変化まで述べられたらいいなぁと思っています。
OpenStack 環境の用意 予め OpenStack 環境は揃っているものとしますです。お持ちでなければ
http://jedipunkz.github.com/blog/2012/11/10/openstack-folsom-install/
この記事を参考に環境を作ってみて下さい。あ、devstack でも大丈夫です。
chef, knife-openstack の用意 chef, knife-openstack を入れるのは OpenStack 環境でも、別のノードでも構いません。
chef が確か 1.9.2 ベースが推奨だったので今回は 1.9.2-p320 使います。 ruby は rbenv で入れるのがオススメです。knife-openstack, chef のインストールは&amp;hellip;</description>
    </item>
    
    <item>
      <title>OpenFlow Trema ハンズオン参加レポート</title>
      <link>https://jedipunkz.github.io/post/2012/11/21/openflow-trema-handson-report/</link>
      <pubDate>Wed, 21 Nov 2012 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2012/11/21/openflow-trema-handson-report/</guid>
      <description>InternetWeek2012 で開かれた &amp;ldquo;OpenFlow Trema ハンズオン&amp;rdquo; に参加してきました。
講師 : Trema 開発チーム 鈴木一哉さま, 高宮安仁さま 開催日 : 2012年11月21日 OpenStack の Quantum Plugin として Trema が扱えるという話だったので興味を持っ たのがきっかけです。また Ruby で簡潔にネットワークをコード化出来る、という点も 個人的に非常に興味を持ちました。OpenStack, CloudStack 等のクラウド管理ソフトウェ アが提供する API といい、Opscode Chef, Puppet 等のインフラソフトウェア構築フレー ムワークといい、この OpenFlow もインフラを形成する技術を抽象化し、技術者がコー ドを書くことでインフラ構築を行える、という点ではイマドキだなと思います。
Google は既にデータセンター間の通信を 100% 、OpenFlow の仕様に沿った機器・ソフ トウェアをを独自に実装しさばいているそうですし、我々が利用する日も近いと想像し ます。
OpenFlow のモチベーション OpenFlow の登場には理由が幾つかあって、既存のネットワークの抱えている下記の幾 つかの問題を解決するためです。
装置仕様の肥大化 多様なプロトコルが標準化 装置のコスト増大 ある意味、自律したシステムが招く複雑さ 一方、OpenFlow を利用すると..
コモディティ化された HW の利用が可能 OpenFlow はコントローラ (神) が集中管理するので楽な場合もある ネットワーク運用の自動化が図れる アプリケーションに合わせた最適化 柔軟な自己修復 等のメリットが。
OpenFlow と Trema とは?</description>
    </item>
    
    <item>
      <title>OpenStack Folsom 構築スクリプト</title>
      <link>https://jedipunkz.github.io/post/2012/11/10/openstack-folsom-install/</link>
      <pubDate>Sat, 10 Nov 2012 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2012/11/10/openstack-folsom-install/</guid>
      <description>※2012/12/04 に内容を修正しました。Network Node を切り出すよう修正。 ※213/01/09 に内容を修正しました。パラメータ修正です。
OpenStack の Folsom リリースからメインコンポーネントの仲間入りした Quantum を 理解するのに時間を要してしまったのだけど、もう数十回とインストールを繰り返して だいぶ理解出来てきました。手作業でインストールしてると日が暮れてしまうのでと思っ て自分用に bash で構築スクリプトを作ったのだけど、それを公開しようと思います。
OpenStack Folsom の構築に四苦八苦している方に使ってもらえたらと思ってます。
http://jedipunkz.github.com/openstack_folsom_deploy/
chef や puppet, juju などデプロイのフレームワークは今流行です。ただどれも環境 を予め構築しなくてはいけないので、誰でもすぐに使える環境ってことで bash スクリ プトで書いています。時間があれば是非 chef の cookbook を書いていきたいです。と いうか予定です。でも、もうすでに opscode 等は書き始めています。(汗
ではでは、紹介を始めます。
前提の構成 management segment 172.16.1.0/24 +--------------------------------------------+------------------+----------------- | | | | | | | eth2 172.16.1.13 | eth2 172.16.1.12 | eth2 172.24.1.11 +------------+ +-----------+ +------------+ | | eth1 ------------------- eth1 | | | | | network | vlan/gre seg = 172.</description>
    </item>
    
    <item>
      <title>Swift で簡単に分散オブジェクトストレージ</title>
      <link>https://jedipunkz.github.io/post/2012/11/04/swift-tempauth/</link>
      <pubDate>Sun, 04 Nov 2012 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2012/11/04/swift-tempauth/</guid>
      <description>最近、OpenStack にどっぷり浸かってる @jedipunkz です。
Folsom がリリースされて Quantum を理解するのにめちゃ苦労して楽しい真っ最中なのだけど、 今日は OpenStack の中でも最も枯れているコンポーネント Swift を使ったオブジェクトストレー ジ構築について少し書こうかなぁと思ってます。
最近は OpenStack を構築・デプロイするのに皆、Swift 入れてないのね。仲間はずれ 感たっぷりだけど、一番安定して動くと思ってる。
これを読んで、自宅にオブジェクトストレージを置いちゃおぅ。
構成は ?&amp;hellip; +--------+ | client | +--------+ | +-------------+ | swift-proxy | +-------------+ 172.16.0.10 | +-------------------+-------------------+ 172.16.0.0/24 | | | +-----------------+ +-----------------+ +-----------------+ | swift-storage01 | | swift-storage02 | | swift-storage03 | +-----------------+ +-----------------+ +-----------------+ 172.16.0.11 172.16.0.12 172.16.0.13 となる。IP アドレスは&amp;hellip;
client : 172.16.0.0/24 のどこか swift-proxy : 172.16.0.10 swift-storage01 : 172.16.0.11 swift-storage02 : 172.</description>
    </item>
    
    <item>
      <title>Secret Training of Opscode Chef</title>
      <link>https://jedipunkz.github.io/post/2012/10/06/secret-training-of-opscode-chef/</link>
      <pubDate>Sat, 06 Oct 2012 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2012/10/06/secret-training-of-opscode-chef/</guid>
      <description>昨日、開かれた &amp;ldquo;Opscode Chef のシークレットトレーニング&amp;rdquo; に参加してきました。
場所はうちの会社で KDDI ウェブコミュニケーションズ。主催はクリエーションオンラ インさんでした。講師は Sean OMeara (@someara) さん。今後 Chef のトレーニングを 日本で開くため、事前に内容についてフィードバックが欲しかったそうで、オープンな レッスンではありませんでしたが、次回以降、日本でも期待できそうです。
内容は chef の基本・メリット・考え方などを網羅した資料で1時間程進められ、その 後はハンズオンがメインでした。今日は実際にハンズオンの内容を書いていこうかと思 います。
chef workstation 環境は揃っている前提にします。また chef server として opscode の hosted chef (opscode が提供している chef のホスティングサービス, chef-server として動作します) を使います。またターゲットホストは当日は ec2 イ ンスタンスを使いましたが、chef ワークステーションから到達できるホストであれば 何でも良いでしょう。
まずは chef-repo のクローン。講習会で使われたものです。
git clone https://github.com/opscode/chef-repo-workshop-sysadmin.git chef-repo 予め cookbook が入っています。
次に、manage.opscode.com へアクセスしアカウントを作ります。Free アカウントが誰 でも作れるようになっています。
https://manage.opscode.com へアクセス -&amp;gt; Sign Up をクリック -&amp;gt; アカウント情報 を入力 -&amp;gt; submit -&amp;gt; メールにて verify -&amp;gt; 自分のアカウント名をクリック -&amp;gt; Get a new key をクリックし &amp;lt;アカウント名&amp;gt;.</description>
    </item>
    
    <item>
      <title>第7回 OpenStack 勉強会参加レポート</title>
      <link>https://jedipunkz.github.io/post/2012/08/29/7th-openstack-meetup/</link>
      <pubDate>Wed, 29 Aug 2012 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2012/08/29/7th-openstack-meetup/</guid>
      <description>第7回 OpenStack 勉強会に参加してきました。
開催日 : 2012年08月28日 開催場所 : 天王洲アイル ビットアイル 1年以上前から OpenStack, CloudStack 界隈はウォッチしていたのだけど、実際に構築 してってなると、今月始めばかりで、OpenStack も先週4日間掛けてやっとこさ構築出来たっ てところ&amp;hellip;orz。前回のブログ記事でへなちょこスクリプト公開しちゃったのを後悔しつ つ現地に向かいましたw あと、その他に Opscode Chef 等の技術にも興味持って調査し ていたので、今回の勉強会はまさに直ぐに活かせる内容だった。
では早速、報告があった内容と自分の感想を交えつつ書いていきます。
HP さんのクラウドサービス HP Cloud Services 日本 HP 真壁さま HP さんは既に Public クラウドサービスを提供し始めていて Ojbect Storage, CDN 部 分は既にリリース済みだそうだ。compute, block storage 等はベータ版状態でこれか らリリース。OpenStack ベースな構成で Horizon 部分は自前で開発したもの。既 にサーバ数は万の桁まで到達！ MySQL な DaaS も登場予定だとか。
あと HP だけにクラウドサービスに特化したサーバ機器も出していて、それが HP Project Moonshot 。ARM/Atom 搭載のサーバで 2,880 nodes/rack が可能だとか！す げぇ。もちろん電源等のボトルネックとなるリソースは他にも出てきそうだけど。
ノード数って増えると嬉しいのかな？コア数が増えるのは嬉しいけど。
Canonical JuJu Canonical 松本さま JuJu は Canonical が提供しているデプロイツールで charms と呼ばれるレシピ集 (っ て言うと語弊があるのか) に従ってソフトウェアの配布を行うツール。MAAS という物 理サーバのプロビジョニングツールと組み合わせればハードウェアを設置した後のプロ ビジョニング操作は一気通貫出来る、といったもの。具体的な操作例を挙げてくれたの で添付してきます。</description>
    </item>
    
    <item>
      <title>OpenStack ESSEX オールインワン インストール</title>
      <link>https://jedipunkz.github.io/post/2012/08/26/all-in-one-openstack-installation/</link>
      <pubDate>Sun, 26 Aug 2012 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2012/08/26/all-in-one-openstack-installation/</guid>
      <description>OpenStack のインストールってしんどいなぁ、って感じて devstack http://devstack.org/ とかで構築して中を覗いていたのですが、そもそも devstack って再起動してしまえば何も起動してこないし、swift がインストールされないしで。 やっぱり公式のマニュアル見ながらインストールするしかないかぁって&amp;hellip;。感じてい たのですが&amp;hellip;。
http://docs.openstack.org/essex/openstack-compute/starter/os-compute-starterguide-trunk.pdf
このマニュアルの前提は、ネットワーク2セグメント・server1, server2 の計2台が前 提なのですが、環境作るのがしんどいので、オールインワンな構築がしたい！サーバ1台で OpenStack ESSEX を インストールしたい！で、シェルスクリプトを作ったのでそれを使ったインストール方法を紹介します。
ぼくの Thinkpad に OpenStack ESSEX をインストールしてブラウザで localhost に接続して いる画面です。ちゃんと KVM VM が起動して noVNC で接続できています。自己満足やぁ。
前提条件 Ubuntu Server 12.04 LTS amd64 がインストールされていること Intel-VT もしくは AMD−Vなマシン NIC が一つ以上ついているマシン /dev/sda6, /dev/sda7 (デバイス名は何でもいい) の2つが未使用で空いていること です。
構成 1 NIC を前提に eth0 と eth0:0 の2つを想定すると、こんな構成になります。eth0:0 は完全にダミーで IP アドレスは何でもいいです。br100 ブリッジデバイス上で VM が NW I/F を持ちます。floating range ってのは OpenStack で言うグローバル IP レン ジ。グローバルである必要は無いですが eth0 と同じレンジの IP アドレスを VM に付 与出来ます。/dev/sda6 が nova-volumes で /dev/sda7 が swift 。なので OS インス トール時に2つのデバイスを未使用で空けておいてください。</description>
    </item>
    
    <item>
      <title>chef-solo で学ぶ chef の基本動作</title>
      <link>https://jedipunkz.github.io/post/2012/08/18/chef-solo/</link>
      <pubDate>Sat, 18 Aug 2012 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2012/08/18/chef-solo/</guid>
      <description>仕事で Opesocd Chef の情報収集をしてたのですが、僕が感じるにこれはインフラエン ジニアの未来だと。逆に言うとインフラエンジニアの危機。AWS のようなクラウドサー ビスがあればアプリケーションエンジニアが今までインフラエンジニアが行っていた作 業を自ら出来てしまうからです。
インフラエンジニアなら身に付けるしかない！って僕が感じる Chef について chef-solo を通して理解するために情報まとめました。
chef には chef-server 構成で動作するものと chef-solo というサーバ無しで動作す るものがある。chef-server は構築するのが少し大変 (後に方法をブログに書きたい) なので今回は chef-solo を使ってみる。ちなみに Opscode が chef-server のホスティ ングサービスを展開している。彼らとしてはこちらがメイン。
chef-solo の入れ方 opscode が推奨している ruby-1.9.2 をインストールする。rvm は色々問題を招き寄せ るので rbenv を使って環境整えます。root ユーザ環境内に入れてください。
必要なパッケージをインストール
% sudo apt-get update % sudo apt-get install build-essential zlib1g-dev libssl-dev root ユーザにてrbenv をインストール
% sudo -i # cd ~ # git clone git://github.com/sstephenson/rbenv.git .rbenv # echo &#39;export PATH=&amp;quot;$HOME/.</description>
    </item>
    
    <item>
      <title>Opscode Bootstrap を使った Chef-Server 構築</title>
      <link>https://jedipunkz.github.io/post/2012/08/18/opscode-bootstrap-chef-server/</link>
      <pubDate>Sat, 18 Aug 2012 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2012/08/18/opscode-bootstrap-chef-server/</guid>
      <description>chef-server の構築は少し面倒だと前回の記事 http://jedipunkz.github.com/blog/2012/08/18/chef-solo/ に書いたのですが、 opscode が提供している bootstrap を用いると、構築作業がほぼ自動化出来ます。 今回はこの手順を書いていきます。
chef のインストール 前回同様に rbenv を使って ruby をインストールし chef を gem でインストールして いきます。
% sudo apt-get update % sudo apt-get install zlib1g-dev build-essential libssl-dev % sudo -i # cd ~ # git clone git://github.com/sstephenson/rbenv.git .rbenv # echo &#39;export PATH=&amp;quot;$HOME/.rbenv/bin:$PATH&amp;quot;&#39; &amp;gt;&amp;gt; ~/.zshrc # echo &#39;eval &amp;quot;$(rbenv init -)&amp;quot;&#39; &amp;gt;&amp;gt; ~/.zshrc ruby-build をインストールします。
# mkdir -p ~/.rbenv/plugins # cd ~/.rbenv/plugins # git clone git://github.com/sstephenson/ruby-build.git ruby-1.</description>
    </item>
    
    <item>
      <title>#DevLOVE に参加してきました。Chef De DevOps</title>
      <link>https://jedipunkz.github.io/post/2012/07/22/number-devlove-nican-jia-sitekimasita.-chef-de-devops/</link>
      <pubDate>Sun, 22 Jul 2012 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2012/07/22/number-devlove-nican-jia-sitekimasita.-chef-de-devops/</guid>
      <description>2012年07月21日に大崎のフィーチャーアーキテクトさんで行われた #DevLOVE (Chef De DevOps) に参加してきました。
開催日 : 2012年07月21日(土曜日) 15:00 - 20:40 場所 : 大崎 フューチャーアーキテクトさま URL : http://www.zusaar.com/event/314003 仕事場でも Chef の利用を考え始めていて今回いい機会でした。半年前と比べるとだい ぶ揃って来ましたが、まだまだ資料の少ない Chef。貴重な機会でした。
プログラムは下の通り。
*『Chefの下準備』by Ryutaro YOSHIBA [ @ryuzee ] *『Chef自慢のレシピ披露』 by 中島弘貴さん [ @nakashii_ ] * ワークショップ『みんなでCooking』 * dialog『試食会』 * Niftyさんのスーパー宣伝タイム!!! *『渾身会』 @ryuzee さんの &amp;ldquo;Chef の下準備&amp;rdquo; は SlideShare に使った資料が公開されています。
20120721 chefの下準備 #devlove from Ryuzee YOSHIBA 印象的だったのが、VirtualBox + Vagrant という環境。実際に使っていらっしゃいま した。MacBook の中に仮想環境とそのインターフェースである Vegrant を使って、デ プロイのテスト等が実施できるそうです。また、Vegrant 設定ファイルは Chef のレシ ピを自動読込して、常に本番環境と同じ状態にしているそうです。CloudFormation と いうキーワードや Capistrano というキーワードが出てきました。最近よく耳にするワー ドです。また CI は Jenkins だよね、だったり。</description>
    </item>
    
    <item>
      <title>DNS の猛威とその対策, 参加レポ #interop2012</title>
      <link>https://jedipunkz.github.io/post/2012/06/16/dns-report-interop2012/</link>
      <pubDate>Sat, 16 Jun 2012 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2012/06/16/dns-report-interop2012/</guid>
      <description>interop 2012 で &amp;lsquo;DNS の猛威とその対策&amp;rsquo; を傍聴してきました。簡単にレポート書い ておきます。ちょっと油断しただけで大きな問題のアップデートが出てくる DNS。怖い です..。
本講義の概要 ブラジルで大規模な ISP への DNS ポイゾニング攻撃が発生。それら猛威について理解 すると同時に技術的対応策や具体的な対応プロセスについて説明。
イントロダクション : インターネットエクスチェンジ 石田さん DNS を取り巻く状況
DNS を乗っ取って悪事を働く試み : DNS Changer DNS そのものをセキュアにする方向性 : DNSSEC DNS に様々な制御を任せる方向性 : SPF, AAAA, DANE, 児童ポルノブロッキング 事例 : IIJ 松崎さん 2011/11 ブラジルの事例 著名サイトへのアクセスを行うと malware が仕込まれたサイトへ誘導。ホームルータ のキャッシュがポイゾニングされた。
とあるホームルータの問題 admin パスワードが管理 web で見える wan からのアクセスが有効 同じチップセットを使っている製品で同様の問題.. wan からルータへアクセスすると html ソースにアカウント情報が平文で書かれてい た これは怖い..。
攻撃活動の実施 攻撃者が行う手順。
脆弱性な CPE 発見 パスワード書き換え CPE が参照する DNS の書き換え 著名サイト向け DNS への応答を書き換え malware サイトへ誘導 銀行の安全客員ツールを無効にする malware をインストールさせる 幾つかの銀行向け DNS 応答を書き換え、目的のフィッシングサイトに誘導。DNS 書 き換えは短い期間のみであった。 規模 2011年時点、450万の CPE の DNS が書き換えられていた。今年も 30万以上の CPE が 影響を受けたまま。</description>
    </item>
    
    <item>
      <title>Vyatta ハンズオン参加レポ #interop2012</title>
      <link>https://jedipunkz.github.io/post/2012/06/14/vyatta-handson-interop2012/</link>
      <pubDate>Thu, 14 Jun 2012 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2012/06/14/vyatta-handson-interop2012/</guid>
      <description>interop 2012 で開催された &amp;ldquo;仮想ルータ Vyatta を使ったネットワーク構築法&amp;rdquo; に参 加してきました。簡単ですがレポートを書いておきます。
開催日 : 2012/06/14(木) 場所 : 幕張メッセ Interop 2012 最初に所感。反省です。サーバエンジニアの視点でしか感じられていなかった。次期バー ジョンの vPlane 実装などエンタープライズ向けとしても利用出来る可能性を感じる し、比較的小さなリソースでハンズオン参加者30人程度の vyatta ルータを動かしてサ クサク動いているのを見て、簡単にパフォーマンスがどうとか 前回の記事で言うべ きじゃなかったぁ。ホント反省。
ではハンズオンの内容。
最初に、基本情報の話 有償版と無償版の違い
メジャーリリースのみの無償版に対して有償版はマイナーリリースもあり 有償版は保守あり 仮想 image template 機能が有償版であり API, Web GUI, Config Sync, Systen Image Cloning が有償版であり image template, config sync, cloning など、有償版ではあると嬉しい機能がモリモ リ。
最近の利用ケース Vyatta + VM で VPN 接続環境構築 キャンパスネットワーク キャンパスネットワークのユースケースが一番多いそうだ。また、会場内にアンケート をとった結果、仮想環境での構築を想定されている方が多数だった。
構成と特徴 Debian Gnu/Linux ベース Quagga, StrongSwam が内部で動作 apt-get など馴染み深いコマンドが使えます。 http://www.</description>
    </item>
    
    <item>
      <title>Vyatta で構築する簡単 VPN サーバ</title>
      <link>https://jedipunkz.github.io/post/2012/06/13/vyatta-vpn/</link>
      <pubDate>Wed, 13 Jun 2012 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2012/06/13/vyatta-vpn/</guid>
      <description>Vyatta で VPN しようと思ったら信じられないくらい簡単に構築できたので共有します。
今回は PPTP (Point-to-Point Tunneling Protocol) を用いました。
さっそく手順に。
$ configure # set vpn pptp remote-access authentication local-users username ${USER} password ${PASSWORD} # set vpn pptp remote-access authentication mode local # set vpn pptp remote-access client-ip-pool start ${IP_START} # set vpn pptp remote-access client-ip-pool stop ${IP_END} # set vpn pptp remote-access outside-address ${GLOBAL_IP} # set vpn pptp remote-access dns-servers server-1 8.8.8.8 # set vpn pptp remote-access dns-servers server-1 8.</description>
    </item>
    
    <item>
      <title>NetBSD インストール直後の初期設定まとめ</title>
      <link>https://jedipunkz.github.io/post/2012/05/12/netbsd-first-setup/</link>
      <pubDate>Sat, 12 May 2012 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2012/05/12/netbsd-first-setup/</guid>
      <description>普段は Mac, Linux がメインなのですが、NetBSD もたまにデスクトップ機として利用 するので、初期設定手順をまとめ。
Console 設定, キーリマップ Caps と Ctrl キーを入れ替えます。お約束。
# wsconsctl -w encoding=us.swapctrlcaps # vi /etc/wscons.conf # 同様の設定を入れる キーリピートを速くします。標準では遅すぎる。値は好みで。
# wsconsctl -w repeat.del1=300 # リピートが始まるまでの時間 # wsconsctl -w repeat.deln=40 # 反復間隔 # vi /etc/wscons.conf # 同様の値を入れる pkgsrc をインストール xx, y は次期に応じて変える。例 :2012Q1
# cd /usr # cvs -q -z3 -d anoncvs@anoncvs.NetBSD.org:/cvsroot checkout -r pkgsrc-20xxQy -P pkgsrc 最新の状態に更新する場合。リリースタグを利用したいなら不要。
# cd /usr/pkgsrc # cvs update -dP grub のインストール grub をインストール。デフォルトでも良いのだけど、安心したいから。他の OS とデュ アル・トリプルブートに設定することが多いので必要を感じて入れている場合もある。 もちろん Linux 側からインストールしても OK。</description>
    </item>
    
    <item>
      <title>Emacs でファイルブラウザ emacs-nav を利用</title>
      <link>https://jedipunkz.github.io/post/2012/05/04/emacs-nav/</link>
      <pubDate>Fri, 04 May 2012 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2012/05/04/emacs-nav/</guid>
      <description>Hacker News で取り上げられていた emacs-nav を使ってみた。
http://code.google.com/p/emacs-nav/
インストール方法は簡単で
% wget http://emacs-nav.googlecode.com/files/emacs-nav-20110220a.tar.gz % tar zxvf emacs-nav-20110220a.tar.gz % mv emacs-nav-20110220a ~/.emacs.d/emacs-nav して
;; emacs-nav (add-to-list &#39;load-path &amp;quot;~/.emacs.d/emacs-nav/&amp;quot;) (require &#39;nav) するだけ。
見た目はこんな感じ。
起動は M-x nav と入力。ウィンドウの左にファイルブラウザが開いてファイルを選択 出来る。これだけだと、使うメリットを感じないが、面白いのがマウスで選択出来ると ころ。TextMate のブラウザのような感じだ。
今だと anything.el が便利すぎて、こちらを利用する価値を見出せるか分からないけ ど、暫く使ってみようと思う。</description>
    </item>
    
    <item>
      <title>powerline.el で emacs モードラインを派手に</title>
      <link>https://jedipunkz.github.io/post/2012/05/04/powerline.el-emacs/</link>
      <pubDate>Fri, 04 May 2012 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2012/05/04/powerline.el-emacs/</guid>
      <description>http://www.emacswiki.org/emacs-en/PowerLine
vim の powerline に似たモードを emacs で実現できる powerline.el を試してみた。 見た目が派手でかわいくなるので、気に入った。w すでに下記のようなサイトで紹介さ れつつある。
http://d.hatena.ne.jp/kenjiskywalker/20120502/1335922233 http://n8.hatenablog.com/entry/2012/03/21/172928
インストールすると見た目はこんな感じになる。
うん、かわいい。
インストール方法は簡単。auto-install 環境が構築されているとして
auto-install-from-emacs-wiki powerline.el する。auto-install が入っていなければ http://www.emacswiki.org/emacs/powerline.el ここにある powerline.el をダウンロードして ~/.emacs.d/lisp/ 配下など path の通っ ている所に入れれば OK。
あとは .emacs 内には下記を追記するだけだ。
;; powerline.el (defun arrow-right-xpm (color1 color2) &amp;quot;Return an XPM right arrow string representing.&amp;quot; (format &amp;quot;/* XPM */ static char * arrow_right[] = { \&amp;quot;12 18 2 1\&amp;quot;, \&amp;quot;. c %s\&amp;quot;, \&amp;quot; c %s\&amp;quot;, \&amp;quot;. \&amp;quot;, \&amp;quot;.</description>
    </item>
    
    <item>
      <title>Vyatta で無線アクセスポイント</title>
      <link>https://jedipunkz.github.io/post/2012/05/04/vyatta-wireless-ap/</link>
      <pubDate>Fri, 04 May 2012 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2012/05/04/vyatta-wireless-ap/</guid>
      <description>自宅ルータを Vyatta で運用開始したのだけど、無線ルータ化ができたのでメモしておき ます。
まずはアクセスポイントとして稼働する無線カードの選定。下記の URL に Linux 系で 動作する無線カードチップ名の一覧が載っている。Vyatta は Linux 系ドライバを利用 しているので、この一覧が有効なはずだ。
http://linuxwireless.org/en/users/Drivers
Intel 系も結構アクセスポイントとしては動作しないことが判ったので Atheros の AR9k のカードを購入した。私が買ったのはAR9280 チップの mini PCI-E 無線カード。
早速装着してみると、認識した！
% dmesg | grep ath [ 11.528390] ath9k 0000:01:00.0: PCI INT A -&amp;gt; GSI 16 (level, low) -&amp;gt; IRQ 16 [ 11.528398] ath9k 0000:01:00.0: setting latency timer to 64 [ 11.961619] ath: EEPROM regdomain: 0x37 [ 11.961620] ath: EEPROM indicates we should expect a direct regpair map [ 11.</description>
    </item>
    
    <item>
      <title>vyatta で UPnP 接続</title>
      <link>https://jedipunkz.github.io/post/2012/04/29/vyatta-upnp/</link>
      <pubDate>Sun, 29 Apr 2012 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2012/04/29/vyatta-upnp/</guid>
      <description>Vyatta を自宅ルータで使い始めて感じたのは、PS3 などのゲーム機や IP 電話など UPnP 接続が必要なことがあるってこと。ただ Vyatta は UPnP に対応していないので、 どうしようかと思っていたら、有志の方が作ってくれたソフトウェアがあり、うちでも これを使うことにした。今回はその方法を記していきます。
https://github.com/kiall/vyatta-upnp
上記のソースを取得して生成するのだが、vyatta 上で構築する環境を作りたくないの で、私は Debian Gnu/Linux マシン上で行いました。Ubuntu でも大丈夫だと思います。
debian% sudo apt-get &amp;amp;&amp;amp; sudo apt-get install build-essential debian% git clone https://github.com/kiall/vyatta-upnp.git debian% cd vyatta-upnp debian% dpkg-buildpackage -us -uc -d 一つ上のディレクトリに vyatta-upnp_0.2_all.deb という .deb ファイルができあがっ ているはずで、これが UPnP パッケージファイル vyatta-upnp_0.2_all.deb です。
次に vyatta 上での作業。packages.vyatta.com から libupnp4 と linux-igd を取得、 その後先ほど生成した vyatta-upnp_0.2_all.deb を vyatta 上に持ってきてからイン ストールします。
vyatta# cd /tmp/ vyatta# wget http://packages.</description>
    </item>
    
    <item>
      <title>vyatta で自宅ルータ構築</title>
      <link>https://jedipunkz.github.io/post/2012/04/28/vyattarouter/</link>
      <pubDate>Sat, 28 Apr 2012 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2012/04/28/vyattarouter/</guid>
      <description>自宅ルータを Vyatta で構築してみたくなり、秋葉原の ark でマシンを調達しました。 Broadcom の BCM57780 チップが搭載された NIC がマザーボード J&amp;amp;W MINIX™ H61M-USB3 だったのですが、Vyatta.org によると Broadcom の NIC が Certificated Hardware に載っていなくて心配でした。まぁ定評のある NIC メーカだから動くだろう と楽観視していたのですけど、案の定動きました。vyatta.org の Certificated Hardware にコミットしたら &amp;ldquo;user tested&amp;rdquo; として掲載してもらえました。
http://www.vyatta.org/hardware/interfaces
こんな感じに見えています。
# dmesg | grep Broadcom [ 3.284646] tg3 0000:03:00.0: eth0: attached PHY driver [Broadcom BCM57780](mii_bus:phy_addr=300:01) [ 3.524122] tg3 0000:05:00.0: eth1: attached PHY driver [Broadcom BCM57780](mii_bus:phy_addr=500:01) 今回は、基本的な設定 (PPPoE, NAT, DHCP) 周りを記していきます。
環境は&amp;hellip;
+--------+ | Modem | +--------+ | | pppoe0 +--eth0--+ | vyatta | +--eth1--+ | 192.</description>
    </item>
    
    <item>
      <title>Mosh を使う</title>
      <link>https://jedipunkz.github.io/post/2012/04/14/ssh-mosh/</link>
      <pubDate>Sat, 14 Apr 2012 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2012/04/14/ssh-mosh/</guid>
      <description>今週/先週？、Hacker News で取り上げられた Mosh を自宅 と会社で使い始めた。SSH 代替なソフトウェアで、SSP (State Synchronization Protocol)over UDP で動作している。MIT が開発したそうだ。
動作は、クライアントがシーケンス番号と共にデータグラムをサーバに送信し、同期し 続ける。クライアントがローミングし IP アドレスが代わる等した時、以前より大きい シーケンス番号と共に正当なパケットが送信されたとサーバが認識した場合のみ、サー バは新しいソース IP アドレスを新たなクライアントだと認識する。もちろん、この場 合のローミングは NAT 越しの IP 再アサイン時やクライアントのネットワークインター フェース切り替えやノート PC を新たな無線アクセスポイント配下へ移動した場合も同 様に動作する。めちゃ便利やん。Mosh は SSP を2経路持ち、1つはクライアントからサー バへユーザの打ったキーの同期を取る。もう一方はサーバからクライアントへで、スク リーンの状態をクライアントへ同期を取るためだ。
つまり、ノート PC やその他モバイル機器の IP アドレスが変わったとしても接続性は 担保され、また ノート PC のスリープ解除後にも接続性は確保され続ける。また、UDP で動作しているので、フルスクリーンの vim や emacs 等での再描画の遅延等も起こり にくそうだ。あと Ctrl-C 。TCP だと、キータイプがサーバプログラムに伝わらない状 況はプログラムプロセスが混雑しているとよくあるのだが、SSP over UDP での Ctrl-C はそういうことが無いそうだ。
また、認証機構は SSH に任せているので sshd は引き続き稼働させておく必要がある。 mosh は接続する先のユーザが一般ユーザ権限で動作させるプログラムでしかない。つ まり mosh daemon は必要ないようだ。</description>
    </item>
    
    <item>
      <title>debian sid on thinkpad</title>
      <link>https://jedipunkz.github.io/post/2012/04/07/debian-sid-on-thinkpad-first-setup/</link>
      <pubDate>Sat, 07 Apr 2012 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2012/04/07/debian-sid-on-thinkpad-first-setup/</guid>
      <description>ノート PC を購入するといつも Debian Gnu/Linux sid をインストールするのだけれど も、X Window System や InputMethod をインストールして利用し始められるところま での手順っていつも忘れる。メモとしてブログに載せておきます。
console 上での ctrl:caps swap 設定 取りあえずこの設定をしないと、何も操作出来ない。Caps Lock と Control キーを入 れ替える設定です。
/etc/default/keyboard を下記のように修正
XKBOPTIONS=&amp;quot;ctrl:swapcaps&amp;quot; 下記のコマンドで設定を反映。
% sudo /etc/init.d/console-setup restart sid の sources.list 設定 Debian Gnu/Linux をノート PC にインストールする時は必ず sid を入れます。新し目 のソフトウェアを使いたいから。
deb http://ftp.riken.jp/Linux/debian/debian/ unstable main contrib non-free deb-src http://ftp.riken.jp/Linux/debian/debian/ unstable main contrib non-free 下記のコマンドで dist-upgrade
% sudo apt-get update % sudo apt-get dist-upgrade iwlwifi のインストールとネットワーク設定 買うノート PC はいつも ThinkPad。大体 intel チップな Wi-Fi モジュールが搭載さ れているので、iwlwifi を使う。</description>
    </item>
    
    <item>
      <title>switching screen-&gt;tmux</title>
      <link>https://jedipunkz.github.io/post/2012/04/01/switching-screen-tmux/</link>
      <pubDate>Sun, 01 Apr 2012 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2012/04/01/switching-screen-tmux/</guid>
      <description>長年 Gnu screen 愛用者だったのだけど完全に tmux に移行しました。
愛用している iterm2 との相性も良く、好都合な点が幾つかあり移行する価値がありました。
ただ、サーバサイドでの利用は諦めました。問題だったコピペ問題をクリアしている tmux のバージョンが Debian sid から取得出来たのだけど、まだまだ完成度高くなく..。
よって、Mac に tmux をインストールして作業するようになりました。インストール方法はこれ。
予め https://github.com/mxcl/homebrew/wiki/installation に したがって homebrew をインストールする必要あり。
% brew update % brew install tmux インストールしたら .tmux.conf の作成に入る。prefix キーは C-t にしたかった。screen 時代から これを使っていて指がそう動くから。
# prefix key set-option -g prefix C-t またステータスライン周りの設定。色なども自分で選択すると良い。
# view set -g status-interval 5 set -g status-left-length 16 set -g status-right-length 50 # status set -g status-fg white set -g status-bg black set -g status-left-length 30 set -g status-left &#39;#[fg=white,bg=black]#H#[fg=white]:#[fg=white][#S#[fg=white]][#[default]&#39; set -g status-right &#39;#[fg=white,bg=red,bold] [%Y-%m-%d(%a) %H:%M]#[default]&#39; # window-status-current setw -g window-status-current-fg white setw -g window-status-current-bg red setw -g window-status-current-attr bold#,underscore # pane-active-border set -g pane-active-border-fg black set -g pane-active-border-bg blue UTF-8 有効化やキーバインド設定等は&amp;hellip;</description>
    </item>
    
    <item>
      <title>github.com で octopress 構築</title>
      <link>https://jedipunkz.github.io/post/2012/03/20/github-dot-com-de-octopress-gou-zhu/</link>
      <pubDate>Tue, 20 Mar 2012 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2012/03/20/github-dot-com-de-octopress-gou-zhu/</guid>
      <description>pages.github.com は github.com の WEB ホスティングサービスです。これを利用して octopress のブログを 構築する方法をメモしていきます。
まず、github.com に &amp;ldquo;${好きな名前}.github.com&amp;rdquo; という名前のレポジトリを github.com 上で作成します。 レポジトリの作成は普通のレポジトリ作成と同じ方法で行えます。しばらくすると &amp;ldquo;${好きな名前}.github.com のページがビルド出来ました&amp;rdquo; という内容でメールが送られてきます。
pages.github.com によると、レポジトリページで &amp;ldquo;GitHub Page&amp;rdquo; にチェックを入れろと書いてありますが、情報が古いようです。 2012/03/20 現在、この操作の必要はありませんでした。
次に octopress の環境構築。
octopress は、jekyll ベースのブログツールです。markdown 形式で記事を書くのですが、emacs や vim 等 好きなエディタを使って記事を書けるので便利です。最近 &amp;ldquo;Blogging with Emacs&amp;rdquo; なんてブログをよく目にしたと 思うのですが、まさにソレですよね。エンジニアにとっては嬉しいブログ環境です。
まずは、rvm の環境構築を。octopress は ruby 1.9.2 以上が必要なので用意するのですが rvm を使うと 手軽に用意出来るので、今回はその方法を記します。
参考 URL は http://octopress.org/docs/setup/rvm/ です。
まずは準備から。私の環境は Ubuntu Server 10.04 LTS なのですが、下記のパッケージが必用になります。
% sudo apt-get install gcc make zlib1g-dev libssl-dev 下記のコマンドを実行すると、rvm がインストールされます。</description>
    </item>
    
    <item>
      <title>cocoa な emacs インストール</title>
      <link>https://jedipunkz.github.io/post/2012/03/07/cocoa-na-emacs-insutoru/</link>
      <pubDate>Wed, 07 Mar 2012 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2012/03/07/cocoa-na-emacs-insutoru/</guid>
      <description>Carbon な API は排除していくべきと Apple も言っているようですし、自宅も会社も Cocoa な emacs を使うようになりました。
その手順を書いていきます。
ソースとパッチでビルドも出来るのですが、homebrew 使うとメチャ楽なので今回はそれを使います。 homebrew は公式サイトに詳しいことが書いてありますけどインストールがワンラインで済みます。 あと、事前に AppStore で Xcode を入れてください。
% /usr/bin/ruby -e &amp;quot;$(curl -fsSL https://raw.github.com/gist/323731)&amp;quot; だけです。
そして Cocoa な emacs インストール。
% brew install --cocoa emacs % sudo mv /usr/local/Cellar/emacs/24.1/Emacs.app /Applications/ 以上です..。簡単すぎる。先人たちのおかげですね。
次は時間見つけて anything.el のことを書こうかなぁと思ってます。
参考 URL : http://mxcl.github.com/homebrew/</description>
    </item>
    
    <item>
      <title>conky statusbar</title>
      <link>https://jedipunkz.github.io/post/2012/03/07/conky-statusbar/</link>
      <pubDate>Wed, 07 Mar 2012 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2012/03/07/conky-statusbar/</guid>
      <description>上の画像は conky というツールのキャプチャです。
conky は x window で使える linux マシンのステータスを文字・グラフ描画で表現してくれるツールです。
透明にしたりグラフ表示を派手にすることも出来るのだけど、わたしは上図のようにステータスバーとして使ってます。Window Manager に openbox という素っ気ないものを使うようにしてるので、これ自体がファイラーもステータスバーも無いんです。なので conky を利用して &amp;lsquo;時間&amp;rsquo;, &amp;lsquo;バッテリ残量&amp;rsquo;, &amp;lsquo;AC アダプタ有無&amp;rsquo;, &amp;lsquo;ネットワーク使用量&amp;rsquo; 等を表示してます。
deviantart に rent0n86 さんという方が投稿した作品があって、それをこちょこちょ自分用にいじって使ってます。
debian gnu/linux な GUI 環境があれば&amp;hellip;
% sudo apt-get conky-all % cd $HOME % wget https://raw.github.com/chobiwan/dotfiles/master/.conkyrc で、この環境を作れます。
表示する内容は環境に合わせて修正すると楽しいです。幅は minimum_size パラメータで合わせてください。
パラメータ一覧は、公式 Wiki サイト に正しい情報が載っています。
話変わるけど、enlightenment 17 が完成度高くならない理由ってなんなのでしょうかね？ 16 を愛用していただけに残念。</description>
    </item>
    
    <item>
      <title>gitosis ssh&#43;git サーバ</title>
      <link>https://jedipunkz.github.io/post/2012/03/07/gitosis-ssh-plus-git-saba/</link>
      <pubDate>Wed, 07 Mar 2012 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2012/03/07/gitosis-ssh-plus-git-saba/</guid>
      <description>github.com は便利なのだけどプライベートなレポジトリを作るのにお金払うのはもったいないので自宅サーバに SSH 経由の Git サーバを構築した。その時の手順をメモしておきます。
gitosis という便利なツールがあって、これを使うとあっという間に環境構築できます。私の環境は debian Gnu/Linux Squeeze なのですが apt-get で必要なモノを入れました。gitosis は git で持ってきます。
remote% sudo apt-get update remote% sudo apt-get install git git-core python python-setuptools remote% cd $HOME/usr/src remote% git clone git://eagain.net/gitosis.git remote% cd gitosis remote% sudo python setup.py install SSH でアクセスする先のユーザを作ります。
remote% sudo adduser --shell /bin/sh -gecos --group \ --disable-password --home /home/git git 作業端末で rsa な SSH 公開鍵を生成して ${remote} サーバは転送する。
local% ssh-keygen -t rsa ... インタラクティブに答える local% scp .</description>
    </item>
    
    <item>
      <title>Heroku JP Meetup #3</title>
      <link>https://jedipunkz.github.io/post/2012/03/07/heroku-jp-meetup-number-3/</link>
      <pubDate>Wed, 07 Mar 2012 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2012/03/07/heroku-jp-meetup-number-3/</guid>
      <description>白金台のクックパッドさんで行われた &amp;ldquo;heroku jp meet up #3&amp;rdquo; に参加してきました。
東京マラソン参加のため来日されていた Christopher Stolt さんや Ruby コミッタの相澤さんなどの話を聞けました。
Christopher さんからは、基本的な使い方や heroku で動作させたアプリケーションをローカル環境で動作させる foreman、また皆が意外と気にするアプリのログを tail する方法などの説明がありました。PaaS での皆の懸念点が結構解決されたんじゃないかなぁ。
相澤さんからは NY マラソンでの実績など、比較的エンタープライズな使われ方もされ初めていると説明がありました。あと、呼び名なのですが heroku は &amp;ldquo;へろく&amp;rdquo; と発音するそうです。確かに about.heroku.com には &amp;ldquo;Heroku (pronounced her-OH-koo) is a cloud application platform&amp;rdquo; と書いてあるのだが、&amp;ldquo;へろく&amp;rdquo; が正しいそうです。w
そのた LT が幾つあって、ちょうど気になっていた Lokka の話があったので、自宅に帰ってから自分の heroku アカウントで lokka を動かしてみました。lokka の公式サイトに手順が書いてあって、そのままなのですが行ったのは、
% gem install heroku bundler % git clone git://github.com/komagata/lokka.git % cd lokka % heroku create % git push heroku master % heroku rake db:setup % heroku open すれば OK。</description>
    </item>
    
    <item>
      <title>OpenFlow 勉強会</title>
      <link>https://jedipunkz.github.io/post/2012/03/07/openflow-mian-qiang-hui/</link>
      <pubDate>Wed, 07 Mar 2012 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2012/03/07/openflow-mian-qiang-hui/</guid>
      <description>2012年2月6日、西新宿にある株式会社ニフティさんで行われた &amp;ldquo;OpenFlow 勉強会&amp;rdquo; に参加したので簡単なレポメモを書いておきます。
まずは OpenFlow の基本動作。
メッセージの切り出し ハンドシェイク コネクションの維持 スイッチから送られてくるメッセージへの応答 構成は
OpenFlow コントローラ, スイッチから成る コントローラは通信制御 スイッチはフロールールをコントローラに問い合わせ通信を受け流し(packet/frame 転送) コントローラは L2 - L4 フィールドを見て制御する そしてコントローラ、スイッチは各社・団体から提供されている。今月も Nicira Networks さんが自社システムの構成を抽象的ではありますが公開され、HP さんも OpenFlow 対応スイッチを12製品ほど発表されました。
コントローラの種別、
Beacon (Java) NOX (Python) Ryu NodeFlow Trema (C/ruby) Nicira Networks Big Switch Networks Midokura NTT Data スイッチの種別、
cisco nexus 3000 IBM BNT rackSwitch G8264 NEC Univerge PF5240/PF5820 Pronto Systems 3240/3290 HP 3500/5400 HP OpenFlow 化 firmware Reference Implementation (Software) Open vSwitch (Software) NEC さんの PF ほにゃららは、GUI なインターフェースと API を持った製品で OpenFlow 1.</description>
    </item>
    
    <item>
      <title>USB Stick で Debian Gnu/Linux インストール</title>
      <link>https://jedipunkz.github.io/post/2012/03/07/usb-stick-de-debian-gnu-slash-linux-insutoru/</link>
      <pubDate>Wed, 07 Mar 2012 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2012/03/07/usb-stick-de-debian-gnu-slash-linux-insutoru/</guid>
      <description>MS Windows なツールを使う方法だったり、vmlinuz, initrd をファイラーでコピーしたり、何故かいつもインターネットで調べると USB スティックを利用した debian のインストール方法が&amp;rsquo;面倒&amp;rsquo;, &amp;lsquo;不確か&amp;rsquo; なので、忘れないようにメモ。
手元に linux 端末用意して、USB スティック挿す。
% wget ftp://ftp.jp.debian.org/pub/Linux/debian-cd/6.0.3/amd64/iso-cd/debian-6.0.3-amd64-netinst.iso % sudo cat debian-6.0.3-amd64-netinst.iso &amp;gt; /dev/sdb # 挿した USB スティックのデバイス名 で終わり。
ただ弱点があって、フルイメージの iso は利用できないでの、今回みたいに netinstall だったり businesscard な iso を利用しかない。 他のディストリビューションもだけど、インストールする環境はネットに繋がっていないと不都合があるって時代だからいいかなぁ。
一方、Ubuntu Server は賢い子なので 公式サイト に行くと USB スティック用の iso がダウンロードできたり iso を焼く環境に合わせて手順まで教えてくれる&amp;hellip;。</description>
    </item>
    
    <item>
      <title>WordPress を nginx &#43; fastcgi で高速化</title>
      <link>https://jedipunkz.github.io/post/2012/03/07/wordpress-wo-nginx-plus-fastcgi-degao-su-hua/</link>
      <pubDate>Wed, 07 Mar 2012 00:00:00 +0000</pubDate>
      
      <guid>https://jedipunkz.github.io/post/2012/03/07/wordpress-wo-nginx-plus-fastcgi-degao-su-hua/</guid>
      <description>ブログを始めるにあたり、wordpress 環境を構築する必要が出てきました。いつもの apache2 + mysql5 + PHP じゃつまらないので、nginx と fastcgi を使って少しだけ高速化してみました。メモですけど、ここに手順を記していきます。
※ wordpress から octopress に移行しました&amp;hellip; (2012/03/07)
ただ、今回は nginx や mysql の基本的なオペレーション手順は割愛させてもらいます。
私の環境について&amp;hellip;
% lsb_release -a No LSB modules are available. Distributor ID: Debian Description: Debian GNU/Linux 6.0.3 (squeeze) Release: 6.0.3 Codename: squeeze インストールしたもの&amp;hellip; メタパッケージを指定したのでその他必要なモノはインストールされます。
% sudo apt-get update % sudo apt-get install spawn-fcgi php5 php5-mysql php5-cgi mysql-server nginx まずはお決まりの gzip 圧縮転送。IE の古いモノ以外は対応しているので心配なし。今回のテーマと関係無いですけど、一応入れておきます。
% diff -u /etc/nginx/nginx.conf.org /etc/nginx/nginx.conf --- /etc/nginx/nginx.conf.org 2012-01-14 15:27:45.</description>
    </item>
    
  </channel>
</rss>
