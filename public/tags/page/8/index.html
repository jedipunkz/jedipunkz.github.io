


    




<!DOCTYPE HTML>

<html>
    <head>
        
            <title>Tags - ジェダイさんのブログ</title>
        

        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1" />
        <meta name="generator" content="Hugo 0.53" />
        


        
        
            
        

        <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Tags"/>
<meta name="twitter:description" content=""/>

        <meta property="og:title" content="Tags" />
<meta property="og:description" content="" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://jedipunkz.github.io/tags/" />
<meta property="og:updated_time" content="2013-06-20T00:00:00&#43;00:00"/>

        
<meta itemprop="name" content="Tags">
<meta itemprop="description" content="">


        

        

        
        
            
        

        
        
            <link rel="stylesheet" href="/css/google-font.css" />
            <link rel="stylesheet" href="/css/font-awesome.min.css" />
            <link rel="stylesheet" href="/css/main.css" />
            <link rel="stylesheet" href="/css/add-on.css" />
            <link rel="stylesheet" href="/css/monokai-sublime.css">
        

        

        
        
        
            
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-30563095-1', 'auto');
	
	ga('send', 'pageview');
}
</script>

        
    </head>
    <body>

        
        <div id="wrapper">

    
<header id="header">
    
        <h2><a href="/"></i></a></h2>
    

    <nav class="links">
        <ul>
            
                <li>
                    <a href="">
                        Blog
                    </a>
                </li>
            
                <li>
                    <a href="about/index.html">
                        About
                    </a>
                </li>
            
        </ul>
    </nav>
    <nav class="main">
        <ul>
            
            <li class="search">
                <a class="fa-search" href="#search">Search</a>
                <form id="search" method="get" action="//google.com/search">
                    <input type="text" name="q" placeholder="Search" />
                    <input type="hidden" name="q" value="site:https://jedipunkz.github.io">
                </form>
            </li>
            <li class="menu">
                <a class="fa-bars" href="#menu">Menu</a>
            </li>
        </ul>
    </nav>
</header>


<section id="menu">

    
        <section>
            <form class="search" method="get" action="//google.com/search">
                <input type="text" name="q" placeholder="Search" />
                <input type="hidden" name="q" value="site:https://jedipunkz.github.io">
            </form>
        </section>

    
        <section>
            <ul class="links">
                
                    <li>
                        <a href="">
                            <h3>
                                
                                Blog
                            </h3>
                        </a>
                    </li>
                
                    <li>
                        <a href="about/index.html">
                            <h3>
                                
                                About
                            </h3>
                        </a>
                    </li>
                
            </ul>
        </section>

    
        <section>
            <ul class="links">
                <header>
                    <h3>Recent Posts</h3>
                </header>
                
                    
                

                
                    <li>
                        <a href="https://jedipunkz.github.io/blog/2018/12/31/istio/"><p>Istio, Helm を使って Getting Started 的なアプリをデプロイ</p></a>
                    </li>
                
                    <li>
                        <a href="https://jedipunkz.github.io/blog/2017/07/02/test-kitchen-cluster/"><p>Docker,Test-Kitchen,Ansible でクラスタを構成する</p></a>
                    </li>
                
                    <li>
                        <a href="https://jedipunkz.github.io/blog/2017/04/13/gke-lb/"><p>GCP ロードバランサと GKE クラスタを Terraform を使って構築する</p></a>
                    </li>
                
                    <li>
                        <a href="https://jedipunkz.github.io/blog/2017/02/12/serverless-fission/"><p>Serverless on Kubernetes : Fission を使ってみた</p></a>
                    </li>
                
                    <li>
                        <a href="https://jedipunkz.github.io/blog/2017/01/13/kubernetes-deployments/"><p>Kubernetes Deployments を使ってみた！</p></a>
                    </li>
                
            </ul>
        </section>

    
        
</section>

    
    <div id="main">
        
        
            
        

        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2013/06/20/sensu-chef-controll/">Sensu 監視システムを Chef で制御</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2013-06-20'>
            June 20, 2013</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは。<a href="https://twitter.com/jedipunkz">@jedipunkz</a> です。</p>

<p>自動化の基盤を導入するために色々調べているのですが、監視も自動化しなくちゃ！と
いうことで Sensu を調べてたのですが Chef との相性バッチリな感じで、自分的にイ
ケてるなと思いました。</p>

<ul>
<li>公式サイト <a href="http://www.sonian.com/cloud-monitoring-sensu/">http://www.sonian.com/cloud-monitoring-sensu/</a></li>
<li>ドキュメント <a href="http://docs.sensuapp.org/0.9/index.html">http://docs.sensuapp.org/0.9/index.html</a></li>
</ul>

<p>開発元が予め Chef の Cookbook (正確にはラッパー Cookbook 開発のための
Cookbook で Include して使う) を用意してくれていたり、インストールを容易にする
ための Omnibus 形式のパッケージの提供だったり。Omnibus なのでインストールと共
に Sensu が推奨する Ruby 一式も一緒にインストールされます。Chef と同じですね。</p>

<p>今回紹介したいのは、Chef で Sensu を構築・制御する方法です。</p>

<pre><code>+--------------+ +--------------+
| chef-server  | | workstation  |
+--------------+ +--------------+
       |                |
       +----------------+ 
       |
+--------------+
| sensu-server |
+--------------+
       |
       +----------------+----------------+----------------+
       |                |                |                |
+--------------+ +--------------+ +--------------+ +--------------+
| sensu-client | | sensu-client | | sensu-client | | sensu-client | ..&gt;
+--------------+ +--------------+ +--------------+ +--------------+
| service node | | service node | | service node | | service node |
+--------------+ +--------------+ +--------------+ +--------------+
</code></pre>

<p>この構成の処理の流れとしては&hellip;</p>

<h4 id="sensu-server-sensu-client-の構築の流れ">sensu-server, sensu-client の構築の流れ</h4>

<ul>
<li>1. workstation から cookbook, role, data_bag を chef-server へアップ</li>
<li>2. workstation から sensu-server を bootstrap で構築</li>
<li>3. workstation から sensu-client を boostrrap で構築</li>
</ul>

<h4 id="監視項目の追加-無効化の流れ">監視項目の追加・無効化の流れ</h4>

<ul>
<li>4. workstation から監視項目 data bag の chef-server へのアップ</li>
<li>5. sensu-server 上の chef-client が chef-server から data bag の取得</li>
<li>6. sensu-server に新たな(削除された)監視項目が追加</li>
<li>7. sensu-client が新たな(削除された)監視項目を検知し、監視開始</li>
</ul>

<p>つまり&hellip; 運用で必要な操作 (監視対象の追加・監視項目の追加・無効化)を
workstation から knife を使って全て行えるっていうことです。しかも Chef も Sensu
も API を持っているので自動化を形成するプログラムの開発も容易です。実際に API
を叩くちょっとしたダッシュボードを Ruby on Rails で作ってみましたが簡単に出来
ました。</p>

<p>今回はこの構成の構築と操作方法について書いていきます。予め私のほうで作っておい
た sensu のための chef-repo を使って環境を作っていきます。</p>

<p><a href="https://github.com/jedipunkz/sensu-chef-repo">https://github.com/jedipunkz/sensu-chef-repo</a></p>

<p>この chef-repo には</p>

<ul>
<li>Berksfile</li>
<li>サンプルの監視項目 data bag</li>
<li>SSL 鍵生成の仕組み (後に data bag に収める)</li>
</ul>

<p>のみが入っています。公式の sensu-chef レポジトリ内にあったサンプルを利用して作っ
ています。また、Berksfile 内で、これまた</p>

<ul>
<li>chef-redis (redis の cookbook)</li>
<li>sensu-chef (公式の sensu cookbook, ラッパー cookbook 内で用いる)</li>
<li>chef-monitor (ラッパー cookbook の例)</li>
</ul>

<p>を取得するようにしています。それぞれ fork して私のレポジトリに置いています。動
く状態を保ちたかったためです。redis に関しては結構手を加えました。そのままでは
全く構築出来ない状態でしたので。chef-monitor は内部で sensu-chef (公式
cookbook) を Include しているラッパー cookbook です。公式 cookbook 内で例とし
て挙げられていたモノです。こちらは手を加えていません。</p>

<p>では手順を&hellip;</p>

<p>(chef 環境の構築方法は割愛します)</p>

<h2 id="sensu-server-のデプロイ">sensu-server のデプロイ</h2>

<p>sensu-chef-repo の取得を行います。ここからの操作は全て上図の workstation 上で
の操作になります。</p>

<pre><code class="language-bash">% git clone https://github.com/jedipunkz/sensu-chef-repo
</code></pre>

<p>SSL 鍵ペアを生成し data bag に投入します。</p>

<pre><code class="language-bash">% cd ~/sensu-chef-repo/data_bags/ssl
% ./ssl_certs.sh generate
% knife data bag create sensu
% knife data bag from file sensu ./ssl.json
</code></pre>

<p>サンプル監視項目 proc_cron.json を data bags に投入します。</p>

<pre><code class="language-bash">% cd ~/sensu-chef-repo/
% knife data bag create sensu_checks
% knife data bag from file sensu_checks data_bags/sensu_checks/proc_cron.json
</code></pre>

<p>proc_cron.json の内容は下記の通り</p>

<pre><code class="language-json">{
  &quot;id&quot;: &quot;proc_cron&quot;,
  &quot;command&quot;: &quot;check-procs.rb -p cron -C 1&quot;,
  &quot;subscribers&quot;: [
    &quot;sensu-client&quot;
  ],
  &quot;interval&quot;: 10
}
</code></pre>

<p>json の構成を説明すると..</p>

<ul>
<li>id : 監視項目名</li>
<li>command : agent が実行するコマンド</li>
<li>subscribers : 監視対象のグループ名, chef role 名が自動で監視対象に割り当てられる</li>
<li>interval : 監視間隔 (秒)</li>
</ul>

<p>となります。</p>

<p>Berkshelf を使って cookbook の取得を行います。</p>

<pre><code class="language-bash">% gem install berksfile --no-ri --no-rdoc
% berks install --path ./cookbooks/
</code></pre>

<p>roles を chef server へアップロードします。</p>

<pre><code class="language-bash">% knife role from file roles/sensu-client.rb # 後の sensu-client デプロイのためついでに準備します
% knife role from file roles/sensu-server.rb
</code></pre>

<p>&ldquo;master_address&rdquo; を sensu-server の IP アドレスに書き換えます。書き換える箇所
は &lsquo;monitor&rsquo; cookbook の attributes です。</p>

<pre><code class="language-bash">% ${EDITOR} cookbooks/monitor/attributes/default.rb
default[&quot;monitor&quot;][&quot;master_address&quot;] = &quot;XXX.XXX.XXX.XXX&quot;
</code></pre>

<p>cookbooks を chef server へアップロードします。</p>

<pre><code class="language-bash">workstation% knife cookbook upload -a
</code></pre>

<p>sensu-server を knife を用いてブートストラップします。</p>

<pre><code class="language-bash">% knife bootstrap &lt;server-ip&gt; -N &lt;server-name&gt; -r 'role[sensu-server]' -x root -i &lt;secret-key&gt;
</code></pre>

<p>sensu ダッシュボード URL : http://<server-ip>:8080 にアクセスし動作確認, アカ
ウント情報は下記の attributes に記載してあります。</p>

<pre><code>cookbooks/sensu/attributes/default.rb
</code></pre>

<h2 id="sensu-client-デプロイ方法">sensu-client デプロイ方法</h2>

<p>knife を用いて sensu-client をデプロイします。</p>

<pre><code class="language-bash">% knife bootstrap &lt;client-ip&gt; -N &lt;client-name&gt; -r 'role[sensu-client]' -x root -i &lt;secret-key&gt;
</code></pre>

<p>この状態で先ほどの &lsquo;proc_cron&rsquo; が監視開始されます。</p>

<h2 id="監視項目の追加方法">監視項目の追加方法</h2>

<p>下記のような json ファイルを生成します。ここでは例として nginx のプロセス監視
のための項目を追加してみます。</p>

<pre><code class="language-bash">% ${EDITOR} data_bags/sensu_checks/proc_nginx.json
</code></pre>

<pre><code class="language-json">{
  &quot;id&quot;: &quot;proc_nginx&quot;,
  &quot;command&quot;: &quot;check-procs.rb -p nginx -w 5 -c 10&quot;,
  &quot;subscribers&quot;: [
    &quot;sensu-client&quot;
  ],
  &quot;interval&quot;: 10
}
</code></pre>

<p>先ほども書きましたが subscribers は chef 的な role 名と一致しています。なので
sensu の監視をグルーピングしたいときは role 名を変えて knife bootstrap すると
良いと思います。ここでは例として &lsquo;sensu-client&rsquo; という先ほど利用した role 名を
用います。</p>

<p>data bags に監視項目情報を投入します。</p>

<pre><code class="language-bash">% knife data bag from file sensu_checks data_bags/sensu_checks/proc_nginx.json
</code></pre>

<p>暫くすると下記のプロセスを経て監視が開始される</p>

<ul>
<li>sensu-server 上の chef-client が interval 間隔後実行</li>
<li>sensu-server に proc_nginx.json が配置</li>
<li>sensu-server が chef により再起動</li>
<li>sensu-client 上の sensu agent が自らの subscribers が属している proc_nginx.json を検知</li>
<li>sensu-client が nginx のプロセス監視開始</li>
</ul>

<h2 id="まとめ">まとめ</h2>

<p>監視対象追加 (agent 仕込み), 監視項目追加を workstation 上から knife を使って
操作出来ました。監視項目の削除についてはダミーの json (command 項に &lsquo;echo ok&rsquo;
など設定) を投入することで、私は対処していますが、本来は data bag が存在しない
ことを検知して sensu-server 上から監視項目を削除する cookbook に仕上げなければ
いけないと思います。</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2013/06/20/sensu-chef-controll/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>

        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2013/06/12/chef-ruby-code/">Chef を Ruby コード内で利用する</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2013-06-12'>
            June 12, 2013</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは。<a href="https://twitter.com/jedipunkz">@jedipunkz</a> です。</p>

<p>require &lsquo;chef&rsquo; して Ruby コードの中で chef を利用したいと思って色々調べていた
のですが、そもそもリファレンスが無くサンプルコードもごくわずかしかネット上に見
つけられない状態でした。結局ソースコードを読んで理解していく世界なわけですが、
サンプルコードが幾つかあると他の人に役立つかなぁと思い、ブログに載せていこうか
なぁと。</p>

<p>まず Chef サーバへアクセスするためには下記の情報が必要です。</p>

<ul>
<li>ユーザ名</li>
<li>ユーザ用のクライアント鍵</li>
<li>Chef サーバの URL</li>
</ul>

<p>これらは Chef::Config で記していきます。</p>

<p>では早速サンプルコードです。まずは data bags 内データの一覧を取得するコードで
す。data bags 内のデータを全で取得し配列で表示します。</p>

<pre><code class="language-ruby">#!/usr/bin/env ruby

require 'rubygems'
require 'chef/rest'
require 'chef/search/query'

Chef::Config[:node_name]='user01'
Chef::Config[:client_key]='/home/user01/user01.pem'
Chef::Config[:chef_server_url]=&quot;https://10.200.9.22&quot;

Chef::DataBag::list.each do |bag_name, url|
  Chef::DataBag::load(bag_name).each do |item_name, url|
    item = Chef::DataBagItem.load(bag_name, item_name).to_hash
    puts item
  end
end
</code></pre>

<p>次は data bags にデータを入力するコードです。json_data という JSON 形式のデー
タを test_data という data bag に放り込んでいます。</p>

<pre><code class="language-ruby">#!/usr/bin/env ruby

require 'rubygems'
require 'chef/rest'
require 'chef/search/query'

Chef::Config[:node_name]='user01'
Chef::Config[:client_key]='/home/user01/user01.pem'
Chef::Config[:chef_server_url]=&quot;https://10.0.0.10&quot;

json_data = {
  &quot;id&quot; =&gt; &quot;test&quot;,
  &quot;command&quot; =&gt; &quot;echo test&quot;
}

databag_item = Chef::DataBagItem.new
databag_item.data_bag('test_data')
databag_item.raw_data = proc_nginx
databag_item.save
</code></pre>

<p>次は nodes 一覧の取得です。</p>

<pre><code class="language-ruby">#!/usr/bin/env ruby

require 'rubygems'
require 'chef/rest'
require 'chef/search/query'


Chef::Config[:node_name]='user01'
Chef::Config[:client_key]='/home/user01/user01.pem'
Chef::Config[:chef_server_url]=&quot;https://10.0.0.10&quot;

Chef::Node.list.each do |node|
  puts node
end
</code></pre>

<p>次は bootstrap するコード。</p>

<pre><code class="language-ruby">#!/usr/bin/env ruby
require 'rubygems'
require &quot;chef&quot;
require &quot;chef/knife/core/bootstrap_context&quot;
require 'chef/knife'
require 'chef/knife/ssh'
require 'net/ssh'
require 'net/ssh/multi'
require 'chef/knife/bootstrap'


Chef::Config[:node_name]='user01'
Chef::Config[:client_key]='/home/user01/user01.pem'
Chef::Config[:validation_key]='/home/user01/chef-validator.pem'
Chef::Config[:chef_server_url]=&quot;https://10.0.0.10&quot;

kb = Chef::Knife::Bootstrap.new
kb.name_args = [&quot;sensu-client04.deathstar.jp&quot;, &quot;10.0.0.20&quot;]
kb.config[:ssh_user] = &quot;root&quot;
kb.config[:identity_file] = &quot;~/novakey01&quot;
kb.config[:ssh_port] = &quot;22&quot;
kb.config[:run_list ] = &quot;role[sensu-client]&quot;
kb.config[:template_file] = &quot;/home/thirai/chef-full.erb&quot;
kb.run
</code></pre>

<p>以上です。他のサンプルもこれから探していこうかと思ってます。knife のソース見る
のが一番はやいかなぁと。もしくは Chef のテストコード見るか。皆さんもご存知であ
れば共有してくださーい。</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2013/06/12/chef-ruby-code/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>

        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2013/05/25/ceph-cluster-network/">Ceph クラスターネットワーク構成</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2013-05-25'>
            May 25, 2013</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは。<a href="https://twitter.com/jedipunkz">@jedipunkz</a> です。</p>

<p>Ceph を運用する上で考慮しなければいけないのがトラフィックの負荷です。特に OSD
同士のレプリケーション・ハートビートには相当トラフィックの負荷が掛かることが想
像出来ます。</p>

<p>このため MDS, MON の通信に影響を与えないよう、OSD レプリケーション・ハートビー
トのためのネットワークを別に設けるのがベストプラクティスな構成の様です。このネッ
トワークのことをクラスターネットワークと Ceph 的に言うそうです。</p>

<p>こんな接続になります。</p>

<pre><code>                +------+
                |Client|
                +------+
                |
+-------+-------+-------+-------+------ public network
|       |       |       |       |
+-----+ +-----+ +-----+ +-----+ +-----+
| MON | | MDS | | OSD | | OSD | | OSD |
+-----+ +-----+ +-----+ +-----+ +-----+
                |       |       |
----------------+-------+-------+------ cluster network
</code></pre>

<p>上図の様に MON, MDS は public ネットワークを介し OSD のレプリケーション・ハー
トビートのみ cluster ネットワークを介します。Client と MDS との通信に影響を与
えない構成になります。</p>

<p>今回はその様な構成を ceph-deploy を使って構築する方法を書いていきます。前提と
なるホストとプロセスとネットワークの関係は下記の図の通りです。</p>

<pre><code>+----------+
| 'client' |
+----------+
|
+---------------+---------------+------------- public network
|               |               |
|10.0.0.11      | 10.0.0.12     | 10.0.0.13
+----------+    +----------+    +----------+
| 'ceph01' |    | 'ceph02' |    | 'ceph03' |
|    osd   |    |    osd   |    |    osd   |
|    mon   |    |    mon   |    |    mon   |
|    mds   |    |    mds   |    |    mds   |
+----------+    +----------+    +----------+
|172.18.0.11    | 172.18.0.12   | 172.18.0.13
|               |               |
+---------------+---------------+------------- cluster network
</code></pre>

<p>特徴としては..</p>

<ul>
<li>ceph01-03 には NIC を2本出します。</li>
<li>ceph01-03 の全てに MDS, MON, OSD を稼働させます。</li>
<li>ceph01-03:/dev/sdb を Ceph 用のディスクとして利用</li>
</ul>

<p>となります。</p>

<h2 id="ceph-deply-を利用するまでの準備">Ceph-Deply を利用するまでの準備</h2>

<p>今回は ceph-deploy を利用し Ceph を構築する。そのための準備として下記の操作を
行う。</p>

<h4 id="ceph-サーバ-ceph01-03-の準備">ceph サーバ (ceph01-03) の準備</h4>

<p>&lsquo;ceph&rsquo; ユーザの作成を行う。</p>

<pre><code>% ssh user@ceph-server
% sudo useradd -d /home/ceph -m ceph
% sudo passwd ceph
</code></pre>

<p>sudoers の設定を行う。</p>

<pre><code>% echo &quot;ceph ALL = (root) NOPASSWD:ALL&quot; | sudo tee /etc/sudoers.d/ceph
% sudo chmod 0440 /etc/sudoers.d/ceph
</code></pre>

<h4 id="client-の準備">&lsquo;client&rsquo; の準備</h4>

<p>ホスト &lsquo;client&rsquo; で準備をします。この準備によって ceph-deploy をそれぞれのホス
トに対して実行できるようになります。</p>

<p>まず、ノンパスフレーズの SSH 公開鍵・秘密鍵を生成します。</p>

<pre><code>client% ssh-keygen
Generating public/private key pair.
Enter file in which to save the key (/ceph-client/.ssh/id_rsa):
Enter passphrase (empty for no passphrase):
Enter same passphrase again:
Your identification has been saved in /ceph-client/.ssh/id_rsa.
Your public key has been saved in /ceph-client/.ssh/id_rsa.pub.
</code></pre>

<p>公開鍵をターゲットホスト (ceph01-03) に配置します。</p>

<pre><code>client% ssh-copy-id ceph@ceph01
client% ssh-copy-id ceph@ceph02
client% ssh-copy-id ceph@ceph03
</code></pre>

<p>ceph-deploy を取得します。</p>

<pre><code>client% git clone https://github.com/ceph/ceph-deploy.git ~/ceph-deploy
</code></pre>

<p>&lsquo;python-virtualenv&rsquo; パッケージをインストールする。</p>

<pre><code>client% sudo apt-get update ; sudo apt-get -y install python-virtualenv
</code></pre>

<p>ceph-deploy をブートストラップする</p>

<pre><code>client% cd ~/ceph-deploy
client% ./bootstrap
</code></pre>

<p>PATH を通す。下記は例。</p>

<pre><code>client% ${EDITOR} ~/.zshrc
export PATH=$HOME/ceph-deploy:$PATH
</code></pre>

<p>ホスト名の解決を行う。</p>

<pre><code>cephclient% sudo ${EDITOR} /etc/hosts
10.0.0.11    ceph01
10.0.0.12    ceph02
10.0.0.13    ceph03
</code></pre>

<p>これで準備は OK です。ceph-deploy が使える状態になりました。</p>

<h2 id="ceph-構築手順">Ceph 構築手順</h2>

<p>今回は ceph01-03 の3台構成を構築しますが、すべての操作はホスト &lsquo;client&rsquo; から行
います。先ほど配置した公開鍵によってそれぞれのホストに対して操作が行えます。</p>

<p>ceph サーバ・クライアント間通信のための鍵の生成とコンフィギュレーションの生成
を下記の操作にて行う。</p>

<pre><code>client% ceph-deploy new ceph01 ceph02 ceph03
</code></pre>

<p>上記の操作で生成されたカレントディレクトリ上の ceph.conf に対して下記の記述を
追記する。</p>

<pre><code>public network = 10.0.0.0/24
cluster network = 172.18.0.0/24

[mon.a]
    host = ceph01
    mon addr = 10.0.0.11:6789

[mon.b]
    host = ceph02
    mon addr = 10.0.0.12:6789

[mon.c]
    host = ceph03
    mon addr = 10.0.0.13:6789

[osd.0]
    public addr = 10.0.0.11
    cluster addr = 172.18.0.11

[osd.1
    public addr = 10.0.0.12
    cluster addr = 172.18.0.12

[osd.2]
    public addr = 10.0.0.13
    cluster addr = 172.18.0.13

[mds.a]
    host = ceph01

[mds.a]
    host = ceph02

[mds.a]
    host = ceph03
</code></pre>

<p>次の操作でそれぞれのホストに対して ceph の公開しているレポジトリを参照させ
ceph をインストールしていきます。</p>

<pre><code>cephclient% ceph-deploy install ceph01 ceph02 ceph03
</code></pre>

<p>MON daemon のデプロイを行う。</p>

<pre><code>cephclient% ceph-deploy mon create ceph01 ceph02 ceph03
</code></pre>

<p>鍵のデプロイを行う。Ceph サーバ間・クライアント間での共有鍵です。1 Cluster
に対して1つの鍵を保有することになります。</p>

<pre><code>cephclient% ceph-deploy gatherkeys ceph01 ceph02 ceph03
</code></pre>

<p>OSD daemon のデプロイを行う。下記の様にパーティションを指定しなければツールが
自動でパーティショニングを行なってくれます。</p>

<pre><code>cephclient% ceph-deploy osd create create ceph01:/dev/sdb ceph02:/dev/sdb ceph03:/dev/sdb
</code></pre>

<p>MDS deamon のデプロイを行う。</p>

<pre><code>cephcleint% ceph-deploy mds create ceph01 ceph02 ceph03
</code></pre>

<p>完成です。</p>

<p>全てのホスト ceph01-03 にて MON, MDS, OSD のプロセスが稼働しているのが確認出来
ると思います。実際にどれかのホストの MDS に対して client から ceph ストレージ
をマウントしてみてください。</p>

<h2 id="まとめ">まとめ</h2>

<p>通常のフラットなネットワーク上にデプロイする方法とほぼ同じ操作で構築出来ます。
異なるところは ceph.conf に対して設定を追加した点です。また、それぞれのホスト
への ceph のインストールの時にオプションが渡せます。&ndash;testing と渡せば RC 版の
ceph が利用できます。今回の様に何も記さなければ stable 版の利用ということにな
ります。</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2013/05/25/ceph-cluster-network/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>

        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2013/05/19/openstack-ceph/">OpenStack &#43; Ceph 連携</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2013-05-19'>
            May 19, 2013</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは。最近 OpenStack の導入に向けて保守性や可用性について調査している
<a href="https://twitter.com/jedipunkz">@jedipunkz</a> です。</p>

<p>OpenStack は MySQL のダンプや OS イメージ・スナップショットのバックアップをとっ
ておけばコントローラの復旧も出来ますし、Grizzly 版の Quantum では冗長や分散が
取れるので障害時に耐えられます。また Quantum の復旧は手動もで可能です。最後の
悩みだった Cinder の接続先ストレージですが、OpenStack のスタンスとしては高価な
ストレージの機能を使ってバックアップ取るか、Ceph, SheepDog のようなオープンソー
スを使うか、でした。で、今回は Ceph を OpenStack に連携させようと思いました。</p>

<p>この作業により Cinder の接続先ストレージが Ceph になるのと Glance の OS イメー
ジ・スナップショットの保管先が Ceph になります。</p>

<p>下記の参考資料が完成度高く、ほぼ内容はそのままです。若干付け足していますが。</p>

<h2 id="参考資料">参考資料</h2>

<p><a href="http://ceph.com/docs/master/rbd/rbd-openstack/">http://ceph.com/docs/master/rbd/rbd-openstack/</a></p>

<h2 id="前提の構成">前提の構成</h2>

<pre><code>+-------------+-------------+--------------------------------------------- Public/API Network
|             |             |             
+-----------+ +-----------+ +-----------+ +-----------+ +-----------+ +-----------+
|           | |           | |vm|vm|..   | |           | |           | |           |
| controller| |  network  | +-----------+ |  ceph01   | |  ceph01   | |  ceph01   |
|           | |           | |  compute  | |           | |           | |           |
|           | |           | |           | |           | |           | |           |
+-----------+ +-----------+ +-----------+ +-----------+ +-----------+ +-----------+
|             |     |       |     |       |             |             |
+-------------+-----)-------+-----)-------+-------------+-------------+-- Management/API Network
                    |             |                       
                    +-------------+-----------------------------------+-- Data Network
</code></pre>

<ul>
<li>Ceph は OpenStack の Management Network 上に配置</li>
<li>Ceph は3台構成 (何台でも可)</li>
<li>OpenStack も3台構成 (何台でも可)</li>
<li>連携処理するのは controller, compute ノード</li>
</ul>

<p>では早速手順ですが、OpenStack と Ceph の構築手順は割愛します。私の他の記事を参
考にしていただければと思います。</p>

<ul>
<li><a href="http://jedipunkz.github.io/blog/2013/04/20/openstack-grizzly-installation-script/">構築スクリプト</a></li>
<li><a href="http://jedipunkz.github.io/blog/2013/05/11/ceph-deploy/">ceph-deploy で Ceph 構築</a></li>
</ul>

<h2 id="ceph-openstack-連携手順">Ceph + OpenStack 連携手順</h2>

<h4 id="openstack-用に-ceph-pool-を作成する">OpenStack 用に Ceph Pool を作成する</h4>

<pre><code>ceph01% sudo ceph pool create volumes 128
ceph01% sudo ceph pool create images 128
</code></pre>

<h4 id="sudoers-の設定">sudoers の設定</h4>

<p>controller, compute ノードにて sudoers の設定</p>

<pre><code>jedipunkz ALL = (root) NOPASSWD:ALL
</code></pre>

<h4 id="ceph-パッケージのインストール">ceph パッケージのインストール</h4>

<p>controller, compute ノードに ceph をインストールする。</p>

<pre><code>controller% wget -q -O- 'https://ceph.com/git/?p=ceph.git;a=blob_plain;f=keys/release.asc' | sudo apt-key add -
controller% echo deb http://ceph.com/debian/ $(lsb_release -sc) main | sudo tee /etc/apt/sources.list.d/ceph.list
controller% sudo apt-get update &amp;&amp; sudo apt-get install -y python-ceph ceph-common
</code></pre>

<h4 id="etc-ceph-作成">/etc/ceph 作成</h4>

<pre><code>controller% sudo mkdir /etc/ceph
compute   % sudo mkdir /etc/ceph
</code></pre>

<h4 id="ceph-コンフィギュレーションのコピー">ceph コンフィギュレーションのコピー</h4>

<p>controller, compute ノードに ceph コンフィギュレーションをコピーする。尚、接続
先の OpenStack ノードでの sudoers 設定は予め済ませること。</p>

<pre><code>ceph01% sudo -i
ceph01# ssh &lt;controller&gt; sudo tee /etc/ceph/ceph.conf &lt;/etc/ceph/ceph.conf
ceph01# ssh &lt;compute&gt; sudo tee /etc/ceph/ceph.conf &lt;/etc/ceph/ceph.conf
</code></pre>

<h4 id="認証設定">認証設定</h4>

<p>nova, cinder, glance 用にユーザを作成する。</p>

<pre><code>ceph01% sudo ceph auth get-or-create client.volumes mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool=volumes, allow rx pool=images'
ceph01% sudo ceph auth get-or-create client.images mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool=images'
</code></pre>

<h4 id="キーリングの作成">キーリングの作成</h4>

<p>Ceph キーリングの作成を行う。Glance, Cinder が起動しているホスト controller ノードに
キーリングを配置する。</p>

<pre><code>ceph01% sudo ceph auth get-or-create client.images | ssh {your-glance-api-server} sudo tee /etc/ceph/ceph.client.images.keyring
ceph01% ssh {your-glance-api-server} sudo chown glance:glance /etc/ceph/ceph.client.images.keyring
ceph01% sudo ceph auth get-or-create client.volumes | ssh {your-volume-server} sudo tee /etc/ceph/ceph.client.volumes.keyring
ceph01% ssh {your-volume-server} sudo chown cinder:cinder /etc/ceph/ceph.client.volumes.keyring
</code></pre>

<p>compute ノードにて libvirt に secret key を格納する。ここで登場する uuid は後
に利用するためメモをとっておくこと。</p>

<pre><code>ceph01 % sudo ceph auth get-key client.volumes | ssh 10.200.10.59 tee client.volumes.key

compute% cat &gt; secret.xml &lt;&lt;EOF
&lt;secret ephemeral='no' private='no'&gt;
  &lt;usage type='ceph'&gt;
    &lt;name&gt;client.volumes secret&lt;/name&gt;
  &lt;/usage&gt;
&lt;/secret&gt;
EOF
comupte% sudo virsh secret-define --file secret.xml
&lt;uuid of secret is output here&gt;
compute% sudo virsh secret-set-value --secret {uuid of secret} --base64 $(cat client.volumes.key) &amp;&amp; rm client.volumes.key secret.xml
</code></pre>

<h4 id="openstack-連携のための設定">OpenStack 連携のための設定</h4>

<p>controller:/etc/glance/glance-api.conf に下記を追記。</p>

<pre><code>default_store=rbd
rbd_store_user=images
rbd_store_pool=images
show_image_direct_url=True
</code></pre>

<p>controller:/etc/cinder/cinder.conf に下記を追記。先ほど登場した uuid を入力す
る。</p>

<pre><code>volume_driver=cinder.volume.driver.RBDDriver
rbd_pool=volumes
rbd_user=volumes
rbd_secret_uuid={uuid of secret}
</code></pre>

<p>controller:/etc/init/cinder-volume.conf の冒頭に下記の記述を追記する。</p>

<pre><code>env CEPH_ARGS=&quot;--id volumes&quot;
</code></pre>

<p>OpenStack の各サービスを再起動もしくはホストの再起動を行う。</p>

<pre><code>sudo service glance-api restart
sudo service nova-compute restart
sudo service cinder-volume restart
</code></pre>

<h4 id="確認">確認</h4>

<p>実際にインスタンスを作成して Volume をアタッチしディスクを消費していくと Ceph
のディスク使用量が増えていきます。</p>

<pre><code>% cinder create --display-name test 5
% nova volumeattach &lt;instance_id&gt; &lt;volume_id&gt; auto
</code></pre>

<h2 id="まとめ">まとめ</h2>

<p>Cinder は分散ストレージですので各ファイルのレプリカが全て失われない限りデータ
はロストしません。ただし Ceph 自体の完成度は以前に比べ高くはなったものの、運用
に耐えられるかどうかまだ私にも分かりません。先日の OpenStack Day に来日してい
たファウンデーションの方が「ベンダロックインするな」と言っていました。僕もオー
プンソースでなんとかしたいと思っています。OpenStack を導入するためには今、Ceph
は欠かすことが出来ないコンポーネントな気がしています。皆で Ceph も盛り上げて行
きたいです。</p>

<p>また、この構成の際のOpenStack 全体の保全について考えると&hellip;</p>

<ul>
<li>MySQL のデータさえダンプの取得すれば OK</li>
<li>OS イメージ・スナップショットは Ceph 上にあるのでバックアップ不要</li>
<li>Ceph はなんとしても守る。バックアップ取るのは難しい</li>
<li>Network ノードは分散・冗長可能, データのバックアップは不要</li>
<li>Compute ノード上のインスタンスデータは Ceph のスナップショットから復旧</li>
</ul>

<p>といったことが考えられます。つまり MySQL のデータさえダンプしておけば
OpenStack 全体が復旧できることになります。実際にやってみましたが可能でした。</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2013/05/19/openstack-ceph/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>

        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2013/05/18/chef-cookbook-adding-users/">Chef Cookbook でユーザ・グループ追加</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2013-05-18'>
            May 18, 2013</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは。<a href="https://twitter.com/jedipunkz">@jedipunkz</a> です。
今回は Opscode Chef でユーザ・グループを作成する方法をまとめます。</p>

<p>&lsquo;users&rsquo; Cookbook を使います。</p>

<pre><code>% cd ${YOUR_CHEF_REPO}
% ${EDITOR} Berksfile
cookbook 'users'
% berks install --path ./cookbooks
</code></pre>

<p>data_bag を使ってユーザ・グループの管理をしたいので管理ディレクトリを作成しま
す。</p>

<pre><code>% mkdir -p data_bags/users
</code></pre>

<p>data_bags/users/jedipunkz.json ファイルを作成します。必要に応じて内容を書き換えてください。</p>

<pre><code>{
  &quot;id&quot;: &quot;jedipunkz&quot;,
  &quot;ssh_keys&quot;: &quot;ssh-rsa AAAABx92tstses jedipunkz@somewhere&quot;,
  &quot;groups&quot;: [ &quot;sysadmin&quot;, &quot;sudo&quot; ],
  &quot;uid&quot;: 2001,
  &quot;shell&quot;: &quot;\/usr\/bin\/zsh&quot;,
  &quot;comment&quot;: &quot;jedipunkz sysadmin&quot;,
  &quot;password&quot;: &quot;$1$s%H8BMHlB$7s3h30y9IB1SklftZXYhvssJ&quot;

}
</code></pre>

<p>json ファイルの説明です。</p>

<ul>
<li>id : ユーザ名</li>
<li>ssh_keys : SSH 公開鍵</li>
<li>groups : 所属させるグループ</li>
<li>uid : unix id</li>
<li>sheell : ログインシェル</li>
<li>comment : コメント</li>
<li>passwd : ハッシュ化したパスワード</li>
</ul>

<p>特にハッシュ化したパスワードは下記のコマンドで生成出来ます。</p>

<pre><code>% openssl passwd -1 'yourPassword'
</code></pre>

<p>data_bag を作成し json ファイルを読み込みます。</p>

<pre><code>% knife data bag create users
% knife data bag from file users data_bags/users/jedipunkz.json
</code></pre>

<p>現在 (2013/05/18 現在) 、&rsquo;users&rsquo; Cookbook に不具合があるらしく groups に記した
グループにユーザが所属してくれませんでした。なので下記の対処をします。
sysadmins.rb を今回は利用します。このファイルに下記の行を追記します。僕は sudo
グループに所属させたかったので (先ほど groups: に記した) こうしましたが、他の
グループが良ければ変更してください。また、Ubuntu Server を扱うことがメインの僕
なので group_id は 27 にしています。適宜変更してください。</p>

<pre><code>% ${EDITOR} cookbooks/users/recipes/sysadmins.rb
# 下記の行を追記
users_manage &quot;sudo&quot; do
  group_id 27
end
</code></pre>

<p>cookbook を Chef サーバにアップロードします。</p>

<pre><code>% knife cookbook upload users
</code></pre>

<p>適用したいノードの run_list に Recipe &lsquo;users::sysadmins&rsquo; を追加します。</p>

<pre><code>% knife node run_list add ${YOUR_NODE_NAME} users::sysadmins
</code></pre>

<p>chef-client の次回実行時にユーザ &lsquo;jedipunkz&rsquo; が作成されているはずです。SSH で
ログインして確認してみてください。待ちきれなかったら knife ssh して
chef-client を実行してください。</p>

<p>この &lsquo;users&rsquo; cookbook は他の Cookbook からも呼び出して利用することが出来るので
応用が利きますね。</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2013/05/18/chef-cookbook-adding-users/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>

        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2013/05/11/ceph-deploy/">Ceph-Deploy で Ceph 分散ストレージ構築</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2013-05-11'>
            May 11, 2013</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>今回は ceph-deploy というツールを使って Ceph ストレージを簡単に構築することが
出来るので紹介します。Ceph は分散ストレージでオブジェクトストレージとしてもブ
ロックストレージとしても動作します。今回の構築ではブロックストレージとしてのみ
の動作です。</p>

<p>Ceph が公開しているのが ceph-deploy なわけですが、マニュアル操作に代わる構築方
法として公開しているようです。その他にも Chef Cookbook も公開されているようで
す。</p>

<p>それでは早速。</p>

<h2 id="今回の構成">今回の構成</h2>

<pre><code>+--------+ +--------+ +--------+
| ceph01 | | ceph02 | | ceph03 |
|  osd   | |  osd   | |  osd   |
|  mon   | |  mon   | |  mon   |
|  mds   | |  mds   | |  mds   |
+--------+ +--------+ +--------+
| 10.0.0.1 | 10.0.0.2 | 10.0.0.3
|          |          |          
+----------+----------+
|
| 10.0.0.10
+-------------+
| workstation |
+-------------+
</code></pre>

<p>特徴は</p>

<ul>
<li>すべてのホストで osd, mon, mds を動作</li>
<li>ceph データ格納用ディスクデバイスを /dev/sdb として利用</li>
<li>workstation は ceph-deploy を実行するホスト</li>
</ul>

<p>です。osd は object store daemon で実際にファイルを格納していくデーモン。mon
はモニタリング用デーモン, mds は metadata server で POSIX 互換のファイルシステ
ムをクライアントに提供するためのデーモンです。</p>

<h2 id="ceph-deploy-を使うまでの準備">ceph-deploy を使うまでの準備</h2>

<p>ceph-deploy を使うまでのターゲットのホスト ceph01-03 と workstation と共に準備
が必要です。</p>

<h4 id="ceph01-03-の準備">ceph01-03 の準備</h4>

<p>&lsquo;ceph&rsquo; ユーザの作成を行う。</p>

<pre><code>% ssh user@ceph-server
% sudo useradd -d /home/ceph -m ceph
% sudo passwd ceph
</code></pre>

<p>sudoers の設定を行う。</p>

<pre><code>% echo &quot;ceph ALL = (root) NOPASSWD:ALL&quot; | sudo tee /etc/sudoers.d/ceph
% sudo chmod 0440 /etc/sudoers.d/ceph
</code></pre>

<h4 id="workstation-の準備">workstation の準備</h4>

<p>ノンパスフレーズの SSH 公開鍵・秘密鍵を生成する。</p>

<pre><code>workstation% ssh-keygen
Generating public/private key pair.
Enter file in which to save the key (/ceph-client/.ssh/id_rsa):
Enter passphrase (empty for no passphrase):
Enter same passphrase again:
Your identification has been saved in /ceph-client/.ssh/id_rsa.
Your public key has been saved in /ceph-client/.ssh/id_rsa.pub.
</code></pre>

<p>公開鍵をターゲットホスト (ceph01-03) に配置</p>

<pre><code>workstation% ssh-copy-id ceph@ceph01
workstation% ssh-copy-id ceph@ceph02
workstation% ssh-copy-id ceph@ceph03
</code></pre>

<p>ceph-deploy の取得を行う。</p>

<pre><code>workstation% git clone https://github.com/ceph/ceph-deploy.git ~/ceph-deploy
</code></pre>

<p>&lsquo;python-virtualenv&rsquo; パッケージをインストールする。</p>

<pre><code>workstation% sudo apt-get update ; sudo apt-get -y install python-virtualenv
</code></pre>

<p>ceph-deploy をブートストラップする</p>

<pre><code>workstation% cd ~/ceph-deploy
workstation% ./bootstrap
</code></pre>

<p>PATH を通す。自分の shell に合わせて登録してください。</p>

<pre><code>workstation% ${EDITOR} ~/.zshrc
export PATH=$HOME/ceph-deploy:$PATH
</code></pre>

<p>ホスト名の解決を行う。</p>

<pre><code>workstation% sudo ${EDITOR} /etc/hosts
10.0.0.1    ceph01
10.0.0.2    ceph02
10.0.0.3    ceph03
</code></pre>

<p>これで準備は終わり。</p>

<h2 id="3台構成構築">3台構成構築</h2>

<p>3台 (ceph01-03) を新規に構築する方法を書きます。すべて workstaiton 上からの操
作です。</p>

<p>ceph サーバ・クライアント間通信のための鍵の生成とコンフィギュレーションの生成
を下記の操作で行う。</p>

<pre><code>workstation% ceph-deploy new ceph01 ceph02 ceph03
</code></pre>

<p>下記の操作で ceph パッケージのインストールを各 Ceph サーバにて行う。&ndash;testing
等と引数を渡せば RC 版の利用が行える。何も渡さなければ stable 版。</p>

<pre><code>workstation% ceph-deploy install ceph01 ceph02 ceph03
</code></pre>

<p>MON daemon のデプロイを行う。</p>

<pre><code>workstation% ceph-deploy mon create ceph01 ceph02 ceph03
</code></pre>

<p>鍵のデプロイを行う。Ceph サーバ間・クライアント間での共有鍵である。1 Cluster
に対して1つの鍵を保有する。</p>

<pre><code>workstation% ceph-deploy gatherkeys create ceph01 ceph02 ceph03
</code></pre>

<p>OSD daemon のデプロイを行う。下記の様にパーティションを指定しなければツールが
自動でパーティショニングを行なってくれる。</p>

<pre><code>workstation% ceph-deploy osd create create ceph01:/dev/sdb ceph02:/dev/sdb ceph03:/dev/sdb
</code></pre>

<p>MDS deamon のデプロイを行う。</p>

<pre><code>cephcleint% ceph-deploy mds create ceph01 ceph02 ceph03
</code></pre>

<p>これで終わりです。これらの操作が終わるとすべてのホスト ceph01-03 で mon, osd,
mds の各デーモンが起動していることが分かると思います。超カンタン！</p>

<h2 id="マウントしてみよう">マウントしてみよう！</h2>

<p>さぁ～、クライアントからマウントしてみましょう。ここでは workstaion ホストを利
用します。Linux 系のマシンで同じネットワークセグメントに属していれば大抵マウン
ト出来ると思います。mds が稼働しているホストに対してであればどこにでもマウント
出来ます。</p>

<h4 id="block-device-としてマウントする方法">Block Device としてマウントする方法</h4>

<p>ストレージ上に block device を生成しそれをマウントする。</p>

<pre><code>workstation% rbd create foo --size 4096
workstation% sudo modprobe rbd
workstation% sudo rbd map foo --pool rbd --name client.admin
workstation% sudo mkfs.ext4 -m0 /dev/rbd/rbd/foo
workstation% sudo mkdir /mnt/myrbd
workstation% sudo mount /dev/rbd/rbd/foo /mnt/myrbd
</code></pre>

<h4 id="kernel-driver-を用いてマウントする方法">Kernel Driver を用いてマウントする方法</h4>

<p>kernel Driver を用いてストレージをマウントする。</p>

<pre><code>workstation% sudo mkdir /mnt/mycephfs
workstation% sudo mount -t ceph 10.0.0.1:6789:/ /mnt/mycephfs -o \
            name=admin,secret=`sudo ceph-authtool -p /etc/ceph/ceph.keyring`
</code></pre>

<h4 id="fuse-driver-ユーザランド-を用いてマウントする方法">Fuse Driver (ユーザランド) を用いてマウントする方法</h4>

<p>ユーザランドソフトウェア FUSE を用いてマウントする。</p>

<pre><code>workstation% sudo mkdir /home/&lt;username&gt;/cephfs
workstation% sudo ceph-fuse -m 10.0.0.1:6789 /home/&lt;username&gt;/cephfs
</code></pre>

<h2 id="まとめ">まとめ</h2>

<p>もし導入するのであればマニュアルでの構築も一度体験した方が良いかもしれません。
ツールを使うと一体どんな作業がされているのか理解出来ないので。ただ今ではマニュ
アル操作で構築している途中に &lsquo;ceph-deploy を使ってください&rsquo; と warning が出る
ので、開発元としてもこちらの構築方法を薦めたいのでしょう。あと Ceph はドキュメ
ントが非常に充実しています。ドキュメントの全てに大事なことが書いてあるので一度
読むことをオススメします。また Ceph が Chef Cookbook も公開しているようで、そ
ちらの方法もドキュメントにチラっと書いてありました。私はまだ試していませんが時
間があればやってみたいです。あとあと！ceph-deploy はまだ未完成な域を脱していま
せん。上記の通り新規構築系の操作はひと通り出来るのですが、ホストの削除系の実装
がまだされていませんでした。ホスト追加系の操作に関しても削除系程ではないのです
が完成度が上がっていません。手作業で少しカバーしてあげる必要があります。</p>

<p>OpenStack の Cinder の先のストレージについて最近考えていました。LVM 管理のロー
カルディスクでもいいのですが運用のことを考えるとバックアップを取らなくちゃい
けないのだけど logcal volume が存在しないのでスナップショットバックアップが出
来なそう。Cinder は比較的高価なストレージも扱えるのでそちらの機能でバックアッ
プ取るのもいいけど、ここはオープンソースでなんとかしたい！と思って Ceph を検討
してみました。</p>

<p>Ceph は分散ストレージでオブジェクトストレージとしてもブロックストレージとして
も動作が可能。OpenStack と組み合わせると Cinder の先のストレージとしても
Glance のイメージ置き場としても利用可能らしい。Cinder の接続先ストレージとして
の動作方法はまた別の機会にブログに書きます。</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2013/05/11/ceph-deploy/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>

        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2013/04/26/quantum-network-distributing/">Quantum Network ノードの分散・冗長</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2013-04-26'>
            April 26, 2013</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは。Grizzly がリリースされてから暫く経ちました。今回は Folsom リリース
まであった Quantum ノードのボトルネックと単一障害点を解決する新しい機能につい
て評価した結果をお伝えします。</p>

<p>Folsom までは</p>

<ul>
<li>Quantum L3-agent が落ちると、その OpenStack 一式の上にある仮想マシン全ての通
信が途絶える</li>
<li>Quantum L3-agent に仮想マシンの全てのトラフィックが集まりボトルネックとなる。</li>
</ul>

<p>という問題がありました。Folsom リリース時代にもし僕が職場で OpenStack を導入す
るのであればこれらを理由に nova-network を選択していたかもしれません。
nova-network は compute ノードが落ちればその上の仮想マシンも同時に落ちるが、他
の compute ノード上の仮想マシンの通信には影響を与えないからです。もちろん仮想
ルータ・仮想ネットワークの生成等を API でユーザに提供したいなどの要望があれば
Quantum を選択するしかありませんが。これに対して Grizzly リリースの Quantum は
改善に向けて大きな機能を提供してくれています。L3-agent, DHCP-agent の分散・冗
長機能です。</p>

<p>下記の構成が想定出来ます。ここでは Network ノードを2台用意しました。それ以上の
台数に増やすことも出来ます。</p>

<pre><code>+-------------+-------------+-------------------------- Public/API Network
|             |             |
+-----------+ +-----------+ +-----------+ +-----------+
|           | |           | |           | |vm|vm|..   |
| controller| |  network  | |  network  | +-----------+
|           | |           | |           | |  compute  |
+-----------+ +-----------+ +-----------+ +-----------+
|             |     |       |     |       |     |
+-------------+-----)-------+-----)-------+-----)------ Management/API Network
                    |             |             |
                    +-------------+-------------+------ Data Network
</code></pre>

<p>L3-agent の分散は仮想ルータ単位で行います。それに対し DHCP-agent は仮想
ネットワーク単位で行います。</p>

<h2 id="agent-一覧の取得">agent 一覧の取得</h2>

<p>上記の構成を構築すると下記のように agent 一覧が取得出来ます。</p>

<pre><code>% quantum agent-list # 'admin' ユーザでアクセス
+--------------------------------------+--------------------+-----------------------+-------+----------------+
| id                                   | agent_type         | host                  | alive | admin_state_up |
+--------------------------------------+--------------------+-----------------------+-------+----------------+
| 44795822-2d9f-434e-ba98-748f7411442f | DHCP agent         | grizzly03.example.com | :-)   | True           |
| a5150a40-0405-4399-ac1a-be012f55d9f5 | DHCP agent         | grizzly02.example.com | :-)   | True           |
| b7bf4e59-06ac-475c-84ab-413d8d29f293 | Open vSwitch agent | grizzly04.example.com | :-)   | True           |
| cc5a6b94-6ddd-4109-8f2f-1b28c6aaf5e6 | L3 agent           | grizzly03.example.com | :-)   | True           |
| d39803cf-19d3-47d7-8205-cf9a143dd0ea | Open vSwitch agent | grizzly02.example.com | :-)   | True           |
| d8e59803-9aad-4c62-a47a-519bc788e0fb | Open vSwitch agent | grizzly03.example.com | :-)   | True           |
| f6f747cf-ffb0-446c-a455-2947fd3e87e8 | L3 agent           | grizzly02.example.com | :-)   | True           |
+--------------------------------------+--------------------+-----------------------+-------+----------------+
</code></pre>

<p>ホスト名は下記。</p>

<ul>
<li>controller : grizzly01.exmaple.com</li>
<li>network01  : grizzly02.exmaple.com</li>
<li>network02  : grizzly03.exmaple.com</li>
<li>compute    : grizzly04.exmaple.com</li>
</ul>

<h2 id="l3-agent-の分散方法-ノード移動">L3-agent の分散方法 (ノード移動)</h2>

<p>仮想ルータ (ここでは &lsquo;router-test01&rsquo; とする) がどの L3-agent に属しているか確
認を取る。</p>

<pre><code>% quantum l3-agent-list-hosting-router router-demo
+--------------------------------------+-----------------------+----------------+-------+
| id                                   | host                  | admin_state_up | alive |
+--------------------------------------+-----------------------+----------------+-------+
| f6f747cf-ffb0-446c-a455-2947fd3e87e8 | grizzly02.example.com | True           | :-)   |
+--------------------------------------+-----------------------+----------------+-------+
</code></pre>

<p>1台目の Network ノード (grizzly02.example.com) 上の L3-agent に属していること
が確認取れた。次にこの親子関係を削除する。</p>

<pre><code>% quantum l3-agent-router-remove f6f747cf-ffb0-446c-a455-2947fd3e87e8 router-test01
Removed Router router-demo to L3 agent
</code></pre>

<p>最後に仮想ルータ &lsquo;router-test01&rsquo; を2台目の Network ノード上の L3-agent の管理
下に設定する。</p>

<pre><code>% quantum l3-agent-router-add cc5a6b94-6ddd-4109-8f2f-1b28c6aaf5e6 router-demo
Added router router-demo to L3 agent
% quantum l3-agent-list-hosting-router router-demo
+--------------------------------------+-----------------------+----------------+-------+
| id                                   | host                  | admin_state_up | alive |
+--------------------------------------+-----------------------+----------------+-------+
| cc5a6b94-6ddd-4109-8f2f-1b28c6aaf5e6 | grizzly0404.cpi.ad.jp | True           | xxx   |
+--------------------------------------+-----------------------+----------------+-------+
</code></pre>

<h2 id="dhcp-agent-の分散方法-ノード移動">DHCP-Agent の分散方法 (ノード移動)</h2>

<p>仮想マシンが所属しているネットワーク (ここでは &lsquo;int_net&rsquo;) がどの DHCP-agent に所属しているか確認する。</p>

<pre><code>% quantum dhcp-agent-list-hosting-net int_net
+--------------------------------------+-----------------------+----------------+-------+
| id                                   | host                  | admin_state_up | alive |
+--------------------------------------+-----------------------+----------------+-------+
| a5150a40-0405-4399-ac1a-be012f55d9f5 | grizzly02.example.com | True           | :-)   |
+--------------------------------------+-----------------------+----------------+-------+
</code></pre>

<p>1台目のノードに所属しているのが確認できる。次に &lsquo;int_net&rsquo; が所属する DHCP-agent を削除行う。</p>

<pre><code>% quantum dhcp-agent-network-remove a5150a40-0405-4399-ac1a-be012f55d9f5 int_net
Removed network int_net to DHCP agent
</code></pre>

<p>2台目のノードの DHCP-agent を仮想ネットワーク &lsquo;int_net&rsquo; に紐付ける。</p>

<pre><code>% quantum dhcp-agent-network-add 44795822-2d9f-434e-ba98-748f7411442f int_net
Added network int_net to DHCP agent
% quantum dhcp-agent-list-hosting-net int_net
+--------------------------------------+-----------------------+----------------+-------+
| id                                   | host                  | admin_state_up | alive |
+--------------------------------------+-----------------------+----------------+-------+
| 44795822-2d9f-434e-ba98-748f7411442f | grizzly0404.cpi.ad.jp | True           | :-)   |
+--------------------------------------+-----------------------+----------------+-------+
</code></pre>

<h2 id="まとめ">まとめ</h2>

<p>この様に仮想ルータ, 仮想ネットワーク単位で Network ノードの agent の分散が行え
る。上記のように仮想ルータ・ネットワークが1つずつでは分散という意味では無いが
運用の過程で仮想ルータ・ネットワークは増えることが想定出来るのでその際にはトラ
フィック・DHCP 機能を分散することが可能になる、と言える。また片系の Network ノー
ドに寄せておいてからの障害テスト -&gt; もう片系への移動も行なってみたが作業ととも
に仮想マシンの通信が復旧した。このテストを行う前まで &lsquo;agent の移動だけ行えるの
であって仮想ルータ自体が移動するわけではないので冗長という意味はない&rsquo; と考えて
いたのだが、実際には上記の操作で namespace が移動していることが判り (Quantum
の仮想ルータの実体は Linux Namespace) 障害テストの結果、うまくいった。
OpenStack を導入するという意味で、この機能は非常に大きな前進だと僕は思っていま
す。</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2013/04/26/quantum-network-distributing/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>

        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2013/04/21/chef-for-openstack-grizzly-roadmap/">Chef for OpenStack</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2013-04-21'>
            April 21, 2013</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>以前にも話題にしたことがある Chef For OpenStack ですが今週新しい情報が入って来
ました。#ChefConf 2013 というイベントがあったのですがここで Opscode の Matt
Ray さんらが集まり OpenStack を Chef で構築する &lsquo;Chef for OpenStack&rsquo; について
語られた模様です。その時の資料が SlideShare に上がっていたので見てみました。</p>

<p><iframe src="http://www.slideshare.net/slideshow/embed_code/19197748"
width="427" height="356" frameborder="0" marginwidth="0" marginheight="0"
scrolling="no" style="border:1px solid #CCC;border-width:1px 1px
0;margin-bottom:5px" allowfullscreen webkitallowfullscreen mozallowfullscreen>
</iframe> <div style="margin-bottom:5px"> <strong> <a
href="http://www.slideshare.net/mattray/chef-for-openstack-grizzly-roadmap"
title="Chef for OpenStack: Grizzly Roadmap" target="_blank">Chef for
OpenStack: Grizzly Roadmap</a> </strong> from <strong><a
href="http://www.slideshare.net/mattray" target="_blank">Matt Ray</a></strong>
</div></p>

<p>気にあった点を幾つか挙げていきます。</p>

<ul>
<li><a href="https://github.com/osops">https://github.com/osops</a> で管理される</li>
<li>各コンポーネントの cookbook の名前には &lsquo;-cookbook&rsquo; を最後に付ける</li>
<li>quantum, cinder, ceilometer, heat 等、比較的新しいコンポーネントも加わる</li>
<li>gerrit でコードレビューされ CI も提供される</li>
<li>Chef11 が用いられる</li>
<li>Ruby 1.9.x に対応した chef-client が用いられる</li>
<li>Foodcritic で可能な限りテストされる</li>
<li>chef-solo はサポートされない</li>
<li>5月に &lsquo;2013.1.0&rsquo; がリリースされる (openstack 2013.1 対応と思われる)</li>
<li>chef-repo の形で提供される</li>
<li>Ubuntu 12.04 が前提</li>
<li>HyperVisor は KVM, LXC がサポートされる</li>
</ul>

<p>以上です。恐らく chef-repo で提供されるということは spiceweasel を使った構成構
築が出来るような形になるでしょう。楽しみです。またコントリビュートする方法も掲
載されているので興味が有る方は協力してみるのも楽しいかもしれません。</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2013/04/21/chef-for-openstack-grizzly-roadmap/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>

        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2013/04/21/openstack-non-virtio/">OpenStack Grizzy で非 Virtio OS 稼働</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2013-04-21'>
            April 21, 2013</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは jedipunkz です。</p>

<p>Virtio に対応していない OS を OpenStack で稼働させることが今まで出来なかったの
ですが Grizzly から非 Virtio な OS イメージが扱えるようになった。今まで NetBSD
やら古い FreeBSD やら virtio ドライバを OS イメージに入れることに苦労していたの
だけど、これで問題無くなった。</p>

<p>最初、この機能のこと調べるのに「どうせ libvirt が生成する xml を書き換えるのだ
から nova 周りの設定なんだろうー」と思っていたら全く方法が見つからず&hellip;。結局
OS イメージを格納している Glance の設定にありました。</p>

<p>ここでは FreeBSD7.4 Release を例に挙げて説明していきます。</p>

<h2 id="前提とする環境">前提とする環境</h2>

<ul>
<li>OpenStack Grizzly が稼働していること</li>
<li>ホスト OS に Ubuntu 12.04.2 LTS が稼働していること</li>
<li>ゲスト OS に FreeBSD 7.4 Release を用いる</li>
</ul>

<p>とします。OS のバージョンはホスト・ゲスト共に、上記以外でも構いません。Grizzly
さえ動いていれば OK です。</p>

<h2 id="os-イメージ作成">OS イメージ作成</h2>

<p>KVM で OS イメージを作成します。もちろん virtio なインターフェースは指定せず</p>

<ul>
<li>IDE ディスクインターフェース</li>
<li>e1000 (intel) ネットワークインターフェース</li>
</ul>

<p>を指定してあげてください。</p>

<pre><code>% kvm-img create -f qcow2 &lt;IMAGE_NAME&gt; 5G
% sudo kvm -m 1024 --cdrom FreeBSD-7.4-RELEASE-amd64-disc1.iso --drive \
  file=./&lt;IMAGE_NAME&gt; -boot d -net nic,model=e1000 -net user -nographic \
  -vnc :9
</code></pre>

<p>VNC クライアントソフトを用いてホスト :9 番に接続し OS をインストールする。</p>

<h2 id="glance-への登録">Glance への登録</h2>

<p>OpenStack API に接続する環境変数等を合わせ下記のコマンドを実行します。</p>

<pre><code>% glance image-create --name=&quot;FreeBSD7.4&quot; --is-public \
  true --container-format bare --disk-format qcow2 &lt; &lt;IMAGE_NAME&gt;
% glance image-update --property hw_vif_model=e1000 &quot;FreeBSD7.4&quot;
% glance image-update --property hw_disk_bus=ide &quot;FreeBSD7.4&quot;
</code></pre>

<p>&ndash;property オプションでディスク・ネットワークインターフェースの指定を変更して
います。</p>

<h2 id="vm-の稼働">VM の稼働</h2>

<p>あとは普段通り nova boot コマンドで VM を稼働させるだけです。</p>

<pre><code>% nova boot --nic net-id=&lt;network_id&gt; --image &lt;image_id&gt; --flavor &lt;flavor_number&gt; &lt;vm_name&gt;
</code></pre>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2013/04/21/openstack-non-virtio/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>

        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2013/04/20/openstack-grizzly-installation-script/">OpenStack Grizzly 構築スクリプト</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2013-04-20'>
            April 20, 2013</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>OpenStack Grizzly がリリースされて2週間ほど経過しました。皆さん動かしてみまし
たか？今回、毎度の構築 Bash スクリプトを開発したので公開します。</p>

<p>下記のサイトで公開しています。</p>

<p><a href="https://github.com/jedipunkz/openstack_grizzly_install">https://github.com/jedipunkz/openstack_grizzly_install</a></p>

<p>このスクリプト、複数台構成とオールインワン構成の両方が構成出来るようなっていま
すが、今回は簡単なオールインワン構成の組み方をを簡単に説明したいと思います。</p>

<h2 id="前提の環境">前提の環境</h2>

<ul>
<li>Ubuntu 12.04 LTS が稼働している</li>
<li>Cinder のためのディスクを OS 領域と別に用意 (/dev/sdb1 など)</li>
<li>オールインワン構成の場合は 2 NICs 準備</li>
</ul>

<p>Ubuntu 13.04 の daily build も完成度上がっている時期ですが OVS 側の対応が
OpenStack 構成に問題を生じさせるため 12.04 LTS + Ubuntu Cloud Archive の組み合
わせで構築するのが主流になっているようです。また、Cinder 用のディスクは OS 領
域を保持しているディスクとは別 (もしくはパーティションを切ってディスクデバイス
を別けても可) が必要です。オールインワン構成の場合は NIC を2つ用意する必要があ
ります。通常 OpenStack を複数台構成する場合は</p>

<ul>
<li>コントローラノード x 1 台</li>
<li>ネットワークノード x 1 台</li>
<li>コンピュートノード x n 台</li>
</ul>

<p>で組み、VM はコンピュートノードからネットワークノードを介してインターネットに
接続します。よってそのため更に NIC が必要になるのですが、オールインワン構成の
場合は</p>

<ul>
<li>マネージメントネットワーク, API ネットワーク(内部通信用)</li>
<li>パブリックネットワーク (VM のためのブリッジインターフェース)</li>
</ul>

<p>の計2つを用意してください。</p>

<h2 id="実行前の準備">実行前の準備</h2>

<h4 id="os-のインストール">OS のインストール</h4>

<p>OS のインストール方法は割愛しますが</p>

<ul>
<li>&lsquo;openssh-server&rsquo; のみをインストール</li>
<li>ディスクが1つしかない場合は cinder 用のパーティションを用意</li>
</ul>

<p>の条件が満たされていれば OK です。</p>

<h4 id="cinder-用のディスクデバイスパーティショニング">Cinder 用のディスクデバイスパーティショニング</h4>

<p>Cinder 用に信頼性のあるディスクを用意している場合は fdisk 等を用いてパーティショ
ニングしてください。近々 loopback デバイスでも構築できるようスクリプトの改修を
する予定です。ディスクが一つしかない場合は先程述べたとおり、OS インストール時
にパーティショニングしたディスクデバイスを使います。</p>

<pre><code>% sudo fdisk /dev/sdb
</code></pre>

<h2 id="ネットワークインターフェースの設定">ネットワークインターフェースの設定</h2>

<p>下記のように2つのネットワークインターフェースを設定してください。</p>

<pre><code>% sudo ${EDITOR} /etc/network/interfaces
auto lo
iface lo inet loopback

# this NIC will be used for VM traffic to the internet
auto eth0
iface eth0 inet static
    up ifconfig $IFACE 0.0.0.0 up
    up ip link set $IFACE promisc on
    down ip link set $IFACE promisc off
    down ifconfig $IFACE down
    address 10.200.9.10
    netmask 255.255.255.0
    dns-nameservers &lt;DNS_RESOLVER1&gt; &lt;DNS_RESOLVER&gt;
    dns-search example.com

# this NIC must be on management network
auto eth1
iface eth1 inet static
    address 10.200.10.10
    netmask 255.255.255.0
    gateway 10.200.10.1
    dns-nameservers &lt;DNS_RESOLVER1&gt; &lt;DNS_RESOLVER&gt;
</code></pre>

<p>eth0 が VM のためのブリッジインターフェースになります。eth1 はマネージメントネッ
トワーク用・内部 API 通信用の兼務です。</p>

<h2 id="スクリプトの取得とパラメータ設定">スクリプトの取得とパラメータ設定</h2>

<p>スクリプトの取得を行います。</p>

<pre><code>% git clone git://github.com/jedipunkz/openstack_grizlly_install.git
% cd openstack_grizzly_install
</code></pre>

<p>パラメータを設定するため setup.conf 内の各パラメータを設定変更します。数多くの
パラメータがありますが、最低限のパラメータということで&hellip;</p>

<pre><code>HOST_IP='10.200.10.10'
HOST_PUB_IP='10.200.9.10'
PUBLIC_NIC='eth0'
</code></pre>

<p>を設定してください。HOST_IP は eth1 の IP アドレス、HOST_PUB_IP は eth0 の IP
アドレス、PUBLIC_NIC は eth0 (HOST_PUB_IP のインターフェース名) を指定します。</p>

<h2 id="スクリプトの実行">スクリプトの実行</h2>

<p>いよいよスクリプトを実行します。</p>

<pre><code>% sudo ./setup.sh allinone
</code></pre>

<p>しばらくすると構築が完了します。あとは</p>

<pre><code>http://${HOST_IP}/horizon/
</code></pre>

<p>にブラウザでアクセスすると WEB I/F である Horizon のログイン画面が表示されます。
パラメータをいじっていなければユーザ : demo, パスワード : demo でアクセス出来
ます。</p>

<h2 id="各-api-にコマンドでアクセスする">各 API にコマンドでアクセスする</h2>

<p>API にアクセスするためにコマンドを用いることも出来ます。スクリプトを実行した結
果、下記のファイルが生成されているはずです。</p>

<pre><code>~/openstackrc-demo # 'demo' ユーザで API にアクセス
~/openstackrc      # 'admin' ユーザで API にアクセス
</code></pre>

<p>&lsquo;demo&rsquo; ユーザでアクセスするためには</p>

<pre><code>% source ~/openstackrc-demo
</code></pre>

<p>を実行してください。環境変数が設定され API にアクセス出来るようになります。例
として下記のコマンドを実行してみてください。</p>

<pre><code>% glnace image-list
+--------------------------------------+---------------------+-------------+------------------+------------+--------+
| ID                                   | Name                | Disk Format | Container Format | Size       | Status |
+--------------------------------------+---------------------+-------------+------------------+------------+--------+
| 1a7943a5-8f8f-4c02-9763-5a6d519c31bb | Cirros 0.3.0 x86_64 | qcow2       | bare             | 9761280    | active |
+--------------------------------------+---------------------+-------------+------------------+------------+--------+
</code></pre>

<p>OS イメージ一覧が取得出来ます。スクリプトで予め Glance に登録した Cirros とい
う小さな OS イメージが確認出来るはずです。</p>

<h2 id="まとめ">まとめ</h2>

<p>本格的な構成を組むのであれば上記の URL にも知る指定ある複数台構成を組んでみて
ください。同じくスクリプトで構築出来ます。また今回から Quantum に実装された
LBaaS も組めるようになっています。構築出来た OpenStack で LB を組んでみてくだ
さい。LBaaS の説明については OpenStack ユーザ会の中島さんのブログが参考になり
ます。</p>

<p><a href="http://aikotobaha.blogspot.jp/2013/04/use-full-function-of-openstack-grizzly.html">http://aikotobaha.blogspot.jp/2013/04/use-full-function-of-openstack-grizzly.html</a></p>

<p>LBaaS で組める負荷分散方式が &lsquo;ROUND_ROBIN&rsquo; 以外にも選択出来るぽいのでもう少し
調べたら、僕のブログでも紹介しようかと思います。また Grizzly になって数多くの
機能が新たに実装されているので引き続き紹介していこうかと思います。</p>

<p>OpenStack は多くの機能がありますし構成の仕方も様々。予め理解しなければいけない
技術も多岐にわたるのでブログだけではなかなか説明し切れないところです。
OpenStack のコミュニティが書いた &lsquo;OpenStack Operations Guide&rsquo; なるドキュメント
が最近リリースされました。</p>

<p><a href="http://docs.openstack.org/ops/">http://docs.openstack.org/ops/</a></p>

<p>日本のユーザ会でもこのドキュメントを翻訳しようという活動がされている最中です。
興味があるかたは一度読むことをオススメしますし、もし更に興味が有る方は翻訳活動
に協力するのはいかがでしょうか。ユーザ会の ML で現在話が進んでいます。</p>

<p>引き続き、OpenStack ネタはアップしていきますー。</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2013/04/20/openstack-grizzly-installation-script/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>

        

        
<ul class="actions pagination">
    
        <li><a href="/tags/page/7/"
                class="button big previous">Previous Page</a></li>
    

    
        <li><a href="/tags/page/9/"
                class="button big next">Next Page</a></li>
    
</ul>

    </div>
    
<section id="sidebar">

    
        <section id="intro">
            
            
            
            <ul class="icons">
                
                    <li><a href="https://jedipunkz.github.io/tags/index.xml" type="application/rss+xml"
                        target="_blank" title="RSS" class="fa fa-rss"></a></li>
                
                
            </ul>
        </section>

    
        <section id="recent-posts">
            <ul class="posts">
                <header>
                    <h3>Recent Posts</h3>
                </header>
                
                    
                

                
                    
                

                
                    <li>
                        <article>
                            <header>
                                <h3><a href="https://jedipunkz.github.io/blog/2018/12/31/istio/">Istio, Helm を使って Getting Started 的なアプリをデプロイ</a></h3>
                                
                                    
                                
                                <time class="published" datetime=
                                    '2018-12-31'>
                                    December 31, 2018</time>
                            </header>
                        </article>
                    </li>
                
                    <li>
                        <article>
                            <header>
                                <h3><a href="https://jedipunkz.github.io/blog/2017/07/02/test-kitchen-cluster/">Docker,Test-Kitchen,Ansible でクラスタを構成する</a></h3>
                                
                                    
                                
                                <time class="published" datetime=
                                    '2017-07-02'>
                                    July 2, 2017</time>
                            </header>
                        </article>
                    </li>
                
                    <li>
                        <article>
                            <header>
                                <h3><a href="https://jedipunkz.github.io/blog/2017/04/13/gke-lb/">GCP ロードバランサと GKE クラスタを Terraform を使って構築する</a></h3>
                                
                                    
                                
                                <time class="published" datetime=
                                    '2017-04-13'>
                                    April 13, 2017</time>
                            </header>
                        </article>
                    </li>
                
                    <li>
                        <article>
                            <header>
                                <h3><a href="https://jedipunkz.github.io/blog/2017/02/12/serverless-fission/">Serverless on Kubernetes : Fission を使ってみた</a></h3>
                                
                                    
                                
                                <time class="published" datetime=
                                    '2017-02-12'>
                                    February 12, 2017</time>
                            </header>
                        </article>
                    </li>
                
                    <li>
                        <article>
                            <header>
                                <h3><a href="https://jedipunkz.github.io/blog/2017/01/13/kubernetes-deployments/">Kubernetes Deployments を使ってみた！</a></h3>
                                
                                    
                                
                                <time class="published" datetime=
                                    '2017-01-13'>
                                    January 13, 2017</time>
                            </header>
                        </article>
                    </li>
                

                
                    <li>
                        <ul class="actions">
                            <li><a href=
                            
                                "/post/"
                            
                            class="button">View more posts</a></li>
                        </ul>
                    </li>
                
            </ul>
        </section>

    
    
    
    
        <section id="categories">
            <ul class="posts">
                <header>
                    <h3><a href="/categories/">Categories</a></h3>
                </header>

                
                    
                

                
                    <li>
                        <article>
                            <header>
                                <a href="/categories/infrastructure/">infrastructure</a>
                                <span style="float:right;">110</span>
                            </header>
                        </article>
                    </li>
                
                    <li>
                        <article>
                            <header>
                                <a href="/categories/report/">report</a>
                                <span style="float:right;">9</span>
                            </header>
                        </article>
                    </li>
                
                    <li>
                        <article>
                            <header>
                                <a href="/categories/tools/">tools</a>
                                <span style="float:right;">11</span>
                            </header>
                        </article>
                    </li>
                
            </ul>
        </section>
    

    
        

    
        <section id="footer">
            <ul class="icons">
                
                    <li><a href="https://jedipunkz.github.io/tags/index.xml" type="application/rss+xml"
                        target="_blank" title="RSS" class="fa fa-rss"></a></li>
                
                
            </ul>

            <p class="copyright">&copy; ジェダイさんのブログ. テーマデザインは <a href="//github.com/jpescador" target="_blank">Julio Pescador</a>さんによるものです。 </p>
        </section>

</section>

            </div>
        <a id="back-to-top" href="#" class="fa fa-arrow-up fa-border fa-2x"></a>
        

        
        
            
        

        
        
            <script src="/js/jquery.min.js"></script>
            <script src="/js/skel.min.js"></script>
            <script src="/js/util.js"></script>
            <script src="/js/main.js"></script>
            <script src="/js/backToTop.js"></script>
            <script src="/js/highlight.pack.js"></script>
        

        

            
            <script>hljs.initHighlightingOnLoad();</script>
            
    </body>
</html>

