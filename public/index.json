[{"content":"こんにちは @jedipunkz 🚀 です。\n最近、職場では ECS/Fargate でサービスを運用しています。そこで ECS Task 上でコマンドを実行する必要に迫られて幾つか調べたのですが、複数の方法があり検証をしてみました。これには 2021/03 にリリースされたばかりの ECS 上のコンテナでコマンドを実行する機能も含まれています。\n自分たちは自動化する必要があったので Go 言語 (aws-sdk-go) を中心に検証実施しましたが同時に awscli でも動作検証しましたので、その方法をこの記事に記そうかと思います。\n下記の2つの ECS の機能についてそれぞれ Go 言語, awscli で動作検証実施しました。\n (1) ECS Execute Command (2021/03 リリース) (2) ECS Run Task  用いるツール類 下記のツールを前提に記事を記します。\n aws-sdk-go Terraform awscli  共通で必要な taskRoleArn まず Task Definition に対して executeRoleArn とは別に TaskRoleArn の付与が必要になります。\nresource \u0026#34;aws_ecs_task_definition\u0026#34; \u0026#34;sample\u0026#34; { family = \u0026#34;sample\u0026#34; cpu = \u0026#34;256\u0026#34; memory = \u0026#34;512\u0026#34; network_mode = \u0026#34;awsvpc\u0026#34; requires_compatibilities = [\u0026#34;FARGATE\u0026#34;] execution_role_arn = module.ecs_task_execution_role.iam_role_arn task_role_arn = module.ecs_task__role.iam_role_arn container_definitions = data.template_file.container_definition_sample.rendered } taksRoleArn の内容については https://docs.aws.amazon.com/ja_jp/AmazonECS/latest/developerguide/task-iam-roles.html に情報があり、下記が必要になります。\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;ecs-tasks.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } ] } コマンド実行 (1) ECS Exuecute Command (2021/03 リリース) まず最近リリースされた ECS コマンド実行について試します。Terraform やその他の周辺の技術 (aws 公式 GitHub Actions 等も)、この機能にはまだ対応していません。awscli (v1, v2), session-manager, aws-sdk-go 等が既に対応しています。ここでは awscli, aws-sdk-go を使ってこの機能を試してみます。\nawscli, session-manager, aws-sdk-go は比較的新しいバージョンを事前にインストールする必要があります。\nまず taskExecute IAM Role に下記の権限が追加で必要です。\nstatement { effect = \u0026#34;Allow\u0026#34; actions = [ \u0026#34;ssmmessages:CreateControlChannel\u0026#34;, \u0026#34;ssmmessages:CreateDataChannel\u0026#34;, \u0026#34;ssmmessages:OpenControlChannel\u0026#34;, \u0026#34;ssmmessages:OpenDataChannel\u0026#34; ] resources = [\u0026#34;*\u0026#34;] } ECS Service に対してコマンド実行の有効化を実施します。\n$ aws ecs update-service \\  --cluster \u0026lt;ClusterArn\u0026gt; \\  --service \u0026lt;ServiceName\u0026gt; \\  --task \u0026lt;TaskName\u0026gt; \\  --enable-execute-command #\u0026lt;--- 有効化 次に --enable-execute-command オプションを付与して Task を起動します。\n$ aws ecs run-task --cluster \u0026lt;ClusterArn\u0026gt; \\  --task-definition \u0026lt;TaskDefinition:Revision\u0026gt; \\  --network-configuration \u0026#34;awsvpcConfiguration={subnets=[\u0026#34;Subnet_ID1\u0026#34;, \u0026#34;Subnet_ID2\u0026#34;],securityGroups=[\u0026#34;SecurityGroupId\u0026#34;],assignPublicIp=DISABLED}\u0026#34; \\  --launch-type FARGATE --enable-execute-command 準備が整ったので、コマンドを実行してみます。\naws ecs execute-command --cluster \u0026lt;ClusterArn\u0026gt; \\  --task \u0026lt;TaskId\u0026gt; --container \u0026lt;ContainerName\u0026gt; \\  --interactive --command \u0026#34;ps ax\u0026#34; The Session Manager plugin was installed successfully. Use the AWS CLI to start a session. Starting session with SessionId: ecs-execute-command-0b2c79e1e775f274c PID USER TIME COMMAND 1 root 0:00 /bin/sh -c nginx -g \u0026#34;daemon off;\u0026#34; 7 root 0:00 nginx: master process nginx -g daemon off; 8 nginx 0:00 nginx: worker process 9 nginx 0:00 nginx: worker process 10 root 0:00 /managed-agents/execute-command/amazon-ssm-agent 23 root 0:00 /managed-agents/execute-command/ssm-agent-worker 72 root 0:00 /managed-agents/execute-command/ssm-session-worker ecs-ex 80 root 0:00 ps ax Exiting session with sessionId: ecs-execute-command-0b2c79e1e775f274c. 上記の通り、コマンド ps ax の結果が得られました。\n次に aws-sdk-go を使ってコマンドを実行してみます。\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;github.com/aws/aws-sdk-go/aws\u0026#34; \u0026#34;github.com/aws/aws-sdk-go/aws/awserr\u0026#34; \u0026#34;github.com/aws/aws-sdk-go/aws/session\u0026#34; \u0026#34;github.com/aws/aws-sdk-go/service/ecs\u0026#34; ) func main() { svc := ecs.New(session.New(aws.NewConfig().WithRegion(\u0026#34;ap-northeast-1\u0026#34;))) input := \u0026amp;ecs.ExecuteCommandInput{ Cluster: aws.String(\u0026#34;\u0026lt;ClusterName\u0026gt;\u0026#34;), Command: aws.String(\u0026#34;ps aux\u0026#34;), Container: aws.String(\u0026#34;\u0026lt;ContainerName\u0026gt;\u0026#34;), Interactive: aws.Bool(true), Task: aws.String(\u0026#34;\u0026lt;TaskId\u0026gt;\u0026#34;), } result, err := svc.ExecuteCommand(input) if err != nil { if aerr, ok := err.(awserr.Error); ok { fmt.Println(aerr.Error()) } fmt.Println(err.Error()) return } fmt.Println(result) } これをビルドし、実行すると下記のようなシンタックスで結果が得られます。(参考: https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_ExecuteCommand.html)\n{ \u0026#34;clusterArn\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;containerArn\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;containerName\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;interactive\u0026#34;: boolean, \u0026#34;session\u0026#34;: { \u0026#34;sessionId\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;streamUrl\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;tokenValue\u0026#34;: \u0026#34;string\u0026#34; }, \u0026#34;taskArn\u0026#34;: \u0026#34;string\u0026#34;: } 残念ながらコマンド結果はレスポンスには入っていないようです。が、コマンドは正常を正常に叩いたかの API レスポンスとしてはエラーも検知出来ます。\n※[2021/04/14] aws の Tori さんから指摘頂きました。API からのレスポンスを利用して session-manager-plugin コマンドを実行するとコンテナに接続できるそうです！\n記事書いていただきありがとうございます！\nAWS SDK for Go ですが、ExecuteCommand API は Session Manager 接続のためのパラメーターを返すのが仕事なので、返り値を使って session-manager-plugin コマンドを実行すると実際にコンテナに接続できるようになります :)\n\u0026mdash; ポジティブな Tori (@toricls) April 14, 2021 (2) ECS Run Task によるコマンドのオーバーライド 次に ECS Run Task によるコマンドのオーバーライドについて記します。こちらは以前からある機能なのでほぼすべての周辺ツールが対応している認識です。まず awscli で動作確認してみます。\n下記の json ファイルを作成して、タスク定義に記してあるコンテナ名と、コマンドを記します。\n{ \u0026#34;containerOverrides\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;\u0026lt;ContainerName\u0026gt;\u0026#34;, \u0026#34;command\u0026#34;: [ \u0026#34;\u0026lt;Command\u0026gt;\u0026#34; ] } ] } awscli を用いてコマンドを Overrides しつつ Run Task 実行します。\naws ecs run-task --cluster \u0026lt;ClusterArn\u0026gt; \\  --task-definition \u0026lt;TaskDefinition:Revision\u0026gt; \\  --network-configuration \u0026#34;awsvpcConfiguration={subnets=[\u0026#34;\u0026lt;SubnetId1\u0026gt;\u0026#34;, \u0026#34;\u0026lt;SubnetId2\u0026gt;\u0026#34;],securityGroups=[\u0026#34;SecurityGroupId\u0026#34;],assignPublicIp=DISABLED}\u0026#34; \\  --launch-type FARGATE \\  --overrides file://\u0026lt;作成した json ファイル\u0026gt;.json 結果は API からの応答内容だけでコマンド実行結果は含まれていません。タスク定義で logConfiguration を記していると、そこにコマンド実行結果が出力されます。\nではつぎに aws-sdk-go を使って Run Task してみます。下記のサンプルコードを記します。環境情報は適宜差し替える必要があります。\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;github.com/aws/aws-sdk-go/aws\u0026#34; \u0026#34;github.com/aws/aws-sdk-go/aws/awserr\u0026#34; \u0026#34;github.com/aws/aws-sdk-go/aws/session\u0026#34; \u0026#34;github.com/aws/aws-sdk-go/service/ecs\u0026#34; ) func main() { // svc := ecs.New(session.New(aws.NewConfig().WithRegion(\u0026#34;ap-northeast-1\u0026#34;))) \tsess := session.Must(session.NewSessionWithOptions(session.Options{ Config: aws.Config{ CredentialsChainVerboseErrors: aws.Bool(true), }, })) svc := ecs.New(sess) input := \u0026amp;ecs.RunTaskInput{ Cluster: aws.String(\u0026#34;\u0026lt;ClusterArn\u0026gt;), TaskDefinition: aws.String(\u0026#34;\u0026lt;TaskDefinition:Revision\u0026gt;), Overrides: \u0026amp;ecs.TaskOverride{ ContainerOverrides: []*ecs.ContainerOverride{ { Name: aws.String(\u0026#34;\u0026lt;ContainerName\u0026gt;\u0026#34;), Command: aws.StringSlice([]string{\u0026#34;\u0026lt;実行したいコマンドを記す\u0026gt;\u0026#34;}), }, }, }, NetworkConfiguration: \u0026amp;ecs.NetworkConfiguration{ AwsvpcConfiguration: \u0026amp;ecs.AwsVpcConfiguration{ Subnets: aws.StringSlice([]string{\u0026#34;\u0026lt;SubnetId1\u0026gt;\u0026#34;, \u0026#34;\u0026lt;SubnetId2\u0026gt;\u0026#34;}), // サブネットID \tAssignPublicIp: aws.String(\u0026#34;DISABLED\u0026#34;), // 必要に応じて \t}, }, LaunchType: aws.String(\u0026#34;FARGATE\u0026#34;), } result, err := svc.RunTask(input) if err != nil { if aerr, ok := err.(awserr.Error); ok { switch aerr.Code() { case ecs.ErrCodeServerException: fmt.Println(ecs.ErrCodeServerException, aerr.Error()) case ecs.ErrCodeClientException: fmt.Println(ecs.ErrCodeClientException, aerr.Error()) case ecs.ErrCodeInvalidParameterException: fmt.Println(ecs.ErrCodeInvalidParameterException, aerr.Error()) case ecs.ErrCodeClusterNotFoundException: fmt.Println(ecs.ErrCodeClusterNotFoundException, aerr.Error()) case ecs.ErrCodeUnsupportedFeatureException: fmt.Println(ecs.ErrCodeUnsupportedFeatureException, aerr.Error()) case ecs.ErrCodePlatformUnknownException: fmt.Println(ecs.ErrCodePlatformUnknownException, aerr.Error()) case ecs.ErrCodePlatformTaskDefinitionIncompatibilityException: fmt.Println(ecs.ErrCodePlatformTaskDefinitionIncompatibilityException, aerr.Error()) case ecs.ErrCodeAccessDeniedException: fmt.Println(ecs.ErrCodeAccessDeniedException, aerr.Error()) case ecs.ErrCodeBlockedException: fmt.Println(ecs.ErrCodeBlockedException, aerr.Error()) default: fmt.Println(aerr.Error()) } } else { fmt.Println(err.Error()) } return } fmt.Println(result) } ビルド \u0026amp; 実行するとレスポンスが得られます。がこちらもレスポンスにはコマンド結果が入っていないので、awscli の際と同様にタスク定義内で logConfiguration を指定してログ送信設定を行うと良いでしょう。Cloudwatch Logs などを介して、コマンド実行結果を確認することが可能です。\nまとめ それぞれの良い点・悪い点を下記にまとめてみました。\nCommand Exec の特徴  Terraform, aws 公式 GitHub Actions 等がまだ対応していない awscli ではコマンド結果が得られるが aws-sdk を用いると API のレスポンスにコマンド結果が入ってない 実行する際には TaskId を指定。よって予め Run Task などで Task を起動させるのが前提になる インタラクティブ(対話的) にコマンドを実行出来るそう (未検証)  Run Task の特徴  古くからある機能で公式・周辺の技術が対応済み コマンド実行結果は Task 定義に記した logConfiguration に転送することが可能 (Cloudwatch, Datadog 等など) 実行する際には Task Defintion を指定。予め Task が起動している必要はない。  それぞれ特徴がありますが、Execute Command の方はコマンド結果を得る方法が今の所、別途仕組みを用意する必要がありそうです。ログ転送や、Task を別途起動しておく必要が無い点、またコマンド実行終了と共に Task が自動的に終了してくれる点など、自動化する上では Run Task が好ましいなぁと感じています。\n今後 Execute Command がどう変わってくるか期待ですが、awscli を使って単純にデバッグしたい等の要望の時には重宝しそうだなと感じてます。\n","permalink":"https://jedipunkz.github.io/post/ecs-execute-command/","summary":"こんにちは @jedipunkz 🚀 です。\n最近、職場では ECS/Fargate でサービスを運用しています。そこで ECS Task 上でコマンドを実行する必要に迫られて幾つか調べたのですが、複数の方法があり検証をしてみました。これには 2021/03 にリリースされたばかりの ECS 上のコンテナでコマンドを実行する機能も含まれています。\n自分たちは自動化する必要があったので Go 言語 (aws-sdk-go) を中心に検証実施しましたが同時に awscli でも動作検証しましたので、その方法をこの記事に記そうかと思います。\n下記の2つの ECS の機能についてそれぞれ Go 言語, awscli で動作検証実施しました。\n (1) ECS Execute Command (2021/03 リリース) (2) ECS Run Task  用いるツール類 下記のツールを前提に記事を記します。\n aws-sdk-go Terraform awscli  共通で必要な taskRoleArn まず Task Definition に対して executeRoleArn とは別に TaskRoleArn の付与が必要になります。\nresource \u0026#34;aws_ecs_task_definition\u0026#34; \u0026#34;sample\u0026#34; { family = \u0026#34;sample\u0026#34; cpu = \u0026#34;256\u0026#34; memory = \u0026#34;512\u0026#34; network_mode = \u0026#34;awsvpc\u0026#34; requires_compatibilities = [\u0026#34;FARGATE\u0026#34;] execution_role_arn = module.","title":"Go 言語と awscli を使って ECS/Fargate 上でコマンド実行してみた"},{"content":"自己紹介 こんにちは。jedipunkz です。 元インフラエンジニア・クラウドエンジニア、最近は AWS を扱う SRE として働いています。ソフトウェアでインフラの課題を解決すべく勉強していきます。学んだことをこのブログに記せたらいいなと思っています。\n","permalink":"https://jedipunkz.github.io/about/profile/","summary":"自己紹介 こんにちは。jedipunkz です。 元インフラエンジニア・クラウドエンジニア、最近は AWS を扱う SRE として働いています。ソフトウェアでインフラの課題を解決すべく勉強していきます。学んだことをこのブログに記せたらいいなと思っています。","title":"自己紹介"},{"content":"こんにちは。jedipunkz です。\n仕事ではこれから会社のサービス環境として AWS ECS の導入を始めていくところなのですが、最近の SRE/インフラ界隈のトレンドを自分たちが追うために自分たち SRE が管理しているボット環境を EKS を使って GitOps 化しようということになり色々と準備を進めています。導入までもう一歩のところまで来たので、構成や技術要素についてここに記したいと思います。\nどんなボットが動いているの？ まずボット開発に用いてる言語は Go 言語です。主に aws-sdk-go を使ってボットを開発しています。私達はコミュニケーションツールとして Slack を使っているので Slack ボット化するためには slack-go を使っています。 ただまだボットの数が少なく主に利用されてるのは Ansible を実行するモノです。開発環境へアプリをデプロイするのに Ansible を使っています。もうすぐ ECS 化するので役割はそろそろ終えるのですが\u0026hellip; 利点は開発者だけでなく非エンジニアの方が GitHub のブランチ上のアプリの動作をしたい際に Slack を使って簡単にアプリの動作ができるところです。今後は自動化目的でもっと沢山のボットを開発していきたいです。\nEKS/Fargate vs EKS/EC2 EKS の利用を検討する際に Fargate タイプと EC2 タイプがあります。2020年の今年頭に評価した際には ALB Ingress Controller と HPA のための Metrics Server が正常に動作しない状態だったので、まだ EC2 タイプを選択すべきなのかな\u0026hellip;と考えたのですが AWS 的にも Fargate を推してる気もするし再度評価実施しました。結果ドキュメントもソフトウェアもだいぶ更新されていて ALB Ingress Controller も Metrics Server もあっけなく動作し、今回のボット環境も EKS/Fargte を選択しました。\n(ちょっと余談) そもそもサービス環境はなぜ EKS でなく ECS？ AWS ECS の導入を始めようとしていると前述しましたが、なぜサービス環境には EKS ではなく ECS を選択したかというと、この両者で下記の観点で評価実施しました。※ () 内はまだ未実施。\n コード化 オートスケール ロードバランサ 機密情報の格納展開 CI/CD, GitOps ロギング メトリクス収集 モニタリング カナリーリリース等のリリース方式 (バッチ) (サービスメッシュ)  結果的にインフラをデプロイする主な機能 (特に ALB Ingress Controller, Metric Server) を時前で Pod として運用するのは運用コストが高い、という判断をしました。Pod の調子が悪いからロードバランサがデプロイできない！というのは辛いです。自分の務めている会社は少人数で運用コストを減らすという視点で技術を選択しているので、今の時点では ECS を選択するのは必然でした。ECS であれば Terraform -\u0026gt; AWS API で確実に必要なインフラがデプロイ出来ますし。\nボット環境を GitOps 化する全体構成 構成は下記のとおりです。\n (3 get manifest +----------------------------------------+ | | +----------------+ (2 +----------------+ | | GitHub Actions |---------\u0026gt; | ECR Repository | | +----------------+ +----------------+ | + + (4 pull image | | (1 merge to master | | | +----------------+ | | | Bots | | | +----------------+ | | | ArgoCD |--+ +----------------+ +----------------+ +---------------+ | Engineer | | EKS/Fargate |---\u0026gt;| EFS Volumes | +----------------+ +----------------+ +---------------+ 処理の流れは\u0026hellip;\n (1 Engineer がアプリ用 Git レポジトリの master ブランチへ Push (2 GitHub Actions により Docker コンテナビルド, ECR への Push \u0026amp; Kustomize によりコンテナイメージタグが更新 (3 ArgoCD が GitHub 上の Manifest ファイルの更新をトリガに EKS へローリングデプロイ開始 (4 ArgoCD によって新たに指定されたイメージを ECR から Pull しデプロイ完了  なぜ EFS を使っているか ? EFS は 永続的なストレージとして利用しています。今後は不要になる可能性が高いのですが前述した Ansible を使ったボットは Playbook を収めた Git レポジトリにアクセスしてデプロイを実行します。コンテナに Git レポジトリを収めるのはコンテナイメージサイズが肥大化しますし良くないと判断しました。結果、EKS から EFS を Volume マウントして Ansible を実行します。また Ansible 以外のボットも環境ごとの情報を格納した yaml ファイルを扱うのですが、その yaml ファイルを EFS 上に Kubernetes Secrets として展開しています。\nArgoCD を選択した理由 ArgoCD が CNCF にジョインしたというニュースが今年の夏頃？ありました。また同時に検討していた Flux がちょうどメジャーバージョンアップをして、だいぶ利用方法やドキュメントが刷新されました。また一世代前のバージョンはメンテナンスモードに突入。新しいバージョンは利用方法が少し複雑化していましたし CLI は劇的に利用方法が変化していました。ということで GitOps をしたいという要望だけ満たせれば良いので CNDF の ArgoCD を利用することにしました。\nArgoCD はボットと同じ EKS クラスタ上にデプロイしました。UI は普段利用しないので Ingress は設定せず EKS Fargate Profile の稼働しているプライベートサブネット上で起動させました。ArgoCD アプリは下記の手順で作成します。\n$ argocd app create hello --repo https://github.com/jedipunkz/no-exsist-bot-repo.git --path bots/hello/manifests/envs/dev --dest-server https://kubernetes.default.svc --dest-namespace infra-bot $ argocd app set hello --sync-policy automated $ arogcd app set hello --auto-prune この操作で ArgoCD が GitHub 上の Manifests の変化に応じてボットのコンテナイメージを ECR から取得して GitOps を実現してくれます。\nKustomize で環境 Manifest の環境を分離し、コンテナイメージのタグを編集するために Kustomize を使っています。イメージの更新するためには Kustomize は必須な技術要素な気がしています。\nbots └── ansible ├── Dockerfile ├── README.md ├── bot.go ├── go.mod ├── go.sum ├── main.go ├── manifests │ ├── base │ │ ├── bot-deployment.yaml │ │ ├── bot-svc.yaml │ │ └── kustomization.yaml │ ├── envs │ │ ├── dev │ │ │ ├── deployment.yaml │ │ │ └── kustomization.yaml │ │ └── prd │ │ ├── deployment.yaml │ │ └── kustomization.yaml │ └── manual │ └── secrets.yaml └── requirements.txt GitHub Actions を用いた Docker ビルド, ECR イメージプッシュ, Kustomize Edit GitHub Actions を使ってボットの追加・編集 PR をマージしたタイミングで\n Docker ビルド ECR へのプッシュ Kustomize Edit をして Manifest に記しているコンテナイメージのタグを更新  をしています。GitHub Actions の処理としてはこれだけで、あとは ArgoCD が GitHub 上の Manifests の更新に反応して自動で GitOps してくれます。\nまとめ 構成する技術要素としては以上です。先に述べましたが kubernetes を SRE として運用することは昨今の SRE/インフラ界隈のトレンドを押さえるという意味で意味があることですし、今回数ヶ月ぶりに EKS を評価・検証して、技術として速いスピードで成熟しているのを実感しました。今後 EKS を本番サービスでも利用することもあるかもしれないです。ALB Ingress Controller, Metrics Server だけではなく ArgoCD も今年の2月に評価した際よりだいぶ完成度が増している気がします。\nEKS 導入前は Docker Compose でボットをデプロイしていて半自動化状態でした。ボットを作る足かせにもなっていたので、EKS の導入で今後のボット開発の敷居が下がり、自動化がますます推進されればいいなと思っています。\n","permalink":"https://jedipunkz.github.io/post/eks-fargate-bot-env/","summary":"こんにちは。jedipunkz です。\n仕事ではこれから会社のサービス環境として AWS ECS の導入を始めていくところなのですが、最近の SRE/インフラ界隈のトレンドを自分たちが追うために自分たち SRE が管理しているボット環境を EKS を使って GitOps 化しようということになり色々と準備を進めています。導入までもう一歩のところまで来たので、構成や技術要素についてここに記したいと思います。\nどんなボットが動いているの？ まずボット開発に用いてる言語は Go 言語です。主に aws-sdk-go を使ってボットを開発しています。私達はコミュニケーションツールとして Slack を使っているので Slack ボット化するためには slack-go を使っています。 ただまだボットの数が少なく主に利用されてるのは Ansible を実行するモノです。開発環境へアプリをデプロイするのに Ansible を使っています。もうすぐ ECS 化するので役割はそろそろ終えるのですが\u0026hellip; 利点は開発者だけでなく非エンジニアの方が GitHub のブランチ上のアプリの動作をしたい際に Slack を使って簡単にアプリの動作ができるところです。今後は自動化目的でもっと沢山のボットを開発していきたいです。\nEKS/Fargate vs EKS/EC2 EKS の利用を検討する際に Fargate タイプと EC2 タイプがあります。2020年の今年頭に評価した際には ALB Ingress Controller と HPA のための Metrics Server が正常に動作しない状態だったので、まだ EC2 タイプを選択すべきなのかな\u0026hellip;と考えたのですが AWS 的にも Fargate を推してる気もするし再度評価実施しました。結果ドキュメントもソフトウェアもだいぶ更新されていて ALB Ingress Controller も Metrics Server もあっけなく動作し、今回のボット環境も EKS/Fargte を選択しました。","title":"EKS/Fargate + ArgoCD でボット環境 GitOps 化"},{"content":"こんにちは。@jedipunkz です。\n今日は Pulumi (https://www.pulumi.com/) について紹介したいと思います。\n最近ではすっかり Terraform がマルチクラウドな IaC ツールとして定着しましたが、巷では AWS CDK を使う現場も増えてきている印象です。AWS CDK は Typescript, Python などのプログラミング言語の中でインフラを定義・操作することができる AWS が提供しているツールです。ただ AWS CDK は名前の通り AWS にのみ対応していて内部的には Cloudformation Template がエキスポートされています。AWS オンリーという点と Cloudformation という点、また 2019 年時点で進化が激しく後方互換性を全く失っているので AWS CDK のアップデートに追従するのに苦労する点、色々ありまだ利用するには早いのかなぁという印象を個人的には持っています。\nそこで今回紹介する Pulumi なのですが CDK 同様にプログラミング言語の中でインフラを定義できて尚且つマルチクラウド対応しています。どちらかというと旧来の SDK の分類だと思うのですが、Terraform 同様にマルチクラウドという点で個人的には以前よりウォッチしていました。\n今回は公式の Getting Started 的なドキュメントに従って作業を進め Kubernetes の上に Pod を起動、その後コードを修正して再デプロイを実施して理解を深めてみたいと思います。\n作業に必要なソフトウェア 下記のソフトウェア・ツールが事前にインストールされていることを前提としたいと思います。また macOS を前提に手順を記します。\n Python3, Pip Minikube  参考  https://www.pulumi.com/docs/get-started/kubernetes/  事前準備 まず macOS を使っている場合 Pulumi は下記の通り brew を使ってインストールできます。\n$ brew install pulumi また minikube を使って kubernetes を起動します。\n$ minikube start --memory=4096 --cpus=2 Pulumi を使った Nginx Pod の起動 早速 Nginx Pod を Kubernetes 情に起動する作業を行ってみます。適当なディレクトリを作成しその中で pulumi new を実行します。\n$ mkdir quickstart \u0026amp;\u0026amp; cd quickstart $ pulumi new kubernetes-python 下記の通り出力され入力を促されます。今回はデフォルト値をそのまま利用するのでエンターを数回入力します。\nproject name: (quickstart) project description: (A minimal Kubernetes Python Pulumi program) Created project 'quickstart' Please enter your desired stack name. To create a stack in an organization, use the format \u0026lt;org-name\u0026gt;/\u0026lt;stack-name\u0026gt; (e.g. `acmecorp/dev`). stack name: (dev) Created stack 'dev' すると __main__.py というファイルが自動生成されています。Kubernetes 上に nginx pod を起動するための Python コードです。中身を見てみましょう。nginx の Deployment が定義されているのが理解できると思います。\nimport pulumi from pulumi_kubernetes.apps.v1 import Deployment app_labels = { \u0026#34;app\u0026#34;: \u0026#34;nginx\u0026#34; } deployment = Deployment( \u0026#34;nginx\u0026#34;, spec={ \u0026#34;selector\u0026#34;: { \u0026#34;match_labels\u0026#34;: app_labels }, \u0026#34;replicas\u0026#34;: 1, \u0026#34;template\u0026#34;: { \u0026#34;metadata\u0026#34;: { \u0026#34;labels\u0026#34;: app_labels }, \u0026#34;spec\u0026#34;: { \u0026#34;containers\u0026#34;: [{ \u0026#34;name\u0026#34;: \u0026#34;nginx\u0026#34;, \u0026#34;image\u0026#34;: \u0026#34;nginx\u0026#34; }] } } }) pulumi.export(\u0026#34;name\u0026#34;, deployment.metadata[\u0026#34;name\u0026#34;]) ここで今回は Python を使っているので pip を使って Pulumi Module をインストールする必要があります。同じディレクトリ内に自動生成された requirements.txt がありますのでそれに従って pip install します。\n$ pip install -r requirements.txt それでは下記の通り pulumi up を実行し Nginx Pod を起動していましょう。\n$ pulumi up Previewing update (dev): Type Name Plan + pulumi:pulumi:Stack quickstart-dev create + └─ kubernetes:apps:Deployment nginx create Resources: + 2 to create Do you want to perform this update? \u0026gt; yes no details 結果として下記のように出力されステータス \u0026lsquo;created\u0026rsquo; となりました。\n Type Name Status + pulumi:pulumi:Stack quickstart-dev created + └─ kubernetes:apps:Deployment nginx created Outputs: name: \u0026quot;nginx-td4rq3xr\u0026quot; Resources: + 2 created Duration: 14s Permalink: https://app.pulumi.com/jedipunkz/quickstart/dev/updates/1 kubectl コマンドでも確認していましょう。下記のように nginx-**** pod が起動していることが分かります。\n$ kubectl get pod NAME READY STATUS RESTARTS AGE nginx-td4rq3xr-86c57db685-nfblb 1/1 Running 0 46m __main__.py コードを修正して外部から Nginx にアクセス 次に __main__.py を下記の通り修正して Nginx Pod に curl を使ってアクセスできるようにしてみます。 コードの中で minikube か否かについての Config pulumi.Config() に関する記述と Deployment に IP を確保する記述があります。\nimport pulumi from pulumi_kubernetes.apps.v1 import Deployment from pulumi_kubernetes.core.v1 import Service # Minikube does not implement services of type `LoadBalancer`; require the user to specify if we\u0026#39;re # running on minikube, and if so, create only services of type ClusterIP. config = pulumi.Config() is_minikube = config.require_bool(\u0026#34;isMinikube\u0026#34;) app_name = \u0026#34;nginx\u0026#34; app_labels = { \u0026#34;app\u0026#34;: app_name } deployment = Deployment( app_name, spec={ \u0026#34;selector\u0026#34;: { \u0026#34;match_labels\u0026#34;: app_labels }, \u0026#34;replicas\u0026#34;: 1, \u0026#34;template\u0026#34;: { \u0026#34;metadata\u0026#34;: { \u0026#34;labels\u0026#34;: app_labels }, \u0026#34;spec\u0026#34;: { \u0026#34;containers\u0026#34;: [{ \u0026#34;name\u0026#34;: app_name, \u0026#34;image\u0026#34;: \u0026#34;nginx\u0026#34; }] } } }) # Allocate an IP to the Deployment. frontend = Service( app_name, metadata={ \u0026#34;labels\u0026#34;: deployment.spec[\u0026#34;template\u0026#34;][\u0026#34;metadata\u0026#34;][\u0026#34;labels\u0026#34;], }, spec={ \u0026#34;type\u0026#34;: \u0026#34;ClusterIP\u0026#34; if is_minikube else \u0026#34;LoadBalancer\u0026#34;, \u0026#34;ports\u0026#34;: [{ \u0026#34;port\u0026#34;: 80, \u0026#34;target_port\u0026#34;: 80, \u0026#34;protocol\u0026#34;: \u0026#34;TCP\u0026#34; }], \u0026#34;selector\u0026#34;: app_labels, }) # When \u0026#34;done\u0026#34;, this will print the public IP. if is_minikube: pulumi.export(\u0026#34;ip\u0026#34;, frontend.spec.apply(lambda v: v[\u0026#34;cluster_ip\u0026#34;] if \u0026#34;cluster_ip\u0026#34; in v else None)) else: pulumi.export(\u0026#34;ip\u0026#34;, frontend.status.apply(lambda v: v[\u0026#34;load_balancer\u0026#34;][\u0026#34;ingress\u0026#34;][0][\u0026#34;ip\u0026#34;] if \u0026#34;load_balancer\u0026#34; in v else None)) 修正が終わったら下記の通り変数 isMinikube に true を設定して、先ほどと同様に pulumi up を実行します。\n$ pulumi config set isMinikube true $ pulumi up 次に下記のコマンドを実行し Pods に付与された IP Addr を調べます。\n$ pulumi stack output ip 最後に minikube のノードにログインし curl を使って先ほど調べた IP Addr 宛に curl でアクセスします。\n$ minikube ssh $ curl 10.108.14.27 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;For online documentation and support please refer to \u0026lt;a href=\u0026#34;http://nginx.org/\u0026#34;\u0026gt;nginx.org\u0026lt;/a\u0026gt;.\u0026lt;br/\u0026gt; Commercial support is available at \u0026lt;a href=\u0026#34;http://nginx.com/\u0026#34;\u0026gt;nginx.com\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;em\u0026gt;Thank you for using nginx.\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; nginx のデフォルトのコンテンツが応答あったことを確認出来ると思います。\nまとめ Python コードの中でインフラを定義することが出来ました。今回は Kubernetes で試しましたが AWS や GCP にも対応しています。同様のことを AWS SDK でも自分は今まで行ってきましたが前述した通りマルチクラウドという点が優位性あるのかなぁという印象です。ただ、クラウド側も常に進化していて、それらに Pulumi が追従し続けられるのか不安な点も感じます。その点ではやはり AWS を利用しているエンジニアにとっては AWS SDK がベストな選択と今時点では言わざるを得ません。また Pulumi という企業自体が安定しているのか、買収されたりしないのかという不安も付き纏います。\nSDK の利用は DevOps・SRE 的に業務を行うのであれば自動化を推進するにあたり必須とも言える技術であると言えます。個人的にはこれはインフラを構築・管理するツールとしての Terraform とは立ち位置が若干異なるという認識でいます。それに対して CDK はどういう立ち位置になるのか。今回紹介した Pulumi が SDK としてのデファクトにのしあがるのかいなか。DevOps・SRE 界隈エンジニアが用いる技術もこれから更に変化していきそうな気がしますし、その過程の中で Pulumi を知って頂くのは良いことだと思います。\n","permalink":"https://jedipunkz.github.io/post/pulumi/","summary":"こんにちは。@jedipunkz です。\n今日は Pulumi (https://www.pulumi.com/) について紹介したいと思います。\n最近ではすっかり Terraform がマルチクラウドな IaC ツールとして定着しましたが、巷では AWS CDK を使う現場も増えてきている印象です。AWS CDK は Typescript, Python などのプログラミング言語の中でインフラを定義・操作することができる AWS が提供しているツールです。ただ AWS CDK は名前の通り AWS にのみ対応していて内部的には Cloudformation Template がエキスポートされています。AWS オンリーという点と Cloudformation という点、また 2019 年時点で進化が激しく後方互換性を全く失っているので AWS CDK のアップデートに追従するのに苦労する点、色々ありまだ利用するには早いのかなぁという印象を個人的には持っています。\nそこで今回紹介する Pulumi なのですが CDK 同様にプログラミング言語の中でインフラを定義できて尚且つマルチクラウド対応しています。どちらかというと旧来の SDK の分類だと思うのですが、Terraform 同様にマルチクラウドという点で個人的には以前よりウォッチしていました。\n今回は公式の Getting Started 的なドキュメントに従って作業を進め Kubernetes の上に Pod を起動、その後コードを修正して再デプロイを実施して理解を深めてみたいと思います。\n作業に必要なソフトウェア 下記のソフトウェア・ツールが事前にインストールされていることを前提としたいと思います。また macOS を前提に手順を記します。\n Python3, Pip Minikube  参考  https://www.pulumi.com/docs/get-started/kubernetes/  事前準備 まず macOS を使っている場合 Pulumi は下記の通り brew を使ってインストールできます。","title":"マルチクラウド対応 SDK の Pulumi を使って Kubernetes を操作"},{"content":"こんにちは。@jedipunkzです。\n今回は AWS ECS についてです。直近の仕事で ECS の Terraform コード開発をしていたのですがコードの構造化について考えていました。一枚岩のコードを書いても運用に耐えられるとは考えられません。また ECS を構成するにあたって ECS のネットワークモードとコンテナのロギングについて考えているうちに、どの構成が一番適しているのか？について時間を掛けて考えました。ここではそれらについてまとめたいと思います。\nTerraform コードの構造化 運用の精神的な負担を軽減するという観点で Terraform のコード開発をする上で一番重要なのはコードの構造化だと思います。前回のブログ記事に書いたのですがコードの構造化をする上で下記に留意して考えると良いと思います。\n 影響範囲 ステートレスかステートフルか 安定度 ライフサイクル  結果、具体的に Terraform のコードはどのような構造になるでしょうか。自分は下記のようにコンポーネント化して Terraform の実行単位を別けました。ここは人それぞれだと思いますが、ECS 本体と ECS の周辺 AWS サービスの一般的な物を考慮しつつ、いかにシンプルに構造化するかを考えると自然と下記の区分けになる気がします。\n   コンポーネント 具体的なリソース     ネットワーク vpc, route table, igw, endpoint, subnet   ECS 本体 ecs, alb, autoscaling, cloudwatch, iam   CI/CD codebuild, codepipeline, ecr, iam   パラメータ ssm, kms   データストア s3, rds, elasticache \u0026hellip;    vpc や subnet に関して頻繁に更新を掛ける人は少ないのではないでしょうか。よってネットワークは \u0026ldquo;影響範囲\u0026rdquo; を考慮しつつコンポーネントを別けました。また、同じ理由でパラメータ・CI/CD も ECS 本体とは実行単位を別けた方が好ましいと思います。また \u0026ldquo;ステートフルかステートレスか\u0026rdquo; という観点でデータベースやストレージは頻繁に更新する ECS 本体とは別けるべきでしょう。\nコンポーネント化する際に Terraform Remote States 機能を用いる 構造化が上記の通りできましたが、具体的にどの様に Terraform の構造化を行うのかについて記したいと思います。\n一枚岩のコードの場合、各 Resources 間で Resource 作成時に得られた id, name, arn の様な Attribute を再利用するケースが多いと思います。ですが Terraform の構造化を行うと Terraform コマンドの実行単位が異なるわけですからそういった処置が行えません。だからと言って構造単位で Variables に値を格納していては問題があるので、ここの対処法として Terraform の Remote State 機能を用います。下記に例を挙げて説明します。\n上記構造の \u0026ldquo;ネットワーク\u0026rdquo; 内の provider.tf で下記のように記して states を s3 へ格納します。\nterraform { required_version = \u0026quot;\u0026gt;= 0.12.0\u0026quot; backend \u0026quot;s3\u0026quot; { bucket = \u0026quot;example-bucket\u0026quot; key = \u0026quot;network/terraform.tfstate\u0026quot; region = \u0026quot;ap-northeast-1\u0026quot; encrypt = true } } \u0026ldquo;ネットワーク\u0026rdquo; 内 vpc.tf で下記のように vpc を作成します。\nresource \u0026quot;aws_vpc\u0026quot; \u0026quot;example\u0026quot; { cidr_block = \u0026quot;10.0.0.0/16\u0026quot; } \u0026ldquo;ネットワーク\u0026rdquo; 内 output.tf で下記のように Terraform Output を指定します。これにより別の構造で値を再利用できます。\noutput \u0026quot;example_vpc_id\u0026quot; { value = aws_vpc.example.id } すると \u0026ldquo;ネットワーク\u0026rdquo; 以外の構造 (例 : ECS 本体等) で下記のように terraform_remote_state を記すと\ndata \u0026quot;terraform_remote_state\u0026quot; \u0026quot;network\u0026quot; { backend = \u0026quot;s3\u0026quot; config = { bucket = \u0026quot;example-bucket\u0026quot; key = \u0026quot;network/terraform.tfstate\u0026quot; region = \u0026quot;ap-northeast-1\u0026quot; encrypt = true } } 下記のように他の構造(この場合ネットワーク VPC) を作成した際の Terraform Attribute が呼び出せます。\nvpc_id = data.terraform_remote_state.network.outputs.example_vpc_id 実際の動きとしては vpc を作成した際の terraform tfstate が s3 に格納されていてその s3 内の tfstate を他のコンポーネント作成時に呼び出す、といったことをしています。この機能によりコンポーネント化を容易に実現することが可能です。\nECS 構成 次に ECS 自体の構成がどの様なものが最適なのか検討していきます。\n自分は ECS の構成を考える上で重要な要素として下記があると考えています。\n networkMode logDriver  これら二つについて記していきます。\nnetworkMode ECS にある networkMode のどれを用いるのがベストなのか検討する必要があります。後述するロギングの情報と合わせて考えるので、ここでは一般的な情報のみを記しておきます。\nbridge  ECS インスタンスの任意のポートをコンテナのポートにマッピング ECS インスタンスの ENI を複数のタスクが共有で利用  host  コンテナで expose したポートをインスタンスでも利用 よって基本、同じポートを用いるコンテナを複数起動できない  awsvpc  ENI がタスク毎にアタッチされる タスク間でのポートマッピングの考慮不要 ネットワークパフォーマンスが優れている ENI 毎に SecurityGroup を紐づけ ALB と NLB に IP ターゲットとして登録が可能  結果的には awsvpc がパフォーマンス的にも優れていて最適解です。また logDriver fluentd の通信のことを考えると bridge も検討するべきだと解りますが詳しくは後術します。\nlogDriver ECS の場合のロギングの選択肢としては Cloudwatch Logs か Fluentd か、となると思います。ログの格納先は何が考えられるでしょうか。s3, cloudwatch logs, elasticsearch を通常考えると思いますが、格納した後のログの再利用を考えるとどうなるでしょう。自分は Elasticsearch にログを格納して Kibana で可視化するケースが多いのですが、ここで注意が必要です。Cloudwatch Logs から Elasticsearch Service へログのストリームをする際に圧縮されたデータを展開しつつストリームする必要があります。これを Terraform でコード化するのは結構厄介です。具体的には Lambda Function を作成して Python or Nodejs でコードを書き Firehose を使って Elasticsearch Service へ繋ぎ込みます。その点では Fluentd -\u0026gt; Elasticsearch Service と直接ストリーム出来れば都合がいいです。ということは Fluentd 一択？と考えたいところなのですが\u0026hellip; Fluentd を ECS で扱うことを考えると結構構成に悩みます。\n考慮点を踏まえた結果の ECS 構成 結果、networkMode, logDriver の検討を行うと下記のような構成パターンが選択肢としてあげられます。\nECS/Fargate 構成の場合  networkMode : awsvpc  logDriver : awslogs  -\u0026gt; cloudwatch logs      ECS/EC2 構成の場合 構成(2)\n networkMode : awsvpc  logDriver : awslogs  -\u0026gt; cloudwatch logs      構成(3)\n networkMode : awsvpc  logDriver : fluentd  通信 : socket  起動タイプ : daemon type  -\u0026gt; s3, cloudwatch logs, elasticsearch service          構成(4)\n networkMode : bridge  logDriver : fluentd  通信 : tcp  起動タイプ : daemon type  -\u0026gt; s3, cloudwatch logs, elasticsearch service          構成(5)\n networkMode : awsvpc  logDriver : fluentd  通信 : socket  起動タイプ : sidecar  -\u0026gt; s3, cloudwatch logs, elasticsearch service          構成(6)\n networkMode : bridge  logDriver : fluentd  通信 : tcp  起動タイプ : sidecar  -\u0026gt; s3, cloudwatch logs, elasticsearch service          上記構成案の説明と結果 結果として ECS/Fargate を用いる場合は logDriver fluentd がまだサポートされていないので (2019/10時点)、awslogs となり cloudwatch logs をログ格納先として選択せざるを得ないです。\nまた ECS/EC2 構成の場合は結論を言うと構成 (5), (6) は sidecar 構成で fluentd コンテナを起動して隣接したコンテナのログを転送するのですが、この構成をとる場合下記が原則になります。\n「1タスク / 1インスタンス」\nこれは fluentd で用いる通信が socket, tcp に関わらずインスタンス内の一つのリソースを複数タスクで共有することができないからです。1タスク/1インスタンスでもサービスは提供できますし考えることは少なくなるのですがそうするとデプロイ時にどのような影響を与えるかは考慮しておいた方がいいです。デプロイ発生時にクラスタインスタンスのオートスケールが発生する事はエンジニアによっては良いとは考えないかもしれません。構成(3), (4) なら「nタスク/1インスタンス」の構成が可能で autoscaling policy configuration をうまく設計すればでデプロイ時もオートスケールが発動せずに済みます。\nこの sidecar 構成のタスクとインスタンスの関係についてもう少し記すと\u0026hellip;、\n下記は tcp の場合のコンテナ定義内 logConfiguration です。この場合、インスタンスの24224ポートを指定しています。また fluentd コンテナは portMappings オプションで public (インスタンス) の 24224 ポートを binding しているため、1インスタンスで1 fluentd コンテナしか起動出来ない。つまり1タスク/1インスタンス。\n \u0026quot;logConfiguration\u0026quot;: { \u0026quot;logDriver\u0026quot;: \u0026quot;fluentd\u0026quot;, \u0026quot;options\u0026quot;: { \u0026quot;fluentd-address\u0026quot;: \u0026quot;localhost:24224\u0026quot;, そしてこちらは socket の場合コンテナ定義内 logConfiguration。この場合はインスタンスの volume を コンテナ上でマウントするため1インスタンス上に1 fluentd コンテナしか起動しない。つまり1タスク/1インスタンス\n \u0026quot;logConfiguration\u0026quot;: { \u0026quot;logDriver\u0026quot;: \u0026quot;fluentd\u0026quot;, \u0026quot;options\u0026quot;: { \u0026quot;fluentd-address\u0026quot;: \u0026quot;unix:///var/run/fluent/fluent.sock\u0026quot;, また(3), (4) の daemon type について簡単に述べると、ECS/EC2 のクラスタインスタンス1つに対して必ず1つの fluentd コンテナを起動させる起動モードのことを言います。(https://docs.aws.amazon.com/ja_jp/AmazonECS/latest/developerguide/ecs_services.html)\nよって、Fargate を用いる場合は構成(1)で EC2 構成の場合は構成(2), (3), (4) となるでしょうか。\nアーキテクチャ 図を描くとこんな感じになると思います。大体構成が決まってくるのではないでしょうか。\n 構成(1), (2), (3), (4)の場合  +---------------------+ + | instance / fargate | ... | +---------------------+ | +------+ +------+ | ap-northeast-1a | task | | task | ... | +------+ +------+ | | + +---------------------+ | alb | +---------------------+ | + +------+ +------+ | | task | | task | ... | +------+ +------+ | ap-northeast-1c +---------------------+ | | instance / fargate | ... | +---------------------+ +  構成 (5), (6) の場合  +----------+ + | instance | ... | +----------+ | +----------+ | ap-northeast-1a | task | ... | +----------+ | | + +---------------------+ | alb | +---------------------+ | + +----------+ | | task | ... | +----------+ | ap-northeast-1c +----------+ | | instance | ... | +----------+ + バッチについて検討 バッチ処理も ECS を使って行うことができます。この場合の構成を考えると下記が選択肢として残りました。\n Fargate  networkMode : awsvpc  logDriver : awslogs      構成イメージは下記の様になります。\n+---------+ +------------------+ | task | \u0026lt;--- | cloudwatch event | +---------+ +------------------+ +---------+ | Fargate | +---------+ この構成になったのは ECS/EC2 構成の場合に構成上の問題があることが発覚したからです。EC2 の場合 ECS Service を起動しないとバッチのための Task が起動しないので daemon として何かしらのタスクをインスタンス上に常時稼働させる必要があります。そして aws_cloudwatch_event_target で下記の通りコンテナ定義を上書きすることでバッチ処理を実行できます。\n{ \u0026quot;containerOverrides\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;nginx\u0026quot;, \u0026quot;command\u0026quot;: [\u0026quot;バッチ処理のコマンド\u0026quot;] } ] } よって常時稼働させるインスタンスや ECS Service のリソースが無駄に使用されることになります。そういった点で ECS/Fargate の場合は無駄なインスタンスを起動しなくていい、というメリットがあります。\nCI/CD CI/CD について考えます。下記のようなアーキテクチャと処理の流れを想定します。ここでは CircleCI ではなく CodeBuild, CodePipeline を想定します。\n+--------+ +-----------+ +--------------+ +---------------------+ | Github | - webhook -\u0026gt; | Codebuild | -\u0026gt; | CodePipeline | -\u0026gt; | ECS Cluster/Service | +--------+ +-----------+ +--------------+ +---------------------+ 処理の流れ\n (1) Github の指定ブランチにプッシュされたのをトリガーに CodeBuild でコンテナがビルドされる。ビルド情報は buildspec.yml に指定 (2) ビルドされた結果 imagedefinitions.json が生成される (3) imagedefinitions.json を元に CodePipeline を経由して ECS Clusnter/Service にデプロイ実施 (4) rolling-update が実施される  下記は buildspec.yml。\nversion: 0.2 phases: pre_build: commands: - $(aws ecr get-login --region $AWS_DEFAULT_REGION --no-include-email) - WEB_REPO=$(aws ecr describe-repositories --repository-names web --output text --query \u0026#34;repositories[0].repositoryUri\u0026#34;) - WEB_IMAGE=$WEB_REPO:latest - APP_REPO=$(aws ecr describe-repositories --repository-names app --output text --query \u0026#34;repositories[0].repositoryUri\u0026#34;) - APP_IMAGE=$APP_REPO:latest build: commands: - docker build -t $WEB_IMAGE ./web - docker push $WEB_IMAGE - docker build -t $APP_IMAGE ./app - docker push $APP_IMAGE post_build: commands: - printf \u0026#39;[{\u0026#34;name\u0026#34;:\u0026#34;web\u0026#34;,\u0026#34;imageUri\u0026#34;:\u0026#34;%s\u0026#34;}, {\u0026#34;name\u0026#34;:\u0026#34;app\u0026#34;,\u0026#34;imageUri\u0026#34;:\u0026#34;%s\u0026#34;}]\u0026#39; $WEB_IMAGE $APP_IMAGE \u0026gt; imagedefinitions.json artifacts: files: - imagedefinitions.json まとめ networkMode, logDriver を考える中で構成が決まりました。また Terraform でコード化する際の構造化についての考えをまとめ、付随する CI/CD, バッチについてもまとめました。完成するコードは必然的にシンプルなものになると思います。また ECS/Fargate 構成での logDriver の fluentd 対応については今進んでいる最中とのことで、期待しています。そうすると2019年初めのコストダウンと相まってより Fargate を選択しやすい状況になるのではないでしょうか。\n","permalink":"https://jedipunkz.github.io/post/ecs/","summary":"こんにちは。@jedipunkzです。\n今回は AWS ECS についてです。直近の仕事で ECS の Terraform コード開発をしていたのですがコードの構造化について考えていました。一枚岩のコードを書いても運用に耐えられるとは考えられません。また ECS を構成するにあたって ECS のネットワークモードとコンテナのロギングについて考えているうちに、どの構成が一番適しているのか？について時間を掛けて考えました。ここではそれらについてまとめたいと思います。\nTerraform コードの構造化 運用の精神的な負担を軽減するという観点で Terraform のコード開発をする上で一番重要なのはコードの構造化だと思います。前回のブログ記事に書いたのですがコードの構造化をする上で下記に留意して考えると良いと思います。\n 影響範囲 ステートレスかステートフルか 安定度 ライフサイクル  結果、具体的に Terraform のコードはどのような構造になるでしょうか。自分は下記のようにコンポーネント化して Terraform の実行単位を別けました。ここは人それぞれだと思いますが、ECS 本体と ECS の周辺 AWS サービスの一般的な物を考慮しつつ、いかにシンプルに構造化するかを考えると自然と下記の区分けになる気がします。\n   コンポーネント 具体的なリソース     ネットワーク vpc, route table, igw, endpoint, subnet   ECS 本体 ecs, alb, autoscaling, cloudwatch, iam   CI/CD codebuild, codepipeline, ecr, iam   パラメータ ssm, kms   データストア s3, rds, elasticache \u0026hellip;    vpc や subnet に関して頻繁に更新を掛ける人は少ないのではないでしょうか。よってネットワークは \u0026ldquo;影響範囲\u0026rdquo; を考慮しつつコンポーネントを別けました。また、同じ理由でパラメータ・CI/CD も ECS 本体とは実行単位を別けた方が好ましいと思います。また \u0026ldquo;ステートフルかステートレスか\u0026rdquo; という観点でデータベースやストレージは頻繁に更新する ECS 本体とは別けるべきでしょう。","title":"ECS の構成と Terraform コード化する際の構造化について"},{"content":"こんにちは。@jedipunkzです。\n今回は電子書籍 \u0026lsquo;Pragmatic Terraform on AWS\u0026rsquo; を読んでとても内容が素晴らしかったので紹介させて頂きます。書籍の購入は下記の URL から可能です。\nhttps://booth.pm/ja/items/1318735\nブログ記事では書籍の細かい内容については述べません。これから購入される方々が買う意欲を無くすようなブログ記事にしたくないからです。なのでこのブログでは自分が Terraform 運用について感じていた問題点とこの電子書籍がどう解決してくれたのかについて記したいと思います。\n自分が Terraform 運用で感じていた問題点 Terraform を使ったインフラコード化と構築は自分の結構経験があるのですが、その構築に使ったコードで構成を運用する難しさはいつも感じていました。Terraform を使った継続的なインフラの運用についてです。具体的には下記のような疑問と言いますか問題点です。\n (1) どのような実行単位で .tf コードを書くか (2) 削除系・修正系の操作も Terraform で行うのか (3) ステートフルなインフラとステートレスなインフラの管理方法  (1) は terraform plan/apply を実行するディレクトリの構造についてです。Terraform は同じディレクトリ上にある .tf ファイル全てを読み込んでくれますし一斉にインフラをデプロイすることも可能です。ですが、何かインフラを修正・削除したい場合、削除してはいけないリソースも同ディレクトリ上の .tf ファイルで管理しているわけですから何かしらのミスで大事なインフラに影響を与えてしまう事になります。影響範囲が大きすぎるのです。\n(2) は、\u0026lsquo;初期の構成のみを Terraform で構築自動化する\u0026rsquo; のかどうか、ということになります。構築に使ったコードで継続的に削除系・修正系の操作も行うのか。これも (1) と同様に管理するインフラの規模が大きくなると影響範囲が大きくなり、運用者の精神的負担が増します。\n(3) は RDS, S3 等のステートフルなインフラと、それ以外のステートレスなインフラを同じ .tf コードで管理していいのか、という疑問です。修正・削除が多発するインフラは .tf コードで継続的に運用出来たとしても、RDS, S3 の様な状態が重要になるインフラは滅多に削除・修正操作は通常行いません。これら二種類のインフラを同じように管理してしまっていいのか？という疑問です。\nこれらの疑問や思っていた問題点について、この \u0026lsquo;Pragmatic Terraform on AWS\u0026rsquo; は全て解決してくれました。\nPragmatic Terraform on AWS の構成 章ごとの説明は詳細には書きませんが、大体の流れは下記のようになっています。\n (1) 基本的な利用方法 (2) Tips 集 (3) ECS Fargate を中心にした実践的な構成のコード化 (4) 構造化 (5) ベストプラクティス (6) モジュール設計  (2) Tips 集について 席に述べたように自分は Terraform の経験が結構ある方だと思うのですが、それでも知らない Terraform の機能が複数ありました。特に Terraform 利用者にとって、要点だけをおさえた説明文と共に次々に多くの Tips を説明してくれるこの章はとても意味があるものではないでしょうか。素晴らしいです。\n(3) ECS Fargate を中心にした実践的な構成のコード化 賛否両論あるかもしれませんが2019年現在でサービスを構成しようとすると GKE か ECS か EKS かの3択なのではないでしょうか。特にサービス・プロバイダにとってマネージドサービスは主流になると思います。その中でも国内では AWS の利用率が高いわけですから ECS を理解しておくことはインフラ系のエンジニアにとって非常に重要になります。\nこの章では ECS Fargate を中心として下記のような特徴を持ったシステムをコード化するという実践的なものになっています。\n ECS とその裏側にある ALB, IAM, VPC 等の基本的な構成構築 ECS による Web サービス・ECS によるバッチサービス構成構築 鍵管理 設定管理 デプロイメントパイプライン ロギング  特に CodePipeline, Github, ECR を使ったコンテナの継続的デプロイに関する説明と、Cloudwatch Logs, S3, Athena, Kinesis Data Firehose を使ったトレーサビリティを上げるという意味でのログ管理の説明は、「ここまで実践的な説明してくれるのか」という印象を持ちました。2019年時点でインフラエンジニアをしている方は駆らず理解しておくべき内容だと思います。\n(4) 構造化 ここが自分的には一番知りたかった内容でした。公式ドキュメントに断片的な情報はあっても、やはり Terraform ユーザの生の声を聞きたいなと感じていたので、この章は非常に自分にとって役立つものでした。\n誰でも思いつく .tf コードの分離・別ディレクトリ管理・別レポジトリ管理から、何を意識してコンポーネント化するかの考え方がソフトウェア設計を元に非常に説得力ある説明と共に解説されています。\nこの章は概略を書いてしまうと内容が把握出来てしまうので、興味ある方は書籍を購入して読んでみてください。きっと損はしないです。\n(5), (6) ベストプラクティスとモジュール設計 運用するなかでおさえておくべきポイントが掲載されています。モジュール化を行う上での注意点等、運用に入ってから苦労しないように色んな技が掲載されています。\nまとめ 以上、自分の読書感想でしたが、素晴らしさは伝わりましたでしょうか。Terraform を使っているエンジニアであれば必ず疑問に感じる点を見事に解決に導いている良書だと思いますし、説明がとても簡潔で良いテンポで理解できる内容でした。また情報量や文字数等、自分にとって最適でしたし、内容的には \u0026ldquo;Terraform を使い始めた人\u0026rdquo; から \u0026ldquo;Terraform を使っているが苦労している人\u0026rdquo; までをカバーしたものになっています。\nネット上には断片的には情報はあっても、ここまでまとまった内容のサイトは無いと思いますし、実際に Terraform を使っているエンジニアだからこその \u0026ldquo;運用を行う上での解決策\u0026rdquo; が満載な書籍でした。\n","permalink":"https://jedipunkz.github.io/post/2019/07/27/pragmatic-terraform/","summary":"こんにちは。@jedipunkzです。\n今回は電子書籍 \u0026lsquo;Pragmatic Terraform on AWS\u0026rsquo; を読んでとても内容が素晴らしかったので紹介させて頂きます。書籍の購入は下記の URL から可能です。\nhttps://booth.pm/ja/items/1318735\nブログ記事では書籍の細かい内容については述べません。これから購入される方々が買う意欲を無くすようなブログ記事にしたくないからです。なのでこのブログでは自分が Terraform 運用について感じていた問題点とこの電子書籍がどう解決してくれたのかについて記したいと思います。\n自分が Terraform 運用で感じていた問題点 Terraform を使ったインフラコード化と構築は自分の結構経験があるのですが、その構築に使ったコードで構成を運用する難しさはいつも感じていました。Terraform を使った継続的なインフラの運用についてです。具体的には下記のような疑問と言いますか問題点です。\n (1) どのような実行単位で .tf コードを書くか (2) 削除系・修正系の操作も Terraform で行うのか (3) ステートフルなインフラとステートレスなインフラの管理方法  (1) は terraform plan/apply を実行するディレクトリの構造についてです。Terraform は同じディレクトリ上にある .tf ファイル全てを読み込んでくれますし一斉にインフラをデプロイすることも可能です。ですが、何かインフラを修正・削除したい場合、削除してはいけないリソースも同ディレクトリ上の .tf ファイルで管理しているわけですから何かしらのミスで大事なインフラに影響を与えてしまう事になります。影響範囲が大きすぎるのです。\n(2) は、\u0026lsquo;初期の構成のみを Terraform で構築自動化する\u0026rsquo; のかどうか、ということになります。構築に使ったコードで継続的に削除系・修正系の操作も行うのか。これも (1) と同様に管理するインフラの規模が大きくなると影響範囲が大きくなり、運用者の精神的負担が増します。\n(3) は RDS, S3 等のステートフルなインフラと、それ以外のステートレスなインフラを同じ .tf コードで管理していいのか、という疑問です。修正・削除が多発するインフラは .tf コードで継続的に運用出来たとしても、RDS, S3 の様な状態が重要になるインフラは滅多に削除・修正操作は通常行いません。これら二種類のインフラを同じように管理してしまっていいのか？という疑問です。\nこれらの疑問や思っていた問題点について、この \u0026lsquo;Pragmatic Terraform on AWS\u0026rsquo; は全て解決してくれました。\nPragmatic Terraform on AWS の構成 章ごとの説明は詳細には書きませんが、大体の流れは下記のようになっています。","title":"Pragmatic Terraform on AWS の内容が素晴らしかったので感想を述べる"},{"content":"こんにちは。@jedipunkzです。\n少し前の話なのですが Google Cloud Platform が Terraformer というツールを出しました。正確には数年前に Google が買収した Waze というサービスのエンジニア達が開発したようです。このツールは GCP, AWS, OpenStack, Kubernetes 等、各クラウド・プラットフォームに対応したリバース Terraform と言えるツールになっていて、構築されたクラウド上の状態を元に terraform の .tf コードと .tfstate ファイルをインポートしてくれます。terraform import は tfstate のインポートのみに対応してたのでこれは夢のツールじゃないか！ということで当初期待して使い始めたのですが、使ってみる中で幾つかの問題点も見えてきました。今回はその気になった問題点を中心に Terraformer の基本的な動作を説明していきたいと思います。\n公式サイト 下記の Github アカウントで公開されています。\nhttps://github.com/GoogleCloudPlatform/terraformer\nRequrements Terraformer を動作させるには下記のソフトウェアが必要です。今回は macos を想定して情報を記していますが Linux でも動作します。適宜読み替えてください。インストール方法と設定方法はここでは割愛させて頂きます。\n macos homebrew terraform awscli  今回の前提のクラウドプラットフォーム 自分がいつも使っているプラットフォームということで今回は aws を前提に記事を書いていきます。ここが結構肝心なところで、Google Cloud Platform が開発したこともあり GCP 向けの機能が一番 Feature されているように読み取れます。つまり aws を対象とした Terraformer の機能が一部追いついていない点も後に述べたいと思います。\n動作させるまでの準備 Terraform と同様に Terraformer でも動作せせるディレクトリが必要になります。\nmkdir working_dir cd working_dir Terraformer を動作させるために Terraform の plugin が必要です。先に述べたようにここでは \u0026lsquo;aws\u0026rsquo; Plugin をダウンロードします。そのために init.tf を下記の通り作成します。\necho \u0026#39;provider \u0026#34;aws\u0026#34; {}\u0026#39; \u0026gt; init.tf terraform init コマンドを実行して Terraform \u0026lsquo;aws\u0026rsquo; plugin をダウンロードします。\nterraform init awscli の profile を用意します。インポートしたい環境を管理している aws アカウントに IAM ユーザを作成して、その secret key, secret access key とリージョン情報を記して下さい。\naws configure 基本的動作の確認 インスタンス・サブネット・セキュリティグループ情報をインポート ここではインスタンスを構成する EC2 インスタンス・サブネット・セキュリティグループのリソースタイプを指定してインポート操作を行ってみます。インポートは下記のコマンド1つ実行するのみです。\nterraformer import aws --resources=ec2_instance,sg,subnet --connect=true --regions=ap-northeast-1 結果、下記のようなディレクトリ構成でファイルがインポートされます。\n. ├ ─ ─ generated │ └ ─ ─ aws │ ├ ─ ─ ec2_instance │ │ └ ─ ─ ap-northeast-1 │ │ ├ ─ ─ instance.tf │ │ ├ ─ ─ outputs.tf │ │ ├ ─ ─ provider.tf │ │ └ ─ ─ terraform.tfstate │ ├ ─ ─ sg │ │ └ ─ ─ ap-northeast-1 │ │ ├ ─ ─ outputs.tf │ │ ├ ─ ─ provider.tf │ │ ├ ─ ─ security_group.tf │ │ └ ─ ─ terraform.tfstate │ └ ─ ─ subnet │ └ ─ ─ ap-northeast-1 │ ├ ─ ─ outputs.tf │ ├ ─ ─ provider.tf │ ├ ─ ─ subnet.tf │ ├ ─ ─ terraform.tfstate │ └ ─ ─ variables.tf └ ─ ─ init.tf 早速インスタンスを構成する generated/aws/ec2_instance/ap-northeast-1/instance.tf を確認してみます。\nresource \u0026quot;aws_instance\u0026quot; \u0026quot;i-************_test-instance\u0026quot; { ami = \u0026quot;ami-***********\u0026quot; associate_public_ip_address = true availability_zone = \u0026quot;ap-northeast-1a\u0026quot; cpu_core_count = \u0026quot;1\u0026quot; cpu_threads_per_core = \u0026quot;1\u0026quot; snip ... subnet_id = \u0026quot;subnet-****************\u0026quot; // サブネット snip... vpc_security_group_ids = [\u0026quot;sg-****************\u0026quot;] // セキュリティグループ このディレクトリ構成と instance.tf の内容から下記のことが言えると思います。\n aws インフラリソース毎に terraform を実行する想定のディレクトリが別れている insntace.tf を見るとリソース ID が直打ちになっていて依存関係が解決されていないコードになっている  filter オプションで特定のリソースのコードだけインポート 下記のように filter オプションを用いて特定の ID のリソースだけ .tf, .tfstate ファイルをインポートすることも可能です。\nterraformer import aws --resources=vpc --filter=aws_vpc=vpc-********* --regions=ap-northeast-1 ここまでで幾つか問題点に気がつくと思います。\n自分が気になった利用する上での問題点その1 Terraform は様々な単位でコードを束ねる事は勿論なのですが、何かしらのサービスを形成するシステム一式という単位でコードを束ねる事はしてくれない、ということになります。例えば instance, subnet 両方に影響を与える修正をする場合 terraform コマンドを2つのディレクトリで実行しなければいけないことになります。なんだかスッキリしません\u0026hellip;。\n自分が気になった利用する上での問題点その2 通常 Terraform のコードを書く際に Resource 間の依存関係保ちます。例えば \u0026lsquo;aws_instance\u0026rsquo; 内の \u0026lsquo;subnet_id\u0026rsquo; には通常下記のように依存関係を記します。\nsubnet_id = \u0026quot;${aws_subnet.foo.id}\u0026quot; ですが生成されたコードを確認すると id がベタ書きされています。Resource 間の依存関係が解決されていません。\n自分が気になった利用する上での問題点その3 Terraform の Module 機能が用いられていないコードが生成されています。コード全体を簡潔に尚且可読性良くするために Module 機能を用いて Terraform コードを書くのが一般的になっているので、全くのベタ書きなコードが生成されているところに若干問題を感じます。\n今現在 2019/07 で対応している aws インフラリソース一覧 2019/07 現在で対応している aws のインフラリソース一覧です。Terraform の Resources 名で記してあります。\n aws_elb aws_lb aws_lb_listener aws_lb_listener_rule aws_lb_listener_certificate aws_lb_target_group aws_lb_target_group_attachment aws_autoscaling_group aws_launch_configuration aws_launch_template aws_db_instance aws_db_parameter_group aws_db_subnet_group aws_db_option_group aws_db_event_subscription aws_iam_role aws_iam_role_policy aws_iam_user aws_iam_user_group_membership aws_iam_user_policy aws_iam_policy_attachment aws_iam_policy aws_iam_group aws_iam_group_membership aws_iam_group_policy aws_internet_gateway aws_network_acl aws_s3_bucket aws_s3_bucket_policy aws_security_group aws_subnet aws_vpc aws_vpn_connection aws_vpn_gateway aws_route53_zone aws_route53_record aws_acm_certificate aws_elasticache_cluster aws_elasticache_parameter_group aws_elasticache_subnet_group aws_elasticache_replication_group aws_cloudfront_distribution aws_instance  自分が気になった利用する上での問題点その3 Google Cloud Platform が開発していることもあり Google Cloud の対応状況は良いと思うのですが(ごめんなさい、あまり確認出来ていません)ですが aws に関して対応されているリソースは下記で全てです。例えば基本的なインフラリソース aws_iam_instance_profile もないのでリソース ID を直打ちする必要がありますし、cloudwatch 等もありません。対応状況はだいぶ良くないのかなぁという印象です。\n考察 以上、簡単にですが基本的な利用方法と自分が気になった箇所を紹介させてもらいました。\nTerraform 自体にもインポート機能がありますが tfstate のみ対応していて .tf をインポートすることが出来ません。なのでこのツールへの期待はとても大きかったのですが、いま現時点では既存環境のコードを書く時にインポートして参考にする、という程度の使い方しか出来ないかもしれません。問題点を中心に Terraformer がインポートするコードの特徴を下記にあげます。\n terraform plan/apply 実行する単位が Resource 単位に別れてしまっている Resource 間の依存関係が解決されていない Module が用いられていない .tf コード 対応している aws Resource がまだまだ追いついていない  ただ、まだ発表されて間もないツールなので今後に期待です。完成度が上がれば最高のツールになるでしょう。対応している aws resource がまだ追い付いていないもの今後開発が進めば時間だけの問題な気もします。また今回私は動作確認出来ていないのですが、GCP であれば対応している Resource 状況も良さそうです。\n","permalink":"https://jedipunkz.github.io/post/2019/07/26/terraformer/","summary":"こんにちは。@jedipunkzです。\n少し前の話なのですが Google Cloud Platform が Terraformer というツールを出しました。正確には数年前に Google が買収した Waze というサービスのエンジニア達が開発したようです。このツールは GCP, AWS, OpenStack, Kubernetes 等、各クラウド・プラットフォームに対応したリバース Terraform と言えるツールになっていて、構築されたクラウド上の状態を元に terraform の .tf コードと .tfstate ファイルをインポートしてくれます。terraform import は tfstate のインポートのみに対応してたのでこれは夢のツールじゃないか！ということで当初期待して使い始めたのですが、使ってみる中で幾つかの問題点も見えてきました。今回はその気になった問題点を中心に Terraformer の基本的な動作を説明していきたいと思います。\n公式サイト 下記の Github アカウントで公開されています。\nhttps://github.com/GoogleCloudPlatform/terraformer\nRequrements Terraformer を動作させるには下記のソフトウェアが必要です。今回は macos を想定して情報を記していますが Linux でも動作します。適宜読み替えてください。インストール方法と設定方法はここでは割愛させて頂きます。\n macos homebrew terraform awscli  今回の前提のクラウドプラットフォーム 自分がいつも使っているプラットフォームということで今回は aws を前提に記事を書いていきます。ここが結構肝心なところで、Google Cloud Platform が開発したこともあり GCP 向けの機能が一番 Feature されているように読み取れます。つまり aws を対象とした Terraformer の機能が一部追いついていない点も後に述べたいと思います。\n動作させるまでの準備 Terraform と同様に Terraformer でも動作せせるディレクトリが必要になります。\nmkdir working_dir cd working_dir Terraformer を動作させるために Terraform の plugin が必要です。先に述べたようにここでは \u0026lsquo;aws\u0026rsquo; Plugin をダウンロードします。そのために init.","title":"期待のツール Terrafomer の基本動作方法と問題点"},{"content":"こんにちは。@jedipunkzです。\n今回は Hashicorp の Consul クラスタを Kubernetes 上で稼働させる方法について調べてみました。\nHashicorp Consul はサービスディスカバリが行えるソフトウェアで、私も以前居た職場で利用していました。アプリケーション間で互いに接続先を確認し合う事が出来ます。以前構築した Consul クラスタはインスタンス上に直に起動していたのですが最近だとどうやってデプロイするのか興味を持ち Kubernetes 上にデプロイする方法を調べた所 Helm を使って簡単にデプロイ出来る事が分かりました。\nまた今回は minikube を使って複数のレプリカを起動するようにしていますが、Helm Chart を用いると Kubernetes のノード毎に Consul Pod が1つずつ起動するようになっていて、ノードの障害を考慮した可用性という点でも優れているなぁと感じました。また Kubernetes の Pod ですのでプロセスが落ちた際に即座に再起動が行われるという点でも優れています。勿論 Consul クラスタの Leader が落ちた場合には Leader Election (リーダ昇格のための選挙) が行われ、直ちに隣接した Kubernetes ノード上の Consul Pod がリーダーに昇格します。といった意味でも Kubernetes 上に Consul をデプロイするという考えは優れているのではないでしょうか。\nRequirements 下記のソフトウェアが事前に必要です。この手順では予めこれらがインストールされていることとして記していきます。\n minikube kubectl helm  Consul クラスタ起動までの手順 早速ですが手順を記していきます。\nHashicorp の Github にて Consul の Helm Chart が公開されています。helm search しても出てきますが、今回は Github のものを用いました。\ngit clone https://github.com/hashicorp/consul-helm.git cd consul-helm git checkout v0.7.0 次にコンフィギュレーションを記した yaml ファイルを生成 (修正) します。本来、Kubernetes のノード毎に1つずつの Pod が起動するようになっていて、逆に言うと1ノードに複数の Consul Pod は起動しません。今回は手元も端末の minikube でお手軽に試せるようレポジトリ上のファイル value.yml を下記のように修正加えました。この手順は Github の Issue https://github.com/hashicorp/consul-helm/issues/13 で記されています。\nもちろん、minikube ではなく Kubernetes 環境を利用できる方はこの手順は飛ばして構いません。\n# 下記はコメントアウト # affinity: | # podAntiAffinity: # requiredDuringSchedulingIgnoredDuringExecution: # - labelSelector: # matchLabels: # app: {{ template \u0026#34;consul.name\u0026#34; . }} # release: \u0026#34;{{ .Release.Name }}\u0026#34; # component: server # topologyKey: kubernetes.io/hostname # minikube 用に下記を有効にする affinity: |podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 podAffinityTerm: topologyKey: kubernetes.io/hostname labelSelector: matchExpressions: - key: component operator: In values: - \u0026#34;{{ .Release.Name }}-{{ .Values.Component }}\u0026#34; kubectl, helm コマンドが minikube を向くように下記のようにコマンドを実行します。\nkubectl config use-context minikube helm init Consul クラスタを helm を用いてデプロイします。下記のコマンドでデプロイが一気に完了します。\nhelm install --name consul . 暫くすると下記の通り Pods, Services の状態が確認出来ると思います。\n$ kubectl get pods NAME READY STATUS RESTARTS AGE consul-q5s62 1/1 Running 0 21m consul-server-0 1/1 Running 0 21m consul-server-1 1/1 Running 0 21m consul-server-2 1/1 Running 0 21m $ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE consul-dns ClusterIP 10.108.171.87 \u0026lt;none\u0026gt; 53/TCP,53/UDP 22m consul-server ClusterIP None \u0026lt;none\u0026gt; 8500/TCP,8301/TCP,8301/UDP,8302/TCP,8302/UDP,8300/TCP,8600/TCP,8600/UDP 22m consul-ui ClusterIP 10.107.40.197 \u0026lt;none\u0026gt; 80/TCP 22m kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 48m 起動した Consul クラスタの状態を確認してみます。consul members コマンドを各 Pod 上で実行すると、クラスタに Join しているノード (この場合 Consul Pod) の状態を一覧表示出来ます。minikube ノードにも consul-agent が起動していることが確認出来ます。\n$ for i in {0..2}; do kubectl exec consul-server-$i -- sh -c \u0026#39;consul members\u0026#39;; done Node Address Status Type Build Protocol DC Segment consul-server-0 172.17.0.6:8301 alive server 1.4.4 2 dc1 \u0026lt;all\u0026gt; consul-server-1 172.17.0.7:8301 alive server 1.4.4 2 dc1 \u0026lt;all\u0026gt; consul-server-2 172.17.0.8:8301 alive server 1.4.4 2 dc1 \u0026lt;all\u0026gt; minikube 172.17.0.5:8301 alive client 1.4.4 2 dc1 \u0026lt;default\u0026gt; Node Address Status Type Build Protocol DC Segment consul-server-0 172.17.0.6:8301 alive server 1.4.4 2 dc1 \u0026lt;all\u0026gt; consul-server-1 172.17.0.7:8301 alive server 1.4.4 2 dc1 \u0026lt;all\u0026gt; consul-server-2 172.17.0.8:8301 alive server 1.4.4 2 dc1 \u0026lt;all\u0026gt; minikube 172.17.0.5:8301 alive client 1.4.4 2 dc1 \u0026lt;default\u0026gt; Node Address Status Type Build Protocol DC Segment consul-server-0 172.17.0.6:8301 alive server 1.4.4 2 dc1 \u0026lt;all\u0026gt; consul-server-1 172.17.0.7:8301 alive server 1.4.4 2 dc1 \u0026lt;all\u0026gt; consul-server-2 172.17.0.8:8301 alive server 1.4.4 2 dc1 \u0026lt;all\u0026gt; minikube 172.17.0.5:8301 alive client 1.4.4 2 dc1 \u0026lt;default\u0026gt; 次に Consul のリーダーがどの Pod なのかを確認してみます。下記の結果から consule-server-2 という Pod がリーダーだと分かります。\n$ for i in {0..2}; do kubectl exec consul-server-$i -- sh -c \u0026#39;consul info | grep leader\u0026#39;; done leader = false leader_addr = 172.17.0.8:8300 leader = false leader_addr = 172.17.0.8:8300 leader = true leader_addr = 172.17.0.8:8300 この状態で consule-server-2 Pod を削除してみます。削除しても Kubernetes Pod なので即座に再作成・起動がされるのですが、Consul 的には「リーダーが落ちた」という判断が下り、リーダー選出のための選挙が consule-server-0, consul-server-1 の 2 Pods で行われます。\n$ kubectl delete pod consul-server-2 下記が consul-server-0 で確認した Consul プロセスのログになります。\u0026ldquo;consul: New leader elected: consul-server-1\u0026rdquo; と表示され新たに consul-server-1 Pod がリーダーに選出されたことが確認出来ます。\n$ kubectl logs consul-server-0 \u0026lt;snip\u0026gt; 2019/04/26 07:31:22 [INFO] serf: EventMemberLeave: consul-server-2.dc1 172.17.0.8 2019/04/26 07:31:22 [INFO] consul: Handled member-leave event for server \u0026quot;consul-server-2.dc1\u0026quot; in area \u0026quot;wan\u0026quot; 2019/04/26 07:31:26 [INFO] serf: EventMemberLeave: consul-server-2 172.17.0.8 2019/04/26 07:31:26 [INFO] consul: Removing LAN server consul-server-2 (Addr: tcp/172.17.0.8:8300) (DC: dc1) 2019/04/26 07:31:29 [WARN] raft: Rejecting vote request from 172.17.0.7:8300 since we have a leader: 172.17.0.8:8300 2019/04/26 07:31:33 [WARN] raft: Heartbeat timeout from \u0026quot;172.17.0.8:8300\u0026quot; reached, starting election 2019/04/26 07:31:33 [INFO] raft: Node at 172.17.0.6:8300 [Candidate] entering Candidate state in term 3 2019/04/26 07:31:36 [ERR] agent: Coordinate update error: No cluster leader 2019/04/26 07:31:36 [INFO] serf: EventMemberJoin: consul-server-2 172.17.0.8 2019/04/26 07:31:36 [INFO] consul: Adding LAN server consul-server-2 (Addr: tcp/172.17.0.8:8300) (DC: dc1) 2019/04/26 07:31:37 [INFO] serf: EventMemberJoin: consul-server-2.dc1 172.17.0.8 2019/04/26 07:31:37 [INFO] consul: Handled member-join event for server \u0026quot;consul-server-2.dc1\u0026quot; in area \u0026quot;wan\u0026quot; 2019/04/26 07:31:37 [INFO] raft: Node at 172.17.0.6:8300 [Follower] entering Follower state (Leader: \u0026quot;\u0026quot;) 2019/04/26 07:31:37 [INFO] consul: New leader elected: consul-server-1 \u0026lt;snip\u0026gt; 先程と同様にリーダーの確認を下記の通り行います。\n$ for i in {0..2}; do kubectl exec consul-server-$i -- sh -c \u0026#39;consul info | grep leader\u0026#39;; done leader = false leader_addr = 172.17.0.7:8300 leader = true leader_addr = 172.17.0.7:8300 leader = false leader_addr = 172.17.0.7:8300 コンフィギュレーションの例 今回レポジトリ上にある value.yaml を一部修正しただけでしたが、yaml 内にある各設定値を変更することで構成を変更することが可能です。各設定値については下記のサイトに詳細が記されています。公式のドキュメントになります。\nhttps://www.consul.io/docs/platform/k8s/helm.html\n設定値の例をあげると\u0026hellip;\n server - replicas  replicas はレプリカ数。Consul クラスタの Pod 数になります。但し冒頭で述べたとおり Consul Helm Chat は各 Kubernetes ノードに対して 1 Pod の原則で起動しますので、その点認識しておく必要があります。\n server - bootstrapExpect  何台の Consul クラスタ Pod が起動していればリーダー選出を行うか？の設定値です。\n server - storageClass  デフォルトはローカルディスクです。Ceph 等を選択することも可能です。\n client - grpc  agent が gRPC リスナを持つかどうかです。true に設定すると gRPC リスナが 8502 番ポートで起動します。\nまとめ Kubernetes 上で Helm を使って簡単に Consul クラスタをデプロイ出来る事が分かりました。また運用を考慮された設計になっていることも確認出来ました。ロードバランサを用いずとも、アプリケーション間、API 間で互いに正常に可動している先をディスカバリし接続し合えるという点で Consul はとても有用なので Kubernetes を用いたアーキテクチャにも適しているのではないでしょうか。\n","permalink":"https://jedipunkz.github.io/post/consul-helm-chart/","summary":"こんにちは。@jedipunkzです。\n今回は Hashicorp の Consul クラスタを Kubernetes 上で稼働させる方法について調べてみました。\nHashicorp Consul はサービスディスカバリが行えるソフトウェアで、私も以前居た職場で利用していました。アプリケーション間で互いに接続先を確認し合う事が出来ます。以前構築した Consul クラスタはインスタンス上に直に起動していたのですが最近だとどうやってデプロイするのか興味を持ち Kubernetes 上にデプロイする方法を調べた所 Helm を使って簡単にデプロイ出来る事が分かりました。\nまた今回は minikube を使って複数のレプリカを起動するようにしていますが、Helm Chart を用いると Kubernetes のノード毎に Consul Pod が1つずつ起動するようになっていて、ノードの障害を考慮した可用性という点でも優れているなぁと感じました。また Kubernetes の Pod ですのでプロセスが落ちた際に即座に再起動が行われるという点でも優れています。勿論 Consul クラスタの Leader が落ちた場合には Leader Election (リーダ昇格のための選挙) が行われ、直ちに隣接した Kubernetes ノード上の Consul Pod がリーダーに昇格します。といった意味でも Kubernetes 上に Consul をデプロイするという考えは優れているのではないでしょうか。\nRequirements 下記のソフトウェアが事前に必要です。この手順では予めこれらがインストールされていることとして記していきます。\n minikube kubectl helm  Consul クラスタ起動までの手順 早速ですが手順を記していきます。\nHashicorp の Github にて Consul の Helm Chart が公開されています。helm search しても出てきますが、今回は Github のものを用いました。","title":"Consul Helm Chart で Kubernetes 上に Consul をデプロイ"},{"content":"こんにちは。@jedipunkzです。\n前回の記事 「Istio, Helm を使って Getting Started 的なアプリをデプロイ」で kubernetes 上で istio をインストールし sidecar injection を有効化しサンプルアプリケーションを起動しました。その結果、sidecar 的に envoy コンテナが起動するところまで確認しました。今回はもう少し単純な pod を用いて \u0026lsquo;sidecar injection\u0026rsquo; の中身をもう少しだけ深掘りして見ていきたいと思います。\nRquirements 記事と同等の動きを確認するために下記のソフトウェアが必要になります。 それぞれのソフトウェアは事前にインストールされた前提で記事を記していきます。\n macos or linux os kubectl istioctl minikube  参考 URL 下記の istio 公式ドキュメントを参考に動作確認しました。\n https://istio.io/docs/setup/kubernetes/additional-setup/sidecar-injection/ https://istio.io/docs/setup/kubernetes/additional-setup/sidecar-injection/  minikube で kubenetes をデプロイ 前回同様に minikube 上で動作を確認していきます。パラメータは適宜、自分の環境に読み替えてください。\nminikube start --memory=8192 --cpus=4 --kubernetes-version=v1.10.0 \\  --extra-config=controller-manager.cluster-signing-cert-file=\u0026#34;/var/lib/minikube/certs/ca.crt\u0026#34; \\  --extra-config=controller-manager.cluster-signing-key-file=\u0026#34;/var/lib/minikube/certs/ca.key\u0026#34; \\  --vm-driver=virtualbox istio を稼働させる 下記のコマンドを用いてカレントディレクトリに istio のサンプル yaml が入ったフォルダを展開します。\ncurl -L https://git.io/getLatestIstio | sh - 次に下記のコマンドで kubernetes 上に istio をインストールします。 istio コンテナ間の通信をプレインテキスト or TLS で行うよう istio-demo.yml を apply しています。\ncd istio-1.1.3/ kubectl apply -f install/kubernetes/helm/istio-init/files/crd-10.yaml kubectl apply -f install/kubernetes/helm/istio-init/files/crd-11.yaml kubectl apply -f install/kubernetes/helm/istio-init/files/crd-certmanager-10.yaml kubectl apply -f install/kubernetes/helm/istio-init/files/crd-certmanager-11.yaml kubectl apply -f install/kubernetes/istio-demo.yaml 稼働状況を確認します。まず Service の状態です。\nkubectl get svc -n istio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE grafana ClusterIP 10.98.111.63 \u0026lt;none\u0026gt; 3000/TCP 9s istio-citadel ClusterIP 10.97.197.128 \u0026lt;none\u0026gt; 8060/TCP,15014/TCP 8s istio-egressgateway ClusterIP 10.96.35.77 \u0026lt;none\u0026gt; 80/TCP,443/TCP,15443/TCP 9s istio-galley ClusterIP 10.100.143.114 \u0026lt;none\u0026gt; 443/TCP,15014/TCP,9901/TCP 9s istio-ingressgateway LoadBalancer 10.105.202.136 \u0026lt;pending\u0026gt; 15020:30773/TCP,80:31380/TCP,443:31390/TCP,31400:31400/TCP,15029:31398/TCP,15030:32184/TCP,15031:31724/TCP,15032:30064/TCP,15443:30160/TCP 9s istio-pilot ClusterIP 10.102.53.62 \u0026lt;none\u0026gt; 15010/TCP,15011/TCP,8080/TCP,15014/TCP 8s istio-policy ClusterIP 10.105.107.53 \u0026lt;none\u0026gt; 9091/TCP,15004/TCP,15014/TCP 8s istio-sidecar-injector ClusterIP 10.104.82.138 \u0026lt;none\u0026gt; 443/TCP 8s istio-telemetry ClusterIP 10.97.117.166 \u0026lt;none\u0026gt; 9091/TCP,15004/TCP,15014/TCP,42422/TCP 8s jaeger-agent ClusterIP None \u0026lt;none\u0026gt; 5775/UDP,6831/UDP,6832/UDP 7s jaeger-collector ClusterIP 10.107.35.224 \u0026lt;none\u0026gt; 14267/TCP,14268/TCP 7s jaeger-query ClusterIP 10.108.172.46 \u0026lt;none\u0026gt; 16686/TCP 7s kiali ClusterIP 10.107.129.129 \u0026lt;none\u0026gt; 20001/TCP 8s prometheus ClusterIP 10.109.114.141 \u0026lt;none\u0026gt; 9090/TCP 8s tracing ClusterIP 10.108.154.22 \u0026lt;none\u0026gt; 80/TCP 7s zipkin ClusterIP 10.96.151.43 \u0026lt;none\u0026gt; 9411/TCP 7s Pod の状態です。\nkubectl get pods -n istio-system NAME READY STATUS RESTARTS AGE grafana-688b8999cd-d4clx 1/1 Running 0 115m istio-citadel-5749f4b6dd-jd6qm 1/1 Running 0 115m istio-cleanup-secrets-1.1.3-mv8lq 0/1 Completed 0 115m istio-egressgateway-666b76dbf7-mjjx6 1/1 Running 0 115m istio-galley-d68bdc684-nwtzz 1/1 Running 0 115m istio-grafana-post-install-1.1.3-gkn4s 0/1 Completed 0 115m istio-ingressgateway-d67598f4-pwddm 1/1 Running 0 115m istio-pilot-865f6997cd-7jmq4 2/2 Running 0 115m istio-policy-56957d4666-vljk9 2/2 Running 5 115m istio-security-post-install-1.1.3-f894p 0/1 Completed 0 115m istio-sidecar-injector-5cf67ccc65-9p69k 1/1 Running 0 115m istio-telemetry-786796559d-dqwr2 2/2 Running 5 115m istio-tracing-5d8f57c8ff-xps9b 1/1 Running 0 115m kiali-95fcf457f-kfdhp 1/1 Running 0 115m prometheus-5554746896-ccs5x 1/1 Running 0 115m istio-injection な状態でサンプル pod コンテナ \u0026lsquo;sleep\u0026rsquo; を起動 ここで sleep コマンドが起動するだけの pod コンテナを istio-injection=enabled な状態でデプロイします。まず先程ダウンロードしたディレクトリ上の sleep.yaml を見てみましょう。\ncat sample/sleep/sleep.yaml apiVersion: v1 kind: ServiceAccount metadata: name: sleep --- apiVersion: v1 kind: Service metadata: name: sleep labels: app: sleep spec: ports: - port: 80 name: http selector: app: sleep --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: sleep spec: replicas: 1 template: metadata: labels: app: sleep spec: serviceAccountName: sleep containers: - name: sleep image: pstauffer/curl command: [\u0026#34;/bin/sleep\u0026#34;, \u0026#34;3650d\u0026#34;] imagePullPolicy: IfNotPresent --- この yaml ファイルを用いるのですが、istioctl コマンドのサブコマンド kube-inject を用いることでこの元となる yaml ファイルを istio-injection=enabled な状態の yaml ファイルに変換することが出来ます。よって kubectl コマンドで apply する手順は下記になります。\nkubectl apply -f \u0026lt;(istioctl kube-inject -f samples/sleep/sleep.yaml) デプロイされた状態を確認し sidecar コンテナを知る デプロイされた Deployment を見てみましょう。\nkubectl get deployments sleep -o yaml apiVersion: extensions/v1beta1 kind: Deployment metadata: annotations: deployment.kubernetes.io/revision: \u0026quot;1\u0026quot; \u0026lt;snip\u0026gt; spec: containers: - command: - /bin/sleep - 3650d image: pstauffer/curl imagePullPolicy: IfNotPresent name: sleep \u0026lt;snip\u0026gt; image: docker.io/istio/proxyv2:1.1.3 imagePullPolicy: IfNotPresent name: istio-proxy ports: - containerPort: 15090 \u0026lt;snip\u0026gt; image: docker.io/istio/proxy_init:1.1.3 imagePullPolicy: IfNotPresent name: istio-init \u0026lt;snip\u0026gt; pstauffer/curl イメージ内で \u0026lsquo;/bin/sleep\u0026rsquo; コマンドが起動しているコンテナと隣接して istio-proxy と istio-init というコンテナが起動していることが確認出来ると思います。それぞれの役割は下記のとおりです。\n istio-proxy : 他の istio からの通信をサービスコンテナに中継するためのコンテナ istio-init : istio-proxy コンテナを介す通信を iptables で制御するためのコンテナ  これらが istio を用いることで起動した sidecar コンテナとなります。\n下記の通り pods を describe することでもこれらのコンテナが稼働していることが分かります。\nkubectl describe pod sleep-759d5cb4ff-btqvl \u0026lt;snip\u0026gt; Init Containers: istio-init: Container ID: docker://ded035851fa141997fdca47a0c317203cda650a0f9dce2f8d46ab264aa0e168b Image: docker.io/istio/proxy_init:1.1.3 Image ID: docker-pullable://istio/proxy_init@sha256:000d022d27c198faa6cc9b03d806482d08071e146423d6e9f81aa135499c4ed3 Port: \u0026lt;none\u0026gt; Host Port: \u0026lt;none\u0026gt; \u0026lt;snip\u0026gt; Containers: sleep: Container ID: docker://b6121c749a2eb394b81728063046eb0eb48ea1b48c464debe395e4b58768513c Image: pstauffer/curl Image ID: docker-pullable://pstauffer/curl@sha256:2663156457abb72d269eb19fe53c2d49e2e4a9fdcb9fa8f082d0282d82eb8e42 Port: \u0026lt;none\u0026gt; Host Port: \u0026lt;none\u0026gt; \u0026lt;snip\u0026gt; istio-proxy: Container ID: docker://0baee18b7d6ecec985b61fd10eff3409131245d390fad8f274d420f0807bc941 Image: docker.io/istio/proxyv2:1.1.3 Image ID: docker-pullable://istio/proxyv2@sha256:b682918f2f8fcca14b3a61bbd58f4118311eebc20799f24b72ceddc5cd749306 Port: 15090/TCP Host Port: 0/TCP sidecar のテンプレートとなる configmap を確認する 今回 istio-injection=enabled な状態で \u0026lsquo;sleep\u0026rsquo; コンテナをデプロイし本体のコンテナとは別に sidecar なコンテナ2つが稼働することが確認できました。次に説明するのが configmap です。どの様な状態でどの様なコンテナを sidecar 的に稼働させるかのルールを記したものが istio-sidecar-injector という configmap になります。その configmap の内容を確認してみましょう。\nkubectl -n istio-system get configmap istio-sidecar-injector -o=jsonpath=\u0026#39;{.data.config}\u0026#39; policy: enabled template: |- rewriteAppHTTPProbe: false initContainers: [[ if ne (annotation .ObjectMeta `sidecar.istio.io/interceptionMode` .ProxyConfig.InterceptionMode) \u0026#34;NONE\u0026#34; ]] - name: istio-init image: \u0026#34;docker.io/istio/proxy_init:1.1.3\u0026#34; args: - \u0026#34;-p\u0026#34; - [[ .MeshConfig.ProxyListenPort ]] - \u0026#34;-u\u0026#34; - 1337 - \u0026#34;-m\u0026#34; - [[ annotation .ObjectMeta `sidecar.istio.io/interceptionMode` .ProxyConfig.InterceptionMode ]] - \u0026#34;-i\u0026#34; - \u0026#34;[[ annotation .ObjectMeta `traffic.sidecar.istio.io/includeOutboundIPRanges` \u0026#34;*\u0026#34; ]]\u0026#34; - \u0026#34;-x\u0026#34; - \u0026#34;[[ annotation .ObjectMeta `traffic.sidecar.istio.io/excludeOutboundIPRanges` \u0026#34;\u0026#34; ]]\u0026#34; - \u0026#34;-b\u0026#34; - \u0026#34;[[ annotation .ObjectMeta `traffic.sidecar.istio.io/includeInboundPorts` (includeInboundPorts .Spec.Containers) ]]\u0026#34; - \u0026#34;-d\u0026#34; - \u0026#34;[[ excludeInboundPort (annotation .ObjectMeta `status.sidecar.istio.io/port` 15020 ) (annotation .ObjectMeta `traffic.sidecar.istio.io/excludeInboundPorts` \u0026#34;\u0026#34; ) ]]\u0026#34; [[ if (isset .ObjectMeta.Annotations `traffic.sidecar.istio.io/kubevirtInterfaces`) -]] - \u0026#34;-k\u0026#34; - \u0026#34;[[ index .ObjectMeta.Annotations `traffic.sidecar.istio.io/kubevirtInterfaces` ]]\u0026#34; [[ end -]] imagePullPolicy: IfNotPresent resources: requests: cpu: 10m memory: 10Mi limits: cpu: 100m memory: 50Mi securityContext: runAsUser: 0 runAsNonRoot: false capabilities: add: - NET_ADMIN restartPolicy: Always [[ end -]] containers: - name: istio-proxy image: [[ annotation .ObjectMeta `sidecar.istio.io/proxyImage` \u0026#34;docker.io/istio/proxyv2:1.1.3\u0026#34; ]] ports: - containerPort: 15090 protocol: TCP name: http-envoy-prom \u0026lt;snip\u0026gt; この configmap から下記のパラメータが記されていることが分かります。\n コンテナイメージ CPU 割当 メモリ割り当て 修正するために必要な権限 ポート指定 プロトコル etc..  pod の iptables を確認してトラヒックを理解する 次に minikube のホストにログインしサービス pod (今回は \u0026lsquo;sleep\u0026rsquo;) に割り当てられた iptables の内容を確認してみましょう。\nminikube ssh docker ps | grep sleep b6121c749a2e pstauffer/curl \u0026#34;/bin/sleep 3650d\u0026#34; 3 hours ago Up 3 hours k8s_sleep_sleep-759d5cb4ff-btqvl_default_6f3f6854-6651-11e9-970b-080027d5d6a7_0 この docker id `b6121c749a2e' と nsenter コマンドを用いて pod (sleep) に適用されている iptables の内容を確認します。\ndocker inspect b6121c749a2e --format \u0026#39;{{ .State.Pid }}\u0026#39; 20066 sudo nsenter -t 20066 -n iptables -t nat -S -P PREROUTING ACCEPT -P INPUT ACCEPT -P OUTPUT ACCEPT -P POSTROUTING ACCEPT -N ISTIO_IN_REDIRECT -N ISTIO_OUTPUT -N ISTIO_REDIRECT -A OUTPUT -p tcp -j ISTIO_OUTPUT -A ISTIO_IN_REDIRECT -p tcp -j REDIRECT --to-ports 15001 -A ISTIO_OUTPUT ! -d 127.0.0.1/32 -o lo -j ISTIO_REDIRECT -A ISTIO_OUTPUT -m owner --uid-owner 1337 -j RETURN -A ISTIO_OUTPUT -m owner --gid-owner 1337 -j RETURN -A ISTIO_OUTPUT -d 127.0.0.1/32 -j RETURN -A ISTIO_OUTPUT -j ISTIO_REDIRECT -A ISTIO_REDIRECT -p tcp -j REDIRECT --to-ports 15001 結果から、この pod は INBOUND 通信のリダイレクト先ポートとして 15001 番ポートが指定されているのが分かります。このポートは istio-proxy が待ち受けているポートになります。また OUTBOUND 通信に関しても同様に 15001 番ポートにリダイレクトされているのが分かります。よって \u0026lsquo;sleep\u0026rsquo; pod コンテナの全ての通信が istio-proxy を介すようになっています。また今回は単純に sleep するだけのコンテナを起動しましたが、http サーバ等を起動する pod を立ち上げた場合 -A ISTIO_INBOUND -p tcp -m tcp --dport 80 -j ISTIO_IN_REDIRECT といった制御も確認出来ると思います。\nまとめ kubernetes 上に istio をインストールすることで、テストで起動した pod コンテナに隣接する形で sidecar コンテナ istio-proxy, istio-init コンテナが起動することが確認出来ました。またそれらのコンテナを起動するテンプレートとなる configmap の内容を確認することができました。この configmap は修正することが可能な様です。そしてこの pod コンテナのインバウンド・アウトバウンドの通信は全て istio-proxy コンテナにリダイレクトされていることも分かりました。また今回は configmap の内容を確認するに留まりましたが、istio の機能としては routing, service discovery 等も有しているため、次回は routing あたりを調べようかと思っています。この routing を操作することで今回確認した iptables の内容も変わってくるのではないでしょうか。\n","permalink":"https://jedipunkz.github.io/post/istio-sidecar-injection/","summary":"こんにちは。@jedipunkzです。\n前回の記事 「Istio, Helm を使って Getting Started 的なアプリをデプロイ」で kubernetes 上で istio をインストールし sidecar injection を有効化しサンプルアプリケーションを起動しました。その結果、sidecar 的に envoy コンテナが起動するところまで確認しました。今回はもう少し単純な pod を用いて \u0026lsquo;sidecar injection\u0026rsquo; の中身をもう少しだけ深掘りして見ていきたいと思います。\nRquirements 記事と同等の動きを確認するために下記のソフトウェアが必要になります。 それぞれのソフトウェアは事前にインストールされた前提で記事を記していきます。\n macos or linux os kubectl istioctl minikube  参考 URL 下記の istio 公式ドキュメントを参考に動作確認しました。\n https://istio.io/docs/setup/kubernetes/additional-setup/sidecar-injection/ https://istio.io/docs/setup/kubernetes/additional-setup/sidecar-injection/  minikube で kubenetes をデプロイ 前回同様に minikube 上で動作を確認していきます。パラメータは適宜、自分の環境に読み替えてください。\nminikube start --memory=8192 --cpus=4 --kubernetes-version=v1.10.0 \\  --extra-config=controller-manager.cluster-signing-cert-file=\u0026#34;/var/lib/minikube/certs/ca.crt\u0026#34; \\  --extra-config=controller-manager.cluster-signing-key-file=\u0026#34;/var/lib/minikube/certs/ca.key\u0026#34; \\  --vm-driver=virtualbox istio を稼働させる 下記のコマンドを用いてカレントディレクトリに istio のサンプル yaml が入ったフォルダを展開します。","title":"Istio Sidecar Injection を理解する"},{"content":"こんにちは。@jedipunkzです。\n最近は kubernetes を触ってなかったのですが Istio や Envoy 等 CNCF 関連のソフトウェアの記事をよく見かけるようになって、少し理解しておいたほうがいいかなと思い Istio と Minikube を使って Getting Started 的な事をやってみました。Istio をダウンロードすると中にサンプルアプリケーションが入っているのでそれを利用してアプリのデプロイまでを行ってみます。\nIstio をダウンロードするとお手軽に Istio 環境を作るための yaml ファイルがあり、それを kubectl apply することで Istio 環境を整えられるのですが、ドキュメントにプロダクション環境を想定した場合は Helm Template を用いた方がいいだろう、と記載あったので今回は Helm Template を用いて Istio 環境を作ります。\n前提の環境 下記の環境でテストを行いました。\n macos Mojave minikube v0.32.0 kubectl v1.10.3 helm v2.12.1 virtualbox  準備 kubectl と helm のインストール kubctl と helm をインストールします。両者共に homebrew でインストールします。\nbrew install kubernetes-cli brew install kubernetes-helm minikube のインストールと起動 minikube をインストールして起動します。\ncurl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.32.0/minikube-darwin-amd64 \u0026amp;\u0026amp; chmod +x minikube \u0026amp;\u0026amp; sudo cp minikube /usr/local/bin/ \u0026amp;\u0026amp; rm minikube minikube start --memory 2048 Istio のダウンロードとインストール Istio のダウンロードとインストールを行います。後術しますがこのディレクトリの中に Istio 環境を構築するためのファイルやサンプルアプリケーションが入っています。\ncurl -L https://git.io/getLatestIstio | sh - cd istio-1.0.5 sudo cp bin/istioctl /usr/local/bin/istioctl 構築作業 Istio の Custom Resource Definitions をインストール Istio の Custom Resource Definitions (以下 CRDs) をインストールします。Kubernetes の CRDs は独自のカスタムリソースを定義し追加するものです。Kubernets API Server を介して作成することで作成したリソースの CRUD の API が Kubernetes API に追加されます。\n先程ダウンロードした Istio のディレクトリに crds.yaml があるのでそれを適用します。\nkubectl apply -f install/kubernetes/helm/istio/templates/crds.yaml Helm Template を使って Istio の Core Components をインストール Helm Template の仕組みをつかって Istio の Core Components をインストールします。まず Helm Template を出力し istio-system というネームスペースを作成、その後生成した Template を用いて kubectl コマンドで適用します。\nhelm template install/kubernetes/helm/istio --name istio --namespace istio-system \u0026gt; ./istio.yaml kubectl create namespace istio-system kubectl apply -f ./istio.yaml 状態の確認 この状態で service と pods の状態を確認してみます。\nkubectl get svc -n istio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istio-citadel ClusterIP 10.100.143.194 \u0026lt;none\u0026gt; 8060/TCP,9093/TCP 55s istio-egressgateway ClusterIP 10.108.243.97 \u0026lt;none\u0026gt; 80/TCP,443/TCP 55s istio-galley ClusterIP 10.98.99.11 \u0026lt;none\u0026gt; 443/TCP,9093/TCP 55s istio-ingressgateway LoadBalancer 10.97.17.220 \u0026lt;pending\u0026gt; 80:31380/TCP,443:31390/TCP,31400:31400/TCP,15011:30080/TCP,8060:31309/TCP,853:31151/TCP,15030:30455/TCP,15031:30836/TCP 55s istio-pilot ClusterIP 10.102.75.110 \u0026lt;none\u0026gt; 15010/TCP,15011/TCP,8080/TCP,9093/TCP 55s istio-policy ClusterIP 10.101.145.62 \u0026lt;none\u0026gt; 9091/TCP,15004/TCP,9093/TCP 55s istio-sidecar-injector ClusterIP 10.107.131.48 \u0026lt;none\u0026gt; 443/TCP 55s istio-telemetry ClusterIP 10.96.248.64 \u0026lt;none\u0026gt; 9091/TCP,15004/TCP,9093/TCP,42422/TCP 55s prometheus ClusterIP 10.98.228.190 \u0026lt;none\u0026gt; 9090/TCP 55s kubectl get pods -n istio-system NAME READY STATUS RESTARTS AGE istio-citadel-55cdfdd57c-64tnn 0/1 ContainerCreating 0 1m istio-cleanup-secrets-x6dmj 0/1 Completed 0 1m istio-egressgateway-7798845f5d-qfx2k 1/1 Running 0 1m istio-galley-76bbb946c8-5l626 0/1 ContainerCreating 0 1m istio-ingressgateway-78c6d8b8d7-68r2z 1/1 Running 0 1m istio-pilot-5fcb895bff-pmg8z 0/2 Pending 0 1m istio-policy-7b6cc95d7b-w7ndg 2/2 Running 0 1m istio-security-post-install-jcwg5 0/1 Completed 0 1m istio-sidecar-injector-9c6698858-8lt92 0/1 ContainerCreating 0 1m istio-telemetry-bfc9ff784-2mzzj 2/2 Running 0 1m prometheus-65d6f6b6c-nttwz 0/1 ContainerCreating 0 1m サンプルアプリケーションのデプロイ 先程ダウンロードした Istio のディレクトリにサンプルアプリケーション \u0026lsquo;bookinfo\u0026rsquo; がありますのでそれをデプロイしてみます。 デプロイ方法は2パターンあります。\n方法1. istioctl を使ってアプリの yaml に対して sidecar を記すよう変換しそれを kubctl apply します。\nkubectl apply -f \u0026lt;(istioctl kube-inject -f samples/bookinfo/platform/kube/bookinfo.yaml) 方法2. Sidecar Injection をデフォルトの動きとして設定し kubectl apply します\nkubectl label namespace default istio-injection=enabled kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yam 最後にアプリケーションの状態を確認します\nkubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE details ClusterIP 10.111.252.7 \u0026lt;none\u0026gt; 9080/TCP 17s kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 6m productpage ClusterIP 10.108.205.216 \u0026lt;none\u0026gt; 9080/TCP 10s ratings ClusterIP 10.102.161.24 \u0026lt;none\u0026gt; 9080/TCP 14s reviews ClusterIP 10.106.18.105 \u0026lt;none\u0026gt; 9080/TCP 12s kubectl get pods NAME READY STATUS RESTARTS AGE details-v1-5458f64c65-svrr7 0/2 PodInitializing 0 48s productpage-v1-577c9594b7-4f49s 0/2 Init:0/1 0 43s ratings-v1-79467df9b5-vrx4w 0/2 PodInitializing 0 47s reviews-v1-5d46b744bd-686s9 0/2 Init:0/1 0 46s reviews-v2-7f7d7f99f7-xsvm8 0/2 Init:0/1 0 46s reviews-v3-7bc67f66-cmt4x 0/2 Init:0/1 0 45s まとめ プロダクション環境を想定した Istio 構築と言っても Helm Template を用いて簡単に操作できることが分かりました。\nまたここで重要なのはサンプルアプリケーション \u0026lsquo;bookinfo\u0026rsquo; の Kubenetes Pods に Envoy プロキシを Sidecar 的に配置するための変換コマンドとして\nkubectl apply -f \u0026lt;(istioctl kube-inject -f samples/bookinfo/platform/kube/bookinfo.yaml) が用いられていることです。下記の操作を実行してみるとよくわかるでしょう。\nistioctl kube-inject -f samples/bookinfo/platform/kube/bookinfo.yaml \u0026gt; ./bookinfo.sidecar.yaml diff -u bookinfo.yaml bookinfo.sidecar.yaml 差分が長くなるためここでは省略しますが元の bookinfo.yaml には記されていなかった sidecar の文字が読み取れると思います。アプリに隣接した pods に Envoy が起動している pods が Sidecar 的に配置され下記のような構成になりました。\n ingress \u0026quot;python\u0026quot; \u0026quot;java\u0026quot; \u0026quot;nodejs\u0026quot; +-----------+ +-------------+ +-------------+ +-------------+ | +-------+ | | +-------+ | | +-------+ | | +-------+ | request -\u0026gt; | | envoy | | -\u0026gt; | | envoy | | -\u0026gt; | | envoy | | -\u0026gt; | | envoy | | | +-------+ | | +-------+ | | +-------+ | | +-------+ | +-----------+ | productpage | | review-v1 | | ratings | +-------------+ +-------------+ +-------------+ +-------------+ | +-------+ | | | envoy | | | +-------+ | | review-v2 | +-------------+ +-------------+ | +-------+ | | | envoy | | | +-------+ | | review-v3 | +-------------+ +-------------+ | +-------+ | | | envoy | | | +-------+ | | details | +-------------+ ruby 参考 URL  https://istio.io/docs/setup/kubernetes/quick-start/ https://istio.io/docs/examples/bookinfo/ https://istio.io/docs/setup/kubernetes/helm-install/  ","permalink":"https://jedipunkz.github.io/post/2018/12/31/istio/","summary":"こんにちは。@jedipunkzです。\n最近は kubernetes を触ってなかったのですが Istio や Envoy 等 CNCF 関連のソフトウェアの記事をよく見かけるようになって、少し理解しておいたほうがいいかなと思い Istio と Minikube を使って Getting Started 的な事をやってみました。Istio をダウンロードすると中にサンプルアプリケーションが入っているのでそれを利用してアプリのデプロイまでを行ってみます。\nIstio をダウンロードするとお手軽に Istio 環境を作るための yaml ファイルがあり、それを kubectl apply することで Istio 環境を整えられるのですが、ドキュメントにプロダクション環境を想定した場合は Helm Template を用いた方がいいだろう、と記載あったので今回は Helm Template を用いて Istio 環境を作ります。\n前提の環境 下記の環境でテストを行いました。\n macos Mojave minikube v0.32.0 kubectl v1.10.3 helm v2.12.1 virtualbox  準備 kubectl と helm のインストール kubctl と helm をインストールします。両者共に homebrew でインストールします。\nbrew install kubernetes-cli brew install kubernetes-helm minikube のインストールと起動 minikube をインストールして起動します。","title":"Istio, Helm を使って Getting Started 的なアプリをデプロイ"},{"content":"こんにちは。@jedipunkzです。\n以前、\u0026ldquo;Test-Kitchen, Docker で Ansible Role 開発サイクル高速化！\u0026rdquo; ってタイトルで Ansible Role の開発を test-kitchen を使って行う方法について記事にしたのですが、やっぱりローカルで Docker コンテナ立ち上げてデプロしてテストして.. ってすごく楽というか速くて今の現場でも便利につかっています。前の記事の URL は下記です。\nhttps://jedipunkz.github.io/blog/2016/07/14/test-kitchen-with-ansible/\n最近？は ansible container って技術もあるけど、僕らが Docker 使う目的はコンテナでデプロイするのではなくて単に Ansible を実行するローカル環境が欲しいってこととか、Serverspec をローカル・実機に実行する環境が欲しいってことなので、今でも test-kitchen 使っています。\nで、最近になって複数ノードの構成の Ansible Role を test-kitchen, Docker を使って開発できることに気がついたので記事にしようと思います。これができるとローカルで Redis Master + Redis Slave(s) + Sentinel って環境も容易にできると思います。\n使うソフトウェア 前提は macOS ですが Linux マシンでも問題なく動作するはずです。\nほぼ前回と同じです。\n Ansible Docker test-kitchen kitchen-docker (test-kitchen ドライバ) kitchen-ansible (test-kitchen ドライバ) Serverspec  インストール ソフトウェアののインストール方法については前回の記事を見てもらうこととして割愛します。\ntest-kitchen の環境を作る test-kitchen の環境を作ります。\u0026lsquo;kitchen init\u0026rsquo; を実行して基本的には生成された .kitchen.yml を弄るんじゃなくて .kitchen.local.yml を修正していきます。こちらの記述が必ず上書きされて優先されます。\n.kitchen.local.yml の例を下記に記します。\n--- driver: name: docker binary: /usr/local/bin/docker socker: unix:///var/run/docker.sock use_sudo: false provisioner: name: ansible_playbook roles_path: ../../roles group_vars_path: ../../group_vars/local/ hosts: kitchen-deploy require_ansible_omnibus: false ansible_platform: centos require_chef_for_busser: true platforms: - name: centos driver_config: image: centos:7.2.1511 # (例) platform: centos require_chef_omnibus: false privileged: true # systemd 使うときの例 run_command: /sbin/init; sleep 3 # 同上 suites: - name: master provisioner: name: ansible_playbook playbook: ./site_master.yml driver_config: run_options: --net=kitchen --ip=172.18.0.11 - name: slave provisioner: name: ansible_playbook playbook: ./site_slave.yml driver_config: run_options: --net=kitchen --ip=172.18.0.12 各パラーメタの詳細については kitchen-ansibke, kitchen-docker のドキュメントを見ていただくとして\u0026hellip;\n https://github.com/neillturner/kitchen-ansible/blob/master/provisioner_options.md https://github.com/test-kitchen/kitchen-docker  特徴としては suites: の項目で mater, slave として Docker コンテナを2つ扱うことを宣言している部分です。\n name: master で site_master.yml という Playbook を実行することを宣言 name: master で 172.18.0.11 という IP アドレスを使うことを宣言 name: slave で site_slave.yml という Playbook を実行することを宣言 name: slave で 172.18.0.12 という IP アドレスを使うことを宣言  勿論、同様の記述で 3, 4.. 個目のコンテナ環境を作ることもできます。\nこんな感じです。ここで \u0026ldquo;固定 IP アドレス\u0026rdquo; を使うよう宣言したことはとても重要で、例えば NSD のクラスタ構成を作りたい時、お互いのコンテナ同士が相手のコンテナの IP アドレスを知り合う必要があります。\n NSD Master は Slave へ Xfer するため Slave コンテナの IP アドレスを知る必要あり NSD Slave は Master からのみ Xfer 受信する設定が必要なので Master IP を知る必要あり  って感じです。RabbitMQ や Redis, Consul などのクラスタの際にも同様に互い・もしくは一方の IP アドレスを知る必要が出てくる場合があります。\nで、固定 IP アドレスなのですが Docker では動的 IP が基本なので、固定 IP アドレスを用いるために macOS + Docker 環境の中に一つネットワークを作る必要があります。下記を実行するだけで作成できます。\ndocker network create --subnet 172.18.0.0/24 kitchen Serverspec のディレクトリ構成 前回の記事でも書いたように、Ansible でコンテナに対してデプロイした結果に対して Servrspec テストが流せます。で今回は、クラスタ構成で複数のコンテナを扱うのでそれぞれのコンテナに対して流すテストが必要になります。下記のようなディレクトリ構成があればそれを実現できます。\n. ├── README.md ├── chefignore ├── site_master.yml ├── site_slave.yml └── test └── integration ├── master │ └── serverspec │ └── nsd_master_spec.rb └── slave └── serverspec └── nsd_slave_spec.rb そして 上記の nsd_master_spec,rb, nsd_slave_spec.rb の記述の仕方ですが下記のようになります。\nrequire 'serverspec' # Required by serverspec set :backend, :exec describe package('nsd') do it { should be_installed } end describe process('nsd') do it { should be_running } end ...\u0026lt;省略\u0026gt;... コンテナデプロイ・テスト実行方法 ここまでで環境が整ったと思うので (実際には site_master.yml 等の Playbook もしくは Ansible Role の指定が必要) 、コンテナに対して Ansible デプロイする方法・Serverspec テスト実行する方法を書いていきます。\nkitchen create # 2 コンテナ作られます kitchen converge # 2 コンテナに対して Ansible デプロイされます kitchen verify # 2 コンテナに対して Serverspec テストされます kitchen test # destroy, create, converge, veriy, destroy が一斉実行されます 各コンテナ毎に操作する場合\nkitchen create master-centos # or slave-centos kitchen converge master-centos # or slave-centos kitchen verify master-centos # or slave-centos kitchen destroy master-centos # or slave-centos まとめ ローカルの Docker コンテナに対してデプロイテストできるのでとても手軽に、尚且つ高速に Ansible の Playbook や Role が開発出来ることは前回の記事でわかったと思うのですが、クラスタ構成についてもローカルで開発出来ることがわかりました。\nちなみに私達の環境だと .kitchen.local.yml に下記の記述をしているので\u0026hellip;\nroles_path: ../../roles site_master(slave).yml の中の記述としては下記のようにしています。\u0026lsquo;nsd\u0026rsquo; っていう Role を作る想定で Role 指定がしてあった、尚且つローカルでの Ansible Variable を上書き(一番優先される) することで、Role のローカルデプロイを実現しています。(variable 名は Role の作り方によります)\n--- - hosts: kitchen-deploy sudo: yes roles: - { role: nsd, tags: nsd } vars: nsd_type: \u0026#39;master\u0026#39; nsd_master_addr: \u0026#39;172.18.0.11\u0026#39; nsd_slave_addr: \u0026#39;172.18.0.12\u0026#39; test-kitchen 自体はリリースされてだいぶ時間が経過している分、技術的に枯れてきていて未だに私達は Ansible の開発に役立てています。元々は Chef 向けの技術ではあるのだけどそこは今回紹介した .kitchen.local.yml の書き方で回避できるし、Serverspec のテストも流せるし..。\nまたローカル(ホストの macOS) へのポートマッピング等もコンテナ毎に記せますのでその辺りはドキュメントを読んでみてください。\n最終的にはここで書いた Serverspec テストを実機へ流せれば文句なしだと思います。方法はあると思います。また気がついたら紹介しようと思います。\n","permalink":"https://jedipunkz.github.io/post/test-kitchen-cluster/","summary":"こんにちは。@jedipunkzです。\n以前、\u0026ldquo;Test-Kitchen, Docker で Ansible Role 開発サイクル高速化！\u0026rdquo; ってタイトルで Ansible Role の開発を test-kitchen を使って行う方法について記事にしたのですが、やっぱりローカルで Docker コンテナ立ち上げてデプロしてテストして.. ってすごく楽というか速くて今の現場でも便利につかっています。前の記事の URL は下記です。\nhttps://jedipunkz.github.io/blog/2016/07/14/test-kitchen-with-ansible/\n最近？は ansible container って技術もあるけど、僕らが Docker 使う目的はコンテナでデプロイするのではなくて単に Ansible を実行するローカル環境が欲しいってこととか、Serverspec をローカル・実機に実行する環境が欲しいってことなので、今でも test-kitchen 使っています。\nで、最近になって複数ノードの構成の Ansible Role を test-kitchen, Docker を使って開発できることに気がついたので記事にしようと思います。これができるとローカルで Redis Master + Redis Slave(s) + Sentinel って環境も容易にできると思います。\n使うソフトウェア 前提は macOS ですが Linux マシンでも問題なく動作するはずです。\nほぼ前回と同じです。\n Ansible Docker test-kitchen kitchen-docker (test-kitchen ドライバ) kitchen-ansible (test-kitchen ドライバ) Serverspec  インストール ソフトウェアののインストール方法については前回の記事を見てもらうこととして割愛します。\ntest-kitchen の環境を作る test-kitchen の環境を作ります。\u0026lsquo;kitchen init\u0026rsquo; を実行して基本的には生成された .","title":"Docker,Test-Kitchen,Ansible でクラスタを構成する"},{"content":"こんにちは。@jedipunkzです。\n少し前まで Google Cloud Platform (GCP) を使っていたのですが、今回はその時に得たノウハウを記事にしようと思います。\nGoogle Container Engine (GKE) とロードバランサを組み合わせてサービスを立ち上げていました。手動でブラウザ上からポチポチして構築すると人的ミスや情報共有という観点でマズイと思ったので Terraform を使って GCP の各リソースを構築するよう仕掛けたのですが、まだまだ Terraform を使って GCP インフラを構築されている方が少ないため情報が無く情報収集や検証に時間が掛かりました。よって今回はまだネット上に情報が少ない GKE とロードバランサの構築を Terraform を使って行う方法を共有します。\n構築のシナリオ 構築するにあたって2パターンの構築の流れがあると思います。\n   GKE クラスタとロードバランサを同時に構築する    GKE クラスタを予め構築しそのクラスタ向けにロードバランサを構築する    両方の方法を記していきます。\n GKE クラスタとロードバランサを同時に構築する   GKE クラスタとロードバランサを同時に作る方法です。\n早速ですが下記に terraform ファイルを記しました。それぞれのシンタックスの意味については Terraform の公式ドキュメントをご覧になってください。\n公式ドキュメント : https://www.terraform.io/docs/providers/google/\nここで特筆すべき点としては下記が挙げられます。\nロードバランサに紐付けるバックエンドの ID 取得のため \u0026ldquo;${replace(element\u0026hellip;\u0026rdquo; 的なことをしている ロードバランサのバックエンドサービスに対して GKE クラスタを作成した際に自動で作成されるインスタンスグループの URI を紐付ける必要があります。ユーザとしては URI ではなく \u0026ldquo;インスタンスグループ名\u0026rdquo; であると扱いやすいのですが、URI が必要になります。この情報は下記のサイトを参考にさせていただきました。\n 参考サイト : GKEでkubernetesのnodesをロードバランサーのバックエンドとして使いたいとき with terraform URL : http://qiita.com/techeten/items/b2ec5f11f4a70dd21d70  ロードバランサ一つ作るために 6 個ものインフラリソースを作っている 一つのロードバランサを作るために6つのインフラリソースが必要になるというのも驚きですが公式ドキュメントを読むとなかなかその感覚がつかめませんでした。それぞれの簡単な意味を下記に記しておきます。\n google_compute_http_health_check : ヘルスチェック google_compute_backend_service : バックエンドサービス google_compute_url_map : ロードバランサ名となるリソース google_compute_target_http_proxy : プロキシ google_compute_global_address : グローバル IP アドレス google_compute_global_forwarding_rule : ポートマッピングによるフォワーディングルール  それでは実際の Terraform コードです\n# 共通変数 variable \u0026quot;credentials\u0026quot; {default = \u0026quot;/path/to/credentials.json\u0026quot;} variable \u0026quot;project\u0026quot; {default = \u0026quot;test01\u0026quot;} variable \u0026quot;region\u0026quot; {default = \u0026quot;asia-northeast\u0026quot;} variable \u0026quot;zone\u0026quot; {default = \u0026quot;asia-northeast1-b\u0026quot;} # GKE クラスタ用の変数 variable \u0026quot;gke_name\u0026quot; {default = \u0026quot;gke-terraform-test\u0026quot;} variable \u0026quot;machine_type\u0026quot; {default = \u0026quot;n1-standard-1\u0026quot;} variable \u0026quot;disk_size_gb\u0026quot; {default = \u0026quot;50\u0026quot;} variable \u0026quot;node_count\u0026quot; {default = \u0026quot;2\u0026quot;} variable \u0026quot;network\u0026quot; {default = \u0026quot;default\u0026quot;} variable \u0026quot;subnetwork\u0026quot; {default = \u0026quot;default\u0026quot;} variable \u0026quot;cluster_ipv4_cidr\u0026quot; {default = \u0026quot;10.0.10.0/14\u0026quot;} variable \u0026quot;username\u0026quot; {default = \u0026quot;username\u0026quot;} variable \u0026quot;password\u0026quot; {default = \u0026quot;password\u0026quot;} # ロードバランサ用の変数 variable \u0026quot;lb_name\u0026quot; {default = \u0026quot;lb-terraform-test\u0026quot;} variable \u0026quot;healthcheck_name\u0026quot; {default = \u0026quot;healthcheck-terraform-test\u0026quot;} variable \u0026quot;healthcheck_host\u0026quot; {default = \u0026quot;test.example.com\u0026quot;} variable \u0026quot;healthcheck_port\u0026quot; {default = \u0026quot;30300\u0026quot;} variable \u0026quot;backend_name\u0026quot; {default = \u0026quot;backend-terraform-test\u0026quot;} variable \u0026quot;http_proxy_name\u0026quot; {default = \u0026quot;terraform-proxy\u0026quot;} variable \u0026quot;global_address_name\u0026quot; {default = \u0026quot;terraform-global-address\u0026quot;} variable \u0026quot;global_forwarding_rule_name\u0026quot; {default = \u0026quot;terraform-global-forwarding-rule\u0026quot;} variable \u0026quot;global_forwarding_rule_port\u0026quot; {default =\u0026quot;80\u0026quot;} variable \u0026quot;port_name\u0026quot; {default = \u0026quot;port-test\u0026quot;} variable \u0026quot;enable_cdn\u0026quot; {default = false} provider \u0026quot;google\u0026quot; { credentials = \u0026quot;${file(\u0026quot;${var.credentials}\u0026quot;)}\u0026quot; project = \u0026quot;${var.project}\u0026quot; region = \u0026quot;${var.region}\u0026quot; } resource \u0026quot;google_container_cluster\u0026quot; \u0026quot;cluster-terraform\u0026quot; { name = \u0026quot;${var.gke_name}\u0026quot; zone = \u0026quot;${var.zone}\u0026quot; initial_node_count = \u0026quot;${var.node_count}\u0026quot; network = \u0026quot;${var.network}\u0026quot; subnetwork = \u0026quot;${var.subnetwork}\u0026quot; cluster_ipv4_cidr = \u0026quot;${var.cluster_ipv4_cidr}\u0026quot; master_auth { username = \u0026quot;${var.username}\u0026quot; password = \u0026quot;${var.password}\u0026quot; } node_config { machine_type = \u0026quot;${var.machine_type}\u0026quot; disk_size_gb = \u0026quot;${var.disk_size_gb}\u0026quot; oauth_scopes = [ \u0026quot;https://www.googleapis.com/auth/compute\u0026quot;, \u0026quot;https://www.googleapis.com/auth/devstorage.read_only\u0026quot;, \u0026quot;https://www.googleapis.com/auth/logging.write\u0026quot;, \u0026quot;https://www.googleapis.com/auth/monitoring\u0026quot; ] } addons_config { http_load_balancing { disabled = true } horizontal_pod_autoscaling { disabled = true } } } resource \u0026quot;google_compute_http_health_check\u0026quot; \u0026quot;healthcheck-terraform\u0026quot; { name = \u0026quot;${var.healthcheck_name}\u0026quot; project = \u0026quot;${var.project}\u0026quot; request_path = \u0026quot;/\u0026quot; host = \u0026quot;${var.healthcheck_host}\u0026quot; check_interval_sec = 5 timeout_sec = 5 port = \u0026quot;${var.healthcheck_port}\u0026quot; } resource \u0026quot;google_compute_backend_service\u0026quot; \u0026quot;backend-terraform\u0026quot; { name = \u0026quot;${var.backend_name}\u0026quot; port_name = \u0026quot;${var.port_name}\u0026quot; protocol = \u0026quot;HTTP\u0026quot; timeout_sec = 10 enable_cdn = \u0026quot;${var.enable_cdn}\u0026quot; region = \u0026quot;${var.region}\u0026quot; project = \u0026quot;${var.project}\u0026quot; backend { group = \u0026quot;${replace(element(google_container_cluster.cluster-terraform.instance_group_urls, 1), \u0026quot;Manager\u0026quot;,\u0026quot;\u0026quot;)}\u0026quot; } health_checks = [\u0026quot;${google_compute_http_health_check.healthcheck-terraform.self_link}\u0026quot;] } resource \u0026quot;google_compute_url_map\u0026quot; \u0026quot;lb-terraform\u0026quot; { name = \u0026quot;${var.lb_name}\u0026quot; default_service = \u0026quot;${google_compute_backend_service.backend-terraform.self_link}\u0026quot; project = \u0026quot;${var.project}\u0026quot; } resource \u0026quot;google_compute_target_http_proxy\u0026quot; \u0026quot;http_proxy-terraform\u0026quot; { name = \u0026quot;${var.http_proxy_name}\u0026quot; url_map = \u0026quot;${google_compute_url_map.lb-terraform.self_link}\u0026quot; } resource \u0026quot;google_compute_global_address\u0026quot; \u0026quot;ip-terraform\u0026quot; { name = \u0026quot;${var.global_address_name}\u0026quot; project = \u0026quot;${var.project}\u0026quot; } resource \u0026quot;google_compute_global_forwarding_rule\u0026quot; \u0026quot;forwarding_rule-terraform\u0026quot; { name = \u0026quot;${var.global_forwarding_rule_name}\u0026quot; target = \u0026quot;${google_compute_target_http_proxy.http_proxy-terraform.self_link}\u0026quot; ip_address = \u0026quot;${google_compute_global_address.ip-terraform.address}\u0026quot; port_range = \u0026quot;${var.global_forwarding_rule_port}\u0026quot; project = \u0026quot;${var.project}\u0026quot; } GKE クラスタを予め構築しそのクラスタ向けにロードバランサを構築する   次に GKE クラスタとロードバランサを別々のタイミングに構築する方法です。 実際には GKE クラスタの上には多数のコンテナが起動されるのですでにクラスタが存在する状態でロードバランサを別サービスのために作成したいというケースが一般的なように思います。\nGKE クラスタのインスタンスグループ URI を取得し設定 既存 GKE クラスタを用いる場合、インスタンスグループ URI がいずれにせよ必要になります。 インスタンスグループ URI を取得するには gcloud CLI を使って下記のように知ることができます。\n$ gcloud compute instance-groups managed describe | grep -rin URI ここで得たインスタンスグループ URI は下記のように variable \u0026ldquo;backend_group\u0026rdquo; に記します。\nPort マッピングを手動で設定 ここは残念なのですが 2017/04 現在、Port マッピングの設定を WebUI 上から行う必要があります。UI から GKE クラスタのインスタンスグループを選択し、Port マッピングの設定を行い名前を記します。ここでは \u0026ldquo;port-test\u0026rdquo; として作成したとし説明します。\nTerraform のコードを記述 ここでロードバランサ単独で構築する際の Terraform コードを見てみます。\nvariable \u0026quot;credentials\u0026quot; {default = \u0026quot;/path/to/credentials.json\u0026quot;} variable \u0026quot;project\u0026quot; {default = \u0026quot;test01\u0026quot;} variable \u0026quot;region\u0026quot; {default = \u0026quot;asia-northeast1\u0026quot;} variable \u0026quot;lb_name\u0026quot; {default = \u0026quot;lb-terraform-test\u0026quot;} variable \u0026quot;healthcheck_name\u0026quot; {default = \u0026quot;healthcheck-terraform-test\u0026quot;} variable \u0026quot;healthcheck_host\u0026quot; {default = \u0026quot;test.example.com\u0026quot;} variable \u0026quot;healthcheck_port\u0026quot; {default = \u0026quot;30300\u0026quot;} variable \u0026quot;backend_name\u0026quot; {default = \u0026quot;backend-terraform-test\u0026quot;} variable \u0026quot;backend_group\u0026quot; {default = \u0026quot;https://www.googleapis.com/compute/v1/projects/test01/zones/asia-northeast1-b/instanceGroups/gke-gke-terraform-test-default-pool-c001020e-grp\u0026quot;} variable \u0026quot;http_proxy_name\u0026quot; {default = \u0026quot;terraform-proxy\u0026quot;} variable \u0026quot;global_address_name\u0026quot; {default = \u0026quot;terraform-global-address\u0026quot;} variable \u0026quot;global_forwarding_rule_name\u0026quot; {default = \u0026quot;terraform-global-forwarding-rule\u0026quot;} variable \u0026quot;global_forwarding_rule_port\u0026quot; {default =\u0026quot;80\u0026quot;} variable \u0026quot;port_name\u0026quot; {default = \u0026quot;port-test\u0026quot;} variable \u0026quot;enable_cdn\u0026quot; {default = false} provider \u0026quot;google\u0026quot; { credentials = \u0026quot;${file(\u0026quot;${var.credentials}\u0026quot;)}\u0026quot; project = \u0026quot;${var.project}\u0026quot; region = \u0026quot;${var.region}\u0026quot; } resource \u0026quot;google_compute_http_health_check\u0026quot; \u0026quot;healthcheck-terraform-test\u0026quot; { name = \u0026quot;${var.healthcheck_name}\u0026quot; project = \u0026quot;${var.project}\u0026quot; request_path = \u0026quot;/\u0026quot; host = \u0026quot;${var.healthcheck_host}\u0026quot; check_interval_sec = 5 timeout_sec = 5 port = \u0026quot;${var.healthcheck_port}\u0026quot; } resource \u0026quot;google_compute_backend_service\u0026quot; \u0026quot;backend-terraform-test\u0026quot; { name = \u0026quot;${var.backend_name}\u0026quot; port_name = \u0026quot;${var.port_name}\u0026quot; protocol = \u0026quot;HTTP\u0026quot; timeout_sec = 10 enable_cdn = \u0026quot;${var.enable_cdn}\u0026quot; region = \u0026quot;${var.region}\u0026quot; project = \u0026quot;${var.project}\u0026quot; backend { group = \u0026quot;${var.backend_group}\u0026quot; } health_checks = [\u0026quot;${google_compute_http_health_check.healthcheck-terraform-test.self_link}\u0026quot;] } resource \u0026quot;google_compute_url_map\u0026quot; \u0026quot;lb-terraform-test\u0026quot; { name = \u0026quot;${var.lb_name}\u0026quot; default_service = \u0026quot;${google_compute_backend_service.backend-terraform-test.self_link}\u0026quot; project = \u0026quot;${var.project}\u0026quot; } resource \u0026quot;google_compute_target_http_proxy\u0026quot; \u0026quot;http_proxy-terraform-test\u0026quot; { name = \u0026quot;${var.http_proxy_name}\u0026quot; url_map = \u0026quot;${google_compute_url_map.lb-terraform-test.self_link}\u0026quot; } resource \u0026quot;google_compute_global_address\u0026quot; \u0026quot;ip-terraform-test\u0026quot; { name = \u0026quot;${var.global_address_name}\u0026quot; project = \u0026quot;${var.project}\u0026quot; } resource \u0026quot;google_compute_global_forwarding_rule\u0026quot; \u0026quot;forwarding_rule-terraform-test\u0026quot; { name = \u0026quot;${var.global_forwarding_rule_name}\u0026quot; target = \u0026quot;${google_compute_target_http_proxy.http_proxy-terraform-test.self_link}\u0026quot; ip_address = \u0026quot;${google_compute_global_address.ip-terraform-test.address}\u0026quot; port_range = \u0026quot;${var.global_forwarding_rule_port}\u0026quot; project = \u0026quot;${var.project}\u0026quot; まとめ GCP のロードバランサが特徴的な構築方法が必要になることが分かったと思います。将来的に URI で記す箇所を名前で記せれば構築がもっと簡単になると思いますので GCP 側の API バージョンアップを期待します。また今回は記しませんでしたが SSL (HTTPS) 証明書をロードバランサに紐付けることも Terraform を使えば容易に出来ます。試してみてください。一旦 Terraform 化すればパラメータを変更するだけで各ロードバランサが一発で構築できるので自動化は是非しておきたいところです。私は以前、この構築を Terraform + Hubot により ChatOps 化していました。作業の見える化と、メンバ間のコードレビューが可能になるからです。\n","permalink":"https://jedipunkz.github.io/post/gke-lb/","summary":"こんにちは。@jedipunkzです。\n少し前まで Google Cloud Platform (GCP) を使っていたのですが、今回はその時に得たノウハウを記事にしようと思います。\nGoogle Container Engine (GKE) とロードバランサを組み合わせてサービスを立ち上げていました。手動でブラウザ上からポチポチして構築すると人的ミスや情報共有という観点でマズイと思ったので Terraform を使って GCP の各リソースを構築するよう仕掛けたのですが、まだまだ Terraform を使って GCP インフラを構築されている方が少ないため情報が無く情報収集や検証に時間が掛かりました。よって今回はまだネット上に情報が少ない GKE とロードバランサの構築を Terraform を使って行う方法を共有します。\n構築のシナリオ 構築するにあたって2パターンの構築の流れがあると思います。\n   GKE クラスタとロードバランサを同時に構築する    GKE クラスタを予め構築しそのクラスタ向けにロードバランサを構築する    両方の方法を記していきます。\n GKE クラスタとロードバランサを同時に構築する   GKE クラスタとロードバランサを同時に作る方法です。\n早速ですが下記に terraform ファイルを記しました。それぞれのシンタックスの意味については Terraform の公式ドキュメントをご覧になってください。\n公式ドキュメント : https://www.terraform.io/docs/providers/google/\nここで特筆すべき点としては下記が挙げられます。\nロードバランサに紐付けるバックエンドの ID 取得のため \u0026ldquo;${replace(element\u0026hellip;\u0026rdquo; 的なことをしている ロードバランサのバックエンドサービスに対して GKE クラスタを作成した際に自動で作成されるインスタンスグループの URI を紐付ける必要があります。ユーザとしては URI ではなく \u0026ldquo;インスタンスグループ名\u0026rdquo; であると扱いやすいのですが、URI が必要になります。この情報は下記のサイトを参考にさせていただきました。","title":"GCP ロードバランサと GKE クラスタを Terraform を使って構築する"},{"content":"こんにちは。 @jedipunkz です。\n今日は Kubernetes を使って Serverless を実現するソフトウェア Fission を紹介します。\nAWS の Lambda とよく似た動きをします。Lambda も内部では各言語に特化したコンテナが起動してユーザが開発した Lambda Function を実行してくれるのですが、Fission も各言語がインストールされた Docker コンテナを起動しユーザが開発したコードを実行し応答を返してくれます。\nそれでは早速なのですが、Fission を動かしてみましょう。\n動作させるための環境 macOS か Linux を前提として下記の環境を用意する必要があります。また Kubernetes 環境は minikube が手っ取り早いので用いますが、もちろん minikube 以外の kubernetes 環境でも動作します。\n macOS or Linux minikube or kubernetes kubectl fission  ソフトウェアのインストール方法 簡単にですが、ソフトウェアのインストール方法を書きます。\nOS 私は Linux で動作させましたが筆者の方は macOS を使っている方が多数だと思いますので、この手順では macOS を使った利用方法を書いていきます。\nminikube ここでは簡単な手順で kubernetes 環境を構築できる minikube をインストールします。\ncurl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.16.0/minikube-darwin-amd64 \u0026amp;\u0026amp; chmod +x minikube \u0026amp;\u0026amp; sudo mv minikube /usr/local/bin/fission kubectl 直接必要ではありませんが、kubectl があると minikube で構築した kubernetes 環境を操作できますのでインストールしておきます。\ncurl -Lo kubectl https://storage.googleapis.com/kubernetes-release/release/v1.5.2/bin/linux/amd64/kubectl \u0026amp;\u0026amp; chmod +x kubectl \u0026amp;\u0026amp; sudo mv kubectl /usr/local/bin/ Fission Fission のインストールです。\ncurl http://fission.io/linux/fission \u0026gt; fission \u0026amp;\u0026amp; chmod +x fission \u0026amp;\u0026amp; sudo mv fission /usr/local/bin/fission kubernetes の起動 ソフトウェアのインストールが完了したら minikube を使って kubernetes を起動します。\n$ minikube start $ minikube status minikubeVM: Running localkube: Running $ kubectl get nodes NAME STATUS AGE minikube Ready 1h Fission の起動と環境変数の設定 Fission を起動します。\n$ kubectl create -f http://fission.io/fission.yaml $ kubectl create -f http://fission.io/fission-nodeport.yaml 次に環境変数を設定します。\n$ export FISSION_URL=http://$(minikube ip):31313 $ export FISSION_ROUTER=$(minikube ip):31314 Fission を使って Python のコードを実行する 例として Python の Hello World を用意します。hello.py として保存します。\ndef main(): return \u0026quot;Hello, world!\\n\u0026quot; ではいよいよ、kubernetes と Fission を使って上記の Hello World を実行させます。\nまず Fission が用意してくれている Docker コンテナを扱うように \u0026lsquo;env\u0026rsquo; を作ります。\n$ fission env create --name python-env --image fission/python-env 次に Fission で Function を作ります。その際に上記の env と python コードを指定します。つまり、hello.py を fission/python-env という Docker コンテナで稼働する、という意味です。\n$ fission function create --name python-hello -env python-env --code ./hello.py 次に Router を作ります。クエリの Path に対して Fuction を関連付けることができます。\n$ fission route add --function python-hello --url /python-hello Function を実行する環境ができました。実際に curl を使ってアクセスしてみましょう。\n$ curl http://$FISSION_ROUTER/python-hello Hello, world! hello.py の実行結果が得られました。\nまとめと考察 結果から Fission は \u0026ldquo;各言語の実行環境として Docker コンテナを用いていて、ユーザが開発したコードをそのコンテナ上で起動し実行結果を得られる。また各コード毎に URL パスが指定することができ、それをルータとして関係性を持たせられる\u0026rdquo; ということが分かりました。AWS の Lambda とほぼ同じことが実現出来ていることが分かると思います。\nAWS には Lambda の実行結果を応答するための API Gateway があり、このマネージド HTTP サーバと併用することで API 環境を用意出来るのですが Fission の場合には HTTP サーバも込みで提供されていることも分かります。\nあとは、この Fission を提供している元が \u0026ldquo;Platform9\u0026rdquo; という企業なのですが、この企業は OpenStack や kubernetes を使ったホスティングサービスを提供しているようです。開発元が一企業ということと、完全な OSS 開発体制になっていない可能性があって、万が一この企業に何かあった場合にこの Fission を使い続けられるのか問題がしばらくはありそうです。Fission 同等のソフトウェアを kubernetes が取り込むという話題は\u0026hellip;あるのかなぁ？\nkubernetes の Job Scheduler が同等の機能を提供してくれるかもしれませんが、まだ Job Scheduler は利用するには枯れていない印象があります。Fission と Job Scheduler 、どちらがいち早く完成度を上げられるのでしょうか。\n","permalink":"https://jedipunkz.github.io/post/serverless-fission/","summary":"こんにちは。 @jedipunkz です。\n今日は Kubernetes を使って Serverless を実現するソフトウェア Fission を紹介します。\nAWS の Lambda とよく似た動きをします。Lambda も内部では各言語に特化したコンテナが起動してユーザが開発した Lambda Function を実行してくれるのですが、Fission も各言語がインストールされた Docker コンテナを起動しユーザが開発したコードを実行し応答を返してくれます。\nそれでは早速なのですが、Fission を動かしてみましょう。\n動作させるための環境 macOS か Linux を前提として下記の環境を用意する必要があります。また Kubernetes 環境は minikube が手っ取り早いので用いますが、もちろん minikube 以外の kubernetes 環境でも動作します。\n macOS or Linux minikube or kubernetes kubectl fission  ソフトウェアのインストール方法 簡単にですが、ソフトウェアのインストール方法を書きます。\nOS 私は Linux で動作させましたが筆者の方は macOS を使っている方が多数だと思いますので、この手順では macOS を使った利用方法を書いていきます。\nminikube ここでは簡単な手順で kubernetes 環境を構築できる minikube をインストールします。\ncurl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.16.0/minikube-darwin-amd64 \u0026amp;\u0026amp; chmod +x minikube \u0026amp;\u0026amp; sudo mv minikube /usr/local/bin/fission kubectl 直接必要ではありませんが、kubectl があると minikube で構築した kubernetes 環境を操作できますのでインストールしておきます。","title":"Serverless on Kubernetes : Fission を使ってみた"},{"content":"こんにちは。 @jedipunkz です。\nいま僕らは職場では GKE 上に Replication Controller と Services を使って Pod を起動しているのですが最近の Kubernetes 関連のドキュメントを拝見すると Deployments を使っている記事をよく見掛けます。Kubernetes 1.2 から実装されたようです。今回は Kubernetes の Replication Controller の次世代版と言われている Deployments について調べてみましたので理解したことを書いていこうかと思います。\n参考資料 今回は Kubernetes 公式の下記のドキュメントに記されているコマンドを一通り実行していきます。追加の情報もあります。\n https://kubernetes.io/docs/user-guide/deployments/  Deployments を使って nginx Pod を起動 nginx をデプロイするための Yaml ファイルを用意します。\napiVersion: extensions/v1beta1 kind: Deployment metadata: name: nginx-deployment spec: replicas: 2 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 作成した yaml ファイルを指定して Pod を作ります。 下記の通りここで \u0026ldquo;\u0026ndash;record\u0026rdquo; と記しているのは、後に Deployments の履歴を表示する際に \u0026ldquo;何を行ったか\u0026rdquo; を出力するためです。このオプションを指定しないと \u0026ldquo;何を行ったか\u0026rdquo; の出力が \u0026ldquo;NONE\u0026rdquo; となります。\n$ kubectl create -f nginx.yaml --record ここで\n deployments replica set pod rollout  の状態をそれぞれ確認してみます。\n$ kubectl get deployments NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-deployment 2 2 2 2 8s $ kubectl get rs NAME DESIRED CURRENT READY AGE nginx-deployment-4087004473 2 2 2 10s $ kubectl get pods --show-labels NAME READY STATUS RESTARTS AGE LABELS nginx-deployment-4087004473-6csa7 1/1 Running 0 21s app=nginx,pod-template-hash=4087004473 nginx-deployment-4087004473-teyzc 1/1 Running 0 21s app=nginx,pod-template-hash=4087004473 $ kubectl rollout status deployment/nginx-deployment deployment nginx-deployment successfully rolled out 結果から、下記の事が分かります。\n yaml に記した通り \u0026ldquo;nginx-deployment\u0026rdquo; という名前で deployment が生成された \u0026ldquo;nginx-deployment-4087004473\u0026rdquo; という名前の rs (レプリカセット) が生成された yaml に記した通り2つの Pod が起動した \u0026ldquo;nginx-deployment\u0026rdquo; が正常に Rollout された  Replication Controller でデプロイした際には作られない replica set, rollout というモノが出てきました。後に Deployments を使うメリットに繋がっていきます。\nnginx イメージの Tag を更新してみる ここで yaml ファイル内で指定していた \u0026ldquo;image: nginx:1.7.9\u0026rdquo; を \u0026ldquo;image: nginx:1.9.1\u0026rdquo; と更新してみます。 Replication Controller で言う Rolling-Update になります。後に述べますが他にも更新方法があります。\n$ kubectl set image deployment/nginx-deployment nginx=nginx:1.9.1 ここで先ほどと同様に状態を確認してみます。\n$ kubectl get deployments NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-deployment 2 2 2 2 2m $ kubectl get rs NAME DESIRED CURRENT READY AGE nginx-deployment-3599678771 2 2 2 39s nginx-deployment-4087004473 0 0 0 2m $ kubectl get pods NAME READY STATUS RESTARTS AGE nginx-deployment-3599678771-0vj9m 1/1 Running 0 53s nginx-deployment-3599678771-t1y62 1/1 Running 0 53s ここで\u0026hellip;\n 新しい Replica Set \u0026ldquo;nginx-deployment-3599678771\u0026rdquo; が作成された 古い Replica Set \u0026ldquo;nginx-deployment-4087004473\u0026rdquo; の Pod は 0 個になった Pod 内コンテナが更新された (NAME より判断)  となったことが分かります。 Replication Controller と異なり、Deployments では以前の状態が Replica Set として保存されていて状態の履歴が追えるようになっています。\nここで Rollout の履歴を確認してみます。\n$ kubectl rollout history deployment/nginx-deployment deployments \u0026quot;nginx-deployment\u0026quot; REVISION CHANGE-CAUSE 1 kubectl create -f nginx.yaml --record 2 kubectl set image deployment/nginx-deployment nginx=nginx:1.9.1 REVISION という名前で履歴番号が付き、どの様な作業を行ったか CHANGE-CAUSE という項目で記されていることがわかります。作業の履歴がリビジョン管理されています。\n下記のように REVSION 番号を付与して履歴内容を表示することも可能です。\n$ kubectl rollout history deployment/nginx-deployment --revision=2 deployments \u0026quot;nginx-deployment\u0026quot; with revision #2 Labels: app=nginx pod-template-hash=3599678771 Annotations: kubernetes.io/change-cause=kubectl set image deployment/nginx-deployment nginx=nginx:1.9.1 Containers: nginx: Image: nginx:1.9.1 Port: 80/TCP Volume Mounts: \u0026lt;none\u0026gt; Environment Variables: \u0026lt;none\u0026gt; No volumes. 作業を切り戻してみる 先程 nginx の Image Tag を更新しましたが、ここで Deployments の機能を使って作業を切り戻してみます。下記の様に実行します。\n$ kubectl rollout undo deployment/nginx-deployment 状態を確認します。\n$ kubectl get deployments NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-deployment 2 2 2 2 5m $ kubectl rollout history deployment/nginx-deployment deployments \u0026quot;nginx-deployment\u0026quot; REVISION CHANGE-CAUSE 2 kubectl set image deployment/nginx-deployment nginx=nginx:1.9.1 3 kubectl create -f nginx.yaml --record ここでは\u0026hellip;\n コンテナが2個、正常に起動した REVISION 番号 3 として初期構築の状態 (kubectl create ..) が新たに保存  ということが分かります。注意したいのは REVSION 番号 1 が削除され 3 が生成されたことです。1 と 3 は同じ作業ということと推測します。\n念のため \u0026lsquo;nginx\u0026rsquo; コンテナの Image Tag が切り戻っているか確認してみます。\n$ kubectl describe pod nginx-deployment-4087004473-nq35u | grep \u0026quot;Image:\u0026quot; Image: nginx:1.7.9 最初の Yaml ファイルに記した \u0026lsquo;nginx\u0026rsquo; イメージ Tag \u0026ldquo;1.7.9\u0026rdquo; となっていることが確認できました。set image \u0026hellip; でイメージ更新をした作業が正常に切り戻ったことになります。\nレプリカ数を 2-\u0026gt;3 へスケールしてみる 更に replicas の数値を 2 から 3 へスケールしてみます。\n$ kubectl scale deployment nginx-deployment --replicas 3 同様に状態を確認してみます。\nkubectl get pods NAME READY STATUS RESTARTS AGE nginx-deployment-4087004473-esj5l 1/1 Running 0 6s nginx-deployment-4087004473-nq35u 1/1 Running 0 4m nginx-deployment-4087004473-tyibo 1/1 Running 0 4m kubectl get rs NAME DESIRED CURRENT READY AGE nginx-deployment-3599678771 0 0 0 9m nginx-deployment-4087004473 3 3 3 11m kubectl rollout history deployment/nginx-deployment deployments \u0026quot;nginx-deployment\u0026quot; REVISION CHANGE-CAUSE 2 kubectl set image deployment/nginx-deployment nginx=nginx:1.9.1 3 kubectl scale deployment nginx-deployment --replicas 3 ここで気になるのは REVISION 3 が上書きされたことです。REVSION 番号 4 が新たに作成されると思っていたからです。先程 REVISION 番号 3 として保存されていた下記の履歴が消えてしまいました。この点については引き続き検証してみます。今の自分には理解できませんでした。ご存知の方いましたら、コメントお願いします！\n3 kubectl create -f nginx.yaml --record Puase, Resume 機能を使ってみる 次は deployments の機能を使って Image Tag を更に 1.9.1 へ変更し、その処理をポーズしてみます。\nkubectl set image deployment/nginx-deployment nginx=nginx:1.9.1; kubectl 同様に状態を確認してみます。\n$ kubectl get rs NAME DESIRED CURRENT READY AGE nginx-deployment-3599678771 2 2 2 10m nginx-deployment-4087004473 2 2 2 12m $ kubectl rollout status deployment/nginx-deployment Waiting for rollout to finish: 2 out of 3 new replicas have been updated... Ctrl-C #\u0026lt;--- キー入力 rollout status で deployment \u0026ldquo;deployment/nginx-deployment\u0026rdquo; を確認すると \u0026ldquo;waiting for rollout to finish\u0026rdquo; と表示され処理がポーズされていることが確認できました。また古い Deployment \u0026ldquo;nginx-deployment-4087004473\u0026rdquo; 上に未だコンテナが残り、新しい Deployment もコンテナが生成中であることが分かります。\nでは Resume します。\n$ kubectl rollout resume deployment/nginx-deployment deployment \u0026quot;nginx-deployment\u0026quot; resumed この時点の状態を確認しましょう。\n$ kubectl rollout status deployment/nginx-deployment deployment nginx-deployment successfully rolled out $ kubectl get rs NAME DESIRED CURRENT READY AGE nginx-deployment-3599678771 3 3 3 11m nginx-deployment-4087004473 0 0 0 14m ここからは\u0026hellip;\n 正常に Deployment \u0026ldquo;nginx-deployment\u0026rdquo; が Rollout されたこと 古い Deployment 上のコンテナ数が 0 に、新しい Deployment 上のコンテナ数が 3 になった  ということが分かります。\nRolling-Update 相当の作業を行う方法 前述した通り、Replication Controller 時代にあった Rolling-Update 作業 (イメージタグ・レプリカ数等の更新) ですが、Deployments では下記の方法をとることが出来ます。\nset オプションを付与する場合 set オプションを付与して Key 項目に対して新しい Value を渡します。\n$ kubectl set image deployment/nginx-deployment nginx=nginx:1.9.1 yaml ファイルを修正する場合 yaml ファイルの内容を更新して適用したい場合、下記のように apply オプションを付与します。\n$ kubectl apply -f \u0026lt;新しいYamlファイル\u0026gt; まとめと考察 REVISION の履歴が上書きされる点など、まだ未完成な感が否めませんでしたが(自分の勘違いかもしれません！)、Replication Controller と比べると作業の切り戻しや、履歴が保存され履歴内容も確認できる点など機能が追加されていることが分かりました。公式ドキュメントを読んでいてもコマンド結果等怪しい点があって流石に API バージョンが \u0026ldquo;v1beta1\u0026rdquo; だなぁという感じではありますが、機能が整理されていて利便性が上がっているので Replication Controller を使っているユーザは自然と今後、Deployments に移行していくのではないかと感じました。\n","permalink":"https://jedipunkz.github.io/post/kubernetes-deployments/","summary":"こんにちは。 @jedipunkz です。\nいま僕らは職場では GKE 上に Replication Controller と Services を使って Pod を起動しているのですが最近の Kubernetes 関連のドキュメントを拝見すると Deployments を使っている記事をよく見掛けます。Kubernetes 1.2 から実装されたようです。今回は Kubernetes の Replication Controller の次世代版と言われている Deployments について調べてみましたので理解したことを書いていこうかと思います。\n参考資料 今回は Kubernetes 公式の下記のドキュメントに記されているコマンドを一通り実行していきます。追加の情報もあります。\n https://kubernetes.io/docs/user-guide/deployments/  Deployments を使って nginx Pod を起動 nginx をデプロイするための Yaml ファイルを用意します。\napiVersion: extensions/v1beta1 kind: Deployment metadata: name: nginx-deployment spec: replicas: 2 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 作成した yaml ファイルを指定して Pod を作ります。 下記の通りここで \u0026ldquo;\u0026ndash;record\u0026rdquo; と記しているのは、後に Deployments の履歴を表示する際に \u0026ldquo;何を行ったか\u0026rdquo; を出力するためです。このオプションを指定しないと \u0026ldquo;何を行ったか\u0026rdquo; の出力が \u0026ldquo;NONE\u0026rdquo; となります。","title":"Kubernetes Deployments を使ってみた！"},{"content":"こんにちは @jedipunkz です。\n今回は Kubernetes を使った構成で Google-Fluentd をどのコンテナに載せるか？ってことを考えてみたので書きたいと思います。\nKubernetes は Docker を利用したソフトウェアなので Docker と同じく \u0026ldquo;1コンテナ, 1プロセス\u0026rdquo; というポリシがあります。つまり、コンテナ上のプロセスが停止したら Kubernetes がそれを検知してコンテナを起動しなおしてくれます。ですが、複数プロセスを1コンテナに稼働させると、それが出来ません。そうは言っても中には複数のプロセスを稼働させたい場面があります。その場面として考えられる具体的な例として HTTPD サーバのログを Google-Fluentd を使って GCP Cloud Logging に転送したい場合があります。\n今回は上記の例を fluentd-sidecar-gcp と kubernetes volumes を使って解決する方法を記したいと思います。\n構成のシナリオ シナリオとしては下記のとおりです。\n マルチコンテナポッドを扱う 1つの Kubernetes Volumes を複数コンテナで共有する HTTPD ログをその Volume に出力 隣接する Google-Fluentd コンテナでその Volume に出力されたログを読み込みログ転送  fluentd-sidecar-gcp とは 次に説明するのは fluentd-sidecar-gcp の概略です。これは Kubernetes が contrib で扱っているコンテナです。下記の URL にあります。\nhttps://github.com/kubernetes/contrib/tree/master/logging/fluentd-sidecar-gcp\nGoogle-Fluentd を稼働させる Dockerfile が用意されているのですが、下記の記述を確認するとこのコンテナに環境変数 $FILES_TO_COLLECT を渡すと Google Fluentd でログを取得してくれることが分かります。\nhttps://github.com/kubernetes/contrib/blob/master/logging/fluentd-sidecar-gcp/config_generator.sh#L22-L37\nつまり、fluentd-sidecar-gcp コンテナに隣接する HTTPD コンテナのログが出力される Kubernetes Volumes 上のファイルパスを指定すれば HTTPD のログが取得でき、Google Cloud Logging へログが転送できます。\nサンプルの Kubernetes YAML 下記にサンプルとして Kubernetes YAML を記します。\napiVersion: v1 kind: Pod metadata: labels: run: my-nginx name: nginx-fluentd-logging-example spec: containers: - name: nginx-container image: nginx ports: - containerPort: 80 volumeMounts: - name: log-storage mountPath: /var/log/nginx - name: sidecar-log-collector image: gcr.io/google_containers/fluentd-sidecar-gcp:1.4 resources: limits: cpu: 100m memory: 200Mi env: - name: FILES_TO_COLLECT value: \u0026quot;/mnt/log/nginx/access.log /mnt/log/nginx/error.log\u0026quot; volumeMounts: - name: log-storage readOnly: true mountPath: /mnt/log/nginx volumes: - name: log-storage emptyDir: {} --- apiVersion: v1 kind: Service metadata: name: my-nginx labels: run: my-nginx spec: ports: - port: 80 protocol: TCP selector: run: my-nginx 特徴を下記に解説します。\n nginx-container, sidecar-log-collector のマルチコンテナポッドです。 sidecar-log-collector の image: としては gcr.io/google_containers/fluentd-sidecar-gcp:1.4 が指定されています \u0026lsquo;log-storage\u0026rsquo; として nginx-container の /var/log/nginx が sidecar-log-collector の /mnt/log/nginx として共有されています FILES_TO_COLLECT として共有 Volume 上の access.log, error.log が指定されています  結果、Nginx コンテナのログが Kubernetes Volume で Google-Fluentd コンテナに読み込み専用で共有され (readOnly 行) 、この Google-Fluentd は環境変数で渡された /mnt/log/nginx/access.log と /mnt/log/nginx/error.log を読み込み開始し、内容を Google Cloud Logging へ転送します。\nデプロイ方法 デプロイは下記の通り実施します。\n$ kubectl create -f \u0026lt;上記のファイル名\u0026gt; 結果とまとめ それぞれのログファイルを Tag を Google-Fluentd で付けた形で Google Cloud Logging へ転送出来ました。ログ毎に結果を Cloud Logging UI 上で確認できます。 本来、Docker なので標準出力にログを出力し Kubernetes がその標準出力を Cloud Logging へ転送してくれるのですが、それだと Tag が付けられないため、ログを分離するのが一苦労だと思います。ですが、今回紹介した方法では Google-Fluentd で Tag を付けてログ転送出来たため、その心配はありません。\nこの Kubernetes Volumes は他にも利用方法がありそうです。\n本来、GKE や Kubernetes を利用される方は Microservice Architecture が採用出来ている方々だと思うのですが、fluentd をアプリコンテナから分離するのは結構悩むところじゃないかと思うので、今回紹介した方法はそう言った場合に有用かと思います。\n","permalink":"https://jedipunkz.github.io/post/fluentd-sidecar-gcp/","summary":"こんにちは @jedipunkz です。\n今回は Kubernetes を使った構成で Google-Fluentd をどのコンテナに載せるか？ってことを考えてみたので書きたいと思います。\nKubernetes は Docker を利用したソフトウェアなので Docker と同じく \u0026ldquo;1コンテナ, 1プロセス\u0026rdquo; というポリシがあります。つまり、コンテナ上のプロセスが停止したら Kubernetes がそれを検知してコンテナを起動しなおしてくれます。ですが、複数プロセスを1コンテナに稼働させると、それが出来ません。そうは言っても中には複数のプロセスを稼働させたい場面があります。その場面として考えられる具体的な例として HTTPD サーバのログを Google-Fluentd を使って GCP Cloud Logging に転送したい場合があります。\n今回は上記の例を fluentd-sidecar-gcp と kubernetes volumes を使って解決する方法を記したいと思います。\n構成のシナリオ シナリオとしては下記のとおりです。\n マルチコンテナポッドを扱う 1つの Kubernetes Volumes を複数コンテナで共有する HTTPD ログをその Volume に出力 隣接する Google-Fluentd コンテナでその Volume に出力されたログを読み込みログ転送  fluentd-sidecar-gcp とは 次に説明するのは fluentd-sidecar-gcp の概略です。これは Kubernetes が contrib で扱っているコンテナです。下記の URL にあります。\nhttps://github.com/kubernetes/contrib/tree/master/logging/fluentd-sidecar-gcp\nGoogle-Fluentd を稼働させる Dockerfile が用意されているのですが、下記の記述を確認するとこのコンテナに環境変数 $FILES_TO_COLLECT を渡すと Google Fluentd でログを取得してくれることが分かります。","title":"fluentd-sidecar-gcp と Kubernetes Volumes で Cloud Logging ログ転送"},{"content":"こんにちは。@jedipunkz です。\n今回は Google Cloud Platform の Google CloudCDN について調べてみたので記したいと思います。\nCloudCDN は GCP のロードバランサのバックエンドサービスに紐付けられるサービスです。このバックエンドサービスで CloudCDN を有効にしていると CDN サービスを機能させることが出来ます。先に書いておくとこの CloudCDN はとてもシンプルで扱いやすいサービスだと判りました。高機能な他の CDN サービスと比べると機能が足らない感ありますが、必要最低限なところを抑えているのと、価格がとても安いです。(価格は下記の URL 参照)\n価格表 : https://cloud.google.com/cdn/pricing\n構成 構成と構成の特徴です。\n+----------+ +---------+ | instance |--+ +-| EndUser | +----------+ | +------------+ +----------+ | +---------+ +--|LoadBalancer|--| CloudCDN |-+-| EndUser | +----------+ | +------------+ +----------+ | +---------+ | instance |--+ +-| EndUser | +----------+ +---------+  コンテンツが初めてリクエストされた場合キャッシュミスする キャッシュミスした際に近くにあるキャッシュからコンテンツを取得しようと試みる 近くのキャッシュがコンテンツがある場合、最初のキャッシュにコンテンツが送信される 近くのキャッシュにコンテンツがない場合、HTTP ロードバランサにリクエストが転送される その後のリクエストはキャッシュが応答する(キャッシュヒット) キャッシュ間のフィルは EndUser のリクエストに応じて実行される キャッシュを事前に読み込むことできない キャッシュは世界各地に配置されている  CloudCDN を導入する方法 導入する方法は簡単で下記のとおりです。\n 新規 LB を WebUI で作成 バックエンドサービスを作成 右パネルの下にある [Cloud CDN を有効にする] チェックボックスをオンに 作成ボタンを押下  HTTPD サーバ側 (今回は Apache を使います) でキャッシュに関する設定を行います。 下記は例です。3600秒、キャッシュさせる設定になります。\nHeader set Cache-control \u0026quot;public, max-age=3600\u0026quot; キャッシュされる条件は RFC7324 に規定されているとおりです。\n特定のキャッシュを削除する方法 Google Cloud SDK https://cloud.google.com/sdk/ を使うと CLI でコンテンツを指定してキャッシュのクリアが出来ます。また WebUI でもクリアは可能です。\n$ gcloud compute url-maps invalidate-cdn-cache \u0026lt;url-map の名前\u0026gt; --path \u0026quot;コンテンツのパス\u0026quot; このときに \u0026ndash;path で指定できるコンテンツの記し方は下記のように指定することも可能です。\n--path \u0026quot;/cat.png # \u0026lt;--- 1つのコンテンツキャッシュを削除 --path \u0026quot;/*\u0026quot; # \u0026lt;--- 全てのコンテンツキャッシュを削除 --path \u0026quot;/pix/*\u0026quot; # \u0026lt;--- ディレクトリ指定で削除 まとめ 価格が低価格で必要最低限な機能に絞られている印象です。シンプルな分、理解し易いですしキャッシュクリアについても Hubot 化などすれば開発者の方に実行してもらいやすいのではないでしょうか。\n","permalink":"https://jedipunkz.github.io/post/cloud-cdn/","summary":"こんにちは。@jedipunkz です。\n今回は Google Cloud Platform の Google CloudCDN について調べてみたので記したいと思います。\nCloudCDN は GCP のロードバランサのバックエンドサービスに紐付けられるサービスです。このバックエンドサービスで CloudCDN を有効にしていると CDN サービスを機能させることが出来ます。先に書いておくとこの CloudCDN はとてもシンプルで扱いやすいサービスだと判りました。高機能な他の CDN サービスと比べると機能が足らない感ありますが、必要最低限なところを抑えているのと、価格がとても安いです。(価格は下記の URL 参照)\n価格表 : https://cloud.google.com/cdn/pricing\n構成 構成と構成の特徴です。\n+----------+ +---------+ | instance |--+ +-| EndUser | +----------+ | +------------+ +----------+ | +---------+ +--|LoadBalancer|--| CloudCDN |-+-| EndUser | +----------+ | +------------+ +----------+ | +---------+ | instance |--+ +-| EndUser | +----------+ +---------+  コンテンツが初めてリクエストされた場合キャッシュミスする キャッシュミスした際に近くにあるキャッシュからコンテンツを取得しようと試みる 近くのキャッシュがコンテンツがある場合、最初のキャッシュにコンテンツが送信される 近くのキャッシュにコンテンツがない場合、HTTP ロードバランサにリクエストが転送される その後のリクエストはキャッシュが応答する(キャッシュヒット) キャッシュ間のフィルは EndUser のリクエストに応じて実行される キャッシュを事前に読み込むことできない キャッシュは世界各地に配置されている  CloudCDN を導入する方法 導入する方法は簡単で下記のとおりです。","title":"Google Cloud CDN を使ってみた"},{"content":"こんにちは @jedipunkz です。\n今日は某アドベントカレンダーに参加していて記事を書いています。 記事だけ先出ししちゃいます..。\n今日は最近 coreos がリリースした \u0026lsquo;etcd-operator\u0026rsquo; を触ってみようかと思います。ほぼ、README に書かれている手順通りに実施するのですが、所感を交えながら作業して記事にしてみたいと思います。\ncoreos が提供している etcd についてご存知ない方もいらっしゃると思いますが etcd は KVS ストレージでありながら Configuration Management Store として利用できる分散型ストレージです。Docker 等の環境を提供する coreos という軽量 OS 内でも etcd が起動していてクラスタで管理された情報をクラスタ内の各 OS が読み書きできる、といった機能を etcd が提供しています。 詳細については公式サイトを御覧ください。\netcd 公式サイト : https://coreos.com/etcd/docs/latest/\netcd-operator はこの etcd クラスタを kubernetes 上でクラスタ管理するための簡単に運用管理するためのソフトウェアになります。\netcd-operator 公式アナウンス : https://coreos.com/blog/introducing-the-etcd-operator.html\n後に実際に触れていきますが下記のような管理が可能になります。\n etcd クラスタの構築 etcd クラスタのスケールアップ・ダウン etcd Pod の障害時自動復旧 etcd イメージをオンラインで最新のモノにアップグレード  では早速利用してみたいと思います。\n必要な環境 下記の環境が事前に用意されている必要があります。\n Docker Kubernetes or minikube+kubernetes (https://github.com/kubernetes/minikube) etcdctl : https://github.com/coreos/etcd/tree/master/etcdctl  作業準備 下記のレポジトリをクローンします。\n$ git clone https://github.com/coreos/etcd-operator.git Operator のデプロイ 下記のような内容のファイルが記さているファイルを利用します。中身を確認しましょう。\n$ cat example/deployment.yaml apiVersion: extensions/v1beta1 kind: Deployment metadata: name: etcd-operator spec: replicas: 1 template: metadata: labels: name: etcd-operator spec: containers: - name: etcd-operator image: quay.io/coreos/etcd-operator env: - name: MY_POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace kind: Deployment で etcd-operator のイメージを使ってレプリカ数1のポッドを起動しているのが分かると思います。実際にデプロイします。\n$ kubectl create -f example/deployment.yaml どんなポッドが起動したか確認します。\nkubectl get pods NAME READY STATUS RESTARTS AGE etcd-operator-217127005-futo0 1/1 Running 0 1m あと、これは自分も知りませんでしたがサードパーティリソースという枠があるらしく下記のように確認することができます。\nkubectl get thirdpartyresources NAME DESCRIPTION VERSION(S) etcd-cluster.coreos.com Managed etcd clusters v1 replica数1 ですが、ポッドなのでポッド内のプロセスに問題があれば kubernetes が自動で修復 (場合によってポッドの再構築) されます。また replica 数を増やす運用も考えられそうです。\netcd クラスタの構築 下記の内容のファイルで etcd をデプロイします。中身を確認しましょう。 API は coreos.com/v1 で kind: EtcdCluster という情報が記されています。また version でイメージバージョンが記されているのかなということが後に確認できます。また size でレプリカ数が記されているようです。\n$ cat example/example-etcd-cluster.yaml apiVersion: \u0026quot;coreos.com/v1\u0026quot; kind: \u0026quot;EtcdCluster\u0026quot; metadata: name: \u0026quot;etcd-cluster\u0026quot; spec: size: 3 version: \u0026quot;v3.1.0-alpha.1\u0026quot; デプロイをしてみます。\n$ kubectl create -f example/example-etcd-cluster.yaml クラスタがデプロイされたか見てみます。3つのポッドが確認できます。やはりファイル中 size: 3 はレプリカ数だったようです。\n$ kubectl get pods --show-all NAME READY STATUS RESTARTS AGE etcd-cluster-0000 1/1 Running 0 1m etcd-cluster-0001 1/1 Running 0 36s etcd-cluster-0002 1/1 Running 0 21s 同様に Service について確認します。etcd-cluster-000[012] の3つが今回作った etcd クラスタです。\n$ kubectl get svc NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE etcd-cluster 10.0.0.80 \u0026lt;none\u0026gt; 2379/TCP 1m etcd-cluster-0000 10.0.0.13 \u0026lt;none\u0026gt; 2380/TCP,2379/TCP 1m etcd-cluster-0001 10.0.0.111 \u0026lt;none\u0026gt; 2380/TCP,2379/TCP 1m etcd-cluster-0002 10.0.0.183 \u0026lt;none\u0026gt; 2380/TCP,2379/TCP 50s kubernetes 10.0.0.1 \u0026lt;none\u0026gt; 443/TCP 5d etcd クラスタをスケールアウト では etcd クラスタのレプリカ数を増やすことでスケールアウトしてみます。が、今現在 (2016/12) 時点では一部不具合があるらしく回避策として下記の通り curl を用いてスケールアウトすることが可能なようです。\n下記の内容で body.json ファイルを用意します。size: 5 になっていることが確認できて、レプリカ数5になるのだなと判断できます。\n$ cat body.json { \u0026quot;apiVersion\u0026quot;: \u0026quot;coreos.com/v1\u0026quot;, \u0026quot;kind\u0026quot;: \u0026quot;EtcdCluster\u0026quot;, \u0026quot;metadata\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;etcd-cluster\u0026quot;, \u0026quot;namespace\u0026quot;: \u0026quot;default\u0026quot; }, \u0026quot;spec\u0026quot;: { \u0026quot;size\u0026quot;: 5 } } ここで curl で実行するためプロキシを稼働します。\n$ kubectl proxy --port=8080 curl で先程作った body.json を PUT してみます。\ncurl -H 'Content-Type: application/json' -X PUT --data @body.json http://127.0.0.1:8080/apis/coreos.com/v1/namespaces/default/etcdclusters/etcd-cluster クラスタがスケールアウトされたか確認しましょう。\nkubectl get pods --show-all NAME READY STATUS RESTARTS AGE etcd-cluster-0000 1/1 Running 0 5m etcd-cluster-0001 1/1 Running 0 4m etcd-cluster-0002 1/1 Running 0 4m etcd-cluster-0003 1/1 Running 0 32s etcd-cluster-0004 1/1 Running 0 17s etcd-operator-217127005-futo0 1/1 Running 0 9m 5台構成のクラスタにスケールアウトしたことが確認できます。\netcd にアクセス 5台構成の etcd クラスタがデプロイできたので etcd に etcdctl を使ってアクセスしてみましょう。事前に etcdctl をインストールする必要があります。また私の環境もそうだったのですがローカルの Mac に minikube を使って kubernetes 環境を構築しているので下記のように nodePort を作るため作業が必要です。\n$ kubectl create -f example/example-etcd-cluster-nodeport-service.json $ export ETCDCTL_API=3 $ export ETCDCTL_ENDPOINTS=$(minikube service etcd-cluster-client-service --url) $ etcdctl put foo bar $ etcdctl get foo foo bar etcdctl でキー・値を入力・読み込みが可能であることが分かりました！\nポッドの自動復旧 kubernetes を普段使っている方は分かると思うのですがポッドを落としても kubernetes が自動復旧してくれます。ここで一つポッドを削除してみようと思います。\n$kubectl get pods NAME READY STATUS RESTARTS AGE etcd-cluster-0000 1/1 Running 0 11m etcd-cluster-0001 1/1 Running 0 11m etcd-cluster-0002 1/1 Running 0 11m etcd-cluster-0003 1/1 Running 0 6m etcd-cluster-0004 1/1 Running 0 6m etcd-operator-217127005-futo0 1/1 Running 0 16m $kubect delete pod etcd-cluster-0004 しばらくすると下記の通り etcd-cluster-0004 に代わり etcd-cluster-0005 が稼働していることが確認できると思います。\n$ kubectl get pods NAME READY STATUS RESTARTS AGE etcd-cluster-0000 1/1 Running 0 12m etcd-cluster-0001 1/1 Running 0 12m etcd-cluster-0002 1/1 Running 0 11m etcd-cluster-0003 1/1 Running 0 7m etcd-cluster-0005 1/1 Running 0 3s etcd-operator-217127005-futo0 1/1 Running 0 17m まとめ kubernetes の上で構成することでうまく kubernetes のメリットを活かしつつ etcd クラスタを構成できていると言えると思います。記事執筆時ではまだ不具合が幾つかありました (上記の curl で実施した箇所や、イメージのアップグレード) が、etcd を手動で構築するより kubernetes で構成するほうがメリットが多いことは明らかです。また kubernetes のポッドから kubernetes dns を介してサービスネームに直接アクセスできるのでポッドから etcd に情報を読み書きすることも容易になりそうです。\nですが etcd に収めるデータの性質によっては運用面で厳しくなることも想定できます。coreos 内で etcd クラスタを介して互いのコンテナ間でコンフィグを共有し合う使い方は非常に意味があると思うのですが、coreos 外の独自のソフトウェアがいろんな種別のデータを etcd クラスタに外から入出力することの意味はそれほど無いように思えます。であれば高耐久性で運用のし易い軽量な KVS ソフトウェアを使うべきだからです。\nまた今回紹介した etcd-operator とは別に coreos が同時にアナウンスした(https://coreos.com/blog/the-prometheus-operator.html) に関しても興味を持っているので後に触ってみたいと思います。\n何と言うか所感として最後に述べるとサーバレスが叫ばれている中でわざわざクラスタソフトウェアを自前で管理するのか？と疑問も確かに残ります。それこそクラウドプラットフォームが提供すべき機能だと思うからです..。\n","permalink":"https://jedipunkz.github.io/post/etcd-operator/","summary":"こんにちは @jedipunkz です。\n今日は某アドベントカレンダーに参加していて記事を書いています。 記事だけ先出ししちゃいます..。\n今日は最近 coreos がリリースした \u0026lsquo;etcd-operator\u0026rsquo; を触ってみようかと思います。ほぼ、README に書かれている手順通りに実施するのですが、所感を交えながら作業して記事にしてみたいと思います。\ncoreos が提供している etcd についてご存知ない方もいらっしゃると思いますが etcd は KVS ストレージでありながら Configuration Management Store として利用できる分散型ストレージです。Docker 等の環境を提供する coreos という軽量 OS 内でも etcd が起動していてクラスタで管理された情報をクラスタ内の各 OS が読み書きできる、といった機能を etcd が提供しています。 詳細については公式サイトを御覧ください。\netcd 公式サイト : https://coreos.com/etcd/docs/latest/\netcd-operator はこの etcd クラスタを kubernetes 上でクラスタ管理するための簡単に運用管理するためのソフトウェアになります。\netcd-operator 公式アナウンス : https://coreos.com/blog/introducing-the-etcd-operator.html\n後に実際に触れていきますが下記のような管理が可能になります。\n etcd クラスタの構築 etcd クラスタのスケールアップ・ダウン etcd Pod の障害時自動復旧 etcd イメージをオンラインで最新のモノにアップグレード  では早速利用してみたいと思います。\n必要な環境 下記の環境が事前に用意されている必要があります。\n Docker Kubernetes or minikube+kubernetes (https://github.com/kubernetes/minikube) etcdctl : https://github.","title":"coreos の etcd operator を触ってみた"},{"content":"こんにちは。@jedipunkz です。\n今回は Helm という kubernetes のパッケージマネージャ的なソフトウェアを使ってみたので記事にしたいと思います。\n 公式サイト : https://helm.sh/  Kubernetes を仕事で使っているのですが \u0026ldquo;レプリケーションコントローラ\u0026rdquo; や \u0026ldquo;サービス\u0026rdquo; といった単位を使って Pod, Service を管理しています。Helm を使うことでこれらの管理方法が変わるのか調べたいと思います。\n依存するソフトウェア 今回は MacOS を使って環境を整えます。\n virtualbox minikube kubectl  これらのソフトウェアをインストールしていきます。\n$ brew cask install virtualbo $ curl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.12.2/minikube-darwin-amd64 \u0026amp;\u0026amp; chmod +x minikube \u0026amp;\u0026amp; sudo mv minikube /usr/local/bin/ $ brew install kubectl minikube を使って簡易的な kubernetes 環境を起動します。\n$ minikube start $ eval $(minikube docker-env) Helm を使ってみる Helm は Charts という単位で Kubernetes をパッケージングします。Charts の一覧を見てみましょう。\n$ helm search NAME VERSION DESCRIPTION stable/drupal 0.3.7 One of the most versatile open source content m... stable/factorio 0.1.1 Factorio dedicated server. stable/ghost 0.4.0 A simple, powerful publishing platform that all... stable/jenkins 0.1.1 A Jenkins Helm chart for Kubernetes. stable/joomla 0.4.0 PHP content management system (CMS) for publish... stable/mariadb 0.5.3 Chart for MariaDB stable/mediawiki 0.4.0 Extremely powerful, scalable software and a fea... stable/memcached 0.4.0 Chart for Memcached stable/minecraft 0.1.0 Minecraft server stable/mysql 0.2.1 Chart for MySQL stable/phpbb 0.4.0 Community forum that supports the notion of use... stable/postgresql 0.2.0 Chart for PostgreSQL stable/prometheus 1.3.1 A Prometheus Helm chart for Kubernetes. Prometh... stable/redis 0.4.1 Chart for Redis stable/redmine 0.3.5 A flexible project management web application. stable/spark 0.1.1 A Apache Spark Helm chart for Kubernetes. Apach... stable/spartakus 1.0.0 A Spartakus Helm chart for Kubernetes. Spartaku... stable/testlink 0.4.0 Web-based test management system that facilitat... stable/traefik 1.1.0-rc3-a A Traefik based Kubernetes ingress controller w... stable/uchiwa 0.1.0 Dashboard for the Sensu monitoring framework stable/wordpress 0.3.2 Web publishing platform for building blogs and ... 各アプリケーションの名前で Charts が管理されていることが分かります。 ここでは stable/mysql を使って kubernetes の中に MySQL 環境を作ってみます。まず stable/mysql に設定できるパラメータ一覧を取得します。\n$ helm inspect stable/mysql Fetched stable/mysql to mysql-0.2.1.tgz description: Chart for MySQL engine: gotpl home: https://www.mysql.com/ keywords: - mysql - database - sql maintainers: - email: viglesias@google.com name: Vic Iglesias name: mysql sources: - https://github.com/kubernetes/charts - https://github.com/docker-library/mysql version: 0.2.1 --- ## mysql image version ## ref: https://hub.docker.com/r/library/mysql/tags/ ## imageTag: \u0026quot;5.7.14\u0026quot; ## Specify password for root user ## ## Default: random 10 character string # mysqlRootPassword: testing ## Create a database user ## # mysqlUser: # mysqlPassword: ## Allow unauthenticated access, uncomment to enable ## # mysqlAllowEmptyPassword: true ## Create a database ## # mysqlDatabase: ## Specify a imagePullPolicy ## 'Always' if imageTag is 'latest', else set to 'IfNotPresent' ## ref: http://kubernetes.io/docs/user-guide/images/#pre-pulling-images ## # imagePullPolicy: ## Persist data to a persitent volume persistence: enabled: true storageClass: generic accessMode: ReadWriteOnce size: 8Gi ## Configure resource requests and limits ## ref: http://kubernetes.io/docs/user-guide/compute-resources/ ## resources: requests: memory: 256Mi cpu: 100m stable/mysql という Charts を構成するパラメータ一覧が取得できました。今回は DB 上のユーザを作ってパスワードを設定してみようと思います。上記のパラメータの中から \u0026lsquo;mysqlUser\u0026rsquo;, \u0026lsquo;mysqlPasswword\u0026rsquo; を編集してみます。下記の内容を config.yaml というファイル名で保存します。\nmysqlUser: user1 mysqlPassword: password1 この config.yaml を使って stable/mysql を起動してみます。\n$ helm install -f config.yaml stable/mysql $ helm ls # \u0026lt;--- 起動した環境を確認する NAME REVISION UPDATED STATUS CHART dealing-ladybug 1 Sun Nov 20 10:44:00 2016 DEPLOYED mysql-0.2.1 \u0026lsquo;dealing-ladybug\u0026rsquo; という名前で mysql が起動したことが分かります。\nkubectl をつかって Services を確認してみます。\n$ kubectl get svc NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE dealing-ladybug-mysql 10.0.0.24 \u0026lt;none\u0026gt; 3306/TCP 33m \u0026lsquo;dealing-ladybug-mysql\u0026rsquo; という Service が kubernetes 環境に作られました。この名前は Kubernetes 内のコンテナから DNS 名前解決できるアドレスとなります。\nここで mysql-client 環境を作るために下記のようなコンテナを起動して mysql-client をインストール、config.yaml で作成したユーザ・パスワードで mysql サーバにアクセス確認します。\n$ kubectl run -i --tty ubuntu02 --image=ubuntu:16.04 --restart=Never -- bash -il # apt-get update; apt-get install -y mysql-client # mysql -h dealing-ladybug-mysql -u user1 -ppassword1 \u0026lt;省略\u0026gt; mysql\u0026gt; config.yaml に記したユーザ情報で MySQL にアクセスできることを確認できました。\nまとめ Helm を使うことでレプリケーションコントローラやサービスを直接 yaml で作らなくても MySQL 環境構築と設定が行えました。Helm の Charts は自分で開発することも可能です。(参考 URL)\nhttps://github.com/kubernetes/helm/blob/master/docs/charts.md\nパッケージングすることで他人の作った資源を有用に活用することもできるため、こういった何かを抽象化する技術を採用することにはとても意味があると思います。自動化を考える上でも抽象化できる技術は有用だと思います。\nですがレプリケーションコントローラを使っても Helm でも Yaml で管理することに変わりはなく、またレプリケーションコントローラで指定できる詳細なパラメータ (replicas や command, image など) を指定できないためすぐに実用するというわけにはいかないなぁと感じました。\n参考 URL  https://github.com/kubernetes/helm/blob/master/docs/charts.md https://github.com/kubernetes/helm/blob/master/docs/quickstart.md  ","permalink":"https://jedipunkz.github.io/post/helm/","summary":"こんにちは。@jedipunkz です。\n今回は Helm という kubernetes のパッケージマネージャ的なソフトウェアを使ってみたので記事にしたいと思います。\n 公式サイト : https://helm.sh/  Kubernetes を仕事で使っているのですが \u0026ldquo;レプリケーションコントローラ\u0026rdquo; や \u0026ldquo;サービス\u0026rdquo; といった単位を使って Pod, Service を管理しています。Helm を使うことでこれらの管理方法が変わるのか調べたいと思います。\n依存するソフトウェア 今回は MacOS を使って環境を整えます。\n virtualbox minikube kubectl  これらのソフトウェアをインストールしていきます。\n$ brew cask install virtualbo $ curl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.12.2/minikube-darwin-amd64 \u0026amp;\u0026amp; chmod +x minikube \u0026amp;\u0026amp; sudo mv minikube /usr/local/bin/ $ brew install kubectl minikube を使って簡易的な kubernetes 環境を起動します。\n$ minikube start $ eval $(minikube docker-env) Helm を使ってみる Helm は Charts という単位で Kubernetes をパッケージングします。Charts の一覧を見てみましょう。","title":"Helm を使って Kubernetes を管理する"},{"content":"こんにちは、@jedipunkz です。今回は Kubernetes1.4 から実装された ScheduledJob を試してみたのでその内容を記したいと思います。\nScheduledJob はバッチ的な処理を Kubernetes の pod を使って実行するための仕組みです。現在は alpha バージョンとして実装されています。 kubernetes の pod, service は通常、永続的に立ち上げておくサーバなどを稼働させるものですが、それに対してこの scheduledJob は cron 感覚でバッチ処理を pod に任せることができます。\nAlpha バージョンということで今回の環境構築は minikube を使って簡易的に Mac 上に構築しました。Docker がインストールされていれば Linux でも環境を作れます。\n参考 URL 今回利用する yaml ファイルなどは下記のサイトを参考にしています。\n http://kubernetes.io/docs/user-guide/scheduled-jobs/ https://github.com/kubernetes/minikube  前提の環境 私の環境では下記の環境を利用しました。\n Mac OSX Docker-machine or Docker for Mac minikube  kubernetes 1.4 以降の構成を minikube で構築する まず minikube のインストールを行います。\n$ curl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.12.0/minikube-darwin-amd64 \u0026amp;\u0026amp; chmod +x minikube \u0026amp;\u0026amp; sudo mv minikube /usr/local/bin/ 早速 minikube を起動します。\n$ docker-machine start default $ minikube start $ eval $(minikube docker-env) Google Cloud の GKE を利用する場合 今回は minikube を使ってローカルマシンに kubernetes 1.4 環境を作りましたが Google Cloud の GKE を用いている場合下記のように gcloud SDK を用いて GKE クラスターノードを構築することで対応できます。ですが Google に確認したところこの構築方法を取った場合には Google からのサポートを得られないのと SLA も対象外になるとのことでした。リスクは大きいと思います。(2016/11現在)\ngcloud alpha container clusters create alpha-test-cluster --zone asia-northeast1-b --enable-kubernetes-alpha gcloud container clusters get-credentials alpha-test-cluster gcloud container node-pools create alpha-test-pool --zone asia-northeast1-b --cluster alpha-test-cluster scheduledjob を試す yaml ファイルの生成 scheduledjob を実行するための yaml ファイルを生成します。公式サイト (http://kubernetes.io/docs/user-guide/scheduled-jobs/) にあるものを一部修正して記述しています。ファイル名は sj.yaml とします。\napiVersion: batch/v2alpha1 kind: ScheduledJob metadata: name: hello spec: schedule: 0/1 * * * ? jobTemplate: spec: template: spec: containers: - name: hello image: busybox args: - /bin/sh - -c - date; echo Hello from the Kubernetes cluster restartPolicy: OnFailure concurrencyPolicy: Allow #suspend: true scheduledJob の実行 生成した yaml ファイルを元に kubectl コマンドで scheduledjob を実行します。\n$ kubectl create -f ./sj.yaml 実行したジョブがどのような状況か確認します。下記のコマンドで生成した scheduledjob 一覧が確認できます。 yaml ファイルの通り、1分間隔で \u0026lsquo;hello\u0026rsquo; という scheduledJob が実行されることが確認できると思います。\n$ kubectl get scheduledjob NAME SCHEDULE SUSPEND ACTIVE LAST-SCHEDULE hello 0/1 * * * ? False 0 \u0026lt;none\u0026gt; scheduledjob を元に実行された(実行される) ジョブ(job) 一覧を確認します。 1分間隔で実行されている様子が確認できます。また、DESIRED:1 で SUCCESSFUL:0 の行は、1分間隔で実行される直前のジョブとして認識されていることがわかります。\n$ kubectl get jobs --watch NAME DESIRED SUCCESSFUL AGE hello-1856276298 1 1 59s NAME DESIRED SUCCESSFUL AGE hello-1780451143 1 0 0s hello-1780451143 1 0 0s hello-1780451143 1 1 5s hello-1476429627 1 0 0s hello-1476429627 1 0 0s hello-1476429627 1 1 5s hello-1628211009 1 0 0s hello-1628211009 1 0 0s hello-1628211009 1 1 5s hello-1552385854 1 0 0s hello-1552385854 1 0 0s hello-1552385854 1 1 5s 下記のコマンドで実行されたジョブ一覧を取得できます。こちらは結果のみですので SUCCESSFULL=1 のみが表示されています。\n$ kubectl get job NAME DESIRED SUCCESSFUL AGE hello-1476429627 1 1 6m hello-1552385854 1 1 4m hello-1552516926 1 1 1m hello-1628211009 1 1 5m hello-1628342081 1 1 2m hello-1704167236 1 1 3m hello-1704298308 1 1 49s hello-1780451143 1 1 7m hello-1856276298 1 1 8m この際に pod の様子も確認してみます。\u0026ndash;show-all オプションで過去の pod を一覧表示します。今回のジョブは一瞬で実行可能な内容なのでこのオプションを付けないと pod 一覧が取得できない可能性が高いです。\nkubectl get pod --show-all NAME READY STATUS RESTARTS AGE hello-1476429627-pxr06 0/1 Completed 0 8m hello-1552385854-y68ci 0/1 Completed 0 6m hello-1552516926-ggjpk 0/1 Completed 0 3m hello-1628211009-iih0i 0/1 Completed 0 7m hello-1628342081-ig5lg 0/1 Completed 0 4m hello-1628473153-9wvn4 0/1 Completed 0 1m hello-1704167236-zqg94 0/1 Completed 0 5m hello-1704298308-eaq8m 0/1 Completed 0 2m hello-1780254535-64mah 0/1 Completed 0 28s hello-1780451143-tjg8r 0/1 Completed 0 9m hello-1856276298-y2o65 0/1 Completed 0 10m 最終行のジョブ \u0026lsquo;hello-1856276298-y2o65\u0026rsquo; の内容を確認します。\n$ kubectl logs hello-1856276298-y2o65 Thu Oct 27 01:18:11 UTC 2016 Hello from the Kubernetes cluster sj.yaml 内に記述したジョブ内容 \u0026lsquo;echo \u0026hellip;\u0026rsquo; の実行結果が表示されていることが確認できます。\ndocker のイメージも確認します。ジョブ内で指定した image: busybox が確認できます。 その他のイメージは minikube を構成するものです。\n$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE busybox latest e02e811dd08f 2 weeks ago 1.093 MB gcr.io/google_containers/kubernetes-dashboard-amd64 v1.4.0 436faaeba2e2 5 weeks ago 86.27 MB gcr.io/google-containers/kube-addon-manager v5.1 735ce4344f7f 3 months ago 255.8 MB gcr.io/google_containers/pause-amd64 3.0 99e59f495ffa 5 months ago 746.9 kB ジョブの削除 ジョブを削除します。削除する対象は scheduledjob とそれを元に生成・実行された jobs です。\n$ kubectl delete scheduledjob hello $ kubectl delete jobs $(kubectl get jobs | awk '{print $1}' | grep -v NAME) job \u0026quot;hello-1476429627\u0026quot; deleted job \u0026quot;hello-1552385854\u0026quot; deleted job \u0026quot;hello-1552516926\u0026quot; deleted job \u0026quot;hello-1628211009\u0026quot; deleted job \u0026quot;hello-1628342081\u0026quot; deleted job \u0026quot;hello-1628473153\u0026quot; deleted job \u0026quot;hello-1628604225\u0026quot; deleted job \u0026quot;hello-1704167236\u0026quot; deleted job \u0026quot;hello-1704298308\u0026quot; deleted job \u0026quot;hello-1704429380\u0026quot; deleted job \u0026quot;hello-1780254535\u0026quot; deleted job \u0026quot;hello-1780385607\u0026quot; deleted job \u0026quot;hello-1780451143\u0026quot; deleted job \u0026quot;hello-1856276298\u0026quot; deleted ここで確認できたのは scheduledjob を削除するとそれ以降の jobs は新規実行されませんでした。が、実行された後の jobs は情報として残ったままでした。\nその他のオプション concurrencyPolicy 下記のように spec.concurrencyPolicy オプションが指定できます。下記のような value を渡すと実行されるジョブの動作を変えることが可能です。\n Allow (Default) : 重複するジョブ実行を許可 Forbid : 直前のジョブが終了していない場合ジョブ実行をスキップする Replace : 直前のジョブが終了していない場合新しいジョブを上書きする  suspend \u0026lsquo;spec.suspend: true\u0026rsquo; に設定すると ScheduledJob は生成されますが次回実行時のジョブがサスペンドされ実行されません。デフォルトは false です。\n$ kubectl get scheduledjob NAME SCHEDULE SUSPEND ACTIVE LAST-SCHEDULE hello 0/1 * * * ? True 0 \u0026lt;none\u0026gt; $ kubectl get job $ オプションの適用例ファイル apiVersion: batch/v2alpha1 kind: ScheduledJob metadata: name: hello spec: schedule: 0/1 * * * ? jobTemplate: spec: template: spec: containers: - name: hello image: busybox args: - /bin/sh - -c - date; echo Hello from the Kubernetes cluster restartPolicy: OnFailure concurrencyPolicy: Forbid suspend: true まとめと考察 今回試してみて、バッチ的な処理を kubernetes で行うならこの scheduledJob しかないな、と思いました。 ですが、2016/11 現在ではまだ Alpha バージョンということで下記のようなリスクを含んでいます。\n このバージョンはバギーです アナウンス無しに機能が削られることがあります アナウンス無しにそれまでの互換性を変更することがあります テスト用としての利用を勧めます。  参考サイト : http://kubernetes.io/docs/api/\nまた上記にも記しましたが、ジョブの結果が残っていくため、通常使う cron のように数分間隔で実行しているとあっという間に job の量が大量に肥大化することが予想されます。この job には実行結果も含まれているため、消されるものではないのは理解できるのですが kubernetes api が持っている DB 上に大量のデータが生成され続けてしまうため、kubernetes api/サーバを管理している場合には問題になると思います。この辺り、仕様の修正が入ることを期待しています。\n","permalink":"https://jedipunkz.github.io/post/kubernetes-scheduledjob/","summary":"こんにちは、@jedipunkz です。今回は Kubernetes1.4 から実装された ScheduledJob を試してみたのでその内容を記したいと思います。\nScheduledJob はバッチ的な処理を Kubernetes の pod を使って実行するための仕組みです。現在は alpha バージョンとして実装されています。 kubernetes の pod, service は通常、永続的に立ち上げておくサーバなどを稼働させるものですが、それに対してこの scheduledJob は cron 感覚でバッチ処理を pod に任せることができます。\nAlpha バージョンということで今回の環境構築は minikube を使って簡易的に Mac 上に構築しました。Docker がインストールされていれば Linux でも環境を作れます。\n参考 URL 今回利用する yaml ファイルなどは下記のサイトを参考にしています。\n http://kubernetes.io/docs/user-guide/scheduled-jobs/ https://github.com/kubernetes/minikube  前提の環境 私の環境では下記の環境を利用しました。\n Mac OSX Docker-machine or Docker for Mac minikube  kubernetes 1.4 以降の構成を minikube で構築する まず minikube のインストールを行います。\n$ curl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.12.0/minikube-darwin-amd64 \u0026amp;\u0026amp; chmod +x minikube \u0026amp;\u0026amp; sudo mv minikube /usr/local/bin/ 早速 minikube を起動します。","title":"kubernetes1.4 で実装された ScheduledJob を試してみた！"},{"content":"こんにちは。@jedipunkz です。\nkubernetes の環境を簡易的に作れる Minikube (https://github.com/kubernetes/minikube) が2ヶ月前ほどにリリースになっていました。簡単ですが少し触ってみたのでその際のメモを記したいと思います。VirtualBox もしくは VMware Fusion がインストールされていればすぐにでも稼働可能です。私は Kubernetes 初心者ですが何も考えずに kubernetes を動かすことが出来ました。\n前提 前提として下記の環境が必要になります。\n Mac OSX がインストールされていること VirtualBox もしくは VMware Fusion がインストールされていること  minikube をインストール minikube をインストールします。\n$ curl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.6.0/minikube-darwin-amd64 \u0026amp;\u0026amp; chmod +x minikube \u0026amp;\u0026amp; sudo mv minikube /usr/local/bin/ kubetl をインストール 次に kubectl をインストールします。\n$ curl -k -o kubectl https://kuar.io/darwin/kubectl \u0026amp;\u0026amp; chmod +x kubectl \u0026amp;\u0026amp; sudo mv kubectl /usr/local/bin/ Minikube で Kurbernates を稼働 Minikube を使って Kubernetes を稼働してみます。下記のコマンドを実行すると Virtualbox 上で仮想マシンが稼働し Kubernetes 一式も立ち上がります。\n$ minikube start Kurbernates を使ってみる Pods を立ち上げてみましょう。下記の内容を redis-django.yaml ファイルに保存します。\napiVersion: v1 kind: Pod metadata: name: redis-django labels: app: web spec: containers: - name: key-value-store image: redis ports: - containerPort: 6379 - name: frontend image: django ports: - containerPort: 8000 kubectl コマンドで Pod を立ち上げます。\n$ kubectl create -f ./redis-django.yaml Pod の様子を確認します。\n$ kubectl get pods NAME READY STATUS RESTARTS AGE redis-django 1/2 CrashLoopBackOff 7 15m Minikube はクラスタといってもノードが1つなので READY 1/2 となるようです。Nodes の様子を見てみます。\n$ kubectl get nodes NAME LABELS STATUS minikubevm beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=minikubevm Ready Docker ホスト上の様子を見てみましょう。Kubernetes を形成するコンテナと共に redis のコンテナが稼働していることが確認できます。\n$ eval $(minikube docker-env) $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 550285614e33 redis \u0026quot;docker-entrypoint.sh\u0026quot; 20 minutes ago Up 20 minutes k8s_key-value-store.a3b8356e_redis-django_default_4440a1d8-5272-11e6-9f19-6e0006e7fb51_90c3fec8 aba3a8c040d4 gcr.io/google_containers/pause-amd64:3.0 \u0026quot;/pause\u0026quot; 20 minutes ago Up 20 minutes k8s_POD.822b267d_redis-django_default_4440a1d8-5272-11e6-9f19-6e0006e7fb51_5bef1d2a 9ea96a3f3e10 gcr.io/google-containers/kube-addon-manager-amd64:v2 \u0026quot;/opt/kube-addons.sh\u0026quot; 48 minutes ago Up 48 minutes k8s_kube-addon-manager.a1c58ca2_kube-addon-manager-minikubevm_kube-system_48abed82af93bb0b941173334110923f_84f4fd38 192e886a5795 gcr.io/google_containers/pause-amd64:3.0 \u0026quot;/pause\u0026quot; 48 minutes ago Up 48 minutes k8s_POD.d8dbe16c_kube-addon-manager-minikubevm_kube-system_48abed82af93bb0b941173334110923f_6c65b482 7b005c68d9d4 gcr.io/google_containers/pause-amd64:3.0 \u0026quot;/pause\u0026quot; 48 minutes ago Up 48 minutes k8s_POD.2225036b_kubernetes-dashboard-pzdxy_kube-system_7005dce1-479a-11e6-a0ce-86b669e45864_c08bd009 まとめ Kubernetes のことを殆ど知らない私でもなんとなくですが稼働させて基本的な操作が出来ました。2016/5/31 にリリースされたツールなのでまだ安定しないところもありますが、より容易に Kubernetes が稼働できるようになったのでエンジニアの敷居が下がったのではないでしょうか。\n","permalink":"https://jedipunkz.github.io/post/minikube/","summary":"こんにちは。@jedipunkz です。\nkubernetes の環境を簡易的に作れる Minikube (https://github.com/kubernetes/minikube) が2ヶ月前ほどにリリースになっていました。簡単ですが少し触ってみたのでその際のメモを記したいと思います。VirtualBox もしくは VMware Fusion がインストールされていればすぐにでも稼働可能です。私は Kubernetes 初心者ですが何も考えずに kubernetes を動かすことが出来ました。\n前提 前提として下記の環境が必要になります。\n Mac OSX がインストールされていること VirtualBox もしくは VMware Fusion がインストールされていること  minikube をインストール minikube をインストールします。\n$ curl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.6.0/minikube-darwin-amd64 \u0026amp;\u0026amp; chmod +x minikube \u0026amp;\u0026amp; sudo mv minikube /usr/local/bin/ kubetl をインストール 次に kubectl をインストールします。\n$ curl -k -o kubectl https://kuar.io/darwin/kubectl \u0026amp;\u0026amp; chmod +x kubectl \u0026amp;\u0026amp; sudo mv kubectl /usr/local/bin/ Minikube で Kurbernates を稼働 Minikube を使って Kubernetes を稼働してみます。下記のコマンドを実行すると Virtualbox 上で仮想マシンが稼働し Kubernetes 一式も立ち上がります。","title":"Minikube で簡易 kubernetes 環境構築"},{"content":"こんにちは。@jedipunkzです。\n今日は Go 言語でサーバのメトリクスデータを InfluxDB に入れてリソース監視を行う方法について書きます。\nAnsible, Terraform, Chef などのソフトウェアを使ってインフラを定義するのが当たり前になった現在ですが、本当の意味でのソフトウェアによるインフラの定義ってなんだろと最近考えています。aws-sdk や fog などを使ったネイティブな言語でインフラを定義することの意味もあるように感じているからです。某サービスプロバイダのエンジニアはこうした言語によるインフラの定義の一番大きなメリットとして \u0026ldquo;再利用性\u0026rdquo; をあげていました。こうしたソフトウェアによるインフラの定義や構成を行う上で監視についてもコード化できるのでは？と考えて今回の記事に至りました。\n使うモノ  https://github.com/influxdata/influxdb/tree/master/client  公式の InfluxDB Go Client です。InfluxDB 自体が Go 言語で書かれていますがクライアントも Go 言語で記述することができます。ここにあるサンプルコードをすこしいじって、今回の記事を書こうと思います。\n https://github.com/shirou/gopsutil  @shirou さんが作られた psutil の Go 言語版です。CPU, Mem などリソースをモニタするのに便利なので利用します。\n環境構築 環境を作っていきます。InfluxDB と Chronograf を構築するのですが Docker で構築するのが簡単なのでこれを利用します。Chronograf は InfluxDB 内のデータを可視化するためのソフトウェアです。\n InfluxDB の起動  InfluxDB のコンテナを起動します。\ndocker run -p 8083:8083 -p 8086:8086 \\ -v $PWD:/var/lib/influxdb \\ influxdb  Chronograf の起動  Chronograf のコンテナを起動します。\ndocker run -p 10000:10000 chronograf この時点で http://${DOCKER_HOST}:10000/ にアクセスすると Chronograf の UI を確認できます。\nInfluxDB にユーザ・データベースを作成する InfluxDB 上にユーザとデータベースを作成します。言語の中でも作ることが出来ますが、今回は手動で。 Mac OSX を使っている場合 homebrew で influxdb をインストールすることが簡単にできます。\nbrew install influxdb ユーザを作ります。\ninflux -host 192.168.99.100 -port 8086 \u0026gt; create user foo with password 'foo' \u0026gt; grant all privileges to foo データベースを作ります。\ninflux -host 192.168.99.100 -port 8086 \u0026gt; CREATE DATABASE IF NOT EXISTS square_holes; Go言語で CPU 時間を取得し InfluxDB にメトリクスデータを挿入 Go 言語でメモリー使用率を取得し得られたメトリクスデータを InfluxDB に挿入するコードを書きます。\npackage main import ( \u0026#34;log\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/influxdata/influxdb/client/v2\u0026#34; \u0026#34;github.com/shirou/gopsutil/cpu\u0026#34; ) const ( MyDB = \u0026#34;square_holes\u0026#34; username = \u0026#34;foo\u0026#34; password = \u0026#34;foo\u0026#34; ) func main() { for { // Make client  c, err := client.NewHTTPClient(client.HTTPConfig{ Addr: \u0026#34;http://192.168.99.100:8086\u0026#34;, Username: username, Password: password, }) if err != nil { log.Fatalln(\u0026#34;Error: \u0026#34;, err) } // Create a new point batch  bp, err := client.NewBatchPoints(client.BatchPointsConfig{ Database: MyDB, Precision: \u0026#34;s\u0026#34;, }) if err != nil { log.Fatalln(\u0026#34;Error: \u0026#34;, err) } // get CPU info  cp, _ := cpu.Times(true) // get CPU status info for each core  var user, system, idle float64 = 0, 0, 0 for _, sub_cpu := range cp { user = user + sub_cpu.User system = system + sub_cpu.System idle = idle + sub_cpu.Idle } // Create a point and add to batch  tags := map[string]string{\u0026#34;cpu\u0026#34;: \u0026#34;cpu\u0026#34;} fields := map[string]interface{}{ \u0026#34;User\u0026#34;: user / float64(len(cp)), \u0026#34;System\u0026#34;: system / float64(len(cp)), \u0026#34;Idle\u0026#34;: idle / float64(len(cp)), } pt, err := client.NewPoint(\u0026#34;cpu\u0026#34;, tags, fields, time.Now()) if err != nil { log.Fatalln(\u0026#34;Error: \u0026#34;, err) } bp.AddPoint(pt) // Write the batch  c.Write(bp) time.Sleep(1 * time.Second) } } ビルドして実行すると下記のように influxdb 上のデータベースにメトリクスデータが挿入されていることを確認できます。\ninflux -host 192.168.99.100 -port 8086 -execute 'SELECT * FROM cpu' -database=square_holes -precision=s | head -8 name: cpu --------- time Idle System User cpu 1469342272 20831.04296875 3700.185546875 3544.90234375 cpu 1469342273 20831.666015625 3700.302734375 3544.966796875 cpu 1469342274 20832.2109375 3700.447265625 3545.068359375 cpu 1469342275 20832.828125 3700.546875 3545.13671875 cpu 1469342291 20841.728515625 3702.482421875 3546.806640625 cpu Chronograf の UI で確認してみましょう。\n得られた CPU に関するデータが可視化されていることが確認できます。変化に乏しいグラフですが\u0026hellip;。 この辺りは CPU 時間から CPU 使用率を得るコードに書き換えるといいかもしれません。\nまとめと考察 InfluxDB の提供元が出している Telegraf というメトリクスデータの送信エージェントがありますが、同じような動きを Go 言語で簡単に開発できることが分かりました。ネイティブな言語で開発するとより柔軟にデータの送信ができることも期待できます。また冒頭に述べた通り再利用も用意になるのではと思います。インフラの状態をメトリクスデータとして時系列 DB に挿入して可視化するということは監視のコード化とも言えると思います。ただし、フレームワークが出てきてもっと簡単に書ける仕組みが出てこないと厳しい気もしますが。果たしてこれらインフラを言語で記述していくことがどれだけ有用なのかまだわかりませんが、いつか現場で実践してみたいと思います。\n","permalink":"https://jedipunkz.github.io/post/influxdb-go/","summary":"こんにちは。@jedipunkzです。\n今日は Go 言語でサーバのメトリクスデータを InfluxDB に入れてリソース監視を行う方法について書きます。\nAnsible, Terraform, Chef などのソフトウェアを使ってインフラを定義するのが当たり前になった現在ですが、本当の意味でのソフトウェアによるインフラの定義ってなんだろと最近考えています。aws-sdk や fog などを使ったネイティブな言語でインフラを定義することの意味もあるように感じているからです。某サービスプロバイダのエンジニアはこうした言語によるインフラの定義の一番大きなメリットとして \u0026ldquo;再利用性\u0026rdquo; をあげていました。こうしたソフトウェアによるインフラの定義や構成を行う上で監視についてもコード化できるのでは？と考えて今回の記事に至りました。\n使うモノ  https://github.com/influxdata/influxdb/tree/master/client  公式の InfluxDB Go Client です。InfluxDB 自体が Go 言語で書かれていますがクライアントも Go 言語で記述することができます。ここにあるサンプルコードをすこしいじって、今回の記事を書こうと思います。\n https://github.com/shirou/gopsutil  @shirou さんが作られた psutil の Go 言語版です。CPU, Mem などリソースをモニタするのに便利なので利用します。\n環境構築 環境を作っていきます。InfluxDB と Chronograf を構築するのですが Docker で構築するのが簡単なのでこれを利用します。Chronograf は InfluxDB 内のデータを可視化するためのソフトウェアです。\n InfluxDB の起動  InfluxDB のコンテナを起動します。\ndocker run -p 8083:8083 -p 8086:8086 \\ -v $PWD:/var/lib/influxdb \\ influxdb  Chronograf の起動  Chronograf のコンテナを起動します。","title":"Go言語とInfluxDBで監視のコード化"},{"content":"こんにちは。@jedipunkzです。\n私もインフラのプロビジョニングツールとして Chef ではなく Ansible を使うことが増えたのですが、Chef を使っていた頃に同じく利用していた test-kitchen が便利だったので ansible と併用できないかと思い試してみました。test-kitchen は Docker コンテナや EC2 等を起動して Chef, Ansible 等で構成をデプロイし serverspec 等のテストツールで構成をテストできるソフトウェアです。AWS EC2 でデプロイしてもいいのですが、EC2 を起動してデプロイして失敗したら削除してのサイクルを回すことを考えるとだいぶ面倒なので Docker + test-kitchen を使ってこのサイクルを高速に回す方がメリットが大きそうです。今回は Docker + test-kitchen を使って Ansible Role (Playbook) を開発するサイクルを高速化する方法を記したいと思います。\nソフトウェアの構成 構成は、私の場合 Mac OSX を使っているので下記のとおりです。\n test-kitchen kitchen-ansible (test-kitchen ドライバ) kitchen-docker (test-kitchen ドライバ) serverspec ansible docker (Docker-machine) VirtualBox  Linux でネイティブな Docker を使っている方は以降、読み替えて下さい。読み替えるのはそれほど難しくないと思います。\nソフトウェアのインストール 今回は上記ソフトウェアのインストール方法は省きます。test-kitchen, kitchen-ansible, kitchen-docker, serverspec は Ruby で開発されたソフトウェアなので Gemfile 等で管理、ansible は pip 等でインストールしてください。\n環境作成 test-kitchen が稼働するように環境を作っていきます。 作業ディレクトリで kitchen コマンドを使って初期設定を行います。今回は試しに nginx のデプロイを実施したいと思います。\n$ mkdir -p test-kitchen/nginx test-kitchen/roles $ cd test-kitchen/nginx $ kitchen init また上記で作成した roles ディレクトリに ansible-galaxy で nginx の role を取得します。\n$ ansible-galaxy install geerlingguy.nginx -p ../roles/nginx 下記の内容を .kitchen.local.yml として保存してください。 Docker ホストの指定、Provisioner として ansible の指定、Platform として \u0026lsquo;ubuntu:16.04\u0026rsquo; の Docker コンテナの指定を行っています。\n--- driver: name: docker binary: /usr/local/bin/docker socker: tcp://192.168.99.100:2376 provisioner: name: ansible_playbook playbook: ./site.yml roles_path: ../roles host_vars_path: ./host_vars hosts: kitchen-deploy require_ansible_omnibus: false ansible_platform: ubuntu require_chef_for_busser: true platforms: - name: ubuntu driver_config: image: ubuntu:16.04 platform: ubuntu require_chef_omnibus: false suites: - name: default run_list: attributes: ここからは上記 .kitchen.local.yml ファイル内で指定したファイルの準備を行っていきます。\nsite.yml ファイルの内容を下記のように書いてください。\n--- - hosts: kitchen-deploy sudo: yes roles: - { role: geerlingguy.nginx, tags: nginx } host_vars/hosts ファイルを作成します。\u0026lsquo;host_vars\u0026rsquo; ディレクトリは手動で作成してください。\nlocalhost ansible_connection=local [kitchen-deploy] localhost 次に serverspec で行うテストの内容を作成します。 serverspec-init コマンドではインタラクティブに回答しますが、SSH ではなく EXEC(Local) を選択することに注意してください。\n$ mkdir -p test/integration/default/serverspec $ cd test/integration/default/serverspec $ serverspec-init # \u0026lt;--- インタラクティブに回答 : 1) UNIX, 2) EXEC(Local) を選択 $ rm localhost/sample_spec.rb # \u0026lt;--- 必要ないので削除 test/integration/default/serverspec/localhost/nginx_spec.rb として下記の内容を試しに書いてみましょう。\nrequire 'spec_helper' describe package('nginx') do it { should be_installed } end describe service('nginx') do it { should be_enabled } it { should be_running } end describe file('/etc/nginx/nginx.conf') do it { should be_file } end 下記のようなファイルとディレクトリ構成になっていることを確認しましょう。\n. ├── nginx │ ├── chefignore │ ├── host_vars │ │ └── hosts │ ├── site.yml │ └── test │ └── integration │ └── default │ ├── Rakefile │ └── serverspec │ ├── localhost │ │ └── nginx_spec.rb │ └── spec_helper.rb └── roles └── geerlingguy.nginx ├── README.md \u0026lt;省略\u0026gt; デプロイ・テストを実行する 環境作成が完了したの Docker コンテナを起動し Ansible でデプロイ、その後 Serverspec でテストしてみます。\n$ cd test-kitchen $ kitchen create # \u0026lt;--- Docker コンテナ起動 $ kitchen setup # \u0026lt;--- Ansible デプロイ $ kitchen verify # \u0026lt;--- Serverspec テスト $ kitchen destroy # \u0026lt;--- コンテナ削除 $ kitchen test # \u0026lt;--- 上の4つのコマンドを一気に実行 まとめ Ansible でも test-kitchen を使ってデプロイ・テストが出来ることが分かりました。インスタンスを使ってデプロイ・テストを実施するよりコンテナを使うほうが失敗した際に削除・起動するのも一瞬で終わりますし Ansible 開発が高速化できることも実際に触っていただいてわかっていただけると思います。\nただ上記の手順ではコンテナの中に Ruby, Chef も一緒にインストールされてしまいます。 test-kitchen 的には下記の記述を .kitchen.local.yml の provisioner: の欄に記述すると Chef のインストールは省けるはず (Ruby は Serverspec で用いる) のですが今現在 (2016/7中旬) では NG でした。これが正常に機能するようになるともっと高速にコンテナデプロイが完了すると思うので残念です。\nrequire_chef_for_busser: false require_ruby_for_busser: true ","permalink":"https://jedipunkz.github.io/post/test-kitchen-with-ansible/","summary":"こんにちは。@jedipunkzです。\n私もインフラのプロビジョニングツールとして Chef ではなく Ansible を使うことが増えたのですが、Chef を使っていた頃に同じく利用していた test-kitchen が便利だったので ansible と併用できないかと思い試してみました。test-kitchen は Docker コンテナや EC2 等を起動して Chef, Ansible 等で構成をデプロイし serverspec 等のテストツールで構成をテストできるソフトウェアです。AWS EC2 でデプロイしてもいいのですが、EC2 を起動してデプロイして失敗したら削除してのサイクルを回すことを考えるとだいぶ面倒なので Docker + test-kitchen を使ってこのサイクルを高速に回す方がメリットが大きそうです。今回は Docker + test-kitchen を使って Ansible Role (Playbook) を開発するサイクルを高速化する方法を記したいと思います。\nソフトウェアの構成 構成は、私の場合 Mac OSX を使っているので下記のとおりです。\n test-kitchen kitchen-ansible (test-kitchen ドライバ) kitchen-docker (test-kitchen ドライバ) serverspec ansible docker (Docker-machine) VirtualBox  Linux でネイティブな Docker を使っている方は以降、読み替えて下さい。読み替えるのはそれほど難しくないと思います。\nソフトウェアのインストール 今回は上記ソフトウェアのインストール方法は省きます。test-kitchen, kitchen-ansible, kitchen-docker, serverspec は Ruby で開発されたソフトウェアなので Gemfile 等で管理、ansible は pip 等でインストールしてください。","title":"Test-Kitchen, Docker で Ansible Role 開発サイクル高速化！"},{"content":"こんにちは。@jedipunkzです。\n今回は StackStorm (https://stackstorm.com/) というイベントドリブンオートメーションツールを使ってみましたので 紹介したいと思います。\nクラウドとプロビジョニングツールの登場で昨今はエンジニアがほぼ全ての操作を自動化出来るようになりました。 ですが監視についてはどうでしょうか？監視システムを自動で構築することが出来ても障害発生時に対応を行う のは手動になっていませんでしょうか。もちろんクラスタ組んでいれば大抵のアラートは放置出来ますが、クラスタ を組むことが出来ないような箇所はクラウドを使ってもどうしても出てきます。\nそこで登場するのが今回紹介する StackStorm のようなツールかなぁと考えるようになりました。\nインストール インストール手順は下記の URL にあります。\nhttps://docs.stackstorm.com/install/index.html\n自分は CentOS7 を使ったので下記のワンライナーでインストールできました。 password は任意のものを入れてください。\ncurl -sSL https://stackstorm.com/packages/install.sh | bash -s -- --user=st2admin --password=foo MongoDB, postgreSQL が依存してインストールされます。\n80番ポートで下記のような WEB UI も起動します。\nStackStorm の基本 基本を知るために幾つかの要素について説明していきます。\nまず CLI で操作するために TOKEN を取得して環境変数にセットする必要があります。 上記で設定したユーザ名・パスワードを入力してください。\nexport ST2_AUTH_TOKEN=`st2 auth -t -p foo st2admin`  Action  Action はイベントが発生した際に実行できるアクションになります。早速アクションの一覧を取得してみましょう。\n$ st2 action list +---------------------------------+---------+-------------------------------------------------------------+ | ref | pack | description | +---------------------------------+---------+-------------------------------------------------------------+ | chatops.format_execution_result | chatops | Format an execution result for chatops | | chatops.post_message | chatops | Post a message to stream for chatops | | chatops.post_result | chatops | Post an execution result to stream for chatops | \u0026lt;省略\u0026gt; | core.http | core | Action that performs an http request. | | core.local | core | Action that executes an arbitrary Linux command on the | | | | localhost. | | core.local_sudo | core | Action that executes an arbitrary Linux command on the | | | | localhost. | | core.remote | core | Action to execute arbitrary linux command remotely. | | core.remote_sudo | core | Action to execute arbitrary linux command remotely. | | core.sendmail | core | This sends an email | | core.windows_cmd | core | Action to execute arbitrary Windows command remotely. | \u0026lt;省略\u0026gt; | linux.cp | linux | Copy file(s) | | linux.diag_loadavg | linux | Diagnostic workflow for high load alert | | linux.dig | linux | Dig action | \u0026lt;省略\u0026gt; | st2.kv.get | st2 | Get value from datastore | | st2.kv.set | st2 | Set value in datastore | +---------------------------------+---------+-------------------------------------------------------------+ 上記のように Linux のコマンドや ChatOps, HTTP でクエリを投げるもの、Key Value の読み書きを行うモノまであります。 上記はかなり咲楽して貼り付けたので本来はもっと沢山のアクションがあります。\n Trigger  Trigger は Action を実行する際のトリガになります。同様に一覧を見てみましょう。\n$ st2 trigger list +--------------------------------------+-------+----------------------------------------------------------------+ | ref | pack | description | +--------------------------------------+-------+----------------------------------------------------------------+ | core.st2.generic.actiontrigger | core | Trigger encapsulating the completion of an action execution. | | core.st2.IntervalTimer | core | Triggers on specified intervals. e.g. every 30s, 1week etc. | | core.st2.generic.notifytrigger | core | Notification trigger. | | core.st2.DateTimer | core | Triggers exactly once when the current time matches the | | | | specified time. e.g. timezone:UTC date:2014-12-31 23:59:59. | | core.st2.action.file_writen | core | Trigger encapsulating action file being written on disk. | | core.st2.CronTimer | core | Triggers whenever current time matches the specified time | | | | constaints like a UNIX cron scheduler. | | core.st2.key_value_pair.create | core | Trigger encapsulating datastore item creation. | | core.st2.key_value_pair.update | core | Trigger encapsulating datastore set action. | | core.st2.key_value_pair.value_change | core | Trigger encapsulating a change of datastore item value. | | core.st2.key_value_pair.delete | core | Trigger encapsulating datastore item deletion. | | core.st2.sensor.process_spawn | core | Trigger indicating sensor process is started up. | | core.st2.sensor.process_exit | core | Trigger indicating sensor process is stopped. | | core.st2.webhook | core | Trigger type for registering webhooks that can consume | | | | arbitrary payload. | | linux.file_watch.line | linux | Trigger which indicates a new line has been detected | +--------------------------------------+-------+----------------------------------------------------------------+ CronTimer はその名の通り Cron であることが分かります。IntervalTimer は同じように一定時間間隔で実行するようです。 その他 webhook や Key Value のペアが生成・削除・変更されたタイミング、等あります。\n Rule  Rule は Trigger が発生して Action を実行する際のルールを記述するものになります。\n$ st2 rule list +----------------+---------+-------------------------------------------------+---------+ | ref | pack | description | enabled | +----------------+---------+-------------------------------------------------+---------+ | chatops.notify | chatops | Notification rule to send results of action | True | | | | executions to stream for chatops | | +----------------+---------+-------------------------------------------------+---------+ 初期では上記の chatops.notify のみがあります。\n実際に使ってみる core.local というアクションを実行してみました。\n$ st2 run core.local -- uname -a id: 5774c022e138230c66f2eefc status: succeeded parameters: cmd: uname -a result: failed: false return_code: 0 stderr: '' stdout: 'Linux localhost.localdomain 3.10.0-327.el7.x86_64 #1 SMP Thu Nov 19 22:10:57 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux' succeeded: true stdout に実行結果が出力されています。また下記のように実行結果の一覧を得ることが出来ます。\n$ st2 execution list +----------------------------+---------------+--------------+-------------------------+-----------------------------+-------------------------------+ | id | action.ref | context.user | status | start_timestamp | end_timestamp | +----------------------------+---------------+--------------+-------------------------+-----------------------------+-------------------------------+ | 5774bdbee138230c66f2eeef | core.local | st2admin | succeeded (0s elapsed) | Thu, 30 Jun 2016 06:35:42 | Thu, 30 Jun 2016 06:35:42 UTC | | | | | | UTC | | +----------------------------+---------------+--------------+-------------------------+-----------------------------+-------------------------------+ 応用した使い方 上記のように core.local, core.remote 等でホスト上のコマンドを実行できることが分かりました。 ここで応用した使い方をしてみます。と言いますか、上記の基本的な使い方だけでは StackStorm を 使うメリットが無いように思えます。\n下記のような Rule を作成します。ファイル名は st2_sample_rule_webhook_remote_command.yaml とします。\n--- name: \u0026quot;st2_sample_rule_webhook_remote_command\u0026quot; pack: \u0026quot;examples\u0026quot; description: \u0026quot;Sample rule of webhook.\u0026quot; enabled: true trigger: type: \u0026quot;core.st2.webhook\u0026quot; parameters: url: \u0026quot;restart\u0026quot; criteria: action: ref: \u0026quot;core.remote\u0026quot; parameters: hosts: \u0026quot;10.0.1.250\u0026quot; username: \u0026quot;thirai\u0026quot; private_key: \u0026quot;/root/.ssh/id_rsa\u0026quot; cmd: \u0026quot;sudo service cron restart\u0026quot; StackStorm の基本要素 Action, Criteria(Rule の基準), Trigger から成っていることが分かります。 Triger は webhoook です。url: \u0026ldquo;restart\u0026rdquo; となっているのは URL : https://\u0026lt;stackstorm_ip_addr\u0026gt;/api/v1/webhooks/restart という名前で アクセスを受けるようになるという意味です。criteria は今回は無条件で action を実行したいので空行にします。 action では core.remote が選択されていて hosts: \u0026lsquo;10.0.1.250\u0026rsquo; に username で SSH してコマンドを実行しています。\n要するに https://\u0026lt;stackstorm_ip_addr\u0026gt;/api/v1/webhooks/restart というアドレスでリクエストを受けると 10.0.1.250 に \u0026lsquo;foo\u0026rsquo; というユーザで SSH してコマンドを実行する、というルールになっています。\n下記のコマンドで上記の yaml ファイルをルールに読み込みます。\nst2 rule create st2_sample_rule_webhook_remote_command.yaml 実際にリクエストを投げてみましょう。Token は読み替えてください。\ncurl -k https://localhost/api/v1/webhooks/restart -d '{}' -H 'Content-Type: application/json' -H 'X-Auth-Token: \u0026lt;Your_Token\u0026gt;' するとリモートホストで \u0026lsquo;cron\u0026rsquo; プロセスの再起動が掛かります。\nまとめと考察 StackStorm は紹介した以外にも沢山のアクションがあり応用が効きます。また監視ツールでアラートが発生した際に webhook 通知するようにして 障害対応を自動で行うといった操作も可能な事がわかりました。ChatOps でも応用が可能なことが分かります。従来、ChatOps ではリモートホストで コマンドなどを実行しようとした場合には Hubot 等のプロセスが稼働しているホストもしくはそのホストから SSH 出来るホストで実行する必要がありましたが StackStorm を介すことで実行結果の閲覧やルールに従った実行等が可能になります。\n自分はまだ少しのアクション・ルールを試用しただけなのですが、他に良い運用上の応用例がないか探してみようと思います。\n","permalink":"https://jedipunkz.github.io/post/stackstorm/","summary":"こんにちは。@jedipunkzです。\n今回は StackStorm (https://stackstorm.com/) というイベントドリブンオートメーションツールを使ってみましたので 紹介したいと思います。\nクラウドとプロビジョニングツールの登場で昨今はエンジニアがほぼ全ての操作を自動化出来るようになりました。 ですが監視についてはどうでしょうか？監視システムを自動で構築することが出来ても障害発生時に対応を行う のは手動になっていませんでしょうか。もちろんクラスタ組んでいれば大抵のアラートは放置出来ますが、クラスタ を組むことが出来ないような箇所はクラウドを使ってもどうしても出てきます。\nそこで登場するのが今回紹介する StackStorm のようなツールかなぁと考えるようになりました。\nインストール インストール手順は下記の URL にあります。\nhttps://docs.stackstorm.com/install/index.html\n自分は CentOS7 を使ったので下記のワンライナーでインストールできました。 password は任意のものを入れてください。\ncurl -sSL https://stackstorm.com/packages/install.sh | bash -s -- --user=st2admin --password=foo MongoDB, postgreSQL が依存してインストールされます。\n80番ポートで下記のような WEB UI も起動します。\nStackStorm の基本 基本を知るために幾つかの要素について説明していきます。\nまず CLI で操作するために TOKEN を取得して環境変数にセットする必要があります。 上記で設定したユーザ名・パスワードを入力してください。\nexport ST2_AUTH_TOKEN=`st2 auth -t -p foo st2admin`  Action  Action はイベントが発生した際に実行できるアクションになります。早速アクションの一覧を取得してみましょう。\n$ st2 action list +---------------------------------+---------+-------------------------------------------------------------+ | ref | pack | description | +---------------------------------+---------+-------------------------------------------------------------+ | chatops.","title":"イベントドリブンな StackStorm で運用自動化"},{"content":"こんにちは。@jedipunkzです。\n今回は DC/OS (https://dcos.io/) を Vagrant を使って構築し評価してみようと思います。 DC/OS はその名の通りデータセンタ OS として利用されることを期待され開発された OS で内部で Docker と Mesos が稼働しています。\n一昔前に Mesos のマルチノード構成は構築したことあったのですが、DC/OS はデプロイ方法が全く変わっていました。 はじめに想定する構成から説明していきます。\n想定する構成 本来 DC/OS は public, private ネットワーク構成ですが利用するレポジトリではこのような構成が想定されていました。\n+----+ +----+ +----+ +------+ | m1 | | a1 | | p1 | | boot | +----+ +----+ +----+ +------+ | | | | +------+------+------+--------- 192.168.65/24 各ノードは下記の通り動作します。\n m1 : Mesos マスタ, Marathon a1 : Mesos スレーブ(Private Agent) p1 : Mesos スレーブ(Public Agent) boot : DC/OS インストレーションノード  前提の環境 Vagrant が動作するマシンであれば問題ないと思いますが私は下記の構成で利用しました。 比較的たくさんのマシンリソースを使うのでメモリ 8GB はほしいところです。\n Mac OSX Vagrant Virtualbox  事前の準備 事前の手順を記します。\n 予め用意された dcos-vagrant を取得する  $ git clone https://github.com/dcos/dcos-vagrant  Mac OSX の hosts ファイルをダイナミック編集する Vagrant プラグインをインストール  $ vagrant plugin install vagrant-hostmanager 構築手順 それでは構築手順です。\n DC/OS が利用する config を環境変数に指定指定  $ export DCOS_CONFIG_PATH=etc/config-1.7.yaml  DC/OS をレポジトリのルートディレクトリに保存  DC/OS 1.7.0 (Early Access)(2016/06/21現在) をダウンロードしてレポジトリのルートディレクトリにインストール\n$ cd dcos-vagrant $ wget https://downloads.dcos.io/dcos/EarlyAccess/dcos_generate_config.sh  VagrantConfig を作成  \u0026lsquo;VagrantConfig.yaml.example\u0026rsquo; というファイルがレポジトリ内ルートディレクトリにあるのでこれを元に下記の通りファイルを生成。元のファイルのままだと比較的大きな CPU/Mem リソースが必要になるので環境に合わせてリソース量を指定。\nm1: ip: 192.168.65.90 cpus: 1 memory: 512 type: master a1: ip: 192.168.65.111 cpus: 1 memory: 1024 memory-reserved: 512 type: agent-private p1: ip: 192.168.65.60 cpus: 1 memory: 1024 memory-reserved: 512 type: agent-public aliases: - spring.acme.org - oinker.acme.org boot: ip: 192.168.65.50 cpus: 1 memory: 1024 type: boot  デプロイを実施  $ vagrant up m1 a1 p1 boot DC/OS の UI インストールが完了すると下記のアドレスで DC/OS の UI にアクセスできます。\nhttp://m1.dcos/\nMarathon の UI 下記のアドレスで Marathon の UI にアクセスできます Marathon は分散型の Linux Init 機構のようなものです。\nhttp://m1.dcos:8080/\nChronos の UI 下記のアドレスで Chronos の UI にアクセスできる Chronos は分散型のジョブスケジューラーであり cron のようなものです。\nhttp://a1.dcos:1370/\nMarathon で redis サーバを立ち上げる テストで redis サーバを立ち上げてみる。Mesos-Slave (今回の環境だと a1 ホスト) 上に Docker コンテナとして redis サーバが立ち上がることになる。\n Marathon の UI にて \u0026ldquo;Create Application\u0026rdquo; を選択 General タブの ID に任意の名前を入力 General タブの Command 欄に \u0026lsquo;redis-server\u0026rsquo; を入力 Docker Container タブの Image 欄に \u0026lsquo;redis\u0026rsquo; を入力 \u0026lsquo;Create Application\u0026rsquo; を選択  結果、下記の通りアプリケーションが生成される\n※ 20160625 下記追記\n構成 ここからは Mesosphere DC/OS の内部構成を理解していきます。主となる mesos-master, mesos-slave の構成は下記の通り。\n Mesos-Master Node 構成  +--------------+ | mesos-master | +--------------+ +-----------+ +----------+ +-------------+ +-----------+ +-----------+ | zookeeper | | mesos-dns | | marathon | | adminRouter | | minuteman | | exhibitor | +--------------+ +-----------+ +----------+ +-------------+ +-----------+ +-----------+ | mesos-master node | +-------------------------------------------------------------------------------------+                Mesos-Slave (Mesos-Agent) Node 構成                +-------------+ +---+---+---+---+ | m-executor | | c | c | c | c | +-------------+ +---+---+---+---+ | mesos-slave | | docker-daemon | +-------------------------------+ | mesos-slave node | +-------------------------------+ 各プロセスの役割 上記の図の各要素を参考資料を元にまとめました。\n mesos-master  masos-slave node からの情報を受け取り mesos-slave へプロセスを分配する 役割。mesos-master は zookeeper によってリーダー選出により冗長構成が保 たれている。\n mesos-dns  mesos フレームワーク内での DNS 解決を行うプロセス。各 mesos-master ノー ド上に稼働している。通常の DNS でいうコンテンツ DNS (Authoritative DNS)になっており mesos-master からクラスタ情報を受け取って DNS レコー ド登録、それを mesos-slave が DNS 参照する。mesos-slave が内部レコード に無い DNS 名を解決しに来た際にはインターネット上の root DNS へ問い合 わせ実施。\n marathon  コンテナオーケストレーションを司り mesos-slave へ支持を出しコンテナを 稼働する役割。各 mesos-master 上で稼働し zookeeper 越しに mesos-master のカレントリーダを探しだしリクエストを創出。他に下記の機能を持っている。 \u0026lsquo;HA\u0026rsquo;, \u0026lsquo;ロードバランス\u0026rsquo;, \u0026lsquo;アプリのヘルスチェック\u0026rsquo;, \u0026lsquo;Web UI\u0026rsquo;, \u0026lsquo;REST API\u0026rsquo;, \u0026lsquo;Metrics\u0026rsquo;。\n adminRouter  実態は nginx。各サービスの認証と Proxy の役割を担っている。\n minuteman  L4 レイヤのロードバランサ。各 mesos-master ノードで稼働。\n zookeeper  mesos-master プロセスを冗長構成させるためのソフトウェア。\n exhibitor  zookeeper のコンフィギュレーションを実施。\n mesos-slave  Task を実行する役割。内部で meosos-executor (上記 m-executor) を実行し ている。\n m-executor (mesos-executor)  mesos-slave ノード上でサービスのための TASK を稼働させる。\n起動シーケンス ここからは mesos-master, mesos-slave の起動シーケンスについて、まとめてきます。\nmesos-master\n exhibitor が起動し zookeeper のコンフィギュレーションを実施し zookeeper を起動 mesos-master が起動。自分自身をレジスト後、他の mesos-master ノードを探索 mesos-dns が起動 mesos-dns が leader.mesos にカレントリーダの mesos-master を登録 marathon が起動し zookeeper 越しに mesos-master を探索。 adminRouter が起動し各 UI (mesos, marathon, exhibitor) が閲覧可能に。  mesos-slave\n leader.mesos に ping を打って mesos-master のカレントリーダを見つけ出し mesos-slave 稼働。 mesos-master に対して自分自身を \u0026lsquo;agent\u0026rsquo; として登録。 mesos-master はその登録された IP アドレスを元に接続を試みる mesos-slave が TASK 実行可能な状態に  まとめと考察 一昔前の Mesos + Marathon + Chronos とはだいぶデプロイ方法が変わっていた。だが構成には大きな変化は見られない。 AWS のような public, private ネットワークが別れたプラットフォームでは mesos-slave (DC/OS 的には Agent とも呼ぶ)は public agent, private agent として別けて管理される模様。public agent は AWS の ELB 等で分散され各コンテナ上のアプリにクエリがインターネットからのリクエストに応える。private agent はプライベートネットワーク上に配置されて public agent からのリクエストにも応える。また、mesos-master 達は別途 admin なネットワークに配置するのが Mesosphare の推奨らしい。\nだがしかし public, private を別けずに DC/OS を構成することも可能なように思えた。下記のように p1 を削除して構成して物理・仮想ロードバランサでリクエストを private agent に送出することでも DC/OS は機能するので。\n$ vagrant up m1 a1 boot ちなみに a2, a3, \u0026hellip; と数を増やすことで private agent ノードを増やすことが可能。\nあとマニュアルインストール手順(公式)を実施してみて解ったが、pulic, private ネットワークを各ノードにアタッチして mesos-master, mesos-slave, またその他の各機能はプライベートネットワークを、外部からのリクエストに応えるためのパブリックネットワーク、といった構成も可能でした。\n参考 URL  手順は右記のものを利用。 https://github.com/dcos/dcos-vagrant/blob/master/docs/deploy.md https://docs.mesosphere.com/1.7/overview/architecture/ https://docs.mesosphere.com/1.7/overview/security/  ","permalink":"https://jedipunkz.github.io/post/mesos-dcos-vagrant/","summary":"こんにちは。@jedipunkzです。\n今回は DC/OS (https://dcos.io/) を Vagrant を使って構築し評価してみようと思います。 DC/OS はその名の通りデータセンタ OS として利用されることを期待され開発された OS で内部で Docker と Mesos が稼働しています。\n一昔前に Mesos のマルチノード構成は構築したことあったのですが、DC/OS はデプロイ方法が全く変わっていました。 はじめに想定する構成から説明していきます。\n想定する構成 本来 DC/OS は public, private ネットワーク構成ですが利用するレポジトリではこのような構成が想定されていました。\n+----+ +----+ +----+ +------+ | m1 | | a1 | | p1 | | boot | +----+ +----+ +----+ +------+ | | | | +------+------+------+--------- 192.168.65/24 各ノードは下記の通り動作します。\n m1 : Mesos マスタ, Marathon a1 : Mesos スレーブ(Private Agent) p1 : Mesos スレーブ(Public Agent) boot : DC/OS インストレーションノード  前提の環境 Vagrant が動作するマシンであれば問題ないと思いますが私は下記の構成で利用しました。 比較的たくさんのマシンリソースを使うのでメモリ 8GB はほしいところです。","title":"Vagrant で Mesosphere DC/OS を構築"},{"content":"こんにちは。@jedipunkzです。\nInfluxdb が Influxdata (https://influxdata.com/) として生まれ変わり公式の メトリクス送信エージェント Telegraf と可視化ツール Chronograf をリリースしたので 使ってみました。\n3つのツールの役割は下記のとおりです。\n Chronograf : 可視化ツール, Grafana 相当のソフトウェアです Telegraf : メトリクス情報を Influxdb に送信するエージェント Influxdb : メトリクス情報を格納する時系列データベース  以前に cAdvisor, influxdb, grafana を使って Docker コンテナのメトリクスを可視 化する記事を書きましたが telegraf を使うとサーバ情報と合わせて Docker コンテナ のメトリクスも influxdb に送信することが出来ます。個人的にはそのコンテナ情報の 扱いもサーバ情報と同様に扱ってくれる点に期待しつつ、評価してみました。\n今回の環境 今回は Ubuntu 15.04 vivid64 を使ってテストしています。\ninfluxdb をインストールして起動 最新リリース版の deb パッケージが用意されていたのでこれを使いました。\nwget http://influxdb.s3.amazonaws.com/influxdb_0.9.5.1_amd64.deb sudo dpkg -i influxdb_0.9.5.1_amd64.deb sudo service influxdb start telegraf のインストールと起動 こちらも deb パッケージで。\nwget http://get.influxdb.org/telegraf/telegraf_0.2.4_amd64.deb sudo dpkg -i telegraf_0.2.4_amd64.deb コンフィギュレーションですが今回は CPU, Disk, Net, Docker のメトリクス情報を送 信するようにしました。\n[agent] interval = \u0026quot;0.1s\u0026quot; [outputs] [outputs.influxdb] urls = [\u0026quot;http://localhost:8086\u0026quot;] database = \u0026quot;telegraf-test\u0026quot; user_agent = \u0026quot;telegraf\u0026quot; [plugins] [[plugins.cpu]] percpu = true totalcpu = false drop = [\u0026quot;cpu_time*\u0026quot;] [[plugins.disk]] [plugins.disk.tagpass] fstype = [ \u0026quot;ext4\u0026quot;, \u0026quot;xfs\u0026quot; ] #path = [ \u0026quot;/home*\u0026quot; ] [[plugins.disk]] pass = [ \u0026quot;disk_inodes*\u0026quot; ] [[plugins.docker]] [[plugins.net]] interfaces = [\u0026quot;eth0\u0026quot;] 他にも色々メトリクス情報を取得できそうです、下記のサイトを参考にしてみてください。 https://github.com/influxdb/telegraf/blob/0.3.0/CONFIGURATION.md\ntelegraf を起動します。Docker コンテナのメトリクスを取得するために root ユーザ で起動する必要があります。\nsudo telegraf -config telegraf.conf chronograf のインストールと起動 こちらも deb でインストールします。\nwget https://s3.amazonaws.com/get.influxdb.org/chronograf/chronograf_0.4.0_amd64.deb sudo dpkg -i chronograf_0.4.0_amd64.deb sudo /opt/chronograf/chronograf -sample-config \u0026gt; /opt/chronograf/config.toml sudo service chronograf start グラフの描画 この状態でブラウザでアクセスしてみましょう。\nhttp://\u0026lt;ホストのIPアドレス\u0026gt;:10000/\nアクセスすると簡単なガイドが走りますのでここでは設定方法は省きます。Grafana を使った場合と 同様に気をつけるポイントは下記のとおりです。\n \u0026lsquo;filter by\u0026rsquo; に描画したいリソース名を選択(CPU,Disk,Net,Dockerの各リソース) Database に telegraf.conf に記した \u0026lsquo;telegraf-test\u0026rsquo; を選択  すると下記のようなグラフやダッシュボードが作成されます。下記は CPU 使用率をグ ラフ化したものです。\nこちらは Docker 関連のグラフ。\n複数のグラフを1つのダッシュボードにまとめることもできるようです。\nまとめ +++\n個人的には Grafana の UI はとてもわかりずらかったので公式の可視化ツールが出てきて良かった と思っています。操作もとても理解しやすくなっています。Telegraf についても公式のメトリクス 情報送信エージェントということで安心感があります。また Grafana は別途 HTTP サー バが必要でしたが Chronograf は HTTP サーバも内包しているのでセットアップが簡単 でした。\nただ configuration guide がまだまだ説明不十分なので凝ったことをしようすとする とソースを読まなくてはいけないかもしれません。\nいずれにしてもサーバのメトリクス情報と共に cAdvisor 等のソフトウェアを用いなく てもサーバ上で稼働しているコンテナ周りの情報も取得できたので個人的にはハッピー。 cAdvisor でしか取得できない情報もありそうですが今後、導入を検討する上で評価し ていきたいと思います。\n","permalink":"https://jedipunkz.github.io/post/2015/12/28/chronograf-telegraf-influxdb/","summary":"こんにちは。@jedipunkzです。\nInfluxdb が Influxdata (https://influxdata.com/) として生まれ変わり公式の メトリクス送信エージェント Telegraf と可視化ツール Chronograf をリリースしたので 使ってみました。\n3つのツールの役割は下記のとおりです。\n Chronograf : 可視化ツール, Grafana 相当のソフトウェアです Telegraf : メトリクス情報を Influxdb に送信するエージェント Influxdb : メトリクス情報を格納する時系列データベース  以前に cAdvisor, influxdb, grafana を使って Docker コンテナのメトリクスを可視 化する記事を書きましたが telegraf を使うとサーバ情報と合わせて Docker コンテナ のメトリクスも influxdb に送信することが出来ます。個人的にはそのコンテナ情報の 扱いもサーバ情報と同様に扱ってくれる点に期待しつつ、評価してみました。\n今回の環境 今回は Ubuntu 15.04 vivid64 を使ってテストしています。\ninfluxdb をインストールして起動 最新リリース版の deb パッケージが用意されていたのでこれを使いました。\nwget http://influxdb.s3.amazonaws.com/influxdb_0.9.5.1_amd64.deb sudo dpkg -i influxdb_0.9.5.1_amd64.deb sudo service influxdb start telegraf のインストールと起動 こちらも deb パッケージで。","title":"Chronograf, Telegraf, Influxdbでサーバとコンテナ情報を可視化する"},{"content":"こんにちは。@jedipunkzです。\n今回は Weave というコンテナ間のネットワークを提供してくれる Docker のネットワークプラ グインを使ってみました。下記のような沢山の機能があるようです。\n Fast Data Path Docker Network Plugin Security Dynamic Netwrok Attachment Service Binding Fault Tolerance etc \u0026hellip;  この記事では上から幾つか抜粋して、Weave ってどのように動かせるのか？を解説します。 そこから Weave が一体ナニモノなのか理解できればなぁと思います。\nVagrant を使った構成 この記事では下記の構成を作って色々と試していきます。使う技術は\n Vagrant Docker Weave  です。\n+---------------------+ +---------------------+ +---------------------+ | docker container a1 | | docker container a2 | | docker container a3 | +---------------------+ +---------------------+ +---------------------+ | vagrant host 1 | | vagrant host 2 | | vagrant host 3 | +---------------------+-+---------------------+-+---------------------+ | Mac or Windows | +---------------------------------------------------------------------+ 特徴としては\n 作業端末(Mac or Windows or Linux)上で Vagrant を動作させる 各 Vagrant VM 同士はホスト OS のネットワークインターフェース上で疎通が取れる  です。\nVagrantfile の作成と host1,2,3 の起動 上記の3台の構成を下記の Vagrantfile で構築します。\nVagrant.configure(2) do |config| config.vm.box = \u0026quot;ubuntu/vivid64\u0026quot; config.vm.define \u0026quot;host1\u0026quot; do |server| server.vm.network \u0026quot;private_network\u0026quot;, ip: \u0026quot;192.168.33.11\u0026quot; end config.vm.define \u0026quot;host2\u0026quot; do |server| server.vm.network \u0026quot;private_network\u0026quot;, ip: \u0026quot;192.168.33.12\u0026quot; end config.vm.define \u0026quot;host3\u0026quot; do |server| server.vm.network \u0026quot;private_network\u0026quot;, ip: \u0026quot;192.168.33.13\u0026quot; end config.vm.provision :shell, inline: \u0026lt;\u0026lt;-SHELL apt-get update apt-get install -y libsqlite3-dev docker.io curl -L git.io/weave -o /usr/local/bin/weave chmod a+x /usr/local/bin/weave SHELL end vagrant コマンドを使って host1, host2, host3 を起動します。\n$ vagrant up $ vagrant ssh host1 # \u0026lt;--- host1 に SSH する場合 $ vagrant ssh host2 # \u0026lt;--- host2 に SSH する場合 $ vagrant ssh host3 # \u0026lt;--- host3 に SSH する場合 物理ノードまたがったコンテナ間で通信をする weave でまず物理ノードをまたがったコンテナ間で通信をさせてみましょう。ここでは 上図の host1, host2 を使います。通常、物理ノードまたがっていると各々のホストで 稼働する Docker コンテナは通信し合えませんが weave を使うと通信しあうことが出 来ます。\nまず weave が用いる Docker コンテナを稼働します。下記のように /16 でレンジを切って 更にそこからデフォルトのレンジを指定することが出来ます。\nhost1# weave launch --ipalloc-range 10.2.0.0/16 --ipalloc-default-subnet 10.2.1.0/24 host1# eval $(weave env) host2# weave launch --ipalloc-range 10.2.0.0/16 --ipalloc-default-subnet 10.2.1.0/24 192.168.33.11 host2# eval $(weave env) この状態で下記のようなコンテナが稼働します。\nhost1# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES c55e96b4bdf9 weaveworks/weaveexec:1.4.0 \u0026#34;/home/weave/weavepr 4 seconds ago Up 3 seconds weaveproxy 394382c9c5d9 weaveworks/weave:1.4.0 \u0026#34;/home/weave/weaver 5 seconds ago Up 4 seconds weave host1, host2 でそれぞれテスト用コンテナを稼働させます。名前を \u0026ndash;name オプションで付けるのを 忘れないようにしてください。\nhost1# docker run --name a1 -ti ubuntu host2# docker run --name a2 -ti ubuntu どちらか一方から ping をもう一方に打ってみましょう。下記では a2 -\u0026gt; a1 の流れで ping を実行しています。\nroot@a2:/# ping 10.2.1.1 -c 3 PING 10.2.1.1 (10.2.1.1) 56(84) bytes of data. 64 bytes from 10.2.1.1: icmp_seq=1 ttl=64 time=0.316 ms 64 bytes from 10.2.1.1: icmp_seq=2 ttl=64 time=0.501 ms 64 bytes from 10.2.1.1: icmp_seq=3 ttl=64 time=0.619 ms --- 10.2.1.1 ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 1999ms rtt min/avg/max/mdev = 0.316/0.478/0.619/0.127 ms また docker コンテナを起動する時に指定した \u0026ndash;name a1, \u0026ndash;name a2 の名前で ping を実行してみましょう。これも weave の機能の１つで dns lookup が行えます。\nroot@b2:/# ping a1 -c 3 PING b1.weave.local (10.2.1.1) 56(84) bytes of data. 64 bytes from a1.weave.local (10.2.1.1): icmp_seq=1 ttl=64 time=1.14 ms 64 bytes from a1.weave.local (10.2.1.1): icmp_seq=2 ttl=64 time=0.446 ms 64 bytes from a1.weave.local (10.2.1.1): icmp_seq=3 ttl=64 time=0.364 ms --- b1.weave.local ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 2001ms rtt min/avg/max/mdev = 0.364/0.653/1.149/0.352 ms 結果から、異なる物理ノード(今回は VM)上で動作させた Docker コンテナ同士が通信し合えた ことがわかります。またコンテナ名の DNS 的は名前解決も可能になりました。\nダイナミックにネットワークをアタッチ・デタッチする 次に weave のネットワークを動的(コンテナがオンラインのまま)にアタッチ・デタッ チすることが出来るので試してみます。\n最初に weave のネットワークに属さない a1-1 という名前のコンテナを作ります。 docker exec で IP アドレスを確認すると eth0, lo のインターフェースしか持っていない ことが判ります。\nhost1# C=$(docker run --name a1-1 -e WEAVE_CIDR=none -dti ubuntu) host1# docker exec -it a1-1 ip a # \u0026lt;--- docker コンテナ内でコマンド実行 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN group default link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 25: eth0: \u0026lt;BROADCAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP group default link/ether 02:42:ac:11:00:05 brd ff:ff:ff:ff:ff:ff inet 172.17.0.5/16 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::42:acff:fe11:5/64 scope link valid_lft forever preferred_lft forever では weave のネットワークを a1-1 コンテナにアタッチしてみましょう。\nhost1# weave attach $C 10.2.1.1 host1# docker exec -it a1-1 ip a 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN group default link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 25: eth0: \u0026lt;BROADCAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP group default link/ether 02:42:ac:11:00:05 brd ff:ff:ff:ff:ff:ff inet 172.17.0.5/16 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::42:acff:fe11:5/64 scope link valid_lft forever preferred_lft forever 27: ethwe: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1410 qdisc pfifo_fast state UP group default qlen 1000 link/ether aa:15:06:51:6a:3b brd ff:ff:ff:ff:ff:ff inet 10.2.1.1/24 scope global ethwe valid_lft forever preferred_lft forever inet6 fe80::a815:6ff:fe51:6a3b/64 scope link valid_lft forever preferred_lft forever 上記のようにインターフェース ethwe が付与され最初に指定したデフォルトのサブネッ ト上の IP アドレスが付きました。\n次に weave ネットワークを複数アタッチしてみましょう。default, 10.2.2.0/24, 10.2.3.0/24 のネットワーク(サブネット)をアタッチします。\nhost1# weave attach net:default net:10.2.2.0/24 net:10.2.3.0/24 $C 10.2.1.1 10.2.2.1 10.2.3.1 root@vagrant-ubuntu-vivid-64:~# docker exec -it b3 ip a 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN group default link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 25: eth0: \u0026lt;BROADCAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP group default link/ether 02:42:ac:11:00:05 brd ff:ff:ff:ff:ff:ff inet 172.17.0.5/16 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::42:acff:fe11:5/64 scope link valid_lft forever preferred_lft forever 33: ethwe: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1410 qdisc pfifo_fast state UP group default qlen 1000 link/ether 9a:74:73:1b:24:a9 brd ff:ff:ff:ff:ff:ff inet 10.2.1.1/24 scope global ethwe valid_lft forever preferred_lft forever inet 10.2.2.1/24 scope global ethwe valid_lft forever preferred_lft forever inet 10.2.3.1/24 scope global ethwe valid_lft forever preferred_lft forever inet6 fe80::9874:73ff:fe1b:24a9/64 scope link valid_lft forever preferred_lft forever 結果、ethwe インターフェースに3つの IP アドレスが付与されました。 この様にダイナミックにコンテナに対して weave ネットワークをアタッチすることが出来ます。\nコンテナ外部から情報を取得する 下記のようにコンテナを起動しているホスト上 (Vagrant VM) からコンテナの情報を取 得する事もできます。シンプルですがオーケストレーション・自動化を行う上で重要な機能に なりそうな予感がします。\nhost1# weave expose 10.2.1.1 host1# weave dns-lookup a2 10.2.1.128 ダイナミックに物理ノードを追加し weave ネットワークへ 物理ノード(今回の場合 vagrant vm)を追加し上記で作成した weave ネットワークへ参 加させることも可能です。なお、今回は上記の vagrant up の時点で追加分の vm (host3) を既に稼働させています。\nhost1 で新しい物理ノードを接続します。\nhost1# weave connect 192.168.33.12 host1# weave status targets 192.168.33.13 192.168.33.12 host3 で weave コンテナ・テストコンテナを起動します。 下記で指定している 192.168.33.11 は host1 の IP アドレスです。\nhost3# weave launch --ipalloc-range 10.2.0.0/16 --ipalloc-default-subnet 10.2.1.0/24 192.168.33.11 host3# docker run --name a3 -ti ubuntu host2 の a2 コンテナに ping を打ってみます。\nroota3:/# ping a2 -c 3 PING a2.weave.local (10.2.1.128) 56(84) bytes of data. 64 bytes from a2.weave.local (10.2.1.128): icmp_seq=1 ttl=64 time=0.366 ms 64 bytes from a2.weave.local (10.2.1.128): icmp_seq=2 ttl=64 time=0.709 ms 64 bytes from a2.weave.local (10.2.1.128): icmp_seq=3 ttl=64 time=0.569 ms host3 上の a3 コンテナが既存の weave ネットワークに参加し通信出来たことが確認 できました。\nまとめと考察 コンフィギュレーションらしきモノを記述することなく Docker コンテナ間の通信 が出来ました。これは自動化する際に優位になるでしょう。また今回紹介したのは \u0026lsquo;weave net\u0026rsquo; と呼ばれるモノですが他にも \u0026lsquo;weave scope\u0026rsquo;, \u0026lsquo;weave run\u0026rsquo; といったモノ があります。\nhttp://weave.works/product/\nまた Docker Swarm, Compose と組み合わせる構成も組めるようです。試してみたい方 がいましたら是非。\nhttp://weave.works/guides/weave-and-docker-platform/compose-scalable-swarm-cluster-with-weave.html\nですが結果、まだ weave をどう自分たちのサービスに組み込めるかは検討が付いてい ません。\u0026lsquo;出来る\u0026rsquo; と \u0026lsquo;運用できる\u0026rsquo; が別物であることと、コンテナまわりのネットワー ク機能全般に理解して選定する必要がありそうです。\n参考サイト +++\nhttp://weave.works/docs/\n","permalink":"https://jedipunkz.github.io/post/2015/12/22/weave-docker-network/","summary":"こんにちは。@jedipunkzです。\n今回は Weave というコンテナ間のネットワークを提供してくれる Docker のネットワークプラ グインを使ってみました。下記のような沢山の機能があるようです。\n Fast Data Path Docker Network Plugin Security Dynamic Netwrok Attachment Service Binding Fault Tolerance etc \u0026hellip;  この記事では上から幾つか抜粋して、Weave ってどのように動かせるのか？を解説します。 そこから Weave が一体ナニモノなのか理解できればなぁと思います。\nVagrant を使った構成 この記事では下記の構成を作って色々と試していきます。使う技術は\n Vagrant Docker Weave  です。\n+---------------------+ +---------------------+ +---------------------+ | docker container a1 | | docker container a2 | | docker container a3 | +---------------------+ +---------------------+ +---------------------+ | vagrant host 1 | | vagrant host 2 | | vagrant host 3 | +---------------------+-+---------------------+-+---------------------+ | Mac or Windows | +---------------------------------------------------------------------+ 特徴としては","title":"Weave を使った Docker ネットワーク"},{"content":"こんにちは。@jedipunkzです。\n最近、業務で CircleCI を扱っていて、だいぶ \u0026ldquo;出来ること・出来ないこと\u0026rdquo; や \u0026ldquo;出来ないこと に対する回避方法\u0026rdquo; 等のノウハウが若干溜まってきたので共有したいなと思います。\nこの記事の前提\u0026hellip; ここでは CodeDeploy の設定方法や、CircleCIの設定方法等に関しては記述しませ ん。あくまで、Tips 的な内容にしています。また運用する上で想定できる問題点と、 それの回避方法等\u0026hellip;についてまとめています。\nCirlceCI と併用するサービスについて CircleCI は Github と連携してレポジトリ内の制御ファイル circle.yml に従ってテ スト・ビルド・デプロイを実施してくれる CI サービスです。ただ CircleCI は自分た ちの管理しているシステム外にあるので、AWS VPC を用いていると VPC 内のプライベー トインスタンスにデプロイするのが難しいです。プロキシ挟んで・・ってことは出来そ うですがプロキシの運用もしたくない、AWS のインフラリソースに任せることが出来た らインスタンス・インスタンス上のミドルウェアを運用しなくて済むので運用コストが 省けそう。ってことで\n AWS S3 (https://aws.amazon.com/jp/s3/) AWS CodeDeploy (https://aws.amazon.com/jp/codedeploy/)  を併用することを考えました。\nS3 は皆さんご存知のオブジェクトストレージです。CircleCI 用のバケットを作って、 ビルドした結果を格納します。私の務めている会社ではプログラミング言語として Scala を用いているので SBT というツールを使ってビルドします。その結果もバージョ ニングしつつ S3 バケットに格納できれば、万が一問題が発生した時にバイナリ毎切り 戻すことが出来そうです。\nまた CodeDeploy は EC2 インスタンス・またオンプレのインスタンスへコードのデプ ロイが可能になるサービスです。東京リージョンでは 2015/08 から利用が可能になり ました。これの便利なところは CircleCI 等の CI サービスから簡単に叩けて、VPC 内 のインスタンスに対してもデプロイが可能なところです。\nTips 的な情報として +++\ncircle.yml という CircleCI の制御ファイルがあります。Git レポジトリ内に格納することで CircleCI の動作を制御することが出来ます。この記事では circle.yml の紹介をメインとしたい と思います。\nGit push からデプロイまでを自動で行う circle.yml Github への push, merge をトリガーとしてデプロイまでの流れを自動で行う流れを組む場合の circle.yml を紹介します。全体の流れとしては\u0026hellip;\n レポジトリに git push, merge ことがトリガで処理が走る circle.yml を元にテスト・ビルド(場合によってはテストのみ) が走る S3 バケットにビルドされた結果が格納される CodeDeploy が実行され S3 バケット内のビルドされた成果物を対象のインスタンスにデプロイする  となります。\nmachine: environment: SBT_VERSION: 0.13.9 SBT_OPTS: \u0026#34;-Xms512M -Xmx1536M -Xss1M -XX:+CMSClassUnloadingEnabled -XX:MaxPermSize=256M\u0026#34; services: - docker dependencies: pre: - (事前に実行したいコマンド・スクリプトを記述) cache_directories: - \u0026#34;~/.sbt\u0026#34; test: override: - sbt compile deployment: production: branch: master codedeploy: codedeploy-sample: application_root: / region: ap-northeast-1 revision_location: revision_type: S3 s3_location: bucket: circleci-sample-bucket key_pattern: filename-{CIRCLE_BRANCH}-{CIRCLE_BUILD_NUM}.zip deployment_group: codedeploy-sample-group それぞれのパラメータの意味 上記 circle.yml の重要なパラメータのみ説明していきます。 私が務めている会社は Scala を使っていると冒頭に説明しましたがテスト・ビルドに SBT を使うのでこのような記述になっています。Ruby や Python でも同様に記述でき ると思いますので読み替えてください。\n machine -\u0026gt; environment : 全体で適用できる環境変数を定義します dependencies -\u0026gt; pre : 事前に実行したいコマンド等を定義できます test -\u0026gt; overide : テストを実行するコマンドを書きます。 deployment -\u0026gt; production -\u0026gt; branch : 適用するブランチ名と本番環境であることを記述します。 \u0026lsquo;codedeploy-sample\u0026rsquo; : CodeDeploy 上にサンプルで作成した \u0026lsquo;Application\u0026rsquo; 名です s3_location -\u0026gt; bucket : ビルドした成果物を S3 へ格納する際のバケット名を記します s3_location -\u0026gt; key_pattern : S3 バケットに収めるファイル名指定です deployment_group : CodeDeploy で定義する \u0026lsquo;Deployment-Group\u0026rsquo; 名です  より詳細な説明を読みたい場合は下記の URL に描いてあります。\nhttps://circleci.com/docs/configuration\nS3 のみににデプロイする例 上記の circle.yml ではビルドとデプロイを一気に処理するのですが、テスト・ビルドとデプロイを別けて 実行したい場面もありそうです。流れとしては\u0026hellip;\n レポジトリに git push, merge ことがトリガで処理が走る circle.yml を元にテスト・ビルド(場合によってはテストのみ) が走る S3 バケットにビルドされた結果が格納される  です。S3 のバケットに格納されたアプリを CodeDeploy を使ってデプロイするのは CodeDeploy の API を直接叩けば出来そうです。\nhttp://docs.aws.amazon.com/codedeploy/latest/APIReference/API_CreateDeployment.html\nこのリファレンスにある \u0026ldquo;CreateDeployment\u0026rdquo; については後に例をあげます。\nただ、同様のサービスとして TravisCI 等は S3 にのみデプロイを実施する仕組みが用意されているのですが CircleCI にはこの機能はありませんでした。サポートに問い合わせもしたのですが、あまり良い回答ではありませんでした。\nよって、下記のように awscli をテストコンテナ起動の度にインストールして S3 にアクセスすれば 上記の流れが組めそうです。\nmachine: environment: SBT_VERSION: 0.13.9 SBT_OPTS: \u0026#34;-Xms512M -Xmx1536M -Xss1M -XX:+CMSClassUnloadingEnabled -XX:MaxPermSize=256M\u0026#34; services: - docker dependencies: pre: - sudo pip install awscli cache_directories: - \u0026#34;~/.sbt\u0026#34; test: override: - sbt compile deployment: master: branch: master commands: - zip -r sample-code-${CIRCLE_BRANCH}-${CIRCLE_BUILD_NUM}.zip . - aws s3 cp sample-code-${CIRCLE_PROJECT_REPONAME}-${CIRCLE_BRANCH}-${CIRCLE_BUILD_NUM}.zip s3://\u0026lt;バケット名\u0026gt;/\u0026lt;ディレクトリ\u0026gt;/ --region ap-northeast-1 事前に awscli をインストールしているだけです。\nS3 バケットに格納された成果物を CodeDeploy を使って手動でデプロイするには下記 のコマンドで実施できます。\n$ aws deploy create-deployment \\  --application-name codedeploy-sample \\  --deployment-config-name CodeDeployDefault.OneAtATime \\  --deployment-group-name codedeploy-sample-group \\  --description \u0026#34;deploy test\u0026#34; \\  --s3-location bucket=\u0026lt;バケット名\u0026gt;,bundleType=zip,key=\u0026lt;ファイル名\u0026gt; { \u0026#34;deploymentId\u0026#34;: \u0026#34;d-2B4OAMT0B\u0026#34; } deploymentId は CodeDeploy 上の Application に紐付いた ID です。CodeDeploy の API を叩くか AWS コンソールで確認可能です。\nCircleCI の問題点とそれの回避方法  production と staging  1つのブランチで管理できる circle.yml は1つです。このファイルの中で定義できる \u0026lsquo;本番用\u0026rsquo;, \u0026lsquo;開発用\u0026rsquo; の定義は deployment -\u0026gt; production, staging の2種類になります。この2つで管理しきれない環境がある場合(例えば staging 以前の development 環境がある) は、レポジトリのブランチを別けて circle.yml を管理する方法があると思います。\n 複数のデプロイ先があるレポジトリの運用  同一のレポジトリ内で管理しているコードのデプロイ先が複数ある場合は CodeDeploy 上で1つの Application に対して複数の Deployment-Group を作成することで対応できます。ただ、cirlce.yml で定義できるデプロイ先は deployment_group: の1つ( 厳密に言うと production, staging の2つ) になるので、こちらもブランチによる circle.yml の別管理で回避できそうです。\nこちらの問題については CircleCI 的にはおそらく「1つのレポジトリで管理するデプロイ先は1つに」というコンセプトなのかもしれません。\nAWS IAM ユーザにアタッチする Policy 作成 IAM ユーザを CircleCI に事前に設定しておくことで直接 AWS のリソースを操作出来るのですが、 そのユーザにアタッチしておくべき Policy について例をあげておきます。\n特定の S3 バケットにオブジェクト Put する Policy\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;Stmt1444196633000\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:PutObject\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::\u0026lt;S3 バケット名\u0026gt;/*\u0026#34; ] } ] } CodeDeploy の各 Action を実行する Policy\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;codedeploy:RegisterApplicationRevision\u0026#34;, \u0026#34;codedeploy:GetApplicationRevision\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;*\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;codedeploy:CreateDeployment\u0026#34;, \u0026#34;codedeploy:GetDeployment\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;*\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;codedeploy:GetDeploymentConfig\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;*\u0026#34; ] } ] } まとめ CodeDeploy, S3 を併用することで CircleCI を使っても VPC 内のプライベートインス タンスにデプロイできることが判りました。もし EC2 インスタンスを使っている場合 は他の方法も取れることが判っています。circle.yml 内の pre: で指定出来るコマン ド・スクリプトで EC2 インスタンスに紐付いているセキュリティグループに穴あけ処 理を記述すれば良さそうです。デプロイが終わったら穴を塞げばいいですね。この辺の 例については国内でもブログ記事にされている方がいらっしゃいますので参考にしてくだ さい。\n","permalink":"https://jedipunkz.github.io/post/2015/11/15/circleci-codedeploy/","summary":"こんにちは。@jedipunkzです。\n最近、業務で CircleCI を扱っていて、だいぶ \u0026ldquo;出来ること・出来ないこと\u0026rdquo; や \u0026ldquo;出来ないこと に対する回避方法\u0026rdquo; 等のノウハウが若干溜まってきたので共有したいなと思います。\nこの記事の前提\u0026hellip; ここでは CodeDeploy の設定方法や、CircleCIの設定方法等に関しては記述しませ ん。あくまで、Tips 的な内容にしています。また運用する上で想定できる問題点と、 それの回避方法等\u0026hellip;についてまとめています。\nCirlceCI と併用するサービスについて CircleCI は Github と連携してレポジトリ内の制御ファイル circle.yml に従ってテ スト・ビルド・デプロイを実施してくれる CI サービスです。ただ CircleCI は自分た ちの管理しているシステム外にあるので、AWS VPC を用いていると VPC 内のプライベー トインスタンスにデプロイするのが難しいです。プロキシ挟んで・・ってことは出来そ うですがプロキシの運用もしたくない、AWS のインフラリソースに任せることが出来た らインスタンス・インスタンス上のミドルウェアを運用しなくて済むので運用コストが 省けそう。ってことで\n AWS S3 (https://aws.amazon.com/jp/s3/) AWS CodeDeploy (https://aws.amazon.com/jp/codedeploy/)  を併用することを考えました。\nS3 は皆さんご存知のオブジェクトストレージです。CircleCI 用のバケットを作って、 ビルドした結果を格納します。私の務めている会社ではプログラミング言語として Scala を用いているので SBT というツールを使ってビルドします。その結果もバージョ ニングしつつ S3 バケットに格納できれば、万が一問題が発生した時にバイナリ毎切り 戻すことが出来そうです。\nまた CodeDeploy は EC2 インスタンス・またオンプレのインスタンスへコードのデプ ロイが可能になるサービスです。東京リージョンでは 2015/08 から利用が可能になり ました。これの便利なところは CircleCI 等の CI サービスから簡単に叩けて、VPC 内 のインスタンスに対してもデプロイが可能なところです。","title":"CodeDeploy, S3 を併用して CircleCI により VPC にデプロイ"},{"content":"こんにちは。@jedipunkzです。\n今回は Docker ネタです。Docker 導入するにしても監視はどうする？という話になる と思うのですが、各 Monitoring as a Service を使うにしてもエージェント入れない といけないしお金掛かることもあるし..で、調べていたら cAdvisor というキーワード が出てきました。今回は cAdvisor を使ってコンテナの監視が出来ないか、について書 いていきたいと想います。\n cAdvisor とは ?  cAdvisor は Kubernates で用いられているコンポーネントで単体でも利用可能とのこ と。Google が開発しています。また Docker コンテナの監視においてこの cAdvisor は一般化しつつあるようです。\nhttps://github.com/google/cadvisor\n 収集したメトリクスの保存  cAdvisor 自体も Docker で起動して、同ホスト上に起動している Docker コンテナの リソースをモニタリングしてくれます。そのメトリクスデータは幾つかの DB に保存出 来るのですが、そのうちの一つが influxDB です。influxDB は時系列データベースで す。システムのメトリクスデータを収めるのにちょうどいいデータベースになります。\nhttps://influxdb.com/\n DB に収めたメトリクスの可視化  influxDB に収めたメトリクスデータを可視化するのが Grafana です。Grafana のデー タソースは influxDB の他にも幾つかあり Elasticsearch, KairosDB, Graphite 等が それです。\nhttp://grafana.org/\nでは早速試してみましょう。\n前提の環境 今回は Vagrant を使います。また Vagrant 上で上記の3つのソフトウェアを Docker で稼働します。またどうせなので docker-compose を使って3つのコンテナを一斉に立 ち上げてみましょう。\nVagrantFile の準備 下記のような VagrantFile を作成します。各ソフトウェアはそれぞれ WebUI を持って いて、そこに手元のコンピュータから接続するため forwarded_port しています。\n# -*- mode: ruby -*- # vi: set ft=ruby : Vagrant.configure(2) do |config| config.vm.box = \u0026quot;ubuntu/trusty64\u0026quot; config.vm.network \u0026quot;forwarded_port\u0026quot;, guest: 8080, host: 8080 config.vm.network \u0026quot;forwarded_port\u0026quot;, guest: 8083, host: 8083 config.vm.network \u0026quot;forwarded_port\u0026quot;, guest: 3000, host: 3000 config.vm.network \u0026quot;private_network\u0026quot;, ip: \u0026quot;192.168.33.10\u0026quot; end Docker コンテナの起動と docker-compose.yml の準備 Vagrant を起動し docker, docker-compose のインストールを行います。\n$ vagrant up $ vagrant ssh vagrant$ sudo apt-get update ; sudo apt-get -y install curl vagrant$ curl -sSL https://get.docker.com/ | sh vagrant$ sudo -i vagrant# export VERSION_NUM=1.4.0 vagrant# curl -L https://github.com/docker/compose/releases/download/VERSION_NUM/docker-compose-`uname -s`-`uname -m` \u0026gt; /usr/local/bin/docker-compose vagrant# chmod +x /usr/local/bin/docker-compose 次に docker-compose.yml を作成します。上記3つのソフトウェアが稼働するコンテナ を起動するため下記のように記述しましょう。カレントディレクトリに作成します。\nInfluxSrv: image: \u0026quot;tutum/influxdb:0.8.8\u0026quot; ports: - \u0026quot;8083:8083\u0026quot; - \u0026quot;8086:8086\u0026quot; expose: - \u0026quot;8090\u0026quot; - \u0026quot;8099\u0026quot; environment: - PRE_CREATE_DB=cadvisor cadvisor: image: \u0026quot;google/cadvisor:0.16.0\u0026quot; volumes: - \u0026quot;/:/rootfs:ro\u0026quot; - \u0026quot;/var/run:/var/run:rw\u0026quot; - \u0026quot;/sys:/sys:ro\u0026quot; - \u0026quot;/var/lib/docker/:/var/lib/docker:ro\u0026quot; links: - \u0026quot;InfluxSrv:influxsrv\u0026quot; ports: - \u0026quot;8080:8080\u0026quot; command: \u0026quot;-storage_driver=influxdb -storage_driver_db=cadvisor -storage_driver_host=influxsrv:8086 -storage_driver_user=root -storage_driver_password=root -storage_driver_secure=False\u0026quot; grafana: image: \u0026quot;grafana/grafana:2.1.3\u0026quot; ports: - \u0026quot;3000:3000\u0026quot; environment: - INFLUXDB_HOST=localhost - INFLUXDB_PORT=8086 - INFLUXDB_NAME=cadvisor - INFLUXDB_USER=root - INFLUXDB_PASS=root links: - \u0026quot;InfluxSrv:influxsrv\u0026quot; コンテナの起動 +++\ndocker コンテナを立ち上げます。\nvagrant$ docker-compose -d influxDB の WebUI に接続する それでは起動したコンテナのうち一つ influxDB の WebUI に接続していましょう。 上記の VagrantFile では IP アドレスを 192.168.33.10 と指定しました。\nURL : http://192.168.33.10:8083\nデータベースに接続します。\nユーザ名 : root パスワード : root\n接続するとデータベース作成画面に飛びますので Database Datails 枠に \u0026ldquo;cadvisor\u0026rdquo; と入力、その他の項目はデフォルトのままで \u0026ldquo;Create Database\u0026rdquo; をクリックします。\ncAdvisor の WebUI に接続する 続いて cAdvisor の WebUI に接続してみましょう。\nURL : http://192.168.33.10:8080\nここでは特に作業の必要はありません。コンテナの監視が行われグラフが描画されてい ることを確認します。\nGrafana の WebUI に接続する 最後に Grafana の WebUI です。\nURL : http://192.168.33.10:3000 ユーザ名 : admin パスワード : admin\nまずデータソースの設定を行います。左上のアイコンをクリックし \u0026ldquo;Data Sources\u0026rdquo; を 選択します。次に \u0026ldquo;Add New Data Source\u0026rdquo; ボタンをクリックします。\n下記の情報を入力しましょう。\n Name : influxdb Type : influxDB 0.8.x Url : http://influxsrv:8086 Access : proxy Basic Auth User admin Basic Auth Password admin Database : cadvisor User : root Password : root  さて最後にグラフを作成していきます。左メニューの \u0026ldquo;Dashboard\u0026rdquo; を選択し上部の \u0026ldquo;Home\u0026rdquo; ボランを押し \u0026ldquo;+New\u0026rdquo; を押します。\n下記の画面を参考にし値に入力していきます。\nMetrics を選択しネットワークの受信転送量をグラフにしています。\n series : \u0026lsquo;stats\u0026rsquo; alias : RX Bytes select mean(rx_bytes)  同じく送信転送量もグラフにします。Add Query を押すと追加できます。\n series : \u0026lsquo;stats\u0026rsquo; alias : TX Bytes select mean(tx_bytes)  時間が経過すると下記のようにグラフが描画されます。\nまとめと考察 3つのソフトウェア共に開発が活発であり、cAdvisor は特に Docker コンテナの監視と して一般化しつつあるよう。Kubernates の一部ということもありそう簡単には廃れな いと想います。コンテナの中にエージェント等を入れることもなく、これで Docker コ ンテナのリソース監視が出来そう。ただサービス監視は別途考えなくてはいけないなぁ という印象です。また、今回 docker-compose に記した各コンテナのバージョンは Docker Hub を確認すると別バージョンもあるので時期が経ってこのブログ記事をご覧 になった方は修正すると良いと想います。ただこの記事を書いている時点では influxDB の 0.9.x 系では動作しませんでした。よって latest ではなくバージョン指 定で記してあります。\n参考にしたサイト  http://qiita.com/atskimura/items/4c4aaaaa554e2814e938 https://www.brianchristner.io/how-to-setup-docker-monitoring/  ","permalink":"https://jedipunkz.github.io/post/2015/09/12/cadvisor-influxdb-grafana-docker/","summary":"こんにちは。@jedipunkzです。\n今回は Docker ネタです。Docker 導入するにしても監視はどうする？という話になる と思うのですが、各 Monitoring as a Service を使うにしてもエージェント入れない といけないしお金掛かることもあるし..で、調べていたら cAdvisor というキーワード が出てきました。今回は cAdvisor を使ってコンテナの監視が出来ないか、について書 いていきたいと想います。\n cAdvisor とは ?  cAdvisor は Kubernates で用いられているコンポーネントで単体でも利用可能とのこ と。Google が開発しています。また Docker コンテナの監視においてこの cAdvisor は一般化しつつあるようです。\nhttps://github.com/google/cadvisor\n 収集したメトリクスの保存  cAdvisor 自体も Docker で起動して、同ホスト上に起動している Docker コンテナの リソースをモニタリングしてくれます。そのメトリクスデータは幾つかの DB に保存出 来るのですが、そのうちの一つが influxDB です。influxDB は時系列データベースで す。システムのメトリクスデータを収めるのにちょうどいいデータベースになります。\nhttps://influxdb.com/\n DB に収めたメトリクスの可視化  influxDB に収めたメトリクスデータを可視化するのが Grafana です。Grafana のデー タソースは influxDB の他にも幾つかあり Elasticsearch, KairosDB, Graphite 等が それです。\nhttp://grafana.org/","title":"cAdvisor/influxDB/GrafanaでDockerリソース監視"},{"content":"こんにちは。@jedipunkz です。\n前回 OpenStack Kilo のオールインワン構成を Chef-Zero を使ってデプロイする方法 を書きましたが、複数台構成についても調べたので結果をまとめていきます。\n使うのは openstack/openstack-chef-repo です。下記の URL にあります。\nhttps://github.com/openstack/openstack-chef-repo\nこの中に Vagrant を使ったファイルが存在しますが、実機でのデプロイには全く役に 立ちません。自分で Environment ファイルを作成する必要があります。今回は前提の 構成を作って、それに合わせた Environment ファイルを記します。ほぼスタンダード な構成にしましたので、自分の環境に合わせて修正するのも比較的簡単だと想います。 参考にしてください。\n今回は knife-zero を使ってデプロイします。Chef サーバが必要なく、knife-zero を 使うホスト上のオンメモリで Chef サーバが稼働するので準備がほとんど必要ありません。\n早速ですが、構成と準備・そしてデプロイ作業を記していきます。\n前提の構成  +------------+ | GW Router | +--+------------+ | | | +--------------+--------------+---------------------------- public network | | eth0 | eth0 | +------------+ +------------+ +------------+ +------------+ | | Controller | | Network | | Compute | | Knife-Zero | | +------------+ +-------+----+ +------+-----+ +------------+ | | eth1 | eth1 | | eth1 | | eth1 +--+--------------+-------)------+------)-------+------------- api/management network | eth2 | eth2 +-------------+--------------------- guest network  特徴としては\u0026hellip;\n public, api/management, guest の3つのネットワークに接続された OpenStack ホスト Controller, Network, Compute の最小複数台構成 knife-zero を実行する \u0026lsquo;Knife-Zero\u0026rsquo; ホスト Knife-zero ホストは api/management network のみに接続で可 デプロイは api/management network を介して行う public, api/management network はインターネットへの疎通が必須 OS は Ubuntu 14.04 amd64  とくに api/management network がインターネットへの疎通が必要なところに注意して ください。デプロイは knife-zero ホストで実行しますが、各ノードへログインしデプ ロイする際にインターネット上からパッケージの取得を試みます。\nまた api/management network を2つに分離するのも一般的ですが、ここでは一本にま とめています。\nIP アドレス IP アドレスは下記を前提にします。\n   interface IP addr     Controller eth0 10.0.1.10   Controller eth1 10.0.2.10   Network eth0 10.0.1.11   Network eth1 10.0.2.11   Network eth2 10.0.3.11   Compute eth1 10.0.2.12   Compute eth2 10.0.3.12   Knife-Zero eth1 10.0.2.13    ネットワークインターフェース設定 それぞれのホストで下記のようにネットワークインターフェースを設定します。\n Controller ホスト  eth0, 1 を使用します。\nauto eth0 iface eth0 inet static address 10.0.1.10 netmask 255.255.255.0 gateway 10.0.1.254 dns-nameservers 8.8.8.8 dns-search jedihub.com auto eth1 iface eth1 inet static address 10.0.2.10 netmask 255.255.255.0 auto eth2 iface eth2 inet manual  Network ホスト  eth0, 1, 2 全てを使用します。\nauto eth0 iface eth0 inet static up ifconfig $IFACE 0.0.0.0 up up ip link set $IFACE promisc on down ip link set $IFACE promisc off down ifconfig $IFACE down address 10.0.1.11 netmask 255.255.255.0 auto eth1 iface eth1 inet static address 10.0.2.11 netmask 255.255.255.0 gateway 10.0.2.248 dns-nameservers 8.8.8.8 dns-search jedihub.com auto eth2 iface eth2 inet static address 10.0.3.11 netmask 255.255.255.0  Compute ホスト  eth1, 2 を使用します。\nauto eth0 iface eth0 inet manual auto eth1 iface eth1 inet static address 10.0.2.12 netmask 255.255.255.0 gateway 10.0.2.248 dns-nameservers 8.8.8.8 dns-search jedihub.com auto eth2 iface eth2 inet static address 10.0.3.12 netmask 255.255.255.0 これらの作業は knife-zero からログインし eth1 を介して行ってください。でないと 接続が切断される可能性があります。\n準備 knife-zero ホストに chef, knife-zero, berkshelf が入っている必要があるので、こ こでインストールしていきます。\nknife-zero ホストに chef をインストールします。Omnibus パッケージを使って手っ 取り早く環境を整えます。\nsudo -i curl -L https://www.opscode.com/chef/install.sh | bash Berkshelf をインストールするのに必要なソフトウェアをインストールします。\napt-get -y install build-essential zlib1g-dev libssl-dev libreadline-dev ruby-dev libxml2-dev libxslt-dev g++ Berkshelf をインストールします。\n/opt/chef/embedded/bin/gem install berkshelf --no-ri --no-rdoc 最後に knife-zero をインストールします。\n/opt/chef/embedded/bin/gem install knife-zero --no-ri --no-rdoc デプロイ作業 それでは openstack-chef-repo を取得してデプロイの準備を行います。 ブランチの指定は行わず master ブランチを取得します。Kilo は master ブランチで 管理されています。次のバージョンの開発が始まるタイミングで \u0026lsquo;stable/kilo\u0026rsquo; ブラ ンチに管理が移されます。\nsudo -i cd ~/ git clone https://github.com/openstack/openstack-chef-repo.git 次に Berkshelf を使って必要な Cookbooks をダウンロードします。\ncd ~/openstack-chef-repo /opt/chef/embedded/bin/berks vendor ./cookbooks Environment を作成します。これは各環境に合わせた設定ファイルのようなもので、各 Cookbooks の Attributes を上書きする仕組みになっています。下記の内容を\nopenstack-chef-repo/environments/multi-neutron-kilo.json というファイル名で保存してください。\n{ \u0026#34;name\u0026#34;: \u0026#34;multi-neutron-kilo\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;test\u0026#34;, \u0026#34;cookbook_versions\u0026#34;: { }, \u0026#34;json_class\u0026#34;: \u0026#34;Chef::Environment\u0026#34;, \u0026#34;chef_type\u0026#34;: \u0026#34;environment\u0026#34;, \u0026#34;default_attributes\u0026#34;: { }, \u0026#34;override_attributes\u0026#34;: { \u0026#34;mysql\u0026#34;: { \u0026#34;bind_address\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;server_root_password\u0026#34;: \u0026#34;mysqlroot\u0026#34;, \u0026#34;server_debian_password\u0026#34;: \u0026#34;mysqlroot\u0026#34;, \u0026#34;server_repl_password\u0026#34;: \u0026#34;mysqlroot\u0026#34;, \u0026#34;allow_remote_root\u0026#34;: true, \u0026#34;root_network_acl\u0026#34;: [\u0026#34;10.0.0.0/8\u0026#34;] }, \u0026#34;rabbitmq\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;5672\u0026#34;, \u0026#34;loopback_users\u0026#34;: [] }, \u0026#34;openstack\u0026#34;: { \u0026#34;auth\u0026#34;: { \u0026#34;validate_certs\u0026#34;: false }, \u0026#34;dashboard\u0026#34;: { \u0026#34;session_backend\u0026#34;: \u0026#34;file\u0026#34; }, \u0026#34;block-storage\u0026#34;: { \u0026#34;syslog\u0026#34;: { \u0026#34;use\u0026#34;: false }, \u0026#34;api\u0026#34;: { \u0026#34;ratelimit\u0026#34;: \u0026#34;False\u0026#34; }, \u0026#34;debug\u0026#34;: true, \u0026#34;image_api_chef_role\u0026#34;: \u0026#34;os-image\u0026#34;, \u0026#34;identity_service_chef_role\u0026#34;: \u0026#34;os-identity\u0026#34;, \u0026#34;rabbit_server_chef_role\u0026#34;: \u0026#34;os-ops-messaging\u0026#34; }, \u0026#34;compute\u0026#34;: { \u0026#34;rabbit\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34; }, \u0026#34;novnc_proxy\u0026#34;: { \u0026#34;bind_interface\u0026#34;: \u0026#34;eth1\u0026#34; }, \u0026#34;libvirt\u0026#34;: { \u0026#34;virt_type\u0026#34;: \u0026#34;qemu\u0026#34;, \u0026#34;bind_interface\u0026#34;: \u0026#34;eth1\u0026#34; }, \u0026#34;novnc_proxy\u0026#34;: { \u0026#34;bind_interface\u0026#34;: \u0026#34;eth1\u0026#34; }, \u0026#34;xvpvnc_proxy\u0026#34;: { \u0026#34;bind_interface\u0026#34;: \u0026#34;eth1\u0026#34; }, \u0026#34;image_api_chef_role\u0026#34;: \u0026#34;os-image\u0026#34;, \u0026#34;identity_service_chef_role\u0026#34;: \u0026#34;os-identity\u0026#34;, \u0026#34;nova_setup_chef_role\u0026#34;: \u0026#34;os-compute-api\u0026#34;, \u0026#34;rabbit_server_chef_role\u0026#34;: \u0026#34;os-ops-messaging\u0026#34;, \u0026#34;network\u0026#34;: { \u0026#34;public_interface\u0026#34;: \u0026#34;eth1\u0026#34;, \u0026#34;service_type\u0026#34;: \u0026#34;neutron\u0026#34; } }, \u0026#34;network\u0026#34;: { \u0026#34;debug\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;dhcp\u0026#34;: { \u0026#34;enable_isolated_metadata\u0026#34;: \u0026#34;True\u0026#34; }, \u0026#34;metadata\u0026#34;: { \u0026#34;nova_metadata_ip\u0026#34;: \u0026#34;10.0.2.10\u0026#34; }, \u0026#34;openvswitch\u0026#34;: { \u0026#34;tunnel_id_ranges\u0026#34;: \u0026#34;1:1000\u0026#34;, \u0026#34;enable_tunneling\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;tenant_network_type\u0026#34;: \u0026#34;gre\u0026#34;, \u0026#34;tunnel_types\u0026#34;: \u0026#34;gre\u0026#34;, \u0026#34;tunnel_type\u0026#34;: \u0026#34;gre\u0026#34;, \u0026#34;bridge_mappings\u0026#34;: \u0026#34;physnet1:br-eth2\u0026#34;, \u0026#34;bridge_mapping_interface\u0026#34;: \u0026#34;br-eth2:eth2\u0026#34; }, \u0026#34;ml2\u0026#34;: { \u0026#34;tenant_network_types\u0026#34;: \u0026#34;gre\u0026#34;, \u0026#34;mechanism_drivers\u0026#34;: \u0026#34;openvswitch\u0026#34;, \u0026#34;tunnel_id_ranges\u0026#34;: \u0026#34;1:1000\u0026#34;, \u0026#34;enable_security_group\u0026#34;: \u0026#34;True\u0026#34; }, \u0026#34;api\u0026#34;: { \u0026#34;bind_interface\u0026#34;: \u0026#34;eth1\u0026#34; }, \u0026#34;l3\u0026#34;: { \u0026#34;external_network_bridge_interface\u0026#34;: \u0026#34;eth0\u0026#34; }, \u0026#34;service_plugins\u0026#34;: [\u0026#34;neutron.services.l3_router.l3_router_plugin.L3RouterPlugin\u0026#34;] }, \u0026#34;db\u0026#34;: { \u0026#34;bind_interface\u0026#34;: \u0026#34;eth1\u0026#34;, \u0026#34;compute\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34; }, \u0026#34;identity\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34; }, \u0026#34;image\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34; }, \u0026#34;network\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34; }, \u0026#34;volume\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34; }, \u0026#34;dashboard\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34; }, \u0026#34;telemetry\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34; }, \u0026#34;orchestration\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34; } }, \u0026#34;developer_mode\u0026#34;: true, \u0026#34;endpoints\u0026#34;: { \u0026#34;network-openvswitch\u0026#34;: { \u0026#34;bind_interface\u0026#34;: \u0026#34;eth1\u0026#34; }, \u0026#34;compute-api-bind\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8774\u0026#34; }, \u0026#34;compute-api\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8774\u0026#34; }, \u0026#34;compute-ec2-admin-bind\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8773\u0026#34; }, \u0026#34;compute-ec2-admin\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8773\u0026#34; }, \u0026#34;compute-ec2-api-bind\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8773\u0026#34; }, \u0026#34;compute-ec2-api\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8773\u0026#34; }, \u0026#34;compute-xvpvnc\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;6081\u0026#34; }, \u0026#34;compute-novnc-bind\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;6080\u0026#34; }, \u0026#34;compute-novnc\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;6080\u0026#34; }, \u0026#34;compute-vnc\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;6080\u0026#34; }, \u0026#34;image-api\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;9292\u0026#34; }, \u0026#34;image-api-bind\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;9292\u0026#34; }, \u0026#34;image-registry\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;9191\u0026#34; }, \u0026#34;image-registry-bind\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;9191\u0026#34; }, \u0026#34;identity-api\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;5000\u0026#34; }, \u0026#34;identity-bind\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;5000\u0026#34; }, \u0026#34;identity-admin\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;35357\u0026#34; }, \u0026#34;identity-internal\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;35357\u0026#34; }, \u0026#34;volume-api-bind\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8776\u0026#34; }, \u0026#34;volume-api\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8776\u0026#34; }, \u0026#34;telemetry-api\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8777\u0026#34; }, \u0026#34;network-api-bind\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.11\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;9696\u0026#34; }, \u0026#34;network-api\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.11, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;9696\u0026#34; }, \u0026#34;block-storage-api-bind\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8776\u0026#34;, \u0026#34;bind_interface\u0026#34;: \u0026#34;eth1\u0026#34; }, \u0026#34;block-storage-api\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8776\u0026#34; }, \u0026#34;orchestration-api\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8004\u0026#34; }, \u0026#34;orchestration-api-cfn\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8000\u0026#34; }, \u0026#34;db\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;3306\u0026#34; }, \u0026#34;bind-host\u0026#34;: \u0026#34;0.0.0.0\u0026#34; }, \u0026#34;identity\u0026#34;: { \u0026#34;admin_user\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;bind_interface\u0026#34;: \u0026#34;eth1\u0026#34;, \u0026#34;debug\u0026#34;: true }, \u0026#34;image\u0026#34;: { \u0026#34;api\u0026#34;: { \u0026#34;bind_interface\u0026#34;: \u0026#34;eth1\u0026#34; }, \u0026#34;debug\u0026#34;: true, \u0026#34;identity_service_chef_role\u0026#34;: \u0026#34;os-identity\u0026#34;, \u0026#34;rabbit_server_chef_role\u0026#34;: \u0026#34;os-ops-messaging\u0026#34;, \u0026#34;registry\u0026#34;: { \u0026#34;bind_interface\u0026#34;: \u0026#34;eth1\u0026#34; }, \u0026#34;syslog\u0026#34;: { \u0026#34;use\u0026#34;: false } }, \u0026#34;mq\u0026#34;: { \u0026#34;bind_interface\u0026#34;: \u0026#34;eth1\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;guest\u0026#34;, \u0026#34;vhost\u0026#34;: \u0026#34;/nova\u0026#34;, \u0026#34;network\u0026#34;: { \u0026#34;rabbit\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;5672\u0026#34; } }, \u0026#34;compute\u0026#34;: { \u0026#34;service_type\u0026#34;: \u0026#34;rabbitmq\u0026#34;, \u0026#34;rabbit\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;5672\u0026#34; } }, \u0026#34;block-storage\u0026#34;: { \u0026#34;service_type\u0026#34;: \u0026#34;rabbitmq\u0026#34;, \u0026#34;rabbit\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;5672\u0026#34; } } } }, \u0026#34;queue\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;guest\u0026#34;, \u0026#34;vhost\u0026#34;: \u0026#34;/nova\u0026#34; } } } 上記ファイルでは virt_type : qemu に設定していますが、KVM リソースを利用出来る 環境であればここを削除してください。デフォルトの \u0026lsquo;kvm\u0026rsquo; が適用されます。また気 をつけることは IP アドレスとネットワークインターフェース名です。環境に合わせて 設定していきましょう。今回は前提構成に合わせて environemnt ファイルを作ってい ます。\n次に openstack-chef-repo/.chef/encrypted_data_bag_secret というファイルが knife-zero ホストにあるはずです。これをデプロイ対象の3ノードに事前に転送してお く必要があります。\nscp openstack-chef-repo/.chef/encrypted_data_bag_secret 10.0.2.10:/tmp/ scp openstack-chef-repo/.chef/encrypted_data_bag_secret 10.0.2.11:/tmp/ scp openstack-chef-repo/.chef/encrypted_data_bag_secret 10.0.2.12:/tmp/ 対象ホストにて\nmkdir /etc/chef mv /tmp/encrypted_data_bag_secret /etc/chef/openstack_data_bag_secret ではいよいよデプロイです。\nController ホストへのデプロイ\nknife zero bootstrap 10.0.2.10 -N kilo01 -r \u0026#39;role[os-compute-single-controller-no-network]\u0026#39; -E multi-neutron-kilo -x \u0026lt;USERNAME\u0026gt; --sudo Network ホストへのデプロイ\nknife zero bootstrap 10.0.2.11 -N kilo02 -r \u0026#39;role[os-client]\u0026#39;,\u0026#39;role[os-network]\u0026#39; -E multi-neutron-kilo -x \u0026lt;USERNAME\u0026gt; --sudo Compute ノードへのデプロイ\nknife zero bootstrap 10.0.2.12 -N kilo03 -r \u0026#39;role[os-compute-worker]\u0026#39; -E multi-neutron-kilo -x \u0026lt;USERNAME\u0026gt; --sudo これで完了です。admin/mypass というユーザ・パスワードでログインが可能です。\nまとめ openstack-chef-repo を使って OpenStack Kilo の複数台構成をデプロイ出来ました。重要なのは Environment をどうやって作るか？ですが、 私は 作成 -\u0026gt; デプロイ -\u0026gt; 修正 -\u0026gt; デプロイ -\u0026gt;\u0026hellip;. を繰り返して作成しています。何度実行しても不具合は発生しない設計なクックブックに なっていますので、このような作業が可能になります。また、「ここの設定を追加したい」という時は\u0026hellip;\n 該当の template を探す 該当のパラメータを確認する recipe 内で template にどうパラメータを渡しているか確認する attribute なり、変数なりを修正するための方法を探す  と行います。比較的難しい作業になるのですが、自らの環境に合わせた Environment を作成するにはこれらの作業が必須となってきます。\n以上、複数台構成のデプロイ方法についてでした。\n","permalink":"https://jedipunkz.github.io/post/2015/07/20/knife-zero-openstack-kilo/","summary":"こんにちは。@jedipunkz です。\n前回 OpenStack Kilo のオールインワン構成を Chef-Zero を使ってデプロイする方法 を書きましたが、複数台構成についても調べたので結果をまとめていきます。\n使うのは openstack/openstack-chef-repo です。下記の URL にあります。\nhttps://github.com/openstack/openstack-chef-repo\nこの中に Vagrant を使ったファイルが存在しますが、実機でのデプロイには全く役に 立ちません。自分で Environment ファイルを作成する必要があります。今回は前提の 構成を作って、それに合わせた Environment ファイルを記します。ほぼスタンダード な構成にしましたので、自分の環境に合わせて修正するのも比較的簡単だと想います。 参考にしてください。\n今回は knife-zero を使ってデプロイします。Chef サーバが必要なく、knife-zero を 使うホスト上のオンメモリで Chef サーバが稼働するので準備がほとんど必要ありません。\n早速ですが、構成と準備・そしてデプロイ作業を記していきます。\n前提の構成  +------------+ | GW Router | +--+------------+ | | | +--------------+--------------+---------------------------- public network | | eth0 | eth0 | +------------+ +------------+ +------------+ +------------+ | | Controller | | Network | | Compute | | Knife-Zero | | +------------+ +-------+----+ +------+-----+ +------------+ | | eth1 | eth1 | | eth1 | | eth1 +--+--------------+-------)------+------)-------+------------- api/management network | eth2 | eth2 +-------------+--------------------- guest network  特徴としては\u0026hellip;","title":"Knife-ZeroでOpenStack Kiloデプロイ(複数台編)"},{"content":"こんにちは。@jedipunkz です。\n久々に openstack-chef-repo を覗いてみたら \u0026lsquo;openstack/openstack-chef-repo\u0026rsquo; とし て公開されていました。今まで stackforge 側で管理されていましたが \u0026lsquo;openstack\u0026rsquo; の方に移動したようです。\nhttps://github.com/openstack/openstack-chef-repo\n結構安定してきているのかな？と想い、ちらっと試したのですが案の定、簡単に動作さ せることが出来ました。\n今回はこのレポジトリを使ってオールインワン構成の OpenStack Kilo を作る方法をま とめていきます。\n前提の構成 このレポジトリは Vagrant で OpenStack を作るための環境一式が最初から用意されて いますが、Vagrant では本番環境を作ることは出来ないため、Ubuntu ホストを前提と した記述に差し替えて説明していきます。前提にする構成は下記のとおりです。\n Uuntu Linux 14.04 x 1 台 ネットワークインターフェース x 3 つ eth0 : External ネットワーク用 eth1 : Internal (API, Manage) ネットワーク用 eth2 : Guest ネットワーク用  特徴としては上記なのですが、eth2 に関してはオールインワンなので必ずしも必要と いうわけではありません。複数台構成を考慮した設定になっています。\n前提のIP アドレス この記事では下記の IP アドレスを前提にします。お手持ちの環境の IP アドレスが違 い場合はそれに合わせて後に示す json ファイルを変更してください。\n 10.0.1.10 (eth0) : external ネットワーク 10.0.2.10 (eth1) : api/management ネットワーク 10.0.3.10 (eth2) : Guest ネットワーク  事前の準備 事前に対象ホスト (OpenStack ホスト) に chef, berkshelf をインストールします。\nsudo -i curl -L https://www.opscode.com/chef/install.sh | bash Berkshelf をインストールするのに必要なソフトウェアをインストールします。\napt-get -y install build-essential zlib1g-dev libssl-dev libreadline-dev ruby-dev libxml2-dev libxslt-dev g++ Berkshelf をインストールします。\n/opt/chef/embedded/bin/gem install berkshelf --no-ri --no-rdoc デプロイ作業 それでは openstack-chef-repo を取得してデプロイの準備を行います。 ブランチの指定は行わず master ブランチを取得します。Kilo は master ブランチで 管理されています。次のバージョンの開発が始まるタイミングで \u0026lsquo;stable/kilo\u0026rsquo; ブラ ンチに管理が移されます。\nsudo -i cd ~/ git clone https://github.com/openstack/openstack-chef-repo.git 次に Berkshelf を使って必要な Cookbooks をダウンロードします。\ncd ~/openstack-chef-repo /opt/chef/embedded/bin/berks vendor ./cookbooks Environment を作成します。これは各環境に合わせた設定ファイルのようなもので、各 Cookbooks の Attributes を上書きする仕組みになっています。下記の内容を\nopenstack-chef-repo/environments/aio-neutron-kilo.json というファイル名で保存してください。\n{ \u0026#34;name\u0026#34;: \u0026#34;aio-neutron-kilo\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;test\u0026#34;, \u0026#34;cookbook_versions\u0026#34;: { }, \u0026#34;json_class\u0026#34;: \u0026#34;Chef::Environment\u0026#34;, \u0026#34;chef_type\u0026#34;: \u0026#34;environment\u0026#34;, \u0026#34;default_attributes\u0026#34;: { }, \u0026#34;override_attributes\u0026#34;: { \u0026#34;mysql\u0026#34;: { \u0026#34;bind_address\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;server_root_password\u0026#34;: \u0026#34;mysqlroot\u0026#34;, \u0026#34;server_debian_password\u0026#34;: \u0026#34;mysqlroot\u0026#34;, \u0026#34;server_repl_password\u0026#34;: \u0026#34;mysqlroot\u0026#34;, \u0026#34;allow_remote_root\u0026#34;: true, \u0026#34;root_network_acl\u0026#34;: [\u0026#34;10.0.0.0/8\u0026#34;] }, \u0026#34;rabbitmq\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;5672\u0026#34;, \u0026#34;loopback_users\u0026#34;: [] }, \u0026#34;openstack\u0026#34;: { \u0026#34;auth\u0026#34;: { \u0026#34;validate_certs\u0026#34;: false }, \u0026#34;dashboard\u0026#34;: { \u0026#34;session_backend\u0026#34;: \u0026#34;file\u0026#34; }, \u0026#34;block-storage\u0026#34;: { \u0026#34;syslog\u0026#34;: { \u0026#34;use\u0026#34;: false }, \u0026#34;api\u0026#34;: { \u0026#34;ratelimit\u0026#34;: \u0026#34;False\u0026#34; }, \u0026#34;debug\u0026#34;: true, \u0026#34;image_api_chef_role\u0026#34;: \u0026#34;os-image\u0026#34;, \u0026#34;identity_service_chef_role\u0026#34;: \u0026#34;os-identity\u0026#34;, \u0026#34;rabbit_server_chef_role\u0026#34;: \u0026#34;os-ops-messaging\u0026#34; }, \u0026#34;compute\u0026#34;: { \u0026#34;rabbit\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34; }, \u0026#34;novnc_proxy\u0026#34;: { \u0026#34;bind_interface\u0026#34;: \u0026#34;eth1\u0026#34; }, \u0026#34;libvirt\u0026#34;: { \u0026#34;virt_type\u0026#34;: \u0026#34;qemu\u0026#34;, \u0026#34;bind_interface\u0026#34;: \u0026#34;eth1\u0026#34; }, \u0026#34;novnc_proxy\u0026#34;: { \u0026#34;bind_interface\u0026#34;: \u0026#34;eth1\u0026#34; }, \u0026#34;xvpvnc_proxy\u0026#34;: { \u0026#34;bind_interface\u0026#34;: \u0026#34;eth1\u0026#34; }, \u0026#34;image_api_chef_role\u0026#34;: \u0026#34;os-image\u0026#34;, \u0026#34;identity_service_chef_role\u0026#34;: \u0026#34;os-identity\u0026#34;, \u0026#34;nova_setup_chef_role\u0026#34;: \u0026#34;os-compute-api\u0026#34;, \u0026#34;rabbit_server_chef_role\u0026#34;: \u0026#34;os-ops-messaging\u0026#34;, \u0026#34;network\u0026#34;: { \u0026#34;public_interface\u0026#34;: \u0026#34;eth1\u0026#34;, \u0026#34;service_type\u0026#34;: \u0026#34;neutron\u0026#34; } }, \u0026#34;network\u0026#34;: { \u0026#34;debug\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;dhcp\u0026#34;: { \u0026#34;enable_isolated_metadata\u0026#34;: \u0026#34;True\u0026#34; }, \u0026#34;metadata\u0026#34;: { \u0026#34;nova_metadata_ip\u0026#34;: \u0026#34;10.0.2.10\u0026#34; }, \u0026#34;openvswitch\u0026#34;: { \u0026#34;tunnel_id_ranges\u0026#34;: \u0026#34;1:1000\u0026#34;, \u0026#34;enable_tunneling\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;tenant_network_type\u0026#34;: \u0026#34;gre\u0026#34;, \u0026#34;local_ip_interface\u0026#34;: \u0026#34;eth2\u0026#34; }, \u0026#34;api\u0026#34;: { \u0026#34;bind_interface\u0026#34;: \u0026#34;eth1\u0026#34; }, \u0026#34;l3\u0026#34;: { \u0026#34;external_network_bridge_interface\u0026#34;: \u0026#34;eth0\u0026#34; }, \u0026#34;service_plugins\u0026#34;: [\u0026#34;neutron.services.l3_router.l3_router_plugin.L3RouterPlugin\u0026#34;] }, \u0026#34;db\u0026#34;: { \u0026#34;bind_interface\u0026#34;: \u0026#34;eth1\u0026#34;, \u0026#34;compute\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34; }, \u0026#34;identity\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34; }, \u0026#34;image\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34; }, \u0026#34;network\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34; }, \u0026#34;volume\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34; }, \u0026#34;dashboard\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34; }, \u0026#34;telemetry\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34; }, \u0026#34;orchestration\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34; } }, \u0026#34;developer_mode\u0026#34;: true, \u0026#34;endpoints\u0026#34;: { \u0026#34;compute-api-bind\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8774\u0026#34; }, \u0026#34;compute-api\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8774\u0026#34; }, \u0026#34;compute-ec2-admin-bind\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8773\u0026#34; }, \u0026#34;compute-ec2-admin\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8773\u0026#34; }, \u0026#34;compute-ec2-api-bind\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8773\u0026#34; }, \u0026#34;compute-ec2-api\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8773\u0026#34; }, \u0026#34;compute-xvpvnc\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;6081\u0026#34; }, \u0026#34;compute-novnc-bind\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;6080\u0026#34; }, \u0026#34;compute-novnc\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;6080\u0026#34; }, \u0026#34;compute-vnc\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;6080\u0026#34; }, \u0026#34;image-api\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;9292\u0026#34; }, \u0026#34;image-api-bind\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;9292\u0026#34; }, \u0026#34;image-registry\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;9191\u0026#34; }, \u0026#34;image-registry-bind\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;9191\u0026#34; }, \u0026#34;identity-api\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;5000\u0026#34; }, \u0026#34;identity-bind\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;5000\u0026#34; }, \u0026#34;identity-admin\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;35357\u0026#34; }, \u0026#34;identity-internal\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;35357\u0026#34; }, \u0026#34;volume-api-bind\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8776\u0026#34; }, \u0026#34;volume-api\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8776\u0026#34; }, \u0026#34;telemetry-api\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8777\u0026#34; }, \u0026#34;network-api-bind\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;9696\u0026#34; }, \u0026#34;network-api\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;9696\u0026#34; }, \u0026#34;orchestration-api\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8004\u0026#34; }, \u0026#34;orchestration-api-cfn\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8000\u0026#34; }, \u0026#34;db\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;3306\u0026#34; }, \u0026#34;bind-host\u0026#34;: \u0026#34;0.0.0.0\u0026#34; }, \u0026#34;identity\u0026#34;: { \u0026#34;admin_user\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;bind_interface\u0026#34;: \u0026#34;eth1\u0026#34;, \u0026#34;debug\u0026#34;: true }, \u0026#34;image\u0026#34;: { \u0026#34;api\u0026#34;: { \u0026#34;bind_interface\u0026#34;: \u0026#34;eth1\u0026#34; }, \u0026#34;debug\u0026#34;: true, \u0026#34;identity_service_chef_role\u0026#34;: \u0026#34;os-identity\u0026#34;, \u0026#34;rabbit_server_chef_role\u0026#34;: \u0026#34;os-ops-messaging\u0026#34;, \u0026#34;registry\u0026#34;: { \u0026#34;bind_interface\u0026#34;: \u0026#34;eth1\u0026#34; }, \u0026#34;syslog\u0026#34;: { \u0026#34;use\u0026#34;: false } }, \u0026#34;mq\u0026#34;: { \u0026#34;bind_interface\u0026#34;: \u0026#34;eth1\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;guest\u0026#34;, \u0026#34;vhost\u0026#34;: \u0026#34;/nova\u0026#34;, \u0026#34;network\u0026#34;: { \u0026#34;rabbit\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;5672\u0026#34; } }, \u0026#34;compute\u0026#34;: { \u0026#34;service_type\u0026#34;: \u0026#34;rabbitmq\u0026#34;, \u0026#34;rabbit\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;5672\u0026#34; } }, \u0026#34;block-storage\u0026#34;: { \u0026#34;service_type\u0026#34;: \u0026#34;rabbitmq\u0026#34;, \u0026#34;rabbit\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;5672\u0026#34; } } } }, \u0026#34;queue\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;guest\u0026#34;, \u0026#34;vhost\u0026#34;: \u0026#34;/nova\u0026#34; } } } 上記ファイルは KVM が使えない環境用に virt_type : qemu にしていますが、KVM が 利用できる環境をご利用であれば該当行を削除してください。デフォルト値の \u0026lsquo;kvm\u0026rsquo; が入るはずです。\n次にデプロイ前に databag 関連の事前操作を行います。Vagrant 用に作成されたファ イルを除くと\u0026hellip;\nmachine \u0026#39;controller\u0026#39; do add_machine_options vagrant_config: controller_config role \u0026#39;allinone-compute\u0026#39; role \u0026#39;os-image-upload\u0026#39; chef_environment env file(\u0026#39;/etc/chef/openstack_data_bag_secret\u0026#39;, \u0026#34;#{File.dirname(__FILE__)}/.chef/encrypted_data_bag_secret\u0026#34;) converge true end となっていて /etc/chef/openstack_data_bag_secret というファイルを事前にコピー する必要がありそうです。下記のように操作します。\ncp .chef/encrypted_data_bag_secret /etc/chef/openstack_data_bag_secret デプロイを実行します。\nこの openstack-chef-repo には .chef ディレクトリが存在していてノード名が記され ています。\u0026lsquo;nodienode\u0026rsquo; というノード名です。これを利用してそのままデプロイを実行 します。\nchef-client -z knife node -z run_list add nodienode \u0026#39;role[allinone-compute]\u0026#39; chef-client -z -E aio-neutron-kilo 上記の説明を行います。 １行目 chef-client -z で Chef-Zero サーバをメモリ上に起動し、2行目で自ノードへ run_list を追加しています。最後、3行目でデプロイ実行、となります。\n数分待つと OpenStack Kilo が構成されているはずです。\nまとめ Chef-Zero を用いることで Chef サーバを利用せずに楽に構築が行えました。ですが、 OpenStack の複数台構成となるとそれぞれのノードのパラメータを連携させる必要が出 てくるので Chef サーバを用いたほうが良さそうです。今度、時間を見つけて Kilo の 複数台構成についても調べておきます。\nまた、master ブランチを使用していますので、まだ openstack-chef-repo 自体が流動 的な状態とも言えます。が launchpad で管理されている Bug リストを見ると、ステー タス Critical, High の Bug が見つからなかったので Kilo に関しては、大きな問題 無く安定してきている感があります。\nhttps://bugs.launchpad.net/openstack-chef\n","permalink":"https://jedipunkz.github.io/post/2015/07/16/chef-zero-openstack-allinone/","summary":"こんにちは。@jedipunkz です。\n久々に openstack-chef-repo を覗いてみたら \u0026lsquo;openstack/openstack-chef-repo\u0026rsquo; とし て公開されていました。今まで stackforge 側で管理されていましたが \u0026lsquo;openstack\u0026rsquo; の方に移動したようです。\nhttps://github.com/openstack/openstack-chef-repo\n結構安定してきているのかな？と想い、ちらっと試したのですが案の定、簡単に動作さ せることが出来ました。\n今回はこのレポジトリを使ってオールインワン構成の OpenStack Kilo を作る方法をま とめていきます。\n前提の構成 このレポジトリは Vagrant で OpenStack を作るための環境一式が最初から用意されて いますが、Vagrant では本番環境を作ることは出来ないため、Ubuntu ホストを前提と した記述に差し替えて説明していきます。前提にする構成は下記のとおりです。\n Uuntu Linux 14.04 x 1 台 ネットワークインターフェース x 3 つ eth0 : External ネットワーク用 eth1 : Internal (API, Manage) ネットワーク用 eth2 : Guest ネットワーク用  特徴としては上記なのですが、eth2 に関してはオールインワンなので必ずしも必要と いうわけではありません。複数台構成を考慮した設定になっています。\n前提のIP アドレス この記事では下記の IP アドレスを前提にします。お手持ちの環境の IP アドレスが違 い場合はそれに合わせて後に示す json ファイルを変更してください。\n 10.","title":"Chef-ZeroでOpenStack Kiloデプロイ(オールインワン編)"},{"content":"こんにちは、@jedipunkz です。\n久々にブログ更新になりましたが、ウォーミングアップで minio というオブジェクト ストレージを使ってみたメモを記事にしたいと想います。\nminio は Minimal Object Storage の名の通り、最小限の小さなオブジェクトストレー ジになります。公式サイトは下記のとおりです。\nhttp://minio.io/\nGolang で記述されていて Apache License v2 の元に公開されています。\n最近、資金調達の話も挙がっていたので、これから一般的になってくるのかもしれません。\n早速ですが、minio を動かしてみます。\nMinio を起動する 方法は mithub.com/minio/minio の README に書かれていますが、バイナリを持ってき て実行権限を与えるだけのシンプルな手順になります。\nLinux でも Mac でも動作しますが、今回私は Mac 上で動作させました。\n% wget https://dl.minio.io:9000/updates/2015/Jun/darwin-amd64/minio % chmod +x minio % ./minio mode memory limit 512MB Starting minio server on: http://127.0.0.1:9000 Starting minio server on: http://192.168.1.123:9000 起動すると Listening Port と共に EndPoint の URL が表示されます。\n次に mc という minio client を使って動作確認します。\nMc を使ってアクセスする mc は下記の URL にあります。\nhttps://github.com/minio/mc\nこちらもダウンロードして実行権限を付与するのみです。mc は minio だけではなく、 Amazon S3 とも互換性がありアクセス出来ますが、せっかくなので上記で起動した minio にアクセスします。\n% wget https://dl.minio.io:9000/updates/2015/Jun/darwin-amd64/mc % chmod +x mc % ./mc config generate /mc ls http://127.0.0.1:9000/bucket01 [2015-06-25 16:21:37 JST] 0B testfile 上記では予め作っておいた bucket01 という名前のバケットの中身を表示しています。 作り方はこれから minio の Golang ライブラリである minio-go を使って作りました。 これから説明します。\nまた ls コマンドの他にも Usage を確認すると幾つかのサブコマンドが見つかります。\nMinio の Golang ライブラリ minio-go を使ってアクセスする さて、せっかくのオブジェクトストレージも手作業でファイルやバケットのアクセスを 行うのはもったいないです。ソフトウェアを使って操作してす。\nminio のサンプルのコードを参考にして、下記のコードを作成してみました。\npackage main import ( \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; \u0026#34;github.com/minio/minio-go\u0026#34; ) func main() { config := minio.Config{ // AccessKeyID: \u0026#34;YOUR-ACCESS-KEY-HERE\u0026#34;,  // SecretAccessKey: \u0026#34;YOUR-PASSWORD-HERE\u0026#34;,  Endpoint: \u0026#34;http://127.0.0.1:9000\u0026#34;, } s3Client, err := minio.New(config) if err != nil { log.Fatalln(err) } err = s3Client.MakeBucket(\u0026#34;bucket01\u0026#34;, minio.BucketACL(\u0026#34;public-read-write\u0026#34;)) if err != nil { log.Fatalln(err) } log.Println(\u0026#34;Success: I made a bucket.\u0026#34;) object, err := os.Open(\u0026#34;testfile\u0026#34;) if err != nil { log.Fatalln(err) } defer object.Close() objectInfo, err := object.Stat() if err != nil { object.Close() log.Fatalln(err) } err = s3Client.PutObject(\u0026#34;bucket01\u0026#34;, \u0026#34;testfile\u0026#34;, \u0026#34;application/octet-stream\u0026#34;, objectInfo.Size(), object) if err != nil { log.Fatalln(err) } for bucket := range s3Client.ListBuckets() { if bucket.Err != nil { log.Fatalln(bucket.Err) } log.Println(bucket.Stat) } for object := range s3Client.ListObjects(\u0026#34;bucket01\u0026#34;, \u0026#34;\u0026#34;, true) { if object.Err != nil { log.Fatalln(object.Err) } log.Println(object.Stat) } } 簡単ですがコードの説明をします。\n 11行目で config の上書きをします。先ほど起動した minio の EndPoint を記します。 17行目で minio にセッションを張り接続を行っています。 22行目で \u0026lsquo;bucket01\u0026rsquo; というバケットを生成しています。その際にACLも設定 28行目から42行目で \u0026lsquo;testfile\u0026rsquo; というローカルファイルをストレージにPUTしています。 44行目でバケット一覧を表示しています。 51行目で上記で作成したバケットの中のオブジェクト一覧を表示しています。  実行結果は下記のとおりです。\n2015/06/25 16:56:21 Success: I made a bucket. 2015/06/25 16:56:21 {bucket01 2015-06-25 07:56:21.155 +0000 UTC} 2015/06/25 16:56:21 {\u0026#34;d41d8cd98f00b204e9800998ecf8427e\u0026#34; testfile 2015-06-25 07:56:21.158 +0000 UTC 0 {minio minio} STANDARD} バケットの作成とオブジェクトの PUT が正常に行えたことをログから確認できます。\nまとめ 上記の通り、今現在出来ることは少ないですが冒頭にも記したとおり資金調達の話も挙 がってきていますので、これからどのような方向に向かうか楽しみでもあります。また 最初から Golang, Python 等のライブラリが用意されているところが今どきだなぁと想 いました。オブジェクトストレージを手作業で操作するケースは現場では殆ど無いと想 いますので、その辺は現在では当たり前になりつつあるかもしれません。ちなみに Python のライブラリは下記の URL にあります。\nhttps://github.com/minio/minio-py\n以上です。\n","permalink":"https://jedipunkz.github.io/post/2015/06/25/minio/","summary":"こんにちは、@jedipunkz です。\n久々にブログ更新になりましたが、ウォーミングアップで minio というオブジェクト ストレージを使ってみたメモを記事にしたいと想います。\nminio は Minimal Object Storage の名の通り、最小限の小さなオブジェクトストレー ジになります。公式サイトは下記のとおりです。\nhttp://minio.io/\nGolang で記述されていて Apache License v2 の元に公開されています。\n最近、資金調達の話も挙がっていたので、これから一般的になってくるのかもしれません。\n早速ですが、minio を動かしてみます。\nMinio を起動する 方法は mithub.com/minio/minio の README に書かれていますが、バイナリを持ってき て実行権限を与えるだけのシンプルな手順になります。\nLinux でも Mac でも動作しますが、今回私は Mac 上で動作させました。\n% wget https://dl.minio.io:9000/updates/2015/Jun/darwin-amd64/minio % chmod +x minio % ./minio mode memory limit 512MB Starting minio server on: http://127.0.0.1:9000 Starting minio server on: http://192.168.1.123:9000 起動すると Listening Port と共に EndPoint の URL が表示されます。\n次に mc という minio client を使って動作確認します。","title":"オブジェクトストレージ minio を使ってみる"},{"content":"こんにちは。@jedipunkz です。\nVyOS に VXLAN が実装されたと聞いて少し触ってみました。この情報を知ったきっかけ は @upaa さんの下記の資料です。\n参考資料 : http://www.slideshare.net/upaa/vyos-users-meeting-2-vyosvxlan\nVyOS は御存知の通り実体は Debian Gnu/Linux 系の OS でその上に OSS なミドル ウェアが搭載されていて CLI でミドルウェアのコンフィギュレーション等が行えるモ ノになっています。Linux で VXLAN といえば OVS を使ったモノがよく知られています が VyOS の VXLAN 機能は Linux Kernel の実装を使っているようです。\n要件  トンネルを張るためのセグメントを用意 VyOS 1.1.1 (現在最新ステーブルバージョン) が必要 Ubuntu Server 14.04 LTS (同じく Linux VXLAN 搭載バージョン)  構成 特徴\n マネージメント用セグメント 10.0.1.0/24 を用意 GRE と同じくトンネル終端が必要なのでそのためのセグメント 10.0.2.0/24 を用意 各 eth1 は IP reachable である必要があるので予め IP アドレスの設定と疎通を確認 VXLAN を喋れる Ubuntu 14.04 LTS x 1 台と VyOS 1.1.1 x 2 台で相互に疎通確認  +-------------+-------------+------------ Management 10.0.1.0/24 |10.0.0.254 |10.0.0.253 |10.0.0.1 |eth0 |eth0 |eth0 +----------+ +----------+ +----------+ | vyos01 | | vyos02 | | ubuntu | +-+--------+ +----------+ +----------+ | |eth1 | |eth1 | |eth1 | |10.0.2.254 | |10.0.2.253 | |10.0.2.1 | +-----------)-+-----------)-+---------- Tunneling 10.0.2.0/24 | | | +-------------+-------------+------------ VXLAN(eth1にlink) 10.0.1.0/24 10.0.1.254 10.0.1.253 10.0.1.1 設定を投入 vyos01 の設定を行う。VXLAN の設定に必要なものは\u0026hellip;\n VNI (VXLAN Network Ideintity)という識別子 Multicast Group Address 互いに IP reachable なトンネルを張るためのインターフェース  です。これらを意識して下記の設定を vyos01 に投入します。\n$ configure % set interfaces vxlan vxlan0 % set interfaces vxlan vxlan0 group 239.1.1.1 % set interfaces vxlan vxlan0 vni 42 % set interfaces vxlan vxlan0 address \u0026#39;10.0.1.254/24\u0026#39; % set interfaces vxlan vxlan0 link eth1 設定を確認します\n% exit $ show int ...\u0026lt;省略\u0026gt;... vxlan vxlan0 { address 10.0.1.254/24 group 239.1.1.1 link eth1 vni 42 } VyOS の CLI を介さず直 Linux の設定を iproute2 で確認してみましょう。 VNI, Multicast Group Address と共に \u0026lsquo;link eth1\u0026rsquo; で設定したトンネルを終端するための物理 NIC が確認できます。\nvyos@vyos01# ip -d link show vxlan0 5: vxlan0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1450 qdisc noqueue state UNKNOWN mode DEFAULT group default link/ether 86:24:26:b2:11:5c brd ff:ff:ff:ff:ff:ff promiscuity 0 vxlan id 42 group 239.1.1.1 dev eth1 port 32768 61000 ttl 16 ageing 300 vyos02 の設定を同様に行います。\n$ congigure % set interfaces vxlan vxlan0 address \u0026#39;10.0.1.253/24\u0026#39; % set interfaces vxlan vxlan0 vni 42 % set interfaces vxlan vxlan0 group 239.1.1.1 % set interfaces vxlan vxlan0 link eth1 設定の確認を行います。\n... 省略 ... vxlan vxlan0 { address 10.0.1.254/24 group 239.1.1.1 link eth1 vni 42 } 同じく Linux の iproute2 で確認します。\nvyos@vyos01# ip -d link show vxlan0 5: vxlan0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1450 qdisc noqueue state UNKNOWN mode DEFAULT group default link/ether 86:24:26:b2:11:5c brd ff:ff:ff:ff:ff:ff promiscuity 0 vxlan id 42 group 239.1.1.1 dev eth1 port 32768 61000 ttl 16 ageing 300 ubuntu ホストの設定を行っていきます。\nUbuntu Server 14.04 LTS であればパッチを当てること無く Linux Kernel の VXLAN 機能を使うことができます。 設定内容は VyOS と同等です。VyOS がこの Linux の実装を使っているのがよく分かります。\nsudo modprobe vxlan sudo ip link add vxlan0 type vxlan id 42 group 239.1.1.1 dev eth1 sudo ip link set up vxlan0 sudo ip a add 10.0.1.1/24 dev vxlan0 同じく Linux iproute2 で確認を行います。\nip -d link show vxlan0 5: vxlan0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1450 qdisc noqueue state UNKNOWN mode DEFAULT group default link/ether d6:ff:c1:27:69:a0 brd ff:ff:ff:ff:ff:ff promiscuity 0 vxlan id 42 group 239.1.1.1 dev eth1 port 32768 61000 ageing 300 疎通確認 疎通確認を行います。\nubuntu -\u0026gt; vyos01 の疎通確認です。ICMP で疎通が取れることを確認できます。\nthirai@ubuntu:~$ ping 10.0.1.254 -c 3 PING 10.0.1.254 (10.0.1.254) 56(84) bytes of data. 64 bytes from 10.0.1.254: icmp_seq=1 ttl=64 time=0.272 ms 64 bytes from 10.0.1.254: icmp_seq=2 ttl=64 time=0.336 ms 64 bytes from 10.0.1.254: icmp_seq=3 ttl=64 time=0.490 ms --- 10.0.1.254 ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 1999ms rtt min/avg/max/mdev = 0.272/0.366/0.490/0.091 ms 次に ubuntu -\u0026gt; vyos02 の疎通確認です。\nthirai@ubuntu:~$ ping 10.0.1.253 -c 3 PING 10.0.1.253 (10.0.1.253) 56(84) bytes of data. 64 bytes from 10.0.1.253: icmp_seq=1 ttl=64 time=0.272 ms 64 bytes from 10.0.1.253: icmp_seq=2 ttl=64 time=0.418 ms 64 bytes from 10.0.1.253: icmp_seq=3 ttl=64 time=0.451 ms --- 10.0.1.253 ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 1998ms rtt min/avg/max/mdev = 0.272/0.380/0.451/0.079 ms この時点で ubuntu ホストの fdb (forwarding db) の内容を確認します。\n$ bridge fdb show dev vxlan0 00:00:00:00:00:00 dst 239.1.1.1 via eth1 self permanent 4e:69:a4:a7:ef:1c dst 10.0.2.253 self 86:24:26:b2:11:5c dst 10.0.2.254 self vyos01, vyos02 のトンネル終端 IP アドレスと Mac アドレスが確認できます。ubuntu ホストから見ると 送信先は vyos0[12] の VXLAN インターフェースではなく、あくまでもトンネル終端を行っているインターフェース になることがわかります。\nまとめ VyOS ver 1.1.0 には VXLAN を物理インターフェースに link する機能に不具合がありそうなので今ら ver 1.1.1 を使うしか なさそう。とは言え、ver 1.1.1 なら普通に動作しました。\nVyOS は仮想ルータという位置付けなので今回紹介したようにインターフェースを VXLAN ネットワークに所属させる 機能があるのみです。VXLAN Trunk を行うような設定はありません。これはハイパーバイザ上で動作させることを前提 に設計されているので仕方ないです..というかスイッチで行うべき機能ですよね..。VM を接続して云々するには OVS のようなソフトウェアスイッチを使えばできます。\nまた fdb は時間が経つと情報が消えます。これは VXLAN のメッシュ構造なトンネルがその都度張られているのかどうか 気になるところです。ICMP の送信で一発目のみマルチキャストでその後ユニキャストになることを確認しましたが、その 一発目のマルチキャストでトンネリングがされるものなのでしょうか\u0026hellip;。あとで調べてみます。OVS のように CLI で トンネルがどのように張られているか確認する手段があれば良いのですが。\n以上です。\n","permalink":"https://jedipunkz.github.io/post/2014/12/16/vyos-vxlan/","summary":"こんにちは。@jedipunkz です。\nVyOS に VXLAN が実装されたと聞いて少し触ってみました。この情報を知ったきっかけ は @upaa さんの下記の資料です。\n参考資料 : http://www.slideshare.net/upaa/vyos-users-meeting-2-vyosvxlan\nVyOS は御存知の通り実体は Debian Gnu/Linux 系の OS でその上に OSS なミドル ウェアが搭載されていて CLI でミドルウェアのコンフィギュレーション等が行えるモ ノになっています。Linux で VXLAN といえば OVS を使ったモノがよく知られています が VyOS の VXLAN 機能は Linux Kernel の実装を使っているようです。\n要件  トンネルを張るためのセグメントを用意 VyOS 1.1.1 (現在最新ステーブルバージョン) が必要 Ubuntu Server 14.04 LTS (同じく Linux VXLAN 搭載バージョン)  構成 特徴\n マネージメント用セグメント 10.0.1.0/24 を用意 GRE と同じくトンネル終端が必要なのでそのためのセグメント 10.0.2.0/24 を用意 各 eth1 は IP reachable である必要があるので予め IP アドレスの設定と疎通を確認 VXLAN を喋れる Ubuntu 14.","title":"VyOS で VXLAN を使ってみる"},{"content":"こんにちは。@jedipunkz です。\n自分は Ruby を普段使うのでいつも Fog というライブラリを使って OpenStack, AWS を操作していました。Fog を使うとクラウドの操作が Ruby のネイティブコードで行え るのでシステムコマンド打つよりミス無く済みます。\nFog より後発で Aviator というライブラリが登場してきたので少し使ってみたのです がまだ未完成なところがあるものの便利な点もあって今後に期待だったので紹介します。\n認証情報を yaml ファイルに記す 接続に必要な認証情報を yaml ファイルで記述します。名前を \u0026lsquo;aviator.yml\u0026rsquo; として 保存。この時に下記のように環境毎に認証情報を別けて書くことができます。こうする ことでコードの中で開発用・サービス用等と使い分けられます。\nproduction: provider: openstack auth_service: name: identity host_uri: \u0026lt;Auth URL\u0026gt; request: create_token validator: list_tenants auth_credentials: username: \u0026lt;User Name\u0026gt; password: \u0026lt;Password\u0026gt; tenant_name: \u0026lt;Tenant Name\u0026gt; development: provider: openstack auth_service: name: identity host_uri: \u0026lt;Auth URL\u0026gt; request: create_token validator: list_tenants auth_credentials: username: \u0026lt;User Name\u0026gt; password: \u0026lt;Password\u0026gt; tenant_name: \u0026lt;Tenant Name\u0026gt; シンタックス確認 +++\n次に aviator のシンタックスを確認します。Fog に無い機能で、コマンドラインでシ ンタックスを確認できてしかも指定可能はパラメータと必須なパラメータと共にサンプ ルコードまで提供してくれます。公式サイトに\u0026rsquo;サーバ作成\u0026rsquo;のメソッドが掲載されてい るので、ここでは仮想ディスクを作るシンタックスを確認してみます。\n% gem install aviator % aviator describe openstack volume # \u0026lt;-- 利用可能な機能を確認 Available requests for openstack volume_service: v1 public list_volume_types v1 public list_volumes v1 public delete_volume v1 public create_volume v1 public get_volume v1 public update_volume v1 public root % aviator describe openstack volume v1 public create_volume # \u0026lt;-- シンタックスを確認 :Request =\u0026gt; create_volume Parameters: +---------------------+-----------+ | NAME | REQUIRED? | +---------------------+-----------+ | availability_zone | N | | display_description | Y | | display_name | Y | | metadata | N | | size | Y | | snapshot_id | N | | volume_type | N | +---------------------+-----------+ Sample Code: session.volume_service.request(:create_volume) do |params| params.volume_type = value params.availability_zone = value params.snapshot_id = value params.metadata = value params.display_name = value params.display_description = value params.size = value end このように create_volume というメソッドが用意されていて、指定出来るパラメータ・ 必須なパラメータが確認できます。必須なモノには \u0026ldquo;Y\u0026rdquo; が REQUIRED に付いています。 またサンプルコードが出力されるので、めちゃ便利です。\nでは create_volume のシンタックスがわかったので、コードを書いてみましょう。\nコードを書いてみる +++\n#!/usr/bin/env ruby require \u0026#39;aviator\u0026#39; require \u0026#39;json\u0026#39; volume_session = Aviator::Session.new( :config_file =\u0026gt; \u0026#39;/home/thirai/aviator/aviator.yml\u0026#39;, :environment =\u0026gt; :production, :log_file =\u0026gt; \u0026#39;/home/thirai/aviator/aviator.log\u0026#39; ) volume_session.authenticate volume_session.volume_service.request(:create_volume) do |params| params.display_description = \u0026#39;testvol\u0026#39; params.display_name = \u0026#39;testvol01\u0026#39; params.size = 1 end puts volume_session.volume_service.request(:list_volumes).body 6行目で先ほど作成した認証情報ファイル aviator.yml とログ出力ファイル aviator.log を指定します。12行目で実際に OpenStack にログインしています。\n14-18行目はサンプルコードそのままです。必須パラメータの display_description, display_name, size のみを指定し仮想ディスクを作成しました。最後の puts \u0026hellip; は 実際に作成した仮想ディスク一覧を出力しています。\n結果は下記のとおりです。\n{ volumes: [{ status: \u0026#39;available\u0026#39;, display_name: \u0026#39;testvol01\u0026#39;, attachments: [], availability_zone: \u0026#39;az3\u0026#39;, bootable: \u0026#39;false\u0026#39;, created_at: description = \u0026#39;testvol\u0026#39;, volume_type: \u0026#39;standard\u0026#39;, snapshot_id: nil, source_volid: nil, metadata: }, id: \u0026#39;3a5f616e-a732-4442-a419-10369111bd4c\u0026#39;, size: 1 }] } まとめ +++\nサンプルコードやパラメータ一覧等がひと目でわかる aviator はとても便利です。ま だ利用できるクラウドプラットフォームが OpenStack しかないのと、Neutron の機能 がスッポリ抜けているので、まだ利用するには早いかもです\u0026hellip;。逆に言えばコントリ ビューションするチャンスなので、もし気になった方がいたら開発に参加してみるのも いいかもしれません。\n","permalink":"https://jedipunkz.github.io/post/2014/12/13/aviator-openstack/","summary":"こんにちは。@jedipunkz です。\n自分は Ruby を普段使うのでいつも Fog というライブラリを使って OpenStack, AWS を操作していました。Fog を使うとクラウドの操作が Ruby のネイティブコードで行え るのでシステムコマンド打つよりミス無く済みます。\nFog より後発で Aviator というライブラリが登場してきたので少し使ってみたのです がまだ未完成なところがあるものの便利な点もあって今後に期待だったので紹介します。\n認証情報を yaml ファイルに記す 接続に必要な認証情報を yaml ファイルで記述します。名前を \u0026lsquo;aviator.yml\u0026rsquo; として 保存。この時に下記のように環境毎に認証情報を別けて書くことができます。こうする ことでコードの中で開発用・サービス用等と使い分けられます。\nproduction: provider: openstack auth_service: name: identity host_uri: \u0026lt;Auth URL\u0026gt; request: create_token validator: list_tenants auth_credentials: username: \u0026lt;User Name\u0026gt; password: \u0026lt;Password\u0026gt; tenant_name: \u0026lt;Tenant Name\u0026gt; development: provider: openstack auth_service: name: identity host_uri: \u0026lt;Auth URL\u0026gt; request: create_token validator: list_tenants auth_credentials: username: \u0026lt;User Name\u0026gt; password: \u0026lt;Password\u0026gt; tenant_name: \u0026lt;Tenant Name\u0026gt; シンタックス確認 +++","title":"Aviator でモダンに OpenStack を操作する"},{"content":"こんにちは。@jedipunkzです。\nOpenStack Juno がリリースされましたが、今日は Icehouse ネタです。\nicehouse 以降、自分の中で OpenStack を自動で作る仕組みが無くなりつつあり、気軽 に OpenStack を作って色々試したい！ッていう時に手段が無く困っていました。例え ば仕事でちょっと OpenStack 弄りたい！って時に DevStack, RDO しかなく。DevStack は御存知の通り動かない可能性が結構あるし RDO は Ubuntu/Debian Gnu Linux ベース じゃないし。\nってことで、以前にも紹介した stackforge 管理の openstack-chef-repo と Chef-Zero を使って OpenStack Icehouse (Neutron) のオールインワン構成を作る方法 を書きます。ちなみに最近 Chef-Solo が Chef-Zero に置き換わりつつあるらしいです。 Chef-Zero はオンメモリで Chef サーバを起動する仕組みです。Chef-Solo と違って Chef サーバを扱う時と何も変更無く操作が出来るのでとても楽です。また、Chef サーバを 別途構、構築・管理しなくて良いので、気軽に OpenStack が作れます。\nちなみに stackforge/openstack-chef-repo の README.md に Chef-Zero での構築方法 が書いてありますが、沢山の問題があります。\n nova-network 構成 API の Endpoint が全て localhost に向いてしまうため外部から操作不可能 各コンポーネントの bind_address が localhost を向いてしまう berkshelf がそのままでは入らない  よって、今回はこれらの問題を解決しつつ \u0026ldquo;オールインワンな Neutron 構成の Icehouse OpenStack を作る方法\u0026rdquo; を書いていきます。\n構成 +----------------- 10.0.0.0/24 (api/management network) | +----------------+ | OpenStack Node | | Controller | | Compute | +----------------+ | | +--(-------------- 10.0.1.0/24 (external network) | +-------------- 10.0.2.0/24 (guest vm network)  IP address 達\n 10.0.0.10 (api/manageent network) : eth0 10.0.1.10 (external network) : eth1 10.0.2.10 (guest vm network) : eth2  注意 : 操作は全て eth0 経由で行う\n前提の環境 stackforge/openstack-chef-repo の依存している Cookbooks の関係上、upstart 周り がうまく制御できていないので Ubuntu Server 12.04.x を使います。\nインストール方法 上記のように3つのネットワークインターフェースが付いたサーバを1台用意します。 KVM が利用出来たほうがいいですが使えないくても構いません。KVM リソースが使えな い場合の修正方法を後に記します。\nサーバにログインし root ユーザになります。その後 Chef をオムニバスインストーラ でインストールします。\n% sudo -i # curl -L https://www.opscode.com/chef/install.sh | bash 次に stable/icehose ブランチを指定して openstack-chef-repo をクローンします。\n# cd ~ # git clone -b stable/icehouse https://github.com/stackforge/openstack-chef-repo #  berkshelf をインストールするのですが依存パッケージが足らないのでここでインストー ルします。\n# apt-get -y install build-essential zlib1g-dev libssl-dev libreadline-dev \\ ruby-dev libxml2-dev libxslt-dev g++ berkshelf をインストールします。\n# /opt/chef/embedded/bin/gem install berkshelf --no-ri --no-rdoc 次に openstack-chef-repo に依存する Cookbooks を取得します。\n# cd ~/openstack-chef-repo # /opt/chef/embedded/bin/berks vendor ./cookbooks ~/openstack-chef-repo/environments ディレクトリ配下に neutron-allinone.json と いうファイル名で作成します。内容は下記の通りです。\n{ [0/215] \u0026#34;name\u0026#34;: \u0026#34;neutron-allinone\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;test\u0026#34;, \u0026#34;cookbook_versions\u0026#34;: { }, \u0026#34;json_class\u0026#34;: \u0026#34;Chef::Environment\u0026#34;, \u0026#34;chef_type\u0026#34;: \u0026#34;environment\u0026#34;, \u0026#34;default_attributes\u0026#34;: { }, \u0026#34;override_attributes\u0026#34;: { \u0026#34;mysql\u0026#34;: { \u0026#34;bind_address\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;server_root_password\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;server_debian_password\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;server_repl_password\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;allow_remote_root\u0026#34;: true, \u0026#34;root_network_acl\u0026#34;: [\u0026#34;10.0.0.0/8\u0026#34;] }, \u0026#34;rabbitmq\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;10.0.1.10\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;5672\u0026#34; }, \u0026#34;openstack\u0026#34;: { \u0026#34;auth\u0026#34;: { \u0026#34;validate_certs\u0026#34;: false }, \u0026#34;dashboard\u0026#34;: { \u0026#34;session_backend\u0026#34;: \u0026#34;file\u0026#34; }, \u0026#34;block-storage\u0026#34;: { \u0026#34;syslog\u0026#34;: { \u0026#34;use\u0026#34;: false }, \u0026#34;api\u0026#34;: { \u0026#34;ratelimit\u0026#34;: \u0026#34;False\u0026#34; }, \u0026#34;debug\u0026#34;: true, \u0026#34;image_api_chef_role\u0026#34;: \u0026#34;os-image\u0026#34;, \u0026#34;identity_service_chef_role\u0026#34;: \u0026#34;os-identity\u0026#34;, \u0026#34;rabbit_server_chef_role\u0026#34;: \u0026#34;os-ops-messaging\u0026#34; }, \u0026#34;compute\u0026#34;: { \u0026#34;rabbit\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34; }, \u0026#34;novnc_proxy\u0026#34;: { \u0026#34;bind_interface\u0026#34;: \u0026#34;eth0\u0026#34; }, \u0026#34;libvirt\u0026#34;: { \u0026#34;bind_interface\u0026#34;: \u0026#34;eth0\u0026#34;, }, \u0026#34;novnc_proxy\u0026#34;: { \u0026#34;bind_interface\u0026#34;: \u0026#34;eth0\u0026#34; }, \u0026#34;xvpvnc_proxy\u0026#34;: { \u0026#34;bind_interface\u0026#34;: \u0026#34;eth0\u0026#34; }, \u0026#34;image_api_chef_role\u0026#34;: \u0026#34;os-image\u0026#34;, \u0026#34;identity_service_chef_role\u0026#34;: \u0026#34;os-identity\u0026#34;, \u0026#34;nova_setup_chef_role\u0026#34;: \u0026#34;os-compute-api\u0026#34;, \u0026#34;rabbit_server_chef_role\u0026#34;: \u0026#34;os-ops-messaging\u0026#34;, \u0026#34;network\u0026#34;: { \u0026#34;public_interface\u0026#34;: \u0026#34;eth0\u0026#34;, \u0026#34;service_type\u0026#34;: \u0026#34;neutron\u0026#34; } }, \u0026#34;network\u0026#34;: { \u0026#34;debug\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;dhcp\u0026#34;: { \u0026#34;enable_isolated_metadata\u0026#34;: \u0026#34;True\u0026#34; }, \u0026#34;metadata\u0026#34;: { \u0026#34;nova_metadata_ip\u0026#34;: \u0026#34;10.0.1.10\u0026#34; }, \u0026#34;openvswitch\u0026#34;: { \u0026#34;tunnel_id_ranges\u0026#34;: \u0026#34;1:1000\u0026#34;, \u0026#34;enable_tunneling\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;tenant_network_type\u0026#34;: \u0026#34;gre\u0026#34;, \u0026#34;local_ip_interface\u0026#34;: \u0026#34;eth2\u0026#34; }, \u0026#34;api\u0026#34;: { \u0026#34;bind_interface\u0026#34;: \u0026#34;eth0\u0026#34; }, \u0026#34;l3\u0026#34;: { \u0026#34;external_network_bridge_interface\u0026#34;: \u0026#34;eth1\u0026#34; }, \u0026#34;service_plugins\u0026#34;: [\u0026#34;neutron.services.l3_router.l3_router_plugin.L3RouterPlugin\u0026#34;] }, \u0026#34;db\u0026#34;: { \u0026#34;bind_interface\u0026#34;: \u0026#34;eth0\u0026#34;, \u0026#34;compute\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34; }, \u0026#34;identity\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34; }, \u0026#34;image\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34; }, \u0026#34;network\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34; }, \u0026#34;volume\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34; }, \u0026#34;dashboard\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34; }, \u0026#34;telemetry\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34; }, \u0026#34;orchestration\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34; } }, \u0026#34;developer_mode\u0026#34;: true, \u0026#34;endpoints\u0026#34;: { \u0026#34;compute-api-bind\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8774\u0026#34; }, \u0026#34;compute-api\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8774\u0026#34; }, \u0026#34;compute-ec2-admin-bind\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8773\u0026#34; }, \u0026#34;compute-ec2-admin\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8773\u0026#34; }, \u0026#34;compute-ec2-api-bind\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8773\u0026#34; }, \u0026#34;compute-ec2-api\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8773\u0026#34; }, \u0026#34;compute-xvpvnc\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;6081\u0026#34; }, \u0026#34;compute-novnc-bind\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;6080\u0026#34; }, \u0026#34;compute-novnc\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;6080\u0026#34; }, \u0026#34;compute-vnc\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;6080\u0026#34; }, \u0026#34;image-api\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;9292\u0026#34; }, \u0026#34;image-api-bind\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;9292\u0026#34; }, \u0026#34;image-registry\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;9191\u0026#34; }, \u0026#34;image-registry-bind\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;9191\u0026#34; }, \u0026#34;identity-api\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;5000\u0026#34; }, \u0026#34;identity-bind\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;5000\u0026#34; }, \u0026#34;identity-admin\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;35357\u0026#34; }, \u0026#34;volume-api-bind\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8776\u0026#34; }, \u0026#34;volume-api\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8776\u0026#34; }, \u0026#34;telemetry-api\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8777\u0026#34; }, \u0026#34;network-api-bind\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;9696\u0026#34; }, \u0026#34;network-api\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;9696\u0026#34; }, \u0026#34;orchestration-api\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8004\u0026#34; }, \u0026#34;orchestration-api-cfn\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8000\u0026#34; } }, \u0026#34;identity\u0026#34;: { \u0026#34;admin_user\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;bind_interface\u0026#34;: \u0026#34;eth0\u0026#34;, \u0026#34;debug\u0026#34;: true }, \u0026#34;image\u0026#34;: { \u0026#34;api\u0026#34;: { \u0026#34;bind_interface\u0026#34;: \u0026#34;eth0\u0026#34; }, \u0026#34;debug\u0026#34;: true, \u0026#34;identity_service_chef_role\u0026#34;: \u0026#34;os-identity\u0026#34;, \u0026#34;rabbit_server_chef_role\u0026#34;: \u0026#34;os-ops-messaging\u0026#34;, \u0026#34;registry\u0026#34;: { \u0026#34;bind_interface\u0026#34;: \u0026#34;eth0\u0026#34; }, \u0026#34;syslog\u0026#34;: { \u0026#34;use\u0026#34;: false }, \u0026#34;upload_images\u0026#34;: [ \u0026#34;precise\u0026#34; ] }, \u0026#34;mq\u0026#34;: { \u0026#34;bind_interface\u0026#34;: \u0026#34;eth0\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;guest\u0026#34;, \u0026#34;vhost\u0026#34;: \u0026#34;/nova\u0026#34;, \u0026#34;network\u0026#34;: { \u0026#34;rabbit\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;5672\u0026#34; } }, \u0026#34;compute\u0026#34;: { \u0026#34;service_type\u0026#34;: \u0026#34;rabbitmq\u0026#34;, \u0026#34;rabbit\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;5672\u0026#34; } }, \u0026#34;block-storage\u0026#34;: { \u0026#34;service_type\u0026#34;: \u0026#34;rabbitmq\u0026#34;, \u0026#34;rabbit\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;5672\u0026#34; } } } }, \u0026#34;queue\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;guest\u0026#34;, \u0026#34;vhost\u0026#34;: \u0026#34;/nova\u0026#34; } } } 内容について全て説明するのは難しいですが、このファイルを作成するのが今回一番苦 労した点です。と言うのは、構成を作りつつそれぞれのコンポーネントのコンフィギュ レーション、エンドポイントのアドレス、バインドアドレス、リスンポート等など、全 てが正常な値になるように Cookbooks を読みつつ作業するからです。この json ファ イルが完成してしまえば、あとは簡単なのですが。\n前述しましたが KVM リソースが使えない環境の場合 Qemu で仮想マシンを稼働するこ とができます。その場合、下記のように \u0026ldquo;libvirt\u0026rdquo; の項目に \u0026ldquo;virt_type\u0026rdquo; を追記して ください。\n\u0026#34;libvirt\u0026#34;: { \u0026#34;bind_interface\u0026#34;: \u0026#34;eth0\u0026#34;, \u0026#34;virt_type\u0026#34;: \u0026#34;qemu\u0026#34; # \u0026lt;------ 追記 }, それではデプロイしていきます。\nここで \u0026lsquo;allinone\u0026rsquo; はホスト名、\u0026lsquo;allinone-compute\u0026rsquo; は Role 名、neutron-allinone は先ほど作成した json で指定している environment 名です。\n# chef-client -z # knife node -z run_list add allinone \u0026#39;role[allinone-compute]\u0026#39; # chef-client -z -E neutron-allinone 環境にもよりますが、数分でオールインワンな OpenStack Icehouse が完成します。\nまとめ +++\nChef サーバを使わなくて良いのでお手軽に OpenStack が構築出来ました。この json ファイルは実は他にも応用出来ると思っています。複数台構成の OpenStack も指定 Role を工夫すれば構築出来るでしょう。が、その場合は chef-zero は使えません。 Chef サーバ構成にする必要が出てきます。\nちなみに OpenStack Paris Summit 2014 で「OpenStack のデプロイに何を使っている か？」という調査結果が下記になります。Chef が2位ですが Pueppet に大きく離され ている感があります。Juno 版の openstack-chef-repo も開発が進んでいますので、頑 張って広めていきたいです。\n 1位 Puppet 2位 Chef 3位 Ansible 4位 DevStack 5位 PackStack 6位 Salt 7位 Juju 8位 Crowbar 9位 CFEngine  参考 URL : http://superuser.openstack.org/articles/openstack-user-survey-insights-november-2014\nちなみに、Puppet を使った OpenStack デプロイも個人的に色々試しています。\n","permalink":"https://jedipunkz.github.io/post/2014/11/15/chef-zero-openstack-icehouse/","summary":"こんにちは。@jedipunkzです。\nOpenStack Juno がリリースされましたが、今日は Icehouse ネタです。\nicehouse 以降、自分の中で OpenStack を自動で作る仕組みが無くなりつつあり、気軽 に OpenStack を作って色々試したい！ッていう時に手段が無く困っていました。例え ば仕事でちょっと OpenStack 弄りたい！って時に DevStack, RDO しかなく。DevStack は御存知の通り動かない可能性が結構あるし RDO は Ubuntu/Debian Gnu Linux ベース じゃないし。\nってことで、以前にも紹介した stackforge 管理の openstack-chef-repo と Chef-Zero を使って OpenStack Icehouse (Neutron) のオールインワン構成を作る方法 を書きます。ちなみに最近 Chef-Solo が Chef-Zero に置き換わりつつあるらしいです。 Chef-Zero はオンメモリで Chef サーバを起動する仕組みです。Chef-Solo と違って Chef サーバを扱う時と何も変更無く操作が出来るのでとても楽です。また、Chef サーバを 別途構、構築・管理しなくて良いので、気軽に OpenStack が作れます。\nちなみに stackforge/openstack-chef-repo の README.md に Chef-Zero での構築方法 が書いてありますが、沢山の問題があります。\n nova-network 構成 API の Endpoint が全て localhost に向いてしまうため外部から操作不可能 各コンポーネントの bind_address が localhost を向いてしまう berkshelf がそのままでは入らない  よって、今回はこれらの問題を解決しつつ \u0026ldquo;オールインワンな Neutron 構成の Icehouse OpenStack を作る方法\u0026rdquo; を書いていきます。","title":"Chef-Zero でお手軽に OpenStack Icehouse を作る"},{"content":"こんにちは。@jedipunkz です。\n昨晩 Midokura さんが Midonet を OSS 化したとニュースになりました。公式サイトは 下記の URL になっています。Midonet は OpenStack Neutron のプラグインとして動作 するソフトウェアです。\nhttp://www.midonet.org\n下記のGithub 上でソースを公開しています。\nhttps://github.com/midonet\n本体の midonet と共に midostack というレポジトリがあってどうやら公式サイトの QuickStart を見ても devstack を交えての簡単な midonet の動作が確認できそう。\nhttps://github.com/midonet/midostack\n早速使ってみる 早速 midostack を使って midonet を体験してみましょう。QuickStart には Vagrant + VirtualBox を用いた使い方が改定ありますが手元の PC 端末だとリソース が足らなくて CirrOS VM 一個すら立ち上がりませんでした。よって普通にリソースの 沢山あるサーバで稼働してみます。Vagrantfile 見ても\nconfig.vm.synced_folder \u0026quot;./\u0026quot;, \u0026quot;/midostack\u0026quot; としているだけなので、Vagrant ではなくても大丈夫でしょう。\nUbuntu Server 14.04 をインストールしたマシンを用意して midostack を取得します。\n% git clone https://github.com/midonet/midostack.git midonet_stack.sh を実行します。\n% cd midostack % ./midonet_stack.sh 暫く待つと Neutron Middonet Plugin が有効になった OpenStack が立ち上がります。 Horizon にアクセスしましょう。ユーザ名 : admin, パスワード : gogomid0 (デフォ ルト) です。\nVM も普通に立ち上がりますし VM 同士の通信も良好です。\nNeutron プロセスを確認する Neutron-Server は下記のように立ち上がっています。\n16229 pts/13 S+ 0:06 python /usr/local/bin/neutron-server --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugins/midonet/midonet.ini /etc/neutron/neutron.conf の midonet の指定はこんな感じ。\ncore_plugin = midonet.neutron.plugin.MidonetPluginV2 api_extensions_path = /opt/stack/midonet/python-neutron-plugin-midonet/midonet/neutron/extensions 次に /etc/neutron/plugins/midonet/midonet.ini を確認してみましょう。\n[midonet] # MidoNet API server URI # midonet_uri = http://localhost:8080/midonet-api # MidoNet admin username # username = admin # MidoNet admin password # password = passw0rd # ID of the project that MidoNet admin user belongs to # project_id = 77777777-7777-7777-7777-777777777777 # Virtual provider router ID # provider_router_id = 00112233-0011-0011-0011-001122334455 # Path to midonet host uuid file # midonet_host_uuid_path = /etc/midolman/host_uuid.properties [MIDONET] project_id = admin password = gogomid0 username = admin midonet_uri = http://localhost:8081/midonet-api Midonet API にアクセスする Midonet API のリファレンスが下記の URL で公開されていました。\nhttp://docs.midonet.org/docs/v1.8/rest-api/api/rest-api-specification.html\n早速使ってみましょう。まず Token を得ます。\ncurl -i \u0026#39;http://127.0.0.1:5000/v2.0/tokens\u0026#39; -X POST -H \u0026#34;Content-Type: application/json\u0026#34; -H \u0026#34;Accept: application/json\u0026#34; -d \u0026#39;{\u0026#34;auth\u0026#34;: {\u0026#34;tenantName\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;passwordCredentials\u0026#34;: {\u0026#34;username\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;gogomid0\u0026#34;}}}\u0026#39; Token ID を取得したら \u0026ldquo;/\u0026rdquo; に対してアクセスしてみましょう。\n% curl -i -X GET http://localhost:8081/midonet-api/ -H \u0026#34;User-Agent: python-keystoneclient\u0026#34; -H \u0026#34;X-Auth-Token: \u0026lt;TokenID\u0026gt;\u0026#34; レスポンス\n{ \u0026#34;routerTemplate\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/routers/{id}\u0026#34;, \u0026#34;portTemplate\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/ports/{id}\u0026#34;, \u0026#34;vipTemplate\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/vips/{id}\u0026#34;, \u0026#34;poolTemplate\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/pools/{id}\u0026#34;, \u0026#34;healthMonitorTemplate\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/health_monitors/{id}\u0026#34;, \u0026#34;healthMonitors\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/health_monitors\u0026#34;, \u0026#34;loadBalancers\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/load_balancers\u0026#34;, \u0026#34;ipAddrGroupTemplate\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/ip_addr_groups/{id}\u0026#34;, \u0026#34;tenants\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/tenants\u0026#34;, \u0026#34;tenantTemplate\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/tenants/{id}\u0026#34;, \u0026#34;portGroupTemplate\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/port_groups/{id}\u0026#34;, \u0026#34;loadBalancerTemplate\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/load_balancers/{id}\u0026#34;, \u0026#34;poolMemberTemplate\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/pool_members/{id}\u0026#34;, \u0026#34;hostVersions\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/versions\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;v1.7\u0026#34;, \u0026#34;bridgeTemplate\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/bridges/{id}\u0026#34;, \u0026#34;hostTemplate\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/hosts/{id}\u0026#34;, \u0026#34;uri\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/\u0026#34;, \u0026#34;vteps\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/vteps\u0026#34;, \u0026#34;tunnelZoneTemplate\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/tunnel_zones/{id}\u0026#34;, \u0026#34;ipAddrGroups\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/ip_addr_groups\u0026#34;, \u0026#34;writeVersion\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/write_version\u0026#34;, \u0026#34;chainTemplate\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/chains/{id}\u0026#34;, \u0026#34;vtepTemplate\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/vteps/{ipAddr}\u0026#34;, \u0026#34;adRouteTemplate\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/ad_routes/{id}\u0026#34;, \u0026#34;bgpTemplate\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/bgps/{id}\u0026#34;, \u0026#34;hosts\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/hosts\u0026#34;, \u0026#34;routeTemplate\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/routes/{id}\u0026#34;, \u0026#34;ruleTemplate\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/rules/{id}\u0026#34;, \u0026#34;systemState\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/system_state\u0026#34;, \u0026#34;vips\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/vips\u0026#34;, \u0026#34;pools\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/pools\u0026#34;, \u0026#34;routers\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/routers\u0026#34;, \u0026#34;bridges\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/bridges\u0026#34;, \u0026#34;chains\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/chains\u0026#34;, \u0026#34;portGroups\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/port_groups\u0026#34;, \u0026#34;poolMembers\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/pool_members\u0026#34;, \u0026#34;tunnelZones\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/tunnel_zones\u0026#34; } なんとなく引数にこれらの文字列を渡せばいいのだなと分かります。\n次に neutron の管理している subnets を確認してみましょう。\n% curl -i -X GET http://localhost:8081/midonet-api/neutron/subnets -H \u0026#34;User-Agent: python-keystoneclient\u0026#34; -H \u0026#34;X-Auth-Token: \u0026lt;TokenID\u0026gt;\u0026#34; レスポンス\n[ { \u0026#34;enable_dhcp\u0026#34;: false, \u0026#34;tenant_id\u0026#34;: \u0026#34;65f7012145d84ac5afc36572eabe5b09\u0026#34;, \u0026#34;host_routes\u0026#34;: [], \u0026#34;dns_nameservers\u0026#34;: [], \u0026#34;id\u0026#34;: \u0026#34;3dbe5cff-8a8c-4790-85b5-b789d8ede863\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;public-subnet\u0026#34;, \u0026#34;cidr\u0026#34;: \u0026#34;200.200.200.0/24\u0026#34;, \u0026#34;shared\u0026#34;: false, \u0026#34;ip_version\u0026#34;: 4, \u0026#34;network_id\u0026#34;: \u0026#34;45269fba-e32f-40b0-a542-f5cfe34ce1a1\u0026#34;, \u0026#34;gateway_ip\u0026#34;: \u0026#34;200.200.200.1\u0026#34;, \u0026#34;allocation_pools\u0026#34;: [ { \u0026#34;last_ip\u0026#34;: null, \u0026#34;first_ip\u0026#34;: null } ] }, { \u0026#34;enable_dhcp\u0026#34;: true, \u0026#34;tenant_id\u0026#34;: \u0026#34;f34b4398015546b8b84f50c731ed6c51\u0026#34;, \u0026#34;host_routes\u0026#34;: [], \u0026#34;dns_nameservers\u0026#34;: [], \u0026#34;id\u0026#34;: \u0026#34;3dbcf04a-9738-4b1f-b084-76f2a4b17cbc\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;private-subnet\u0026#34;, \u0026#34;cidr\u0026#34;: \u0026#34;10.0.0.0/24\u0026#34;, \u0026#34;shared\u0026#34;: false, \u0026#34;ip_version\u0026#34;: 4, \u0026#34;network_id\u0026#34;: \u0026#34;2edb78c3-0f23-4e29-a3e6-cc97f55baa6a\u0026#34;, \u0026#34;gateway_ip\u0026#34;: \u0026#34;10.0.0.1\u0026#34;, \u0026#34;allocation_pools\u0026#34;: [ { \u0026#34;last_ip\u0026#34;: null, \u0026#34;first_ip\u0026#34;: null } ] } ] ２つのサブネットが確認出来ました。\nまとめ 勉強不足でまだ全く midonet で出来る事がわからない..汗。でもとりあえず動かせた し、API も引っ張れるのでこれから色々試せそうですね。OSS 化されたことで、コミュ ニティの間でも使われていくことも想像出来ますし、自分たち技術者としてはとても有 り難いことでした。\n","permalink":"https://jedipunkz.github.io/post/2014/11/04/midostack/","summary":"こんにちは。@jedipunkz です。\n昨晩 Midokura さんが Midonet を OSS 化したとニュースになりました。公式サイトは 下記の URL になっています。Midonet は OpenStack Neutron のプラグインとして動作 するソフトウェアです。\nhttp://www.midonet.org\n下記のGithub 上でソースを公開しています。\nhttps://github.com/midonet\n本体の midonet と共に midostack というレポジトリがあってどうやら公式サイトの QuickStart を見ても devstack を交えての簡単な midonet の動作が確認できそう。\nhttps://github.com/midonet/midostack\n早速使ってみる 早速 midostack を使って midonet を体験してみましょう。QuickStart には Vagrant + VirtualBox を用いた使い方が改定ありますが手元の PC 端末だとリソース が足らなくて CirrOS VM 一個すら立ち上がりませんでした。よって普通にリソースの 沢山あるサーバで稼働してみます。Vagrantfile 見ても\nconfig.vm.synced_folder \u0026quot;./\u0026quot;, \u0026quot;/midostack\u0026quot; としているだけなので、Vagrant ではなくても大丈夫でしょう。\nUbuntu Server 14.04 をインストールしたマシンを用意して midostack を取得します。\n% git clone https://github.com/midonet/midostack.git midonet_stack.sh を実行します。\n% cd midostack % .","title":"MidoStack を動かしてみる"},{"content":"こんにちは。@jedipunkzです。\n昨晩 Chef が Chef-Container を発表しました。\n http://www.getchef.com/blog/2014/07/15/release-chef-container-0-2-0-beta/ http://docs.opscode.com/containers.html  まだ Beta リリースでバージョンは 0.2.0 です。(gem だと 0.1.1)\nDocker を代表とするコンテナ周りの技術が最近、盛んにリリースされていますし、今 後クラウドプラットフォーム上でコンテナを使ってアプリを動かすケースも増えてくる のではないでしょうか。Dockerfile を使っても Chef-Solo を使ってソフトウェアをデ プロイ出来るのだけどそれだけだとしんどいので、コンテナに特化した Chef が出てき たってことだと思います。特徴として SSH でログインしてブートストラップするので はなくて Runit + Chef-init を用いてコンテナにデプロイすることが挙げられます。\nでは実際に使ってみたのでその時の手順をまとめてみます。\n事前に用意する環境 下記のソフトウェアを予めインストールしておきましょう。\n docker chef berkshelf  ここで注意なのですが後に knife コマンドを使って Docker イメージをビルドします。 つまり root 権限が必要です。rbenv 等を使って ruby, chef をインストールすると、 辛いかもしれませんので OS のパッケージを使ってインストールすると良いと思います。 この辺りは今後改善策が出てくるかも\u0026hellip;。\n尚、インストール方法はここでは割愛します。\nChef-Container のインストール 下記の2つの Gems をインストールします。\n knife-container chef-container  % sudo gem install knife-container % sudo gem install chef-container 使用方法 まず knife コマンドを使って操作に必要なディレクトリとファイルを生成します。\n% knife container docker init chef/ubuntu-12.04 -r \u0026#39;recipe[apache2]\u0026#39; -z -b ここで \u0026lsquo;chef/ubuntu-12.04\u0026rsquo; は Docker のイメージ名です。chef-init 等の環境が予 め入っていました。このイメージ以外では今のところ動作を確認していません..。これは後にまとめで触れます。\n上記のコマンドの結果で得られるディレクトリとファイル達です。\n. └── dockerfiles └── chef └── ubuntu-12.04 ├── Berksfile ├── chef │ ├── first-boot.json │ └── zero.rb └── Dockerfile また dockerfiles/chef/ubuntu-12.04/Dockerfile を確認すると\u0026hellip;\n# BASE chef/ubuntu-12.04:latest FROM chef/ubuntu-12.04 ADD chef/ /etc/chef/ RUN chef-init --bootstrap RUN rm -rf /etc/chef/secure/* ENTRYPOINT [\u0026quot;chef-init\u0026quot;] CMD [\u0026quot;--onboot\u0026quot;] イメージを取得 -\u0026gt; ディレクトリ同期 -\u0026gt; chef-init 実行 -\u0026gt; /etc/chef/secure 配下削除、と 実行しているようです。\n次に first-boot.json という名前のファイルを生成します。chef-init が解釈するファ イルです。\n{ \u0026#34;run_list\u0026#34;: [ \u0026#34;recipe[apache2]\u0026#34; ], \u0026#34;container_service\u0026#34;: { \u0026#34;apache2\u0026#34;: { \u0026#34;command\u0026#34;: \u0026#34;/usr/sbin/apache2 -k start\u0026#34; } } } ではいよいよ knife コマンドで Docker イメージをビルドします。\n% sudo knife container docker build chef/ubuntu-12.04 -z すると、下記のように Docker イメージが出来上がります。\n% sudo docker images REPOSITORY TAG IMAGE ID CREATED VIRTUAL SIZE chef/ubuntu-12.04 11 03fd2357596f 4 days ago 397.7 MB chef/ubuntu-12.04 11.12 03fd2357596f 4 days ago 397.7 MB chef/ubuntu-12.04 11.12.8 03fd2357596f 4 days ago 397.7 MB 出来上がったイメージを利用してコンテナを稼働します。\n% sudo docker run chef/ubuntu-12.04 % sudo docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 191cfdaf0bdb 650a89f73ed8 chef-init --onboot 39 minutes ago Up 39 minutes agitated_almeida まとめ コンテナと言っても今現在は Docker のみに対応しているようです。また init の際に指定する Docker イメージ の中に chef-init が入っている必要がありそうです。Build する前に予めイメージを作っておく必要があるという のはしんどいので、今後改善されるかもしれません。\nそもそも Docker やコンテナ技術の登場で Puppet, Chef を代表とするツール類が不要になるのでは？という議論が 幾つかの場面であったように思います。つまりコンテナのイメージに予めソフトウェアを配布しそれを用いて稼働 することで、マシンが起動した後にデプロイすることが必要ないよね？という発想です。今回紹介したようにコンテナの イメージを生成するのに Chef を用いるということであれば、また別の議論になりそうです。また稼働したコンテナに ソフトウェアをデプロイすることも場合によっては必要なので、この辺りの技術の完成度が上がることを期待したいです。\n参考 URL  CreationLine さんブログ http://www.creationline.com/lab/5346 公式サイト http://docs.opscode.com/containers.html  ","permalink":"https://jedipunkz.github.io/post/2014/07/16/chef-container/","summary":"こんにちは。@jedipunkzです。\n昨晩 Chef が Chef-Container を発表しました。\n http://www.getchef.com/blog/2014/07/15/release-chef-container-0-2-0-beta/ http://docs.opscode.com/containers.html  まだ Beta リリースでバージョンは 0.2.0 です。(gem だと 0.1.1)\nDocker を代表とするコンテナ周りの技術が最近、盛んにリリースされていますし、今 後クラウドプラットフォーム上でコンテナを使ってアプリを動かすケースも増えてくる のではないでしょうか。Dockerfile を使っても Chef-Solo を使ってソフトウェアをデ プロイ出来るのだけどそれだけだとしんどいので、コンテナに特化した Chef が出てき たってことだと思います。特徴として SSH でログインしてブートストラップするので はなくて Runit + Chef-init を用いてコンテナにデプロイすることが挙げられます。\nでは実際に使ってみたのでその時の手順をまとめてみます。\n事前に用意する環境 下記のソフトウェアを予めインストールしておきましょう。\n docker chef berkshelf  ここで注意なのですが後に knife コマンドを使って Docker イメージをビルドします。 つまり root 権限が必要です。rbenv 等を使って ruby, chef をインストールすると、 辛いかもしれませんので OS のパッケージを使ってインストールすると良いと思います。 この辺りは今後改善策が出てくるかも\u0026hellip;。\n尚、インストール方法はここでは割愛します。\nChef-Container のインストール 下記の2つの Gems をインストールします。\n knife-container chef-container  % sudo gem install knife-container % sudo gem install chef-container 使用方法 まず knife コマンドを使って操作に必要なディレクトリとファイルを生成します。","title":"Chef-Container Beta を使ってみる"},{"content":"こんにちは。@jedipunkzです。\n今日、JTF2014 (July Tech Festa 2014) というイベントで Ceph のことを話してきま した。Ceph ユーザ会の会員として話してきたのですが Ceph ユーザ会は実は最近発足 したばかりのユーザ会で、まだまだ活動が活発ではありません。もし興味がある方いらっ しゃいましたら是非参加よろしくお願いしますー。下記の Google Groups になります。\nhttps://groups.google.com/forum/#!forum/ceph-jp\nユーザ会としての勉強会として初になるのですが、今回このイベントで自分は Ceph-Deploy について話してきました。とりあえず皆さんに使ってもらいたかったので この話をしてきました。が、予定時間がメチャ短かったので超絶早口で頑張った分、皆 さんに理解してもらえなかった気がしてちょっと反省\u0026hellip;。なので、このブログを利用 して少し細くさせてもらいます。\n今日の発表資料はこちらです！\n今日のテーマは 「Ceph-Deploy を使って Ceph を構築してみる」だったのですが、下 記のテーマを持って資料を作っています。\n 単にミニマム構成ではなく運用を考慮した実用性のある構成 OSD, MON, MDS の各プロセスとノード・ディスクの数の関係を知ってもらう  特に「実用性のある..」は意識したつもりでした。そのために前提とした構成に下記の 特徴を持たせています。(資料 6 ページ目に構成図があります。確認してみてください。)\n オブジェクト格納用ディスクは複数/ノードを前提 OSD レプリケーションのためのクラスタネットワークを用いる構成 OSD の扱うジャーナル格納用ディスクは高速な SSD を用いる MDS は利用する HW リソースの特徴が異なるので別ノードへ配置  ストレージ全体を拡張したければ\n 図中 ceph01-03 の様なノードを増設する ceph01-03 にディスクとそれに対する OSD を増設する  ですが、前者がベストでしょう。ノード増設の場合 ceph-deploy を用いて\n ceph-deploy mon create \u0026lt;新規ホスト名\u0026gt; で MON を稼働 ceph-dploy disk zap, osd create で OSD を稼働  で簡単に可能です。MDS の増設も負荷状況を見ながらするといいでしょう。自分はまだ Ceph を運用していないので、各プロセスがどのようなリソースの消費の仕方をするの か知りません。MDS がどのような数で運用していくべきなのか。早く運用から得たノウ ハウが共有されないかなぁと期待しています。\nまた今回話すのを忘れたのですが SSD をジャーナル格納用ディスクとして用いたのは ハードディスクに対して高速でアクセス出来ること・またメタデータはファイルオブジェ クトに対して小容量で済む、といった理由からです。メタデータを扱うのに適している と思います。また将来的には幾つかの KVS データベースソフトウェアをメタデータ管 理に使う実装がされるそうです。\n以上です。皆さん、是非 Ceph を使ってみてください！ また興味のある方はユーザ会 への加入をご検討くださいー。\n","permalink":"https://jedipunkz.github.io/post/2014/06/22/jtf2014-ceph/","summary":"こんにちは。@jedipunkzです。\n今日、JTF2014 (July Tech Festa 2014) というイベントで Ceph のことを話してきま した。Ceph ユーザ会の会員として話してきたのですが Ceph ユーザ会は実は最近発足 したばかりのユーザ会で、まだまだ活動が活発ではありません。もし興味がある方いらっ しゃいましたら是非参加よろしくお願いしますー。下記の Google Groups になります。\nhttps://groups.google.com/forum/#!forum/ceph-jp\nユーザ会としての勉強会として初になるのですが、今回このイベントで自分は Ceph-Deploy について話してきました。とりあえず皆さんに使ってもらいたかったので この話をしてきました。が、予定時間がメチャ短かったので超絶早口で頑張った分、皆 さんに理解してもらえなかった気がしてちょっと反省\u0026hellip;。なので、このブログを利用 して少し細くさせてもらいます。\n今日の発表資料はこちらです！\n今日のテーマは 「Ceph-Deploy を使って Ceph を構築してみる」だったのですが、下 記のテーマを持って資料を作っています。\n 単にミニマム構成ではなく運用を考慮した実用性のある構成 OSD, MON, MDS の各プロセスとノード・ディスクの数の関係を知ってもらう  特に「実用性のある..」は意識したつもりでした。そのために前提とした構成に下記の 特徴を持たせています。(資料 6 ページ目に構成図があります。確認してみてください。)\n オブジェクト格納用ディスクは複数/ノードを前提 OSD レプリケーションのためのクラスタネットワークを用いる構成 OSD の扱うジャーナル格納用ディスクは高速な SSD を用いる MDS は利用する HW リソースの特徴が異なるので別ノードへ配置  ストレージ全体を拡張したければ\n 図中 ceph01-03 の様なノードを増設する ceph01-03 にディスクとそれに対する OSD を増設する  ですが、前者がベストでしょう。ノード増設の場合 ceph-deploy を用いて\n ceph-deploy mon create \u0026lt;新規ホスト名\u0026gt; で MON を稼働 ceph-dploy disk zap, osd create で OSD を稼働  で簡単に可能です。MDS の増設も負荷状況を見ながらするといいでしょう。自分はまだ Ceph を運用していないので、各プロセスがどのようなリソースの消費の仕方をするの か知りません。MDS がどのような数で運用していくべきなのか。早く運用から得たノウ ハウが共有されないかなぁと期待しています。","title":"JTF2014 で Ceph について話してきた！"},{"content":"こんにちは。@jedipunkz です。\n以前 Mesos, Docker について記事にしました。\nhttp://jedipunkz.github.io/blog/2013/09/28/mesos-architecture-number-1/ http://jedipunkz.github.io/blog/2013/10/01/methos-architecture-number-2-docker-on-mesos/\nTwitter で Docker 関連のオーケストレーションツールについて呟いていたら @everpeace さんから こんな情報をもらいました。\nDeimos !!! 知らなかった。Mesos の Docker プラグインらしく下記の場所にありました。\nhttps://github.com/mesosphere/deimos\n色々調べいたら、こんな資料が見つかりました。どうやらまだ公開されて4日しか経っていないようです。\nhttp://mesosphere.io/learn/run-docker-on-mesosphere/\nMesos + Marathon + Deimos + Docker をオールインワン構成で構築する手順が書かれています。\n内容はほぼ同じですが、一応自分がやってみて理解したことをまとめたいので下記に記していきます。\n構築してみる 手順をまとめてスクリプトにしました。パッケージは Ubuntu 13.10 用のようですが 14.04 のホスト で実行出来ました。14.04 のパッケージはまだ見つかっていません。\n#!/bin/bash # disable ipv6 echo \u0026#39;net.ipv6.conf.all.disable_ipv6 = 1\u0026#39; | sudo tee -a /etc/sysctl.conf echo \u0026#39;net.ipv6.conf.default.disable_ipv6 = 1\u0026#39; | sudo tee -a /etc/sysctl.conf sudo sysctl -p # install related tools sudo apt-get update sudo apt-get -y install curl python-setuptools python-pip python-dev python-protobuf # install zookeeper sudo apt-get -y install zookeeperd echo 1 | sudo dd of=/var/lib/zookeeper/myid # install docker sudo apt-get -y install docker.io sudo ln -sf /usr/bin/docker.io /usr/local/bin/docker sudo sed -i \u0026#39;$acomplete -F _docker docker\u0026#39; /etc/bash_completion.d/docker.io sudo docker pull libmesos/ubuntu # install mesos curl -fL http://downloads.mesosphere.io/master/ubuntu/13.10/mesos_0.19.0-xcon3_amd64.deb -o /tmp/mesos.deb sudo dpkg -i /tmp/mesos.deb sudo mkdir -p /etc/mesos-master echo in_memory | sudo dd of=/etc/mesos-master/registry curl -fL http://downloads.mesosphere.io/master/ubuntu/13.10/mesos_0.19.0-xcon3_amd64.egg -o /tmp/mesos.egg sudo easy_install /tmp/mesos.egg # install marathon curl -fL http://downloads.mesosphere.io/marathon/marathon_0.5.0-xcon2_noarch.deb -o /tmp/marathon.deb sudo dpkg -i /tmp/marathon.deb # restart each services sudo service docker.io restart sudo service zookeeper restart sudo service mesos-master restart sudo service mesos-slave restart # install deimos sudo pip install deimos sudo mkdir -p /etc/mesos-slave ## Configure Deimos as a containerizer echo /usr/bin/deimos | sudo dd of=/etc/mesos-slave/containerizer_path echo external | sudo dd of=/etc/mesos-slave/isolation プロセスの確認 実行が終わると各プロセスが確認出来ます。オプションでどのプロセスが何を見ているか大体 わかりますので見ていきます。\nmesos-master mesos-master は zookeeper を参照して 5050 番ポートで起動しているようです。\n% ps ax | grep mesos-master 1224 ? Ssl 0:30 /usr/local/sbin/mesos-master --zk=zk://localhost:2181/mesos --port=5050 --log_dir=/var/log/mesos --registry=in_memory mesos-slave mesos-slave は同じく zookeeper を参照して containerizer を deimos として稼働していることが わかります。\n% ps ax | grep mesos-slave 1225 ? Ssl 0:12 /usr/local/sbin/mesos-slave --master=zk://localhost:2181/mesos --log_dir=/var/log/mesos --containerizer_path=/usr/bin/deimos --isolation=external zookeeper zookeeper は OpenJDK7 で稼働している Java プロセスです。\n% ps ax | grep zookeeper 1073 ? Ssl 1:07 /usr/bin/java -cp /etc/zookeeper/conf:/usr/share/java/jline.jar:/usr/share/java/log4j-1.2.jar:/usr/share/java/xercesImpl.jar:/usr/share/java/xmlParserAPIs.jar:/usr/share/java/netty.jar:/usr/share/java/slf4j-api.jar:/usr/share/java/slf4j-log4j12.jar:/usr/share/java/zookeeper.jar -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.local.only=false -Dzookeeper.log.dir=/var/log/zookeeper -Dzookeeper.root.logger=INFO,ROLLINGFILE org.apache.zookeeper.server.quorum.QuorumPeerMain /etc/zookeeper/conf/zoo.cfg docker docker が起動していることも確認できます。設定は特にしていないです。\n% ps axuw | grep docker root 831 0.0 0.3 364776 14924 ? Sl 01:30 0:01 /usr/bin/docker.io -d Marathon の WebUI にアクセス Marathon の WebUI にアクセスしてみましょう。\nまだ何も Tasks が実行されていないので一覧には何も表示されないと思います。\nTasks の実行 Marathon API に対してクエリを発行することで Mesos の Tasks として Docker コンテナを稼働させることが出来ます。 下記のファイルを ubuntu.json として保存。\n{ \u0026#34;container\u0026#34;: { \u0026#34;image\u0026#34;: \u0026#34;docker:///libmesos/ubuntu\u0026#34; }, \u0026#34;id\u0026#34;: \u0026#34;ubuntu\u0026#34;, \u0026#34;instances\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;cpus\u0026#34;: \u0026#34;.5\u0026#34;, \u0026#34;mem\u0026#34;: \u0026#34;512\u0026#34;, \u0026#34;uris\u0026#34;: [ ] } 下記の通り localhost:8080 が Marathon API の Endpoint になっているのでここに対して作成した JSON を POST します。\n% curl -X POST -H \u0026#34;Content-Type: application/json\u0026#34; localhost:8080/v2/apps -d@ubuntu.json Tasks の一覧を取得してみます。\n% curl -X GET -H \u0026#34;Content-Type: application/json\u0026#34; localhost:8080/v2/apps {\u0026#34;apps\u0026#34;:[{\u0026#34;id\u0026#34;:\u0026#34;ubuntu\u0026#34;,\u0026#34;cmd\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;env\u0026#34;:{},\u0026#34;instances\u0026#34;:1,\u0026#34;cpus\u0026#34;:0.5,\u0026#34;mem\u0026#34;:512.0,\u0026#34;executor\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;constraints\u0026#34;:[],\u0026#34;uris\u0026#34;:[],\u0026#34;ports\u0026#34;:[13049],\u0026#34;taskRateLimit\u0026#34;:1.0,\u0026#34;container\u0026#34;:{\u0026#34;image\u0026#34;:\u0026#34;docker:///libmesos/ubuntu\u0026#34;,\u0026#34;options\u0026#34;:[]},\u0026#34;version\u0026#34;:\u0026#34;2014-06-13T01:45:58.693Z\u0026#34;,\u0026#34;tasksStaged\u0026#34;:1,\u0026#34;tasksRunning\u0026#34;:0}]} Tasks の一覧が JSON で返ってきます。id : ubuntu, インスタンス数 : 1, CPU 0.5, メモリー : 512MB で Task が稼働していることが確認出来ます。\nここで WebUI 側も見てみましょう。\n一つ Task が稼働していることが確認出来ると思います。\nその Task をクリックすると詳細が表示されます。\n次に Tasks のスケーリングを行ってみましょう。 下記の通り ubuntu.json を修正し instances : 2 とする。これによってインスタンス数が2に増えます。\n{ \u0026#34;container\u0026#34;: { \u0026#34;image\u0026#34;: \u0026#34;docker:///libmesos/ubuntu\u0026#34; }, \u0026#34;id\u0026#34;: \u0026#34;ubuntu\u0026#34;, \u0026#34;instances\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;cpus\u0026#34;: \u0026#34;.5\u0026#34;, \u0026#34;mem\u0026#34;: \u0026#34;512\u0026#34;, \u0026#34;uris\u0026#34;: [ ] } 修正した JSON を POST します。\n% curl -X PUT -H \u0026#34;Content-Type: application/json\u0026#34; localhost:8080/v2/apps/ubuntu -d@ubuntu.json Tasks の一覧を取得し containers が 2 になっていることが確認できます。\n% curl -X GET -H \u0026#34;Content-Type: application/json\u0026#34; localhost:8080/v2/apps {\u0026#34;apps\u0026#34;:[{\u0026#34;id\u0026#34;:\u0026#34;ubuntu\u0026#34;,\u0026#34;cmd\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;env\u0026#34;:{},\u0026#34;instances\u0026#34;:2,\u0026#34;cpus\u0026#34;:0.5,\u0026#34;mem\u0026#34;:512.0,\u0026#34;executor\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;constraints\u0026#34;:[],\u0026#34;uris\u0026#34;:[],\u0026#34;ports\u0026#34;:[17543],\u0026#34;taskRateLimit\u0026#34;:1.0,\u0026#34;container\u0026#34;:{\u0026#34;image\u0026#34;:\u0026#34;docker:///libmesos/ubuntu\u0026#34;,\u0026#34;options\u0026#34;:[]},\u0026#34;version\u0026#34;:\u0026#34;2014-06-13T02:40:04.536Z\u0026#34;,\u0026#34;tasksStaged\u0026#34;:3,\u0026#34;tasksRunning\u0026#34;:0}]} 最後に Tasks を削除してみましょう。\n% curl -X DELETE -H \u0026#34;Content-Type: application/json\u0026#34; localhost:8080/v2/apps/ubuntu Tasks が削除されたことを確認します。\n% curl -X GET -H \u0026#34;Content-Type: application/json\u0026#34; localhost:8080/v2/apps {\u0026#34;apps\u0026#34;:[]} Marathon API v2 Marathon API v2 について下記の URL に仕様が載っています。上記に記したクエリ以外にも色々載っているので 動作を確認してみるといいと思います。\nhttps://github.com/mesosphere/marathon/blob/master/docs/api/http/REST_template.md\nまとめ オールインワン構成が出来ました。また動作確認も無事出来ています。 以前試した時よりも大分、手順が簡潔になった印象があります。また参考資料中に\n\u0026ldquo;checkout our other multi-node tutorials on how to scale Docker in your data center.\u0026rdquo;\nとありますが、まだ見つかっていません(´・ω・｀)見つかった方教えてくださいー。\n以前試した時は Mesos-Master の冗長化が出来なかったので今回こそ Multi Mesos-Masters, Multi Mesos-Slaves の構成を作ってみたいと思います。\nまた今月？になって続々と Docker のオーケストレーションツールを各社が公開しています。\ncenturion New Relic が開発したオーケストレーションツール。 https://github.com/newrelic/centurion\nhelios Spotify が開発したオーケストレーションツール。 https://github.com/spotify/helios\nfleet CoreOS 標準搭載。 https://github.com/coreos/fleet\ngeard RedHat が Red Hat Enterprise Linux Atomic Host に搭載しているツール。 http://openshift.github.io/geard/\nKubernetes Google が開発したオーケストレーションツール。 https://github.com/GoogleCloudPlatform/kubernetes\nshipper Python のコードで Docker をオーケストレーション出来るツール。 https://github.com/mailgun/shipper\n幾つか試したのですが、まだまだ動く所までいかないツールがありました。github の README にも \u0026ldquo;絶賛開発中なのでプロダクトレディではない\u0026rdquo; と書かれています。これからでしょう。\n","permalink":"https://jedipunkz.github.io/post/2014/06/13/mesos-marathon-deimos-docker/","summary":"こんにちは。@jedipunkz です。\n以前 Mesos, Docker について記事にしました。\nhttp://jedipunkz.github.io/blog/2013/09/28/mesos-architecture-number-1/ http://jedipunkz.github.io/blog/2013/10/01/methos-architecture-number-2-docker-on-mesos/\nTwitter で Docker 関連のオーケストレーションツールについて呟いていたら @everpeace さんから こんな情報をもらいました。\nDeimos !!! 知らなかった。Mesos の Docker プラグインらしく下記の場所にありました。\nhttps://github.com/mesosphere/deimos\n色々調べいたら、こんな資料が見つかりました。どうやらまだ公開されて4日しか経っていないようです。\nhttp://mesosphere.io/learn/run-docker-on-mesosphere/\nMesos + Marathon + Deimos + Docker をオールインワン構成で構築する手順が書かれています。\n内容はほぼ同じですが、一応自分がやってみて理解したことをまとめたいので下記に記していきます。\n構築してみる 手順をまとめてスクリプトにしました。パッケージは Ubuntu 13.10 用のようですが 14.04 のホスト で実行出来ました。14.04 のパッケージはまだ見つかっていません。\n#!/bin/bash # disable ipv6 echo \u0026#39;net.ipv6.conf.all.disable_ipv6 = 1\u0026#39; | sudo tee -a /etc/sysctl.conf echo \u0026#39;net.ipv6.conf.default.disable_ipv6 = 1\u0026#39; | sudo tee -a /etc/sysctl.conf sudo sysctl -p # install related tools sudo apt-get update sudo apt-get -y install curl python-setuptools python-pip python-dev python-protobuf # install zookeeper sudo apt-get -y install zookeeperd echo 1 | sudo dd of=/var/lib/zookeeper/myid # install docker sudo apt-get -y install docker.","title":"Mesos + Marathon + Deimos + Docker を試してみた!"},{"content":"こんにちは。@jedipunkzです。\n最近 OpenStack でサービスを開発！.. じゃなくて AWS でプロトタイプサービス作っ ているのですが、Ruby で開発したかったので Fog を使っています。EC2 と ELB の API を叩くコードになりつつあるのですが、サンプルコードって世の中に中々無いと気 がついたので、このブログ記事にサンプルコードを載せたいと思います。\nFog とは ? Fog http://fog.io/ はクラウドライブラリソフトウェアです。AWS, Rackspace, CloudStack, OpenStack .. と数ある世の中のクラウドプラットフォームを扱うために 用意されたソフトウェアです。対応しているプラットフォームの種別は下記を見ると参 考になります。\nhttp://fog.io/about/provider_documentation.html\nドキュメントがまだまだ揃っていなく、Fog のコードを覗きながら実装するしかない状 況です。なので「こう使えば良い！」というお手本があまりネット上にも無い気がしま す。\nドキュメントは一応下記にあります。 が使い方がよくわからない・・！(´；ω；｀)ﾌﾞﾜｯ\nhttp://rubydoc.info/gems/fog/frames/index\nEC2 インスタンスを使ってみる まずは AWS EC2 の API を叩いて t1.micro インスタンスを立ち上げてみましょう。\nrequire \u0026#39;fog\u0026#39; compute = Fog::Compute.new({ :provider =\u0026gt; \u0026#39;AWS\u0026#39;, :aws_access_key_id =\u0026gt; \u0026#39;....\u0026#39;, :aws_secret_access_key =\u0026gt; \u0026#39;....\u0026#39;, :region =\u0026gt; \u0026#39;ap-northeast-1\u0026#39; }) server = compute.servers.create( :image_id =\u0026gt; \u0026#39;ami-cedaa2bc\u0026#39;, :flavor_id =\u0026gt; \u0026#39;t1.micro\u0026#39;, :key_name =\u0026gt; \u0026#39;test_key\u0026#39;, :tags =\u0026gt; {\u0026#39;Name\u0026#39; =\u0026gt; \u0026#39;test\u0026#39;}, :groups =\u0026gt; \u0026#39;ssh-secgroup\u0026#39; ) server.wait_for { print \u0026#34;.\u0026#34;; ready? } puts \u0026#34;created instance name :\u0026#34;, server.dns_name 解説  compute = \u0026hellip; とあるところで接続情報を記しています。  \u0026ldquo;ACCESS_KEY_ID\u0026rdquo; や \u0026ldquo;SECRET_ACCESS_KEY\u0026rdquo; はみなさん接続する時にお持ちですよね。それ とリージョン名やプロバイダ名 (ここでは AWS) を記して AWS の API に接続します。\n server = \u0026hellip; とあるところで実際にインスタンスを作成しています。  ここではインスタンス生成に必要な情報を盛り込んでいます。Flavor 名や AMI イメー ジ名・SSH 鍵の名前・セキュリティグループ名等です。\n便利なメソッド server = \u0026hellip; でインスタンスを生成すると便利なメソッドを扱って情報を読み込むこ とが出来ます。\nserver.dns_name # =\u0026gt; public な DNS 名を取得 server.private_dns_name # =\u0026gt; private な DNS 名を取得 server.id # =\u0026gt; インスタンス ID を取得 server.availability_zone # =\u0026gt; Availability Zone を取得 server.public_ip_address # =\u0026gt; public な IP アドレスを取得 server.private_ip_address # =\u0026gt; private な IP アドレスを取得 これは便利\u0026hellip;\nモジュール化して利用 +++\n毎回コードの中でこれらの接続情報を書くのはしんどいので、Ruby のモジュールを作 りましょう。\nmodule AWSCompute def self.connect() conn = Fog::Compute.new({ :provider =\u0026gt; \u0026#39;AWS\u0026#39;, :aws_access_key_id =\u0026gt; \u0026#39;...\u0026#39;, :aws_secret_access_key =\u0026gt; \u0026#39;...\u0026#39;, :region =\u0026gt; \u0026#39;...\u0026#39; }) begin yield conn ensure # conn.close end rescue Errno::ECONNREFUSED end end こう書いておくと例えば\u0026hellip;\nインスタンスのターミネイト AWSCompute.connect() do |sock| server = sock.servers.get(instance_id) server.destroy return server.id end インスタンスの起動 AWSCompute.connect() do |sock| server = sock.servers.get(instance_id) server.start return server.id end インスタンスの停止 AWSCompute.connect() do |sock| server = sock.servers.get(instance_id) server.stop return server.id end 等と出来ます。\nELB (Elastic LoadBalancer) を使ってみる 同様に ELB を扱うコードのサンプルも載せておきます。同じくモジュール化して書くと\nmodule AWSELB def self.connect() conn = Fog::AWS::ELB.new( :aws_access_key_id =\u0026gt; \u0026#39;...\u0026#39;, :aws_secret_access_key =\u0026gt; \u0026#39;...\u0026#39;, :region =\u0026gt; \u0026#39;...\u0026#39;, ) begin yield conn ensure # conn.close end rescue Errno::ECONNREFUSED end end としておいて\u0026hellip;\nELB の新規作成 下記のコードで ELB を新規作成出来ます。\nAWSELB.connect() do |sock| availability_zone = \u0026#39;...\u0026#39; elb_name = \u0026#39;...\u0026#39; listeners = [{ \u0026#34;Protocol\u0026#34; =\u0026gt; \u0026#34;HTTP\u0026#34;, \u0026#34;LoadBalancerPort\u0026#34; =\u0026gt; 80, \u0026#34;InstancePort\u0026#34; =\u0026gt; 80, \u0026#34;InstanceProtocol\u0026#34; =\u0026gt; \u0026#34;HTTP\u0026#34; }] result = sock.create_load_balancer(availability_zone, elb_name, listeners) p result end この状態では ELB に対してインスタンスが紐付けられていないので使えません。下記の操作で インスタンスを紐付けてみましょう。\nAWSELB.connect() do |sock| insntance_id = \u0026#39;...\u0026#39; elb_name = \u0026#39;...\u0026#39; result = sock.register_instances_with_load_balancer(instance_id, elbname) p result end insntance_id には紐付けたいインスタンスの ID を、elb_name には先ほど作成した ELB の名前を 入力します。 この操作を繰り返せば AWS 上にクラスタが構成出来ます。\n逆にクラスタからインスタンスの削除したい場合は下記の通り実行します。\nAWSELB.connect() do |sock| result = sock.deregister_instances_from_load_balancer(instance_id, elbname) p result end まとめ 今回は Fog を紹介しましたが Python 使いの方には libcloud をおすすめします。\nhttps://libcloud.apache.org/\nApache ファウンデーションが管理しているクラウドライブラリです。こちらも複数の クラウドプラットフォームに対応しているようです。\nFog で OpenStack も操作したことがあるのですが、AWS 用のコードの方が完成度が高 いのか、戻り値などが綺麗に整形されていて扱いやすかったり、メソッドも豊富に用意 されていたりという印象でした。これは\u0026hellip; OpenStack 用の Fog コードにコントリビュー トするチャンス・・！\n皆さんもサンプルコードお持ちでしたら、ブログ等で公開していきましょうー。 ではでは。\n","permalink":"https://jedipunkz.github.io/post/2014/05/29/fog-aws-ec2-elb/","summary":"こんにちは。@jedipunkzです。\n最近 OpenStack でサービスを開発！.. じゃなくて AWS でプロトタイプサービス作っ ているのですが、Ruby で開発したかったので Fog を使っています。EC2 と ELB の API を叩くコードになりつつあるのですが、サンプルコードって世の中に中々無いと気 がついたので、このブログ記事にサンプルコードを載せたいと思います。\nFog とは ? Fog http://fog.io/ はクラウドライブラリソフトウェアです。AWS, Rackspace, CloudStack, OpenStack .. と数ある世の中のクラウドプラットフォームを扱うために 用意されたソフトウェアです。対応しているプラットフォームの種別は下記を見ると参 考になります。\nhttp://fog.io/about/provider_documentation.html\nドキュメントがまだまだ揃っていなく、Fog のコードを覗きながら実装するしかない状 況です。なので「こう使えば良い！」というお手本があまりネット上にも無い気がしま す。\nドキュメントは一応下記にあります。 が使い方がよくわからない・・！(´；ω；｀)ﾌﾞﾜｯ\nhttp://rubydoc.info/gems/fog/frames/index\nEC2 インスタンスを使ってみる まずは AWS EC2 の API を叩いて t1.micro インスタンスを立ち上げてみましょう。\nrequire \u0026#39;fog\u0026#39; compute = Fog::Compute.new({ :provider =\u0026gt; \u0026#39;AWS\u0026#39;, :aws_access_key_id =\u0026gt; \u0026#39;....\u0026#39;, :aws_secret_access_key =\u0026gt; \u0026#39;....\u0026#39;, :region =\u0026gt; \u0026#39;ap-northeast-1\u0026#39; }) server = compute.servers.create( :image_id =\u0026gt; \u0026#39;ami-cedaa2bc\u0026#39;, :flavor_id =\u0026gt; \u0026#39;t1.","title":"クラウドライブラリ Fog で AWS を操作！..のサンプル"},{"content":"こんにちは。@jedipunkzです。\nまたまた OpenStack のデプロイをどうするか？についてです。\n今まで自分の中では Rackspace Private Cloud で使われている Rackspace 管理の rcbops/chef-cookbooks が今現在使うならベストの選択だと思っていました。これは内 部で Chef が使われていてしかも Cookbooks が Github 上で公開されています。 Apache ライセンスで使えるので、サービス構築にも使えちゃうというモノ。\n先日、ある OpenStack コアデベロッパーの方から「jedipunkz さん、やっぱり rcbops がいいですか？運営とかどうなっているんでしょう？マージの規準とかどうなのかな？」 と質問受けました。確かにマージの基準は Rackspace Private Cloud がベースになり ますし、管理しているエンジニアの一覧を見ていると Rackspace 社のエンジニアがメ インですし、今後どうなるのか分からない\u0026hellip;。\n逃げ道は用意したほうが良さそう。\nということで、以前自分も暑かったことのある StackForge の openstack-chef-repo を久々に使ってみました。Icehouse 構成がこの時点で既に組めるようになっていて、 以前よりだい〜ぶ完成度増した感があります。今回は nova-network 構成を作ってみた のですが、Neutron 構成ももちろん出来そうなので後に調べてまた公開したいです。\nStackForge とは StackForge は OpenStack のデプロイ・CI の仕組みとして公式に用いられているもの。 公式サイトは下記の場所にある。\nhttp://ci.openstack.org/stackforge.html\nStackForge の openstack-chef-repo は下記の場所にある。\nhttps://github.com/stackforge/openstack-chef-repo\nopenstack-chef-repo はまだ \u0026lsquo;stable/icehouse\u0026rsquo; ブランチが生成されていない。が直 ちに master からブランチが切られる様子。\n目的 StackForge の openstack-chef-repo を用いて Icehouse リリース版の OpenStack を デプロイするための方法を記す。今回は未だ \u0026lsquo;stable/icehouse\u0026rsquo; ブランチが無いので master ブランチを用いて Icehouse リリース版 OpenStack を構築する。\n構成 構成はこんな感じ。\n +-----------------+ | GW Router | +--+-----------------+ | | | +-------------------+-------------------+-------------------------------------- | |eth0 |eth0 |eth0 VM Network (fixed) | +-----------------+ +-----------------+ +-----------------+ +-----------------+ | | Controller Node | | Compute Node | | Compute Node | | Chef Workstation| | +-----------------+ +-----------------+ +-----------------+ +-----------------+ | |eth1 |eth1 |eth1 | +--+-------------------+-------------------+-------------------+------------------ API/Management Network  Nova-Network 構成 今回は fixed network 用の NIC を eth0(物理 NIC) にアサイン fixed network 用のネットワークをパブリックにする API/Management Network 側に全ての API を出す。またここから The Internet に迂回出来るようにする VM Network も GW を介して The Internet へ迂回出来るようにする 全ての操作は \u0026lsquo;Chef Workstaion\u0026rsquo; から行う Compute ノードはキャパシティの許す限り何台でも可  IP 一覧 (この記事での例)\n Controller : 10.200.9.46 (eth0), 10.200.10.46 (eth1) Compute : 10.200.9.47 (eth0), 10.200.10.47 (eth1) Compute : 10.200.9.48 (eth0), 10.200.10.48 (eth1)  手順 openstack-chef-repo をワークステーションノード上で取得する。\n% git clone https://github.com/stackforge/openstack-chef-repo.git % cd openstack-chef-repo Berksfile があるのでこれを用いて Chef Cookbooks を取得する。\n% berks install --path=./cookbooks Roles, Cookbooks を Chef サーバにアップロードする。\n% knife cookbook upload -o cookbooks -a % knife role from file roles/*.rb 1 Environment に対して 1 OpenStack クラスタである。今回構築するクラスタのため の Environment を作成する。\n下記を environments/icehouse-nova-network.rb として生成する。\nname \u0026#34;icehouse-nova-network\u0026#34; description \u0026#34;separated nodes environment\u0026#34; override_attributes( \u0026#34;release\u0026#34; =\u0026gt; \u0026#34;icehouse\u0026#34;, \u0026#34;osops_networks\u0026#34; =\u0026gt; { \u0026#34;management\u0026#34; =\u0026gt; \u0026#34;10.200.10.0/24\u0026#34;, \u0026#34;public\u0026#34; =\u0026gt; \u0026#34;10.200.10.0/24\u0026#34;, \u0026#34;nova\u0026#34; =\u0026gt; \u0026#34;10.200.10.0/24\u0026#34; }, \u0026#34;mysql\u0026#34; =\u0026gt; { \u0026#34;bind_address\u0026#34; =\u0026gt; \u0026#34;0.0.0.0\u0026#34;, \u0026#34;root_network_acl\u0026#34; =\u0026gt; \u0026#34;%\u0026#34;, \u0026#34;allow_remote_root\u0026#34; =\u0026gt; true, \u0026#34;server_root_password\u0026#34; =\u0026gt; \u0026#34;secrete\u0026#34;, \u0026#34;server_repl_password\u0026#34; =\u0026gt; \u0026#34;secrete\u0026#34;, \u0026#34;server_debian_password\u0026#34; =\u0026gt; \u0026#34;secrete\u0026#34; }, \u0026#34;nova\u0026#34; =\u0026gt; { \u0026#34;network\u0026#34; =\u0026gt; { \u0026#34;fixed_range\u0026#34; =\u0026gt; \u0026#34;172.18.0.0/24\u0026#34;, \u0026#34;public_interface\u0026#34; =\u0026gt; \u0026#34;eth0\u0026#34; } }, \u0026#34;rabbitmq\u0026#34; =\u0026gt; { \u0026#34;address\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;5672\u0026#34; }, \u0026#34;openstack\u0026#34; =\u0026gt; { \u0026#34;developer_mode\u0026#34; =\u0026gt; true, \u0026#34;compute\u0026#34; =\u0026gt; { \u0026#34;rabbit\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34; }, \u0026#34;novnc_proxy\u0026#34; =\u0026gt; { \u0026#34;bind_interface\u0026#34; =\u0026gt; \u0026#34;eth1\u0026#34; }, \u0026#34;libvirt\u0026#34; =\u0026gt; { \u0026#34;bind_interface\u0026#34; =\u0026gt; \u0026#34;eth1\u0026#34; }, \u0026#34;network\u0026#34; =\u0026gt; { \u0026#34;fixed_range\u0026#34; =\u0026gt; \u0026#34;10.200.9.0/24\u0026#34; }, \u0026#34;rabbit_server_chef_role\u0026#34; =\u0026gt; \u0026#34;os-ops-messaging\u0026#34;, \u0026#34;networks\u0026#34; =\u0026gt; [ { \u0026#34;label\u0026#34; =\u0026gt; \u0026#34;private\u0026#34;, \u0026#34;ipv4_cidr\u0026#34; =\u0026gt; \u0026#34;10.200.9.0/24\u0026#34;, \u0026#34;num_networks\u0026#34; =\u0026gt; \u0026#34;1\u0026#34;, \u0026#34;network_size\u0026#34; =\u0026gt; \u0026#34;255\u0026#34;, \u0026#34;bridge\u0026#34; =\u0026gt; \u0026#34;br200\u0026#34;, \u0026#34;bridge_dev\u0026#34; =\u0026gt; \u0026#34;eth0\u0026#34;, \u0026#34;dns1\u0026#34; =\u0026gt; \u0026#34;8.8.8.8\u0026#34;, \u0026#34;dns2\u0026#34; =\u0026gt; \u0026#34;8.8.4.4\u0026#34;, \u0026#34;multi_host\u0026#34; =\u0026gt; \u0026#34;T\u0026#34; } ] }, \u0026#34;identity\u0026#34; =\u0026gt; { \u0026#34;bind_interface\u0026#34; =\u0026gt; \u0026#34;eth1\u0026#34;, \u0026#34;users\u0026#34; =\u0026gt; { \u0026#34;demo\u0026#34; =\u0026gt; { \u0026#34;password\u0026#34; =\u0026gt; \u0026#34;demo\u0026#34;, \u0026#34;default_tenant\u0026#34; =\u0026gt; \u0026#34;service\u0026#34;, \u0026#34;roles\u0026#34; =\u0026gt; { \u0026#34;Member\u0026#34; =\u0026gt; [ \u0026#34;Member\u0026#34; ] } } } }, \u0026#34;image\u0026#34; =\u0026gt; { \u0026#34;api\u0026#34; =\u0026gt; { \u0026#34;bind_interface\u0026#34; =\u0026gt; \u0026#34;eth1\u0026#34; }, \u0026#34;debug\u0026#34; =\u0026gt; true, \u0026#34;identity_service_chef_role\u0026#34; =\u0026gt; \u0026#34;os-identity\u0026#34;, \u0026#34;rabbit_server_chef_role\u0026#34; =\u0026gt; \u0026#34;os-ops-messaging\u0026#34;, \u0026#34;registry\u0026#34; =\u0026gt; { \u0026#34;bind_interface\u0026#34; =\u0026gt; \u0026#34;eth1\u0026#34; }, \u0026#34;syslog\u0026#34; =\u0026gt; { \u0026#34;use\u0026#34; =\u0026gt; false }, \u0026#34;upload_image\u0026#34; =\u0026gt; { \u0026#34;cirros\u0026#34; =\u0026gt; \u0026#34;http://hypnotoad/cirros-0.3.0-x86_64-disk.img\u0026#34;, }, \u0026#34;upload_images\u0026#34; =\u0026gt; [ \u0026#34;cirros\u0026#34; ] }, \u0026#34;network\u0026#34; =\u0026gt; { \u0026#34;api\u0026#34; =\u0026gt; { \u0026#34;bind_interface\u0026#34; =\u0026gt; \u0026#34;eth1\u0026#34;, } }, \u0026#34;db\u0026#34; =\u0026gt; { \u0026#34;bind_interface\u0026#34; =\u0026gt; \u0026#34;eth1\u0026#34;, \u0026#34;compute\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34; }, \u0026#34;identity\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34; }, \u0026#34;image\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34; }, \u0026#34;network\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34; }, \u0026#34;volume\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34; }, \u0026#34;dashboard\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34; } }, \u0026#34;mq\u0026#34; =\u0026gt; { \u0026#34;bind_interface\u0026#34; =\u0026gt; \u0026#34;eth1\u0026#34;, \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34;, \u0026#34;user\u0026#34; =\u0026gt; \u0026#34;guest\u0026#34;, \u0026#34;vhost\u0026#34; =\u0026gt; \u0026#34;/nova\u0026#34;, \u0026#34;servers\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34;, \u0026#34;compute\u0026#34; =\u0026gt; { \u0026#34;service_type\u0026#34; =\u0026gt; \u0026#34;rabbitmq\u0026#34;, \u0026#34;rabbit\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;5672\u0026#34; } }, \u0026#34;block-storage\u0026#34; =\u0026gt; { \u0026#34;service_type\u0026#34; =\u0026gt; \u0026#34;rabbitmq\u0026#34;, \u0026#34;rabbit\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;5672\u0026#34; } } }, \u0026#34;endpoints\u0026#34; =\u0026gt; { \u0026#34;compute-api-bind\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34;, \u0026#34;scheme\u0026#34; =\u0026gt; \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;8774\u0026#34;, \u0026#34;path\u0026#34; =\u0026gt; \u0026#34;/v2/%(tenant_id)s\u0026#34; }, \u0026#34;compute-api\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34;, \u0026#34;scheme\u0026#34; =\u0026gt; \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;8774\u0026#34;, \u0026#34;path\u0026#34; =\u0026gt; \u0026#34;/v2/%(tenant_id)s\u0026#34; }, \u0026#34;compute-ec2-admin-bind\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34;, \u0026#34;scheme\u0026#34; =\u0026gt; \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;8773\u0026#34;, \u0026#34;path\u0026#34; =\u0026gt; \u0026#34;/services/Admin\u0026#34; }, \u0026#34;compute-ec2-admin\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34;, \u0026#34;scheme\u0026#34; =\u0026gt; \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;8773\u0026#34;, \u0026#34;path\u0026#34; =\u0026gt; \u0026#34;/services/Admin\u0026#34; }, \u0026#34;compute-ec2-api-bind\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34;, \u0026#34;scheme\u0026#34; =\u0026gt; \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;8773\u0026#34;, \u0026#34;path\u0026#34; =\u0026gt; \u0026#34;/services/Cloud\u0026#34; }, \u0026#34;compute-ec2-api\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34;, \u0026#34;scheme\u0026#34; =\u0026gt; \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;8773\u0026#34;, \u0026#34;path\u0026#34; =\u0026gt; \u0026#34;/services/Cloud\u0026#34; }, \u0026#34;compute-xvpvnc-bind\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34;, \u0026#34;scheme\u0026#34; =\u0026gt; \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;6081\u0026#34;, \u0026#34;path\u0026#34; =\u0026gt; \u0026#34;/console\u0026#34; }, \u0026#34;compute-xvpvnc\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34;, \u0026#34;scheme\u0026#34; =\u0026gt; \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;6081\u0026#34;, \u0026#34;path\u0026#34; =\u0026gt; \u0026#34;/console\u0026#34; }, \u0026#34;compute-novnc-bind\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34;, \u0026#34;scheme\u0026#34; =\u0026gt; \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;6080\u0026#34;, \u0026#34;path\u0026#34; =\u0026gt; \u0026#34;/vnc_auto.html\u0026#34; }, \u0026#34;compute-novnc\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34;, \u0026#34;scheme\u0026#34; =\u0026gt; \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;6080\u0026#34;, \u0026#34;path\u0026#34; =\u0026gt; \u0026#34;/vnc_auto.html\u0026#34; }, \u0026#34;compute-vnc\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;0.0.0.0\u0026#34;, \u0026#34;scheme\u0026#34; =\u0026gt; \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;6080\u0026#34;, \u0026#34;path\u0026#34; =\u0026gt; \u0026#34;/vnc_auto.html\u0026#34; }, \u0026#34;image-api-bind\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34;, \u0026#34;scheme\u0026#34; =\u0026gt; \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;9292\u0026#34;, \u0026#34;path\u0026#34; =\u0026gt; \u0026#34;/v2\u0026#34; }, \u0026#34;image-api\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34;, \u0026#34;scheme\u0026#34; =\u0026gt; \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;9292\u0026#34;, \u0026#34;path\u0026#34; =\u0026gt; \u0026#34;/v2\u0026#34; }, \u0026#34;image-registry-bind\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34;, \u0026#34;scheme\u0026#34; =\u0026gt; \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;9191\u0026#34;, \u0026#34;path\u0026#34; =\u0026gt; \u0026#34;/v2\u0026#34; }, \u0026#34;image-registry\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34;, \u0026#34;scheme\u0026#34; =\u0026gt; \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;9191\u0026#34;, \u0026#34;path\u0026#34; =\u0026gt; \u0026#34;/v2\u0026#34; }, \u0026#34;identity-bind\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34;, \u0026#34;scheme\u0026#34; =\u0026gt; \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;5000\u0026#34;, \u0026#34;path\u0026#34; =\u0026gt; \u0026#34;/v2.0\u0026#34; }, \u0026#34;identity-api\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34;, \u0026#34;scheme\u0026#34; =\u0026gt; \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;5000\u0026#34;, \u0026#34;path\u0026#34; =\u0026gt; \u0026#34;/v2.0\u0026#34; }, \u0026#34;identity-admin\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34;, \u0026#34;scheme\u0026#34; =\u0026gt; \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;35357\u0026#34;, \u0026#34;path\u0026#34; =\u0026gt; \u0026#34;/v2.0\u0026#34; }, \u0026#34;volume-api-bind\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34;, \u0026#34;scheme\u0026#34; =\u0026gt; \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;8776\u0026#34;, \u0026#34;path\u0026#34; =\u0026gt; \u0026#34;/v1/%(tenant_id)s\u0026#34; }, \u0026#34;block-storage-api\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34;, \u0026#34;scheme\u0026#34; =\u0026gt; \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;8776\u0026#34;, \u0026#34;path\u0026#34; =\u0026gt; \u0026#34;/v1/%(tenant_id)s\u0026#34; }, \u0026#34;telemetry-api-bind\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34;, \u0026#34;scheme\u0026#34; =\u0026gt; \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;8777\u0026#34;, \u0026#34;path\u0026#34; =\u0026gt; \u0026#34;/v1\u0026#34; }, \u0026#34;telemetry-api\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34;, \u0026#34;scheme\u0026#34; =\u0026gt; \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;8777\u0026#34;, \u0026#34;path\u0026#34; =\u0026gt; \u0026#34;/v1\u0026#34; }, \u0026#34;network-api-bind\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34;, \u0026#34;scheme\u0026#34; =\u0026gt; \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;9696\u0026#34;, \u0026#34;path\u0026#34; =\u0026gt; \u0026#34;/v2\u0026#34; }, \u0026#34;network-api\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34;, \u0026#34;scheme\u0026#34; =\u0026gt; \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;9696\u0026#34;, \u0026#34;path\u0026#34; =\u0026gt; \u0026#34;/v2\u0026#34; }, \u0026#34;orchestration-api-bind\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34;, \u0026#34;scheme\u0026#34; =\u0026gt; \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;8004\u0026#34;, \u0026#34;path\u0026#34; =\u0026gt; \u0026#34;/v1/%(tenant_id)s\u0026#34; }, \u0026#34;orchestration-api\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34;, \u0026#34;scheme\u0026#34; =\u0026gt; \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;8004\u0026#34;, \u0026#34;path\u0026#34; =\u0026gt; \u0026#34;/v1/%(tenant_id)s\u0026#34; }, \u0026#34;orchestration-api-cfn-bind\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34;, \u0026#34;scheme\u0026#34; =\u0026gt; \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;8000\u0026#34;, \u0026#34;path\u0026#34; =\u0026gt; \u0026#34;/v1\u0026#34; }, \u0026#34;orchestration-api-cfn\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34;, \u0026#34;scheme\u0026#34; =\u0026gt; \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;8000\u0026#34;, \u0026#34;path\u0026#34; =\u0026gt; \u0026#34;/v1\u0026#34; }, \u0026#34;mq\u0026#34; =\u0026gt; { \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;5672\u0026#34; } } } ) 生成した environment を Chef サーバにアップロードする。\n% knife environment from file environments/icehouse-nova-network.rb Spiceweasel をインストールする。Spiceweasel は yml ファイルを元に knife の操作 を書き出して、またそれを一気に実行することが出来るツールです。\n% gem install spiceweasel --no-ri --no-rdoc % rbenv rehash infrastructure.yml を下記の通り修正する。\nberksfile: options: \u0026#39;--no-freeze --halt-on-frozen\u0026#39; cookbooks: - apache2: - apt: - aws: - build-essential: - chef_handler: - database: - dmg: - erlang: - git: - homebrew: - iptables: - logrotate: - memcached: - mysql: - openssl: - openstack-block-storage: - openstack-common: - openstack-compute: - openstack-dashboard: - openstack-identity: - openstack-image: - openstack-network: - openstack-object-storage: - openstack-ops-messaging: - openstack-ops-database: - openstack-orchestration: - openstack-telemetry: - pacman: - postgresql: - python: - rabbitmq: - runit: - selinux: - statsd: - windows: - xfs: - yum: - yum-epel: - yum-erlang_solutions: roles: - allinone-compute: - os-compute-single-controller: - os-base: - os-ops-caching: - os-ops-messaging: - os-ops-database: - os-block-storage: - os-block-storage-api: - os-block-storage-scheduler: - os-block-storage-volume: - os-client: - os-compute-api: - os-compute-api-ec2: - os-compute-api-metadata: - os-compute-api-os-compute: - os-compute-cert: - os-compute-conductor: - os-compute-scheduler: - os-compute-setup: - os-compute-vncproxy: - os-compute-worker: - os-dashboard: - os-identity: - os-image: - os-image-api: - os-image-registry: - os-image-upload: - os-telemetry-agent-central: - os-telemetry-agent-compute: - os-telemetry-api: - os-telemetry-collector: - os-network: - os-network-server: - os-network-l3-agent: - os-network-dhcp-agent: - os-network-metadata-agent: - os-network-openvswitch: - os-object-storage: - os-object-storage-account: - os-object-storage-container: - os-object-storage-management: - os-object-storage-object: - os-object-storage-proxy: environments: - icehouse-nova-network: nodes: - 10.200.10.46: run_list: role[os-compute-single-controller] options: -N opstall01 -E icehouse-nova-network --sudo -x thirai - 10.200.10.47: run_list: role[os-compute-worker] options: -N opstall02 -E icehouse-nova-network --sudo -x thirai - 10.200.10.48: run_list: role[os-compute-worker] options: -N opstall03 -E icehouse-nova-network --sudo -x thirai ※ nodes: 項にはデプロイしたいノードと Roles を割り当て列挙する。\nspiceweasel を実行する。この時点ではこれから実行されるコマンドの一覧が表示され るのみである。\n% spiceweasel infrastructure.yml berks upload --no-freeze --halt-on-frozen -b ./Berksfile knife cookbook upload apache2 apt aws build-essential chef_handler database dmg erlang git homebrew iptables logrotate memcached mysql openssl openstack-block-storage openstack-common openstack-compute openstack-dashboard openstack-identity openstack-image openstack-network openstack-object-storage openstack-ops-messaging openstack-ops-database openstack-orchestration openstack-telemetry pacman postgresql python rabbitmq runit selinux statsd windows xfs yum yum-epel yum-erlang_solutions knife environment from file separated.rb knife role from file allinone-compute.rb os-base.rb os-block-storage-api.rb os-block-storage-scheduler.rb os-block-storage-volume.rb os-block-storage.rb os-client.rb os-compute-api-ec2.rb os-compute-api-metadata.rb os-compute-api-os-compute.rb os-compute-api.rb os-compute-cert.rb os-compute-conductor.rb os-compute-scheduler.rb os-compute-setup.rb os-compute-single-controller.rb os-compute-vncproxy.rb os-compute-worker.rb os-dashboard.rb os-identity.rb os-image-api.rb os-image-registry.rb os-image-upload.rb os-image.rb os-network-dhcp-agent.rb os-network-l3-agent.rb os-network-metadata-agent.rb os-network-openvswitch.rb os-network-server.rb os-network.rb os-object-storage-account.rb os-object-storage-container.rb os-object-storage-management.rb os-object-storage-object.rb os-object-storage-proxy.rb os-object-storage.rb os-ops-caching.rb os-ops-database.rb os-ops-messaging.rb os-telemetry-agent-central.rb os-telemetry-agent-compute.rb os-telemetry-api.rb os-telemetry-collector.rb knife bootstrap 10.200.10.46 -N opstall01 -E separated --sudo -x thirai -r \u0026#39;role[os-compute-single-controller]\u0026#39; knife bootstrap 10.200.10.47 -N opstall02 -E separated --sudo -x thirai -r \u0026#39;role[os-compute-worker]\u0026#39; knife bootstrap 10.200.10.48 -N opstall03 -E separated --sudo -x thirai -r \u0026#39;role[os-compute-worker]\u0026#39; -e オプションを付与すると実際にこれらのコマンドが実行される。実行してデプロイを行う。\n% spiceweasel -e infrastructure.yml この時点で、下記の操作も行われる。\n Nova-Network 上に仮想ネットワークの生成 OS イメージのダウンロードと登録 (Environment に記したモノ)  Cinder の設定 デプロイ完了したところで cinder-volume プロセスは稼働しているが物理ディスクの アサインが済んでいない。これは Chef では指定出来ないので手動で行う。\n予め Cinder 用の物理ディスクを Controller ノードに付与する。(ここでは /dev/sdb1)\ncontroller% sudo -i controller# pvcreate /dev/sdb1 controller# vgcreate cinder-volumes /dev/sdb1 controller# service cinder-volume restart これで完了。\n使ってみる ではデプロイした OpenStack を使って仮想マシンを作ってみる。\ncontroller% sudo -i controller# source openrc controller# nova keypair-add novakey01 \u0026gt; novakey01 controller# chmod 400 novakey01 controller# nova boot --image cirros --flavor 1 --key_name novakey01 cirros01 controller# nova list +--------------------------------------+----------+--------+------------+-------------+--------------------+ | ID | Name | Status | Task State | Power State | Networks | +--------------------------------------+----------+--------+------------+-------------+--------------------+ | e6687359-1aef-4105-a8db-894600001610 | cirros01 | ACTIVE | - | Running | private=10.200.9.2 | +--------------------------------------+----------+--------+------------+-------------+--------------------+ controller# ssh -i novakey01 -l cirros 10.200.9.2 vm# ping www.goo.ne.jp 仮想マシンが生成され The Internet に対して通信が行えたことを確認。\n次に仮想ディスクを生成して上記で作成した仮想マシンに付与する。\ncontroller# cinder create --display-name vol01 1 +--------------------------------------+-----------+--------------+------+-------------+----------+--------------------------------------+ | ID | Status | Display Name | Size | Volume Type | Bootable | Attached to | +--------------------------------------+-----------+--------------+------+-------------+----------+--------------------------------------+ | 0f3726a0-c2f5-45bc-bcf2-1f6eb746f5c8 | available | vol02 | 1 | None | false | b286387c-2311-4134-ab59-850fee3e4650 | +--------------------------------------+-----------+--------------+------+-------------+----------+--------------------------------------+ controller# nova volume-attach e6687359-1aef-4105-a8db-894600001610 0f3726a0-c2f5-45bc-bcf2-1f6eb746f5c8 auto controller# ssh -i novakey01 -l cirros 10.200.9.2 vm# mkfs.ext4 /dev/vdb vm# mount -t ext4 /dev/vdb /mnt vm# df -h | grep mnt /dev/vdb 976M 1.3M 908M 1% /mnt 仮想ディスクを仮想マシンに付与しマウントできることを確認出来た。\nまとめ 今回は Nova-Network 構成の Icehouse を構築出来た。Nova-Network は Havana でサポート終了との話が延期 になっていることは聞いていたが Icehouse でもしっかり動いている。また後に Neutron 構成も調査を行う。 Neutron 構成は下記の Environments を参考にすると動作するかもしれない。\nhttps://github.com/stackforge/openstack-chef-repo/blob/master/environments/vagrant-multi-neutron.json\nキーとなるのは、\n\u0026#34;network\u0026#34;: { \u0026#34;debug\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;dhcp\u0026#34;: { \u0026#34;enable_isolated_metadata\u0026#34;: \u0026#34;True\u0026#34; }, \u0026#34;metadata\u0026#34;: { \u0026#34;nova_metadata_ip\u0026#34;: \u0026#34;192.168.3.60\u0026#34; }, \u0026#34;openvswitch\u0026#34;: { \u0026#34;tunnel_id_ranges\u0026#34;: \u0026#34;1:1000\u0026#34;, \u0026#34;enable_tunneling\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;tenant_network_type\u0026#34;: \u0026#34;gre\u0026#34;, \u0026#34;local_ip_interface\u0026#34;: \u0026#34;eth2\u0026#34; }, \u0026#34;api\u0026#34;: { \u0026#34;bind_interface\u0026#34;: \u0026#34;eth1\u0026#34; } }, この辺り。\n残っている問題点 VNC コンソールにアクセス出来ない。調べたのですが、environment の修正で直せる問 題ではないように見えました。\ncookbooks/openstack-compute/templates/default/nova.conf.rb を確認すると下記の ようになっています。\n##### VNCPROXY ##### novncproxy_base_url=\u0026lt;%= @novncproxy_base_url %\u0026gt; xvpvncproxy_base_url=\u0026lt;%= @xvpvncproxy_base_url %\u0026gt; # This is only required on the server running xvpvncproxy xvpvncproxy_host=\u0026lt;%= @xvpvncproxy_bind_host %\u0026gt; xvpvncproxy_port=\u0026lt;%= @xvpvncproxy_bind_port %\u0026gt; # This is only required on the server running novncproxy novncproxy_host=\u0026lt;%= @novncproxy_bind_host %\u0026gt; novncproxy_port=\u0026lt;%= @novncproxy_bind_port %\u0026gt; vncserver_listen=\u0026lt;%= @vncserver_listen %\u0026gt; vncserver_proxyclient_address=\u0026lt;%= @vncserver_proxyclient_address %\u0026gt; vncserver_listen, vncserver_proxyclient_address はそれぞれ\n @vncserver_listen @vncserver_proxyclient_address  という変数が格納されることになっている。\nでは cookbooks/openstack-compute/recipes/nova-common.rb を確認すると、\ntemplate \u0026#39;/etc/nova/nova.conf\u0026#39; do source \u0026#39;nova.conf.erb\u0026#39; owner node[\u0026#39;openstack\u0026#39;][\u0026#39;compute\u0026#39;][\u0026#39;user\u0026#39;] group node[\u0026#39;openstack\u0026#39;][\u0026#39;compute\u0026#39;][\u0026#39;group\u0026#39;] mode 00644 variables( sql_connection: sql_connection, novncproxy_base_url: novnc_endpoint.to_s, xvpvncproxy_base_url: xvpvnc_endpoint.to_s, xvpvncproxy_bind_host: xvpvnc_bind.host, xvpvncproxy_bind_port: xvpvnc_bind.port, novncproxy_bind_host: novnc_bind.host, novncproxy_bind_port: novnc_bind.port, vncserver_listen: vnc_endpoint.host, vncserver_proxyclient_address: vnc_endpoint.host, 以下略 となっている。vncserver_listen, vncserver_proxyclient_address 共に vnc_endpoint.host が格納されることになっている。vnc_endpont.host は\nvnc_endpoint = endpoint \u0026#39;compute-vnc\u0026#39; || {} となっており、Attributes の\n\u0026#34;compute-vnc\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;0.0.0.0\u0026#34;, \u0026#34;scheme\u0026#34; =\u0026gt; \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;6080\u0026#34;, \u0026#34;path\u0026#34; =\u0026gt; \u0026#34;/vnc_auto.html\u0026#34; }, の設定が入ることになる。つまり上記のような Attributes だと vncserver_listen, vncserver_proxyclient_address 共に \u0026lsquo;0.0.0.0\u0026rsquo; のアドレスが controller, compute ノードの双方に入ってしまい、NoVNC が正しく格納しないことになる。\n解決したらまたここに更新版を載せたいと思いまーす！\n以上です。\n","permalink":"https://jedipunkz.github.io/post/2014/04/25/stackforge-openstack-chef-repo-icehouse-deploy/","summary":"こんにちは。@jedipunkzです。\nまたまた OpenStack のデプロイをどうするか？についてです。\n今まで自分の中では Rackspace Private Cloud で使われている Rackspace 管理の rcbops/chef-cookbooks が今現在使うならベストの選択だと思っていました。これは内 部で Chef が使われていてしかも Cookbooks が Github 上で公開されています。 Apache ライセンスで使えるので、サービス構築にも使えちゃうというモノ。\n先日、ある OpenStack コアデベロッパーの方から「jedipunkz さん、やっぱり rcbops がいいですか？運営とかどうなっているんでしょう？マージの規準とかどうなのかな？」 と質問受けました。確かにマージの基準は Rackspace Private Cloud がベースになり ますし、管理しているエンジニアの一覧を見ていると Rackspace 社のエンジニアがメ インですし、今後どうなるのか分からない\u0026hellip;。\n逃げ道は用意したほうが良さそう。\nということで、以前自分も暑かったことのある StackForge の openstack-chef-repo を久々に使ってみました。Icehouse 構成がこの時点で既に組めるようになっていて、 以前よりだい〜ぶ完成度増した感があります。今回は nova-network 構成を作ってみた のですが、Neutron 構成ももちろん出来そうなので後に調べてまた公開したいです。\nStackForge とは StackForge は OpenStack のデプロイ・CI の仕組みとして公式に用いられているもの。 公式サイトは下記の場所にある。\nhttp://ci.openstack.org/stackforge.html\nStackForge の openstack-chef-repo は下記の場所にある。\nhttps://github.com/stackforge/openstack-chef-repo\nopenstack-chef-repo はまだ \u0026lsquo;stable/icehouse\u0026rsquo; ブランチが生成されていない。が直 ちに master からブランチが切られる様子。","title":"stackforge/openstack-chef-repo で OpenStack Icehouse デプロイ"},{"content":"こんにちは。@jedipunkzです。\n皆さん、Mirantis OpenStack はご存知ですか？ OpenStack ディストリビューションの 1つです。以下、公式サイトです。\nhttp://software.mirantis.com/main/\nこの Mirantis OpenStack を使って OpenStack Havana (Neutron GRE) 構成を作ってみ ました。その時のメモを書いていきたいと思います。\n構成は? 構成は下記の通り。\n※ CreativeCommon\n特徴は\n Administrative Network : Fuel Node, DHCP + PXE ブート用 Management Network : 各コンポーネント間 API 用 Public/Floating IP Network : パブリック API, VM Floating IP 用 Storage Network : Cinder 配下ストレージ \u0026lt;-\u0026gt; インスタンス間用 要インターネット接続 : Public/Floating Networks Neutron(GRE) 構成  です。タグ VLAN 使って物理ネットワークの本数を減らすことも出来るはずですが、僕 の環境では何故かダメだったので上記の4つの物理ネットワークを別々に用意しました。\nFuel ノードの構築 Fuel ノードとは、OpenStack の各ノードをデプロイするための管理ノードのことです。 DHCP + PXE を管理する Cobbler やデプロイツールの Puppet が内部で稼働し、 Administrative Network 上で稼働したノードを管理・その後デプロイします。\n構築方法は\u0026hellip;\n下記の URL より ISO ファイルをダウンロード。\nhttp://software.mirantis.com/main/\nAdministrative Network に NIC を出したノードを上記の ISO から起動。\nGrub メニューが表示されたタイミングで \u0026ldquo;Tab\u0026rdquo; キーを押下。\n上記画面にてカーネルオプションにて hostname, ip, gw, dns を修正。下記は例。\nvmlinuz initrd=initrd.img biosdevname=0 ks=cdrom:/ks.cfg ip=10.200.10.76 gw=10.200.10.1 dns1=8.8.8.8 netmask=255.255.255.0 hostname=fuel.cpi.ad.jp showmenu=no_ ブラウザで http://10.200.10.76:8080 (上記例)にアクセスし、新しい \u0026lsquo;OpenStack Environment\u0026rsquo; を作成する。\nName : 任意 OpenStack Release : Havana on CentOS6.4 なお、Ubuntu 構成も組めるが、私の環境では途中でエラーが出力した。\nNext を押下し、ネットワーク設定を行う。今回は \u0026lsquo;Nuetron GRE\u0026rsquo; を選択。\n\u0026lsquo;Save Settings\u0026rsquo; を押下し設定を保存。この時点では \u0026lsquo;Verify Networks\u0026rsquo; は行えない。 少なくとも 2 ノードが必要。次のステップで2ノードの追加を行う。\nノードの追加 下記の4つのネットワークセグメントに NIC を出したノードを用意し、起動する。起動 すると Administrative Network 上で稼働している Cobbler によりノードが PXE 起動 しミニマムな OS がインストールされる。これは後に Fuel ノードよりログインがされ、 各インターフェースの Mac アドレス等の情報を知るためです。ネットワークベリファ イ等もこのミニマムな OS 越しに実施されます。\n Administrative Network Public/Floating IP Network Storage Network Management Network  ノードが稼働した時点で Fuel によりノードが認識されるので、ここでは2つのノード をそれぞれ\n Controller ノード Compute ノード  として画面上で割り当てます。\nインターフェースの設定 http://10.200.10.76:8000/#cluster/1/nodes にログインし作成した Environment を選択。その後、\u0026lsquo;Nodes\u0026rsquo; タブを押下。ノードを選択し、\u0026lsquo;Configure Interfaces\u0026rsquo; を 選択。各ノードのインターフェースの Mac アドレスを確認し、各々のネットワークセ グメントを紐付ける。尚、Fuel ノードから \u0026lsquo;root\u0026rsquo; ユーザで SSH(22番ポート) にノン パスフレーズで公開鍵認証ログインが可能となっている。Fuel ノードに対しては SSH (22番ポート) にて下記のユーザにてログインが可能です。\nusername : root password : r00tme  ネットワークの確認 http://10.200.10.76:8000/#cluster/1/network にて \u0026lsquo;Networks\u0026rsquo; タブを開き、\u0026lsquo;Verify Networks\u0026rsquo; を押下。ネットワーク設定が正しく行われているか否かを確認。\nデプロイ http://10.200.10.76:8000/#cluster/1/nodes にて \u0026lsquo;Deploy Changes\u0026rsquo; を押下しデプ ロイ開始。kickstart にて OS が自動でインストールされ puppet にて fuel ノードか ら自動で OpenStack がインストールされます。\nOpenStack へのアクセス SSH では下記のステップで OpenStack コントローラノードにログイン。\nfuel ノード (SSH) -\u0026gt; node-1 (OpenStack コントローラノード)(SSH)\nブラウザで Horizon にアクセスするには\nhttp://10.200.10.2\nにアクセス。これは例。Administrative Network に接続している NIC の IP アドレス へ HTTP でログイン。\nまとめ Mirantis OpenStack Neutron (GRE) 構成が組めた。上記構成図を見て疑問に思ってい た \u0026ldquo;VM 間通信のネットワークセグメント\u0026rdquo; であるが、Administrative Network のセグ メントを用いている事が判った。これは利用者が意図しているとは考えづらいので、正 直、あるべき姿では無いように思える。が、上記構成図に VM ネットワークが無い理由 はこれにて判った。\n下記はノード上で ovs-vsctl show コマンドを打った結果の抜粋です。\nBridge \u0026#34;br-eth1\u0026#34; Port \u0026#34;br-eth1\u0026#34; Interface \u0026#34;br-eth1\u0026#34; type: internal Port \u0026#34;br-eth1--br-fw-admin\u0026#34; trunks: [0] Interface \u0026#34;br-eth1--br-fw-admin\u0026#34; type: patch options: {peer=\u0026#34;br-fw-admin--br-eth1\u0026#34;} Port \u0026#34;eth1\u0026#34; Interface \u0026#34;eth1\u0026#34; 今回の構成は eth1 は Administrative Network に割り当てていました。\n一昔前は OS のディストリビュータが有料サポートをビジネスにしていました。Redhat がその代表格だと思いますが、今は OS 上で何かを実現するにもソフトウェアの完成度 が高く、エンジニアが困るシチュエーションがそれほど無くなった気がします。そこで その OS の上で稼働する OpenStack のサポートビジネスが出てきたか！という印象で す。OpenStack はまだまだエンジニアにとって敷居の高いソフトウェアです。自らクラ ウドプラットフォームを構築出来るのは魅力的ですが、サポート無しに構築・運用する には、まだ難しい技術かもしれません。こういったディストリビューションが出てくる 辺りが時代だなぁと感じます。\n尚、ISO をダウンロードして利用するだけでしたら無償で OK です。\n","permalink":"https://jedipunkz.github.io/post/2014/04/23/mirantis-openstack/","summary":"こんにちは。@jedipunkzです。\n皆さん、Mirantis OpenStack はご存知ですか？ OpenStack ディストリビューションの 1つです。以下、公式サイトです。\nhttp://software.mirantis.com/main/\nこの Mirantis OpenStack を使って OpenStack Havana (Neutron GRE) 構成を作ってみ ました。その時のメモを書いていきたいと思います。\n構成は? 構成は下記の通り。\n※ CreativeCommon\n特徴は\n Administrative Network : Fuel Node, DHCP + PXE ブート用 Management Network : 各コンポーネント間 API 用 Public/Floating IP Network : パブリック API, VM Floating IP 用 Storage Network : Cinder 配下ストレージ \u0026lt;-\u0026gt; インスタンス間用 要インターネット接続 : Public/Floating Networks Neutron(GRE) 構成  です。タグ VLAN 使って物理ネットワークの本数を減らすことも出来るはずですが、僕 の環境では何故かダメだったので上記の4つの物理ネットワークを別々に用意しました。\nFuel ノードの構築 Fuel ノードとは、OpenStack の各ノードをデプロイするための管理ノードのことです。 DHCP + PXE を管理する Cobbler やデプロイツールの Puppet が内部で稼働し、 Administrative Network 上で稼働したノードを管理・その後デプロイします。","title":"Mirantis OpenStack (Neutron GRE)を組んでみた！"},{"content":"こんにちは。@jedipunkz です。\n今週 Redhat が \u0026lsquo;Redhat Enterprise Linux Atomic Host\u0026rsquo; しましたよね。Docker を特 徴としたミニマムな OS だとのこと。その内部で用いられている技術 Geard について 少し調べてみました。複数コンテナの関連付けが可能なようです。ここでは調べた結果 について簡単にまとめていきます。\n参考資料 http://openshift.github.io/geard/deploy_with_geard.html\n利用方法 ここではホスト OS に Fedora20 を用意します。\nまず Geard をインストール\n% sudo yum install --enablerepo=updates-testing geard 下記の json ファイルを作成します。ここにはコンテナをデプロイするための情報と関 連付けのための情報を記します。\n$ ${EDITOR} rockmongo_mongodb.json { \u0026#34;containers\u0026#34;:[ { \u0026#34;name\u0026#34;:\u0026#34;rockmongo\u0026#34;, \u0026#34;count\u0026#34;:1, \u0026#34;image\u0026#34;:\u0026#34;derekwaynecarr/rockmongo\u0026#34;, \u0026#34;publicports\u0026#34;:[{\u0026#34;internal\u0026#34;:80,\u0026#34;external\u0026#34;:6060}], \u0026#34;links\u0026#34;:[{\u0026#34;to\u0026#34;:\u0026#34;mongodb\u0026#34;}] }, { \u0026#34;name\u0026#34;:\u0026#34;mongodb\u0026#34;, \u0026#34;count\u0026#34;:1, \u0026#34;image\u0026#34;:\u0026#34;ccoleman/ubuntu-mongodb\u0026#34;, \u0026#34;publicports\u0026#34;:[{\u0026#34;internal\u0026#34;:27017}] } ] } 上記のファイルの解説。\n コンテナ \u0026lsquo;rockmongo\u0026rsquo; と \u0026lsquo;mongodb\u0026rsquo; を作成 それぞれ1個ずつコンテナを作成 \u0026lsquo;image\u0026rsquo; パラメータにて docker イメージの指定 \u0026lsquo;publicports\u0026rsquo; パラメータにてコンテナ内部とホスト側のポートマッピングを行う \u0026lsquo;links\u0026rsquo; パラメータで \u0026lsquo;rockmongo\u0026rsquo; を \u0026lsquo;mongodb\u0026rsquo; に関連付け  では、デプロイ開始します。\n$ sudo gear deploy rockmongo_mongo.json 2014/04/22 07:21:12 Deploying rockmongo_mongo.json.20140422-072112 2014/04/22 07:21:12 appending 27017 on rockmongo-1: \u0026amp;{PortPair:{Internal:27017 External:0} Target:127.0.0.1:27017} \u0026amp;{Id:rockmongo-1 Image:derekwaynecarr/rockmongo From:rockmongo On:0xc2100bb980 Ports:[{PortPair:{Internal:80 External:6060} Target::0}] add:true remove:false container:0xc21000be00 links:[]} 2014/04/22 07:21:12 ports: Releasing 2014/04/22 07:21:12 systemd: Reloading daemon local PortMapping: 80 -\u0026gt; 6060 local Container rockmongo-1 is installed 2014/04/22 07:21:12 ports: Releasing 2014/04/22 07:21:12 systemd: Reloading daemon local PortMapping: 27017 -\u0026gt; 4000 local Container mongodb-1 is installed linking rockmongo: 127.0.0.1:27017 -\u0026gt; localhost:4000 local Links set on local local Container rockmongo-1 starting local Container mongodb-1 starting ブラウザにてホスト OS に接続することで rockmongo の管理画面にアクセスが可能。\nhttp://\u0026lt;ホストOSのIP:6060\u0026gt;/ ポートマッピング管理 デプロイが完了して、実際に RockMongo の管理画面にアクセス出来たと思います。 関連付けが特徴と言えそうなのでその解析をしてみました。\nGeard のコンテナ関連付けはホストとコンテナのポート管理がキモとなっていることが 解ります。これを紐解くことで geard のコンテナ管理を理解します。\n 'rockmongo' コンテナ 'mongodb' コンテナ +----------+ +--------------------------+ +-------------------+ | Client |-\u0026gt;6060-\u0026gt;|-\u0026gt;80-\u0026gt; RockMongo -\u0026gt;27017-\u0026gt;|-\u0026gt;4000-\u0026gt;|-\u0026gt;27017-\u0026gt; Mongodb | +----------+ +--------------------------+ +-------------------+ |-------------------- docker ホスト --------------------| 上記 \u0026lsquo;gear deploy\u0026rsquo; コマンド発行時のログと json ファイルにより上図のような構成 になっていることが理解できます。一つずつ読み解いていきましょう。\n \u0026lsquo;rockmongo\u0026rsquo; コンテナの内部でリスンしている 80 番ポートはホスト OS の 6060 番へ変換 \u0026lsquo;rockmongo\u0026rsquo; コンテナ内で稼働する RockMongo の config.php から \u0026lsquo;127.0.0.1:27017\u0026rsquo; でリスンしていることが解る  試しに \u0026lsquo;derekwaynecarr/rockmongo:latest\u0026rsquo; に /bin/bash でログインし config.php を確認。\n$ sudo docker run -i -t derekwaynecarr/rockmongo:latest /bin/bash bash-4.1# grep mongo_host /var/www/html/config.php $MONGO[\u0026#34;servers\u0026#34;][$i][\u0026#34;mongo_host\u0026#34;] = \u0026#34;127.0.0.1\u0026#34;;//mongo host $MONGO[\u0026#34;servers\u0026#34;][$i][\u0026#34;mongo_host\u0026#34;] = \u0026#34;127.0.0.1\u0026#34;;  デプロイ時のログと json ファイルの \u0026lsquo;links\u0026rsquo; パラメータより、ホストのポートに動的に(ここでは 4000番ポート) に変換されることが解ります。  linking rockmongo: 127.0.0.1:27017 -\u0026gt; localhost:4000  ホストの 4000 番ポートは動的に \u0026lsquo;mongodb\u0026rsquo; コンテナの内部ポート 27017 にマッピングされる  これらのポートマッピングによりそれぞれのコンテナの連携が取れている。\nまとめと考察 Geard は下記の2つを特徴とした技術だと言えるとことがわかりました。\n コンテナを json で管理しデプロイする仕組みを提供する コンテナ間の関連付けをホスト OS のポートを動的に管理・マッピングすることで行う  同じような OS に CoreOS https://coreos.com/ がありますが、こちらも docker, sytemd 等を特徴としています。さらに etcd を使ってクラスタの構成等が可能になっ ていますが、Geard はホストのポートを動的に管理することで関連付けが可能なことが わかりました。\n実際に触ってみた感覚から言えば、まだまだ実用化は厳しい状況に思えますが、今後へ の展開に期待したい技術です。\n","permalink":"https://jedipunkz.github.io/post/2014/04/22/geard-port-mapping/","summary":"こんにちは。@jedipunkz です。\n今週 Redhat が \u0026lsquo;Redhat Enterprise Linux Atomic Host\u0026rsquo; しましたよね。Docker を特 徴としたミニマムな OS だとのこと。その内部で用いられている技術 Geard について 少し調べてみました。複数コンテナの関連付けが可能なようです。ここでは調べた結果 について簡単にまとめていきます。\n参考資料 http://openshift.github.io/geard/deploy_with_geard.html\n利用方法 ここではホスト OS に Fedora20 を用意します。\nまず Geard をインストール\n% sudo yum install --enablerepo=updates-testing geard 下記の json ファイルを作成します。ここにはコンテナをデプロイするための情報と関 連付けのための情報を記します。\n$ ${EDITOR} rockmongo_mongodb.json { \u0026#34;containers\u0026#34;:[ { \u0026#34;name\u0026#34;:\u0026#34;rockmongo\u0026#34;, \u0026#34;count\u0026#34;:1, \u0026#34;image\u0026#34;:\u0026#34;derekwaynecarr/rockmongo\u0026#34;, \u0026#34;publicports\u0026#34;:[{\u0026#34;internal\u0026#34;:80,\u0026#34;external\u0026#34;:6060}], \u0026#34;links\u0026#34;:[{\u0026#34;to\u0026#34;:\u0026#34;mongodb\u0026#34;}] }, { \u0026#34;name\u0026#34;:\u0026#34;mongodb\u0026#34;, \u0026#34;count\u0026#34;:1, \u0026#34;image\u0026#34;:\u0026#34;ccoleman/ubuntu-mongodb\u0026#34;, \u0026#34;publicports\u0026#34;:[{\u0026#34;internal\u0026#34;:27017}] } ] } 上記のファイルの解説。\n コンテナ \u0026lsquo;rockmongo\u0026rsquo; と \u0026lsquo;mongodb\u0026rsquo; を作成 それぞれ1個ずつコンテナを作成 \u0026lsquo;image\u0026rsquo; パラメータにて docker イメージの指定 \u0026lsquo;publicports\u0026rsquo; パラメータにてコンテナ内部とホスト側のポートマッピングを行う \u0026lsquo;links\u0026rsquo; パラメータで \u0026lsquo;rockmongo\u0026rsquo; を \u0026lsquo;mongodb\u0026rsquo; に関連付け  では、デプロイ開始します。","title":"Geard のポートマッピングについて調べてみた"},{"content":"こんにちは！@jedipunkzです。\n今回は Havana 版の OpenStack Glance, Cinder と分散ストレージの Ceph を連携させ る手順を書いていきます。元ネタはこちら。下記の Ceph の公式サイトに手順です。\nhttps://ceph.com/docs/master/rbd/rbd-openstack/\nこの手順から下記の変更を行って、ここでは記していきます。\n Nova + Ceph 連携させない cinder-backup は今のところ動作確認出来ていないので省く 諸々の手順がそのままでは実施出来ないので補足を入れていく。  cinder-backup は Cinder で作成した仮想ディスクのバックアップを Ceph ストレージ 上に取ることが出来るのですが、そもそも Ceph 上にある仮想ディスクを Ceph にバッ クアップ取っても意味が薄いですし、まだ動いていないので今回は省きます。LVM やそ の他ストレージを使った Cinder 連携をされている方にとっては cinder-backup の Ceph 連携は意味が大きくなってくると思います。\n構成 下記の通りの物理ネットワーク6つの構成です。 OpenStack, Ceph 共に最小構成を前提にします。\n +--------------------------------------------------------------------- external | +--------------+--(-----------+--------------+------------------------------------------ public | | | | | +------------+ +------------+ +------------+ +------------+ +------------+ +------------+ | controller | | network | | compute | | ceph01 | | ceph02 | | ceph03 | +------------+ +------------+ +------------+ +------------+ +------------+ +------------+ | | | | | | | | | | | | | | | | +--------------+--(-----------+--(-----------+--(--(--------+--(--(--------+--(--(------- management | | | | | | | | | | | +--------------+--(-----------(--(-----------(--(-----------(--(------- guest | | | | | | | | +--------------------------------+-----------+--(-----------+--(-----------+--(------- storage | | | +--------------+--------------+------- cluster 特徴\n 最小構成 controller x 1 + network x 1 + compute x 1 + ceph x 3 OpenStack API の相互通信は management ネットワーク OpenStack VM 間通信は guest ネットワーク OpenStack 外部通信は public ネットワーク OpenStack VM の floating-ip (グローバル) 通信は external ネットワーク Ceph と OpenStack 間の通信は storage ネットワーク Ceph の OSD 間通信は cluster ネットワーク ここには記されていないホスト \u0026lsquo;workstation\u0026rsquo; から OpenStack, Ceph をデプロイ  前提の構成 前提構成の OpenStack と Ceph ですが、ここでは構築方法は割愛します。OpenStack は rcbops/chef-cookbooks を。Ceph は ceph-deploy を使って自分は構築しました。 下記の自分のブログに構築手順が載っているので参考にしてみてください。\nOpenStack 構築方法\nhttp://jedipunkz.github.io/blog/2013/11/17/openstack-havana-chef-deploy/\nCeph 構築方法\nhttp://jedipunkz.github.io/blog/2014/02/27/journal-ssd-ceph-deploy/\nOpenStack + Ceph 連携手順 では実際に連携するための手順を記していきます。\nrbd pool の作成 Ceph ノードの何れかで下記の手順を実施し rbd pool を作成する。\nceph01% sudo ceph osd pool create volumes 128 ceph01% sudo ceph osd pool create images 128 ceph01% sudo ceph osd pool create backups 128 ssh 鍵の配置 Ceph ノードから OpenStack の controller, compute ノードへ接続出来るよう鍵を配 布します。\nceph01% ssh-copy-id \u0026lt;username\u0026gt;@\u0026lt;controller_ip\u0026gt; ceph01% ssh-copy-id \u0026lt;username\u0026gt;@\u0026lt;compute_ip\u0026gt; sudoers の設定 controller, compute ノード上で sudoers の設定を予め実施する。 /etc/sudoers.d/として保存する。\n\u0026lt;username\u0026gt; ALL = (root) NOPASSWD:ALL ceph パッケージのインストール controller ノード, compute ノード上で Ceph パッケージをインストールする。\ncontroller% wget -q -O- \u0026#39;https://ceph.com/git/?p=ceph.git;a=blob_plain;f=keys/release.asc\u0026#39; | sudo apt-key add - controller% echo deb http://ceph.com/debian/ $(lsb_release -sc) main | sudo tee /etc/apt/sources.list.d/ceph.list controller% sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y python-ceph ceph-common compute% wget -q -O- \u0026#39;https://ceph.com/git/?p=ceph.git;a=blob_plain;f=keys/release.asc\u0026#39; | sudo apt-key add - compute% echo deb http://ceph.com/debian/ $(lsb_release -sc) main | sudo tee /etc/apt/sources.list.d/ceph.list compute% sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y python-ceph ceph-common controller ノード, compute ノード上でディレクトリを作成する。\ncontroller% sudo mkdir /etc/ceph compute % sudo mkdir /etc/ceph ceph.conf を controller, compute ノードへ配布する。\nceph01% sudo -i ceph01# ssh \u0026lt;controller_ip\u0026gt; sudo tee /etc/ceph/ceph.conf \u0026lt;/etc/ceph/ceph.conf ceph01# ssh \u0026lt;compute_ip\u0026gt; sudo tee /etc/ceph/ceph.conf \u0026lt;/etc/ceph/ceph.conf ceph 上にユーザを作成 Ceph 上に cinder, glance 用の新しいユーザを作成する。\nceph auth get-or-create client.cinder mon \u0026#39;allow r\u0026#39; osd \u0026#39;allow class-read object_prefix rbd_children, allow rwx pool=volumes, allow rx pool=images\u0026#39; ceph auth get-or-create client.glance mon \u0026#39;allow r\u0026#39; osd \u0026#39;allow class-read object_prefix rbd_children, allow rwx pool=images\u0026#39; ceph auth get-or-create client.cinder-backup mon \u0026#39;allow r\u0026#39; osd \u0026#39;allow class-read object_prefix rbd_children, allow rwx pool=backups\u0026#39; キーリングの作成と配置 client.cinder, client.glance, client.cinder-backup のキーリングを作成する。また作成したキーリングを controller ノードに配布する。\nceph01% sudo ceph auth get-or-create client.glance | ssh {your-glance-api-server} sudo tee /etc/ceph/ceph.client.glance.keyring ceph01% ssh {your-glance-api-server} sudo chown glance:glance /etc/ceph/ceph.client.glance.keyring ceph01% sudo ceph auth get-or-create client.cinder | ssh {your-volume-server} sudo tee /etc/ceph/ceph.client.cinder.keyring ceph01% ssh {your-cinder-volume-server} sudo chown cinder:cinder /etc/ceph/ceph.client.cinder.keyring ceph01% sudo ceph auth get-or-create client.cinder-backup | ssh {your-cinder-backup-server} sudo tee /etc/ceph/ceph.client.cinder-backup.keyring ceph01% ssh {your-cinder-backup-server} sudo chown cinder:cinder /etc/ceph/ceph.client.cinder-backup.keyring client.cinder のキーリングを compute ノードに配置する。\nceph01% sudo ceph auth get-key client.cinder | ssh {your-compute-node} tee client.cinder.key libvirt への secret キー追加 compute ノード上で secret キーを libvirt に追加する。\ncompute% uuidgen 457eb676-33da-42ec-9a8c-9293d545c337 cat \u0026gt; secret.xml \u0026lt;\u0026lt;EOF \u0026lt;secret ephemeral=\u0026#39;no\u0026#39; private=\u0026#39;no\u0026#39;\u0026gt; \u0026lt;uuid\u0026gt;457eb676-33da-42ec-9a8c-9293d545c337\u0026lt;/uuid\u0026gt; \u0026lt;usage type=\u0026#39;ceph\u0026#39;\u0026gt; \u0026lt;name\u0026gt;client.cinder secret\u0026lt;/name\u0026gt; \u0026lt;/usage\u0026gt; \u0026lt;/secret\u0026gt; EOF compute % sudo virsh secret-define --file secret.xml Secret 457eb676-33da-42ec-9a8c-9293d545c337 created compute% sudo virsh secret-set-value --secret 457eb676-33da-42ec-9a8c-9293d545c337 --base64 $(cat client.cinder.key) \u0026amp;\u0026amp; rm client.cinder.key secret.xml glance 連携手順 controller ノードの /etc/glance/glance-api.conf に下記を追記。 default_store=file と標準ではなっているので下記の通り rbd に書き換える。\ndefault_store=rbd rbd_store_user=glance rbd_store_pool=images cinder 連携手順 controller ノードの /etc/cinder/cinder.conf に下記を追記。 volume_driver は予め LVM の設定が入っていると思われるので書き換える。 また rbd_secret_uuid は先ほど生成した uuid を記す。\nvolume_driver=cinder.volume.drivers.rbd.RBDDriver rbd_pool=volumes rbd_ceph_conf=/etc/ceph/ceph.conf rbd_flatten_volume_from_snapshot=false rbd_max_clone_depth=5 glance_api_version=2 rbd_user=cinder rbd_secret_uuid=457eb676-33da-42ec-9a8c-9293d545c337 ceph.conf への追記 上記で配布した ceph.conf にはキーリングのパスが記されていない。controller ノー ド上の /etc/ceph/ceph.conf に下記の通り追記する。\nここは公式サイトには印されていないのでハマりました。ポイントです。\n[client.keyring] keyring = /etc/ceph/ceph.client.cinder.keyring OpenStack のそれぞれのコンポーネントを再起動かける controller% sudo glance-control api restart compute % sudo service nova-compute restart controller% sudo service cinder-volume restart controller% sudo service cinder-backup restart 動作確認 では動作確認を。Glance に OS イメージを登録。その後そのイメージを元にインスタ ンスを作成。Cinder 上に仮想ディスクを作成。その後先ほど作成したインスタンスに 接続しマウント。そのマウントした仮想ディスク上で書き込みが行えるか確認をします。\nテストで Ubuntu Precise 12.04 のイメージを glance を用いて登録する。\ncontroller% wget http://cloud-images.ubuntu.com/precise/current/precise-server-cloudimg-amd64-disk1.img controller% glance image-create --name precise-image --is-public true --container-format bare --disk-format qcow2 \u0026lt; precise-server-cloudimg-amd64-disk1.img テスト VM を稼働する。\ncontroller% nova boot --nic net-id=\u0026lt;net_id\u0026gt; --flavor 2 --image precise-image --key_name novakey01 vm01 controller% cinder create --display-name vol01 1 controller% nova volume-attach \u0026lt;instance_id\u0026gt; \u0026lt;volume_id\u0026gt; auto テスト VM へログインしファイルシステムを作成後、マウントする。\ncontroller% ssh -i \u0026lt;key_file_path\u0026gt; -l ubuntu \u0026lt;instance_ip\u0026gt; vm01% sudo mkfs -t ext4 /dev/vdb vm01% sudo mount -t ext4 /dev/vdb /mnt マウントした仮想ディスク上でデータを書き込んでみる。\nvm01% sudo dd if=/dev/zero of=/mnt/500M bs=1M count=5000 まとめ Ceph, Cinder の Ceph 連携が出来ました！\nOpenStack Grizzly 版時代に Ceph 連携は取っていたのですが Havana では\n cinder-bacup の Ceph 連携 nova の Ceph 連携  が追加されていました。Nova 連携をとるとインスタンスを稼働させる際に通常は controller ノードの /var/lib/nova 配下にファイルとしてインスタンスイメージが作 成されますが、これが Ceph 上に作成されるとのことです。Nova 連携は是非とってみ たいのですが、今のところ動いていません。引き続き調査を行います。\ncinder-backup も少し連携取ってみましたが backup_driver に Ceph ドライバの指定 をしているにも関わらず Swift に接続しにいってしまう有り様でした..。こちらも引 き続き調査します。またステートが \u0026lsquo;in-use\u0026rsquo; の場合バックアップが取れず \u0026lsquo;available\u0026rsquo; なステートでないといけないようです。確かに書き込み中に操作が行われ てしまってもバックアップの整合性が取れないですし、ここは仕方ないところですね。\n","permalink":"https://jedipunkz.github.io/post/2014/04/04/openstack-havana-cinder-glance-ceph/","summary":"こんにちは！@jedipunkzです。\n今回は Havana 版の OpenStack Glance, Cinder と分散ストレージの Ceph を連携させ る手順を書いていきます。元ネタはこちら。下記の Ceph の公式サイトに手順です。\nhttps://ceph.com/docs/master/rbd/rbd-openstack/\nこの手順から下記の変更を行って、ここでは記していきます。\n Nova + Ceph 連携させない cinder-backup は今のところ動作確認出来ていないので省く 諸々の手順がそのままでは実施出来ないので補足を入れていく。  cinder-backup は Cinder で作成した仮想ディスクのバックアップを Ceph ストレージ 上に取ることが出来るのですが、そもそも Ceph 上にある仮想ディスクを Ceph にバッ クアップ取っても意味が薄いですし、まだ動いていないので今回は省きます。LVM やそ の他ストレージを使った Cinder 連携をされている方にとっては cinder-backup の Ceph 連携は意味が大きくなってくると思います。\n構成 下記の通りの物理ネットワーク6つの構成です。 OpenStack, Ceph 共に最小構成を前提にします。\n +--------------------------------------------------------------------- external | +--------------+--(-----------+--------------+------------------------------------------ public | | | | | +------------+ +------------+ +------------+ +------------+ +------------+ +------------+ | controller | | network | | compute | | ceph01 | | ceph02 | | ceph03 | +------------+ +------------+ +------------+ +------------+ +------------+ +------------+ | | | | | | | | | | | | | | | | +--------------+--(-----------+--(-----------+--(--(--------+--(--(--------+--(--(------- management | | | | | | | | | | | +--------------+--(-----------(--(-----------(--(-----------(--(------- guest | | | | | | | | +--------------------------------+-----------+--(-----------+--(-----------+--(------- storage | | | +--------------+--------------+------- cluster 特徴","title":"OpenStack Havana Cinder,Glance の分散ストレージ Ceph 連携"},{"content":"こんにちは。@jedipunkzです。\n追記 2014/03/20 : 一旦削除していた記事なのですが、無事動作が確認出来たので再度アッ プします！\n第17回 OpenStack 勉強会で rcbops/chef-cookbooks の話をしてきたのですが会場から 質問がありました。「Havana の Swift 構成を作る Cookbooks はどこにありますか？」 と。私が試したのが Grizzly 時代のモノで、よく rcbops/chef-cookbooks を見てみる と Havana ブランチ又は Havana に対応したリリースタグのファイル構成に Swift が 綺麗サッパリ消えているではありませんか・・！下記の Swift の Cookbooks は幸い github 上に残っていました。\nhttps://github.com/rcbops-cookbooks/swift\nが rcbops/chef-cookbooks との関連付けが切れています・・。ぐあぁ。\nということで Havana 構成の Keystone 2013.2.2 と Swift 1.10.0 の構成を Chef で 作らねば！と思い色々試していたら結構あっさりと出来ました。今回はその方法を書い ていきたいと思います！\n構成 構成は\u0026hellip;以前の記事 http://jedipunkz.github.io/blog/2013/10/27/swift-chef/ と同じです。\n+-----------------+ | load balancer | +-----------------+ | +-------------------+-------------------+-------------------+-------------------+---------------------- proxy network | | | | | +-----------------+ +-----------------+ +-----------------+ +-----------------+ +-----------------+ | chef server | | chef workstation| | swift-mange | | swift-proxy01 | | swift-proxy02 | +-----------------+ +-----------------+ +-----------------+ +-----------------+ +-----------------+ ...\u0026gt; scaling | | | | | +-------------------+-------------------+-------------------+-------------------+-------------------+-- storage network | | | | | | +-----------------+ +-----------------+ +-----------------+ +-----------------+ +-----------------+ +-----------------+ | swift-storage01 | | swift-storage02 | | swift-storage03 | | swift-account01 | | swift-account02 | | swift-account03 | +-----------------+ +-----------------+ +-----------------+ +-----------------+ +-----------------+ +-----------------+ ..\u0026gt; scaling 手順 では早速手順を記していきますね。毎回なのですが Chef ワークステーション・Chef サーバの環境構築については割愛します。オムニバスインストーラを使えば Chef サー バの構築は簡単ですし、ワークステーションの構築も Ruby インストール -\u0026gt; gem で Chef をインストール -\u0026gt; .chef 配下を整える、で出来ます。\nrcbops/chef-cookbooks の取得。現在 Stable バージョンの refs/tags/v4.2.1 を用いる。\n% git clone https://github.com/rcbops/chef-cookbooks.git ./chef-cookbooks-4.2.1 % cd ./chef-cookbooks-4.2.1 % git checkout -b v4.2.1 refs/tags/v4.2.1 % git submodule init % git submodule sync % git submodule update ここで本家 rcbops/chef-cookbooks と関連付けが消えている rcbops-cookbooks/swift を cookbooks ディレクトリ配下にクローンします。あと念のため \u0026lsquo;havana\u0026rsquo; ブランチ を指定します。コードを確認したところ何も変化はありませんでしたが。\n% git clone https://github.com/rcbops-cookbooks/swift.git cookbooks/swift % cd cookbooks/swift % git checkout -b havana remotes/origin/havana % cd ../.. cookbooks, roles の Chef サーバへのアップロードを行います。\n% knife cookbook upload -o cookbooks -a % knife role from file role/*.rb 今回の構成 (1クラスタ) 用の environments/swift-havana.json を作成します。json ファイルの名前は任意です。\n{ \u0026#34;name\u0026#34;: \u0026#34;swift-havana\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;cookbook_versions\u0026#34;: { }, \u0026#34;json_class\u0026#34;: \u0026#34;Chef::Environment\u0026#34;, \u0026#34;chef_type\u0026#34;: \u0026#34;environment\u0026#34;, \u0026#34;default_attributes\u0026#34;: { }, \u0026#34;override_attributes\u0026#34;: { \u0026#34;package_component\u0026#34;: \u0026#34;havana\u0026#34;, \u0026#34;osops_networks\u0026#34;: { \u0026#34;management\u0026#34;: \u0026#34;10.200.9.0/24\u0026#34;, \u0026#34;public\u0026#34;: \u0026#34;10.200.10.0/24\u0026#34;, \u0026#34;swift\u0026#34;: \u0026#34;10.200.9.0/24\u0026#34; }, \u0026#34;keystone\u0026#34;: { \u0026#34;pki\u0026#34;: { \u0026#34;enabled\u0026#34;: false }, \u0026#34;admin_port\u0026#34;: \u0026#34;35357\u0026#34;, \u0026#34;admin_token\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;admin_user\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;tenants\u0026#34;: [ \u0026#34;admin\u0026#34;, \u0026#34;service\u0026#34; ], \u0026#34;users\u0026#34;: { \u0026#34;admin\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;secrete\u0026#34;, \u0026#34;roles\u0026#34;: { \u0026#34;admin\u0026#34;: [ \u0026#34;admin\u0026#34; ] } }, \u0026#34;demo\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;demo\u0026#34;, \u0026#34;default_tenant\u0026#34; : \u0026#34;service\u0026#34;, \u0026#34;roles\u0026#34;: { \u0026#34;admin\u0026#34;: [ \u0026#34;admin\u0026#34; ] } } }, \u0026#34;db\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;keystone\u0026#34; } }, \u0026#34;mysql\u0026#34;: { \u0026#34;root_network_acl\u0026#34;: \u0026#34;%\u0026#34;, \u0026#34;allow_remote_root\u0026#34;: true, \u0026#34;server_root_password\u0026#34;: \u0026#34;secrete\u0026#34;, \u0026#34;server_repl_password\u0026#34;: \u0026#34;secrete\u0026#34;, \u0026#34;server_debian_password\u0026#34;: \u0026#34;secrete\u0026#34; }, \u0026#34;monitoring\u0026#34;: { \u0026#34;procmon_provider\u0026#34;: \u0026#34;monit\u0026#34;, \u0026#34;metric_provider\u0026#34;: \u0026#34;collectd\u0026#34; }, \u0026#34;vips\u0026#34;: { \u0026#34;keystone-admin-api\u0026#34;: \u0026#34;10.200.9.11\u0026#34;, \u0026#34;keystone-service-api\u0026#34;: \u0026#34;10.200.9.11\u0026#34;, \u0026#34;keystone-internal-api\u0026#34;: \u0026#34;10.200.9.11\u0026#34;, \u0026#34;swift-proxy\u0026#34;: \u0026#34;10.200.9.11\u0026#34;, \u0026#34;config\u0026#34;: { \u0026#34;10.200.9.112\u0026#34;: { \u0026#34;vrid\u0026#34;: 12, \u0026#34;network\u0026#34;: \u0026#34;management\u0026#34; } } }, \u0026#34;developer_mode\u0026#34;: false, \u0026#34;swift\u0026#34;: { \u0026#34;swift_hash\u0026#34;: \u0026#34;307c0568ea84\u0026#34;, \u0026#34;authmode\u0026#34;: \u0026#34;keystone\u0026#34;, \u0026#34;authkey\u0026#34;: \u0026#34;20281b71-ce89-4b27-a2ad-ad873d3f2760\u0026#34; } } } 作成した environment ファイル environments/swift-havana.json を chef-server へアップ ロードする。\n% knife environment from file environments/swift-havana.json Cookbooks の修正 swift cookbooks を修正します。havana からは keystone を扱うクライアントは keystone.middleware.auth_token でなく keystoneclient.middleware.auth_token を 使うように変更掛かっていますので、下記のように修正しました。\n% cd cookbooks/swift/templates/default % diff -u proxy-server.conf.erb.org proxy-server.conf.erb --- proxy-server.conf.erb.org 2014-03-20 16:28:28.000000000 +0900 +++ proxy-server.conf.erb 2014-03-20 16:28:13.000000000 +0900 @@ -243,7 +243,8 @@ use = egg:swift#keystoneauth [filter:authtoken] -paste.filter_factory = keystone.middleware.auth_token:filter_factory +#paste.filter_factory = keystone.middleware.auth_token:filter_factory +paste.filter_factory = keystoneclient.middleware.auth_token:filter_factory auth_host = \u0026lt;%= @keystone_api_ipaddress %\u0026gt; auth_port = \u0026lt;%= @keystone_admin_port %\u0026gt; auth_protocol = \u0026lt;%= @keystone_admin_protocol %\u0026gt; % cd ../../../.. デプロイ かきのとおり knife bootstrap する。\n% knife bootstrap \u0026lt;manage_ip_addr\u0026gt; -N swift-manage -r \u0026#39;role[base]\u0026#39;,\u0026#39;role[mysql-master]\u0026#39;,\u0026#39;role[keystone]\u0026#39;,\u0026#39;role[swift-management-server]\u0026#39; -E swift-havana --sudo -x thirai % knife bootstrap \u0026lt;proxy01_ip_addr\u0026gt; -N swift-proxy01 -r \u0026#34;role[base]\u0026#34;,\u0026#34;role[swift-proxy-server]\u0026#34;,\u0026#39;role[swift-setup]\u0026#39;,\u0026#39;role[openstack-ha]\u0026#39; -E swift-havana --sudo -x thirai % knife bootstrap \u0026lt;proxy02_ip_addr\u0026gt; -N swift-proxy02 -r \u0026#34;role[base]\u0026#34;,\u0026#34;role[swift-proxy-server]\u0026#34;,\u0026#39;role[openstack-ha]\u0026#39; -E swift-havana --sudo -x thirai % knife bootstrap \u0026lt;storage01_ip_addr\u0026gt; -N swift-storage01 -r \u0026#39;role[base]\u0026#39;,\u0026#39;role[swift-object-server]\u0026#39; -E swift-havana --sudo -x thirai % knife bootstrap \u0026lt;storage02_ip_addr\u0026gt; -N swift-storage02 -r \u0026#39;role[base]\u0026#39;,\u0026#39;role[swift-object-server]\u0026#39; -E swift-havana --sudo -x thirai % knife bootstrap \u0026lt;storage03_ip_addr\u0026gt; -N swift-storage03 -r \u0026#39;role[base]\u0026#39;,\u0026#39;role[swift-object-server]\u0026#39; -E swift-havana --sudo -x thirai % knife bootstrap \u0026lt;account01_ip_addr\u0026gt; -N swift-account01 -r \u0026#39;role[base]\u0026#39;,\u0026#39;role[swift-account-server]\u0026#39;,\u0026#39;role[swift-container-server]\u0026#39; -E swift-havana --sudo -x thirai % knife bootstrap \u0026lt;account02_ip_addr\u0026gt; -N swift-account02 -r \u0026#39;role[base]\u0026#39;,\u0026#39;role[swift-account-server]\u0026#39;,\u0026#39;role[swift-container-server]\u0026#39; -E swift-havana --sudo -x thirai % knife bootstrap \u0026lt;account03_ip_addr\u0026gt; -N swift-account03 -r \u0026#39;role[base]\u0026#39;,\u0026#39;role[swift-account-server]\u0026#39;,\u0026#39;role[swift-container-server]\u0026#39; -E swift-havana --sudo -x thirai ここでバグ対策。Swift 1.10.0 にはバグがあるので下記の通り対処します。\nkeystone.middleware.s3_token に既知のバグがあり、下記のように対処します。この 状態ではバグにより swift-proxy が稼働してない状態ですが後の各ノードでの chef-client 実行時に稼働する予定です。\n% diff /usr/lib/python2.7/dist-packages/keystone/exception.py.org /usr/lib/python2.7/dist-packages/keystone/exception.py --- exception.py.org 2014-03-12 16:45:00.181420694 +0900 +++ exception.py 2014-03-12 16:44:47.173177081 +0900 @@ -18,6 +18,7 @@ from keystone.common import config from keystone.openstack.common import log as logging +from keystone.openstack.common.gettextutils import _ from keystone.openstack.common import strutils 上記のバグ報告は下記の URL にあります。\nhttps://bugs.launchpad.net/ubuntu/+source/swift/+bug/1231339\nzone 番号を付与します。\n% knife exec -E \u0026#34;nodes.find(:name =\u0026gt; \u0026#39;swift-storage01\u0026#39;) {|n| n.set[\u0026#39;swift\u0026#39;][\u0026#39;zone\u0026#39;] = \u0026#39;1\u0026#39;; n.save }\u0026#34; % knife exec -E \u0026#34;nodes.find(:name =\u0026gt; \u0026#39;swift-account01\u0026#39;) {|n| n.set[\u0026#39;swift\u0026#39;][\u0026#39;zone\u0026#39;] = \u0026#39;1\u0026#39;; n.save }\u0026#34; % knife exec -E \u0026#34;nodes.find(:name =\u0026gt; \u0026#39;swift-storage02\u0026#39;) {|n| n.set[\u0026#39;swift\u0026#39;][\u0026#39;zone\u0026#39;] = \u0026#39;2\u0026#39;; n.save }\u0026#34; % knife exec -E \u0026#34;nodes.find(:name =\u0026gt; \u0026#39;swift-account02\u0026#39;) {|n| n.set[\u0026#39;swift\u0026#39;][\u0026#39;zone\u0026#39;] = \u0026#39;2\u0026#39;; n.save }\u0026#34; % knife exec -E \u0026#34;nodes.find(:name =\u0026gt; \u0026#39;swift-storage03\u0026#39;) {|n| n.set[\u0026#39;swift\u0026#39;][\u0026#39;zone\u0026#39;] = \u0026#39;3\u0026#39;; n.save }\u0026#34; % knife exec -E \u0026#34;nodes.find(:name =\u0026gt; \u0026#39;swift-account03\u0026#39;) {|n| n.set[\u0026#39;swift\u0026#39;][\u0026#39;zone\u0026#39;] = \u0026#39;3\u0026#39;; n.save }\u0026#34; zone 番号が付与されたこと下記の通りを確認します\naccount-server の確認\n% knife exec -E \u0026#39;search(:node,\u0026#34;role:swift-account-server\u0026#34;) \\ { |n| z=n[:swift][:zone]||\u0026#34;not defined\u0026#34;; puts \u0026#34;#{n.name} has the role \\ [swift-account-server] and is in swift zone #{z}\u0026#34;; }\u0026#39; swift-account01 has the role [swift-account-server] and is in swift zone 1 swift-account02 has the role [swift-account-server] and is in swift zone 2 swift-account03 has the role [swift-account-server] and is in swift zone 3 container-server の確認\n% knife exec -E \u0026#39;search(:node,\u0026#34;role:swift-container-server\u0026#34;) \\ { |n| z=n[:swift][:zone]||\u0026#34;not defined\u0026#34;; puts \u0026#34;#{n.name} has the role \\ [swift-container-server] and is in swift zone #{z}\u0026#34;; }\u0026#39; swift-account01 has the role [swift-container-server] and is in swift zone 1 swift-account02 has the role [swift-container-server] and is in swift zone 2 swift-account03 has the role [swift-container-server] and is in swift zone 3 object-server の確認\n% knife exec -E \u0026#39;search(:node,\u0026#34;role:swift-object-server\u0026#34;) \\ { |n| z=n[:swift][:zone]||\u0026#34;not defined\u0026#34;; puts \u0026#34;#{n.name} has the role \\ [swift-object-server] and is in swift zone #{z}\u0026#34;; }\u0026#39; swift-storage01 has the role [swift-object-server] and is in swift zone 1 swift-storage02 has the role [swift-object-server] and is in swift zone 2 swift-storage03 has the role [swift-object-server] and is in swift zone 3 Chef が各々のノードに搭載された Disk を検知出来るか否かを確認する。\n% knife  exec -E \\ \u0026#39;search(:node,\u0026#34;role:swift-object-server OR \\ role:swift-account-server \\ OR role:swift-container-server\u0026#34;) \\ { |n| puts \u0026#34;#{n.name}\u0026#34;; \\ begin; n[:swift][:state][:devs].each do |d| \\ puts \u0026#34;\\tdevice #{d[1][\u0026#34;device\u0026#34;]}\u0026#34;; \\ end; rescue; puts \\ \u0026#34;no candidate drives found\u0026#34;; end; }\u0026#39; swift-storage02 device sdb1 swift-storage03 device sdb1 swift-account01 device sdb1 swift-account02 device sdb1 swift-account03 device sdb1 swift-storage01 device sdb1 swift-manage ノードにて chef-client を実行し /etc/swift/ring-workspace/generate-rings.sh を更新します。\nswift-manage% sudo chef-client generate-rings.sh の \u0026lsquo;exit 0\u0026rsquo; 行をコメントアウトし実行します。\nswift-manage% sudo ${EDITOR} /etc/swift/ring-workspace/generage-rings.sh swift-manage% sudo /etc/swift/ring-workspace/generate-rings.sh この操作で /etc/swift/ring-workspace/rings 配下に account, container, object 用の Rings ファイル群が生成されたことを確認出来るはずです。これらを swift-manage 上で既に稼働している git サーバに push し管理します。\nswift-manage# cd /etc/swift/ring-workspace/rings swift-manage# git add account.builder container.builder object.builder swift-manage# git add account.ring.gz container.ring.gz object.ring.gz swift-manage# git commit -m \u0026#34;initial commit\u0026#34; swift-manage# git push 各々のノードにて chef-client を実行することで git サーバ上の Rings ファイル群 を取得し、swift プロセスを稼働させます。\nswift-proxy01# chef-client swift-proxy02# chef-client swift-storage01# chef-client swift-storage02# chef-client swift-storage03# chef-client swift-account01# chef-client swift-account02# chef-client swift-account03# chef-client 3台のノードが登録されたかどうかを下記の通り確認行います。\nswift-proxy01% sudo swift-recon --md5 [sudo] password for thirai: =============================================================================== --\u0026gt; Starting reconnaissance on 3 hosts =============================================================================== [2013-10-18 11:14:43] Checking ring md5sums 3/3 hosts matched, 0 error[s] while checking hosts. =============================================================================== 動作確認 構築が出来ました！ということで動作の確認をしてみましょう。\nテストコンテナ \u0026lsquo;container01\u0026rsquo; にテストファイル \u0026lsquo;test\u0026rsquo; をアップロードしてみる。\nswift-storage01% swift -V 2 -A http://\u0026lt;ip_addr_keystone\u0026gt;:5000/v2.0/ -U admin:admin -K secrete stat swift-storage01% swift -V 2 -A http://\u0026lt;ip_addr_keystone\u0026gt;:5000/v2.0/ -U admin:admin -K secrete post container01 swift-storage01% echo \u0026#34;test\u0026#34; \u0026gt; test swift-storage01% swift -V 2 -A http://\u0026lt;ip_addr_keystone\u0026gt;:5000/v2.0/ -U admin:admin -K secrete upload container01 test swift-storage01% swift -V 2 -A http://\u0026lt;ip_addr_keystone\u0026gt;:5000/v2.0/ -U admin:admin -K secrete list container01 swift-storage01% swift -V 2 -A http://\u0026lt;ip_addr_keystone\u0026gt;:5000/v2.0/ -U admin:admin -K secrete list container01 test まとめ 前回「実用的な Swift 構成を Chef でデプロイ」の記事で記した内容とほぼ手順は変 わりませんでした。rcbops-cookbooks/rcbops-utils 内にソフトウェアの取得先レポジ トリを記すレシピが下記の場所にあります。\nhttps://github.com/rcbops-cookbooks/osops-utils/blob/master/recipes/packages.rb\nそして havana ブランチの attributes を確認すると Ubuntu Cloud Archive の URL が記されていることが確認出来ます。下記のファイルです。\nhttps://github.com/rcbops-cookbooks/osops-utils/blob/havana/attributes/repos.rb\nファイルの中身の抜粋です。\n\u0026#34;havana\u0026#34; =\u0026gt; { \u0026#34;uri\u0026#34; =\u0026gt; \u0026#34;http://ubuntu-cloud.archive.canonical.com/ubuntu\u0026#34;, \u0026#34;distribution\u0026#34; =\u0026gt; \u0026#34;precise-updates/havana\u0026#34;, \u0026#34;components\u0026#34; =\u0026gt; [\u0026#34;main\u0026#34;], \u0026#34;keyserver\u0026#34; =\u0026gt; \u0026#34;hkp://keyserver.ubuntu.com:80\u0026#34;, \u0026#34;key\u0026#34; =\u0026gt; \u0026#34;EC4926EA\u0026#34; }, これらのことより、rcbops-utils の attibutes で havana (実際には \u0026lsquo;havana-proposed\u0026rsquo;) をレポジトリ指定するように Cookbooks 構成を管理してあげれば Havana 構成の Keystone, Swift が構築出来ることになります。ちなみに havana-proposed で Swift や Keystone のどのバージョンがインストールされるかは、 下記の Packages ファイルを確認すると判断出来ます。\nhttp://ubuntu-cloud.archive.canonical.com/ubuntu/dists/precise-proposed/havana/main/binary-amd64/Packages\n以上です。\n","permalink":"https://jedipunkz.github.io/post/2014/03/16/rcbops-chef-cookbooks-keystone-havana-swift-1-10-0.deploy/","summary":"こんにちは。@jedipunkzです。\n追記 2014/03/20 : 一旦削除していた記事なのですが、無事動作が確認出来たので再度アッ プします！\n第17回 OpenStack 勉強会で rcbops/chef-cookbooks の話をしてきたのですが会場から 質問がありました。「Havana の Swift 構成を作る Cookbooks はどこにありますか？」 と。私が試したのが Grizzly 時代のモノで、よく rcbops/chef-cookbooks を見てみる と Havana ブランチ又は Havana に対応したリリースタグのファイル構成に Swift が 綺麗サッパリ消えているではありませんか・・！下記の Swift の Cookbooks は幸い github 上に残っていました。\nhttps://github.com/rcbops-cookbooks/swift\nが rcbops/chef-cookbooks との関連付けが切れています・・。ぐあぁ。\nということで Havana 構成の Keystone 2013.2.2 と Swift 1.10.0 の構成を Chef で 作らねば！と思い色々試していたら結構あっさりと出来ました。今回はその方法を書い ていきたいと思います！\n構成 構成は\u0026hellip;以前の記事 http://jedipunkz.github.io/blog/2013/10/27/swift-chef/ と同じです。\n+-----------------+ | load balancer | +-----------------+ | +-------------------+-------------------+-------------------+-------------------+---------------------- proxy network | | | | | +-----------------+ +-----------------+ +-----------------+ +-----------------+ +-----------------+ | chef server | | chef workstation| | swift-mange | | swift-proxy01 | | swift-proxy02 | +-----------------+ +-----------------+ +-----------------+ +-----------------+ +-----------------+ .","title":"rcbops/chef-cookbooks で Keystone 2013.2.2(Havana) + Swift 1.10.0 を構築"},{"content":"こんにちは。@jedipunkz です。\n今まで監視システムの Sensu やクラウドプラットフォームの OpenStack、コンフィギュ レーションマネージメントツールの Chef やクラウドライブラリの Fog 等使ってきま したが、これらを組み合わせるとオレオレオートスケーラ作れるんじゃないか？と思い、 ちょろっと作ってみました。\nちなみに自分はインフラエンジニアでしかも運用の出身なので Ruby に関しては初心者 レベルです。Chef で扱っているのと Rails アプリを作った経験はありますが、その程 度。Fog というクラウドライブラリにコントリビュートしたことはアリますが..。ちな みに Fog のコントリビュート内容は OpenStack Neutron(当時 Quantum) の仮想ルータ の操作を行う実装です。\nそんな自分ですが\u0026hellip;設計1周間・実装1周間でマネージャと CLI が出来ました。 また暫く放置していたマネージャと CLI に WebUI くっつけようかなぁ？と思って sinatra の学習を始めたのですが、学習を初めて 1.5 日で WebUI が動くところまで行 きました。何故か？Ruby には有用な技術が揃っているから・・！(´；ω；｀)ﾌﾞﾜｯ\nオレオレオートスケーラ \u0026lsquo;sclman\u0026rsquo; の置き場所 https://github.com/jedipunkz/sclman\nスクリーンショット +++\n構成は？ +-------------- public network +-------------+ | |sclman-api.rb| +----+----+---+ | sclman.rb | | vm | vm |.. | |sclman-cli.rb| +-------------+ +-------------+ +-------------+ +-------------+ | openstack | | chef server | | sensu server| | workstation | +-------------+ +-------------+ +-------------+ +-------------+ | | | | +---------------+---------------+---------------+--------------- management network \u0026lsquo;sclman\u0026rsquo; っていう名前です。上図の workstation ノードで稼働します。処理の流れは\n処理の流れ    sclman-cli.rb もしくは WebUI から HTTP クラスタのセットを OpenStack 上に生成    生成された VM に対して Chef で nginx をインストール    Chef の Roles である \u0026lsquo;LB\u0026rsquo; と \u0026lsquo;Web\u0026quot; が同一 Envrionment になるようにブートストラップ    LB VM のバックエンドに Web VM が指し示される    bootstrap と同時に sensu-client をインストール    Web VM の load を sensu で監視    sclman.rb (マネージャ) は Sensu AP を定期的に叩いて Web VM の load を監視    load が高い environment があれが該当 environment の HTTP クラスタに VM を追加    LB VM は追加された VM へのリクエストを追加    引き続き監視し一定期間 load が上がらなけれ Web VM を削除    LB VM は削除された VM へのリクエストを削除    といった感じです。要約すると CLI/WebUI から HA クラスタを作成。その時に LB, Web ミドルウェアと同時に sensu クライアントを VM に放り込む。監視を継続して負 荷が上昇すれば Web インスタンスの数を増加させ LB のリクエスト振り先にもその追 加した VM のアドレスを追加。逆に負荷が下がれば VM 削除と共にリクエスト振り先も 削除。この間、人の手を介さずに処理してくれる。です。\n使い方 詳細な使い方は github の README.md を見て下さい。ここには簡単な説明だけ書いて おきます。\n sclman を github から取得して bundler で必要な gems をインストールします。 chef-repo に移動して Berkshelf で必要な cookbooks をインストールします。 cookbooks を用いて sensu をデプロイします。 Omnibus インストーラーを使って chef サーバをインストールします。 OpenStack をインストールします。 sclman.conf を環境に合わせて修正します。 sclman.rb (マネージャ) を稼働します。 sclman-api.rb (WebUI/API) を稼働します。 sclman-cli.rb (CLI) もしくは WebUI から最小構成の HTTP クラスタを稼働します。 この状態で \u0026lsquo;Web\u0026rsquo; Role のインスタンスに負荷が掛かると \u0026lsquo;Web\u0026rsquo; Role のインスタンスの数が増加します。 また逆に負荷が下がるとインスタンス数が減ります。  負荷の増減のシビアさは sclman.conf のパラメータ \u0026lsquo;man_sensitivity\u0026rsquo; で決定します。 値が長ければ長いほど増減のし易さは低下します。\nまとめ +++\nこんな僕でも Ruby の周辺技術使ってなんとなくの形が出来ましたー。ただまだまだ課 題はあって、インフラを制御するアプリってエラーハンドリングが難しいっていうこと です。帰ってくるエラーの一覧などがクラウドプラットフォーム・クラウドライブラリ のドキュメントにあればいいのだけど、まだそこまで行ってない。Fog もまだまだ絶賛 開発中と言うかクラウドプラットフォームの進化に必死で追いついている状態なので、 僕らがアプリを作るときには自分でエラーを全部洗い出す等の作業が必要になるかもし れない。大変だけど面白い所でもありますよね！これからも楽しみです。\n","permalink":"https://jedipunkz.github.io/post/2014/03/05/sensu-chef-openstack-fog-autoscaler/","summary":"こんにちは。@jedipunkz です。\n今まで監視システムの Sensu やクラウドプラットフォームの OpenStack、コンフィギュ レーションマネージメントツールの Chef やクラウドライブラリの Fog 等使ってきま したが、これらを組み合わせるとオレオレオートスケーラ作れるんじゃないか？と思い、 ちょろっと作ってみました。\nちなみに自分はインフラエンジニアでしかも運用の出身なので Ruby に関しては初心者 レベルです。Chef で扱っているのと Rails アプリを作った経験はありますが、その程 度。Fog というクラウドライブラリにコントリビュートしたことはアリますが..。ちな みに Fog のコントリビュート内容は OpenStack Neutron(当時 Quantum) の仮想ルータ の操作を行う実装です。\nそんな自分ですが\u0026hellip;設計1周間・実装1周間でマネージャと CLI が出来ました。 また暫く放置していたマネージャと CLI に WebUI くっつけようかなぁ？と思って sinatra の学習を始めたのですが、学習を初めて 1.5 日で WebUI が動くところまで行 きました。何故か？Ruby には有用な技術が揃っているから・・！(´；ω；｀)ﾌﾞﾜｯ\nオレオレオートスケーラ \u0026lsquo;sclman\u0026rsquo; の置き場所 https://github.com/jedipunkz/sclman\nスクリーンショット +++\n構成は？ +-------------- public network +-------------+ | |sclman-api.rb| +----+----+---+ | sclman.rb | | vm | vm |.. | |sclman-cli.rb| +-------------+ +-------------+ +-------------+ +-------------+ | openstack | | chef server | | sensu server| | workstation | +-------------+ +-------------+ +-------------+ +-------------+ | | | | +---------------+---------------+---------------+--------------- management network \u0026lsquo;sclman\u0026rsquo; っていう名前です。上図の workstation ノードで稼働します。処理の流れは","title":"Sensu,Chef,OpenStack,Fog を使ったオレオレオートスケーラを作ってみた！"},{"content":"こんにちは、@jedipunkz です。\n前回、\u0026lsquo;Ceph のプロセス配置ベストプラクティス\u0026rsquo; というタイトルで記事を書きました。\nhttp://jedipunkz.github.io/blog/2014/01/29/ceph-process-best-practice/\n今回はこの記事にあるポリシに従って下記のような特徴を持った構成を作る手順を具体 的に書いていきたいと思います。\n ceph01 - ceph04 の4台構成 ノードに HDD 2台搭載されていることを前提 (/dev/sdb, /dev/sdc) ノードに Journal 用 SSD 1台搭載されていることを前提 (/dev/ssd) ceph04 は mds サービス稼働 ceph05 は ceph-deploy を実行するためのワークステーション 最終的に ceph04 から Ceph をマウントする mon は ノード単位で稼働 osd は HDD 単位で稼働 mds は ceph04 に稼働  構成 : ハードウェアとノードとネットワークの関係  public network +-------------------+-------------------+-------------------+-------------------+--------- | | | | | +--------+--------+ +--------+--------+ +--------+--------+ +--------+--------+ +--------+--------+ | ceph01 | | ceph02 | | ceph03 | | ceph04 | | ceph05 | | +-----+ +-----+ | | +-----+ +-----+ | | +-----+ +-----+ | | | | | | | sdb | | sdc | | | | sdb | | sdc | | | | sdb | | sdc | | | | | | | +-----+-+-----+ | | +-----+-+-----+ | | +-----+-+-----+ | | | | | | | ssd | | | | ssd | | | | ssd | | | | | | | +-------------+ | | +-------------+ | | +-------------+ | | | | | +--------+--------+ +--------+--------+ +--------+--------+ +-----------------+ +-----------------+ | | | cluster network +-------------------+-------------------+------------------------------------------------- 構成 : プロセスとノードとネットワークの関係  public network +-------------------+-------------------+-------------------+-------------------+--------- | | | | | +--------+--------+ +--------+--------+ +--------+--------+ +--------+--------+ +--------+--------+ | ceph01 | | ceph02 | | ceph03 | | ceph04 | | ceph05 | | +-----+ +-----+ | | +-----+ +-----+ | | +-----+ +-----+ | | +-------------+ | | | | | osd | | osd | | | | osd | | osd | | | | osd | | osd | | | | mds | | | | | +-----+-+-----+ | | +-----+-+-----+ | | +-----+-+-----+ | | +-------------+ | | | | | mon | | | | mon | | | | mon | | | | | | | +-------------+ | | +-------------+ | | +-------------+ | | | | | +--------+--------+ +--------+--------+ +--------+--------+ +-----------------+ +-----------------+ | | | cluster network +-------------------+-------------------+------------------------------------------------- 注意 : 上記の図だと ssd : mon が対に見えますが、そうではありません。\nでは構築方法について書いていきます。\n作業用ホストの準備 ノンパスフレーズの SSH 公開鍵・秘密鍵を生成する。\n% ssh-keygen 公開鍵をターゲットホスト (ceph01-03) に配置\n% ssh-copy-id ceph@ceph01 % ssh-copy-id ceph@ceph02 % ssh-copy-id ceph@ceph03 ceph-deploy の取得を行う。\n% git clone https://github.com/ceph/ceph-deploy.git ~/ceph-deploy \u0026lsquo;python-virtualenv\u0026rsquo; パッケージをインストールする。\n% sudo apt-get update ; sudo apt-get -y install python-virtualenv ceph-deploy をブートストラップする\n% cd ~/ceph-deploy % ./bootstrap PATH を通す。下記は例。\n% ${EDITOR} ~/.zshrc export PATH=$HOME/ceph-deploy:$PATH ホスト名の解決を行う。IP アドレスは例。\n% sudo ${EDITOR} /etc/hosts 10.200.10.11 ceph01 10.200.10.12 ceph02 10.200.10.13 ceph03 10.200.10.14 ceph04 10.200.10.15 ceph05 上記の構成の構築方法 以前の記事同様に ceph-deploy をデプロイツールとして用いる。\nceph-deploy に関しては下記の URL を参照のこと。\nhttps://github.com/ceph/ceph-deploy\n下記の手順でコンフィギュレーションと鍵の生成を行う。またこれからの操作はすべて public network 上の ceph-deploy 専用 node からの操作とする。\n% mkdir ~/ceph-work % cd ~/ceph-work % ceph-deploy --cluster cluster01 new ceph01 ceph02 ceph03 ceph04 ~/ceph-work ディレクトリ上に cluster01.conf が生成されているので下記の通り cluster network を扱う形へと追記を行う。\npublic network = \u0026lt;public_network_addr\u0026gt;/\u0026lt;netmask\u0026gt; cluster network = \u0026lt;cluster_network_addr\u0026gt;/\u0026lt;netmask\u0026gt; [mon.a] host = ceph01 mon addr = \u0026lt;ceph01_ip_addr\u0026gt;:6789 [mon.b] host = ceph02 mon addr = \u0026lt;ceph02_ip_addr\u0026gt;:6789 [mon.c] host = ceph03 mon addr = \u0026lt;ceph03_ip_addr\u0026gt;:6789 [osd.0] public addr = \u0026lt;ceph01_public_ip_addr\u0026gt; cluster addr = \u0026lt;ceph01_cluster_ip_addr\u0026gt; [osd.1] public addr = \u0026lt;ceph01_public_ip_addr\u0026gt; cluster addr = \u0026lt;ceph01_cluster_ip_addr\u0026gt; [osd.2] public addr = \u0026lt;ceph01_public_ip_addr\u0026gt; cluster addr = \u0026lt;ceph01_cluster_ip_addr\u0026gt; [mds.a] host = ceph04 ceph の各 nodes へのインストールを行う。ceph はワークステーションである ceph05 にも インストールしておきます。後に Ceph ストレージをマウントするためです。\n% ceph-deploy --cluster cluster01 install ceph01 ceph02 ceph03 ceph04 ceph04 mon プロセスを各 nodes で稼働する。\n% ceph-deploy --cluster cluster01 mon create ceph01 ceph02 ceph03 鍵の配布を各 nodes に行う。\n% ceph-deploy --cluster cluster01 gatherkeys ceph01 ceph02 ceph03 ceph04 ceph05 disk のリストを確認。\n各 node 毎に用いることが可能は disk の一覧を確認する。\n% ceph-deploy --cluster cluster01 disk list ceph01 % ceph-deploy --cluster cluster01 disk list ceph02 % ceph-deploy --cluster cluster01 disk list ceph03 disk の初期化を行う。この作業を行うと指定ディスク上のデータは消去される。\n% ceph-deploy --cluster cluster01 disk zap ceph01:/dev/sdb ceph01:/dev/sdc % ceph-deploy --cluster cluster01 disk zap ceph02:/dev/sdb ceph02:/dev/sdc % ceph-deploy --cluster cluster01 disk zap ceph03:/dev/sdb ceph03:/dev/sdc journal 用の ssd のパーティションを切る。ここでは 10GB 毎に切った /dev/ssd1, /dev/ssd2 が存在することを前提に記す。ceph と同時にインストールされた gdisk を用いる。\n% sudo gdisk /dev/ssd (注意) 下記の公式ドキュメントでは osd prepare, osc activate の手順が掲載されて いるがその後の osd create のコマンドにて prepare が実行されるようでこれら2つの 手順を行うと正常に osd create コマンドが実行できなかった。よってこのタイミング にて osd create を行うものとする。\n http://ceph.com/docs/master/rados/deployment/ceph-deploy-osd/#prepare-osds http://ceph.com/docs/dumpling/start/quick-ceph-deploy/  2 つの disk に対してそれぞれ osd を稼働させる。\n% ceph-deploy --cluster cluster01 osd create ceph01:sdb:/dev/ssd1 ceph02:sdb:/dev/ssd1 ceph03:sdb:/dev/ssd1 % ceph-deploy --cluster cluster01 osd create ceph01:sdc:/dev/ssd2 ceph02:sdc:/dev/ssd2 ceph03:sdc:/dev/ssd2 mds の稼働を行う。ここでは1号機にのみ稼働を行う。\n% ceph-deploy --cluster cluster01 mds create ceph04 クライアントからのマウント方法各種 上記で構築した Ceph ストレージを利用する方法を3つ説明する。先に述べたように POSIX 互換 filesystem として利用が可能。それぞれ mds が稼働しているホストに対 して接続を行う。\nBlock Device としてマウントする方法 ストレージ上に block device を生成しそれをマウントする\ncephclient% rbd create foo --size 4096 cephclient% sudo modprobe rbd cephclient% sudo rbd map foo --pool rbd --name client.admin cephclient% sudo mkfs.ext4 -m0 /dev/rbd/rbd/foo cephclient% sudo mkdir /mnt/myrbd cephclinet% sudo mount /dev/rbd/rbd/foo /mnt/myrbd Kernel Driver を用いてマウントする方法 kernel Driver を用いてストレージをマウントする\ncephclient% sudo mkdir /mnt/mycephfs cephclient% sudo mount -t ceph 10.200.10.26:6789:/ /mnt/mycephfs -o \\  name=admin,secret=`sudo ceph-authtool -p /etc/ceph/cluster01.client.admin.keyring` Fuse Driver (ユーザランド) を用いてマウントする方法 ユーザランドソフトウェア FUSE を用いてマウントする方法\ncephclient% sudo mkdir /home/\u0026lt;username\u0026gt;/cephfs cephclient% sudo ceph-fuse -m 10.200.10.26:6789 /home/\u0026lt;username\u0026gt;/cephfs ","permalink":"https://jedipunkz.github.io/post/2014/02/27/journal-ssd-ceph-deploy/","summary":"こんにちは、@jedipunkz です。\n前回、\u0026lsquo;Ceph のプロセス配置ベストプラクティス\u0026rsquo; というタイトルで記事を書きました。\nhttp://jedipunkz.github.io/blog/2014/01/29/ceph-process-best-practice/\n今回はこの記事にあるポリシに従って下記のような特徴を持った構成を作る手順を具体 的に書いていきたいと思います。\n ceph01 - ceph04 の4台構成 ノードに HDD 2台搭載されていることを前提 (/dev/sdb, /dev/sdc) ノードに Journal 用 SSD 1台搭載されていることを前提 (/dev/ssd) ceph04 は mds サービス稼働 ceph05 は ceph-deploy を実行するためのワークステーション 最終的に ceph04 から Ceph をマウントする mon は ノード単位で稼働 osd は HDD 単位で稼働 mds は ceph04 に稼働  構成 : ハードウェアとノードとネットワークの関係  public network +-------------------+-------------------+-------------------+-------------------+--------- | | | | | +--------+--------+ +--------+--------+ +--------+--------+ +--------+--------+ +--------+--------+ | ceph01 | | ceph02 | | ceph03 | | ceph04 | | ceph05 | | +-----+ +-----+ | | +-----+ +-----+ | | +-----+ +-----+ | | | | | | | sdb | | sdc | | | | sdb | | sdc | | | | sdb | | sdc | | | | | | | +-----+-+-----+ | | +-----+-+-----+ | | +-----+-+-----+ | | | | | | | ssd | | | | ssd | | | | ssd | | | | | | | +-------------+ | | +-------------+ | | +-------------+ | | | | | +--------+--------+ +--------+--------+ +--------+--------+ +-----------------+ +-----------------+ | | | cluster network +-------------------+-------------------+------------------------------------------------- 構成 : プロセスとノードとネットワークの関係  public network +-------------------+-------------------+-------------------+-------------------+--------- | | | | | +--------+--------+ +--------+--------+ +--------+--------+ +--------+--------+ +--------+--------+ | ceph01 | | ceph02 | | ceph03 | | ceph04 | | ceph05 | | +-----+ +-----+ | | +-----+ +-----+ | | +-----+ +-----+ | | +-------------+ | | | | | osd | | osd | | | | osd | | osd | | | | osd | | osd | | | | mds | | | | | +-----+-+-----+ | | +-----+-+-----+ | | +-----+-+-----+ | | +-------------+ | | | | | mon | | | | mon | | | | mon | | | | | | | +-------------+ | | +-------------+ | | +-------------+ | | | | | +--------+--------+ +--------+--------+ +--------+--------+ +-----------------+ +-----------------+ | | | cluster network +-------------------+-------------------+------------------------------------------------- 注意 : 上記の図だと ssd : mon が対に見えますが、そうではありません。","title":"Journal 用 SSD を用いた Ceph 構成の構築"},{"content":"Ceph はブロック型の分散ストレージファイルシステムです。POSIX のファイルシステ ムとしてマウント出来ます。Linux の Kernel ドライバや FUSE ドライバを用いてマウ ントします。またブロックデバイスとしてマウントする方法もあります。\nだいぶ前ですが、Ceph に関する記事を以前下記の通り書きました。\n http://jedipunkz.github.io/blog/2013/05/25/ceph-cluster-network/ http://jedipunkz.github.io/blog/2013/05/11/ceph-deploy/  Ceph の構築方法について記したブログだったのですが、今まで mon, osd, mds の各プ ロセスをそれぞれ何台のノードに対して配置し、またそれぞれのプロセス幾つを何に対 して配置するのか？という疑問が付きまとわっていました。node, disk, process のそ れぞれの数の関係について知りたいなぁと思っていました。幾つかのドキュメントを読 んでいて、ぼんやり見えてきた事があるのでそれを今回はまとめたいと思います。\nまた、皆さん気になるトコロだと思われる容量設計についても軽く触れたいと思います。\n参考資料  http://ceph.com/docs/master/rados/configuration/mon-config-ref/ http://www.sebastien-han.fr/blog/2013/12/02/ceph-performance-interesting-things-going-on/  各要素の数の関係 ハードウェア要素である node, disk(hdd), ssd そしてソフトウェア要素である mon, osd, mds の数の関係はどのようにするべきか？基本となる関係は\n 1 mds process / node 1 mon process / node 1 osd process / disk n jornal ssd device / disk / node  だと考えられます。僕が今のところ理想かなぁと思っている構成をまとめたいと思いま す。\n下記の図がそれです。\n+------------------------+ | client | +------------------------+ | +--------------------------+--------------------------+-------------------------------+------------------------- | | | | public network +------------------------+ +------------------------+ +------------------------+ +------------------------+ | mon | | mon | | mon | | mds | +------+ +------+ +------+ +------+ +------+ +------+ +------+ +------+ +------+ +------------------------+ | osd | | osd | | osd | | osd | | osd | | osd | | osd | | osd | | osd | | | +------+ +------+ +------+ +------+ +------+ +------+ +------+ +------+ +------+ | | | disk | | disk | | disk | | disk | | disk | | disk | | disk | | disk | | disk |....\u0026gt; | | +------+ +------+ +------+ +------+ +------+ +------+ +------+ +------+ +------+scale | node | | ssd | | ssd | | ssd | | | +------------------------+ +------------------------+ +------------------------+ | | | node | | node | | node | | | +------------------------+ +------------------------+ +------------------------+ +------------------------+ | | | | +--------------------------+--------------------------+-------------------------------+------------------------- cluster network mds と node の関係 mds はリモートクライアントへのファイルシステムサービスの提供を行うことや特性が 全く異なることから別ノードに切り出しました。また mds は幾つかのノードで稼働さ せる事も可能。が、mds はそれぞれのサービスを HA 組む仕組みは持っていないので どれか一方の mds をクライアントは指し示す必要があり、その mds が停止すれば直 ちに障害に発展します。\nmon と node の関係 mon は比較的少量のリソースで稼働します。今回 osd と同じノードの搭載しましたが 別ノードに切り出すことも勿論可能です。mon は CRUSH Maps アルゴリズムの元に連携 が取れますので複数のプロセスを稼働することが推奨されていることからも、比較的少 ないノード数のクラスタの場合は osd と同ノードに搭載するのが容易かなと考えまし た。\nosd と node の関係 1 osd プロセスに対して 1 disk が基本となります。osd は実データのレプリケーショ ンを行うことからコンフィギュレーションに対して上図の様にクラスタ用のネットワー クを紐付け、高トラヒックに対応する必要があります。また osd 用の disk device で すが RAID を組まないことが推奨されています。CEPH 自体に HA の仕組みがあること、 また RAID 構成にもよりますがディスクアクセスが遅くなることは Ceph にとってボト ルネックを早く招くことになりますし、小さいディスク容量しか扱えなくなることは Ceph にとって不利になると思います。\njournal 用の ssd device と disk, node の関係 現在の Stable Release 版の Ceph は journal を用いてメタデータを管理します。各 osd の disk 単位に journal 用の disk device を指定出来るようになっています。メ タデータですので実データ用の disk よりだいぶ小さな容量で構わないこと、また比較 的高速なアクセスを要求されることからも SSD を選択することが推奨されつつあるよ うです。実際にストアされるデータの特性にもよりますが 1 node に対して 1 ssd device を配置すれば十分かと思います。また osd のプロセスの数 (disk の数) に対 して一つのパーティションを切ることで対応出来るかと思います。\n設定方法の例を記します。ここに ceph01-03 の3台のノードがありそれぞれ 2 disk, 1 ssd が搭載されているとします。/dev/ssd は gdisk 等を用いて2つ以上のパーティショ ンに切り分けます。\n下記のように /dev/sdb (hdd) に対して /dev/ssd1 (ssd), /dev/sdc (hdd) に対して /dev/ssd2 (ssd) を割り当てることが出来ます。\n% ceph-deploy --cluster cluster01 osd create ceph01:sdb:/dev/ssd1 ceph02:sdb:/dev/ssd1 ceph03:sdb:/dev/ssd1 % ceph-deploy --cluster cluster01 osd create ceph01:sdc:/dev/ssd2 ceph02:sdc:/dev/ssd2 ceph03:sdc:/dev/ssd2 Ceph ストレージ全体の容量設計と mon の ratio の関係 3TB のディスクを持ったノードが 33 台並んでいるとします。各ノードには osd プロ セスが1つ稼働します。合計容量は 99 TB となりますが mon が持っているコンフィギュ レーションである full ratio というパラメータがありデフォルト値が 0.95 となって います。よってこのクラスタで扱える全体のディスク容量は 95TB となります。\nまた、ラックに数台のノードを積むのが通常ですが、電源故障等で一気にラック単位で 障害が発生したとします。この場合 Ceph はすべてのデータに関してレプリカを取り復 旧作業を行います。しかしながら停止するノード数によってはストレージ全体の扱える 容量をオーバーすることも懸念されます。これに対応するために先ほど登場した ratio パラメータを調整することが出来ます。\n[global] mon osd full ratio = .80 mon osd nearfull ratio = .70 上記の例では full ステートの ratio が 0.80, nearfull ステートの ratio が 0.70 となります。想定の障害ノード数を考慮し ratio パラメータにてその台数分を減算す れば良いわけです。\nまとめ 前述した通り上図は比較的少ないノード数のクラスタを組む場合を想定しています。ノー ド数が増える場合は mon は mds, osd とも必要とするリソースの特性が異なりますの で別ノードに切り出すことも考えたほうが良さそうです。2014年の2月には Firefly と いう新しいリリース版が出ます。ここでのブループリント(設計書)を確認すると\u0026hellip;\nhttp://wiki.ceph.com/Planning/Blueprints/Firefly/osd%3A_new_key%2F%2Fvalue_backend\njournal に変わる新たなメタデータ管理方法として KVS データベースを扱うことも視 野に入っているようです。上記の URL 見る限りでは Facebook がオープンソースにし た rocksdb や fusionio の nvmkv, seagate の kinetic 等が挙がっています。2月に 期待しましょう！\n","permalink":"https://jedipunkz.github.io/post/2014/01/29/ceph-process-best-practice/","summary":"Ceph はブロック型の分散ストレージファイルシステムです。POSIX のファイルシステ ムとしてマウント出来ます。Linux の Kernel ドライバや FUSE ドライバを用いてマウ ントします。またブロックデバイスとしてマウントする方法もあります。\nだいぶ前ですが、Ceph に関する記事を以前下記の通り書きました。\n http://jedipunkz.github.io/blog/2013/05/25/ceph-cluster-network/ http://jedipunkz.github.io/blog/2013/05/11/ceph-deploy/  Ceph の構築方法について記したブログだったのですが、今まで mon, osd, mds の各プ ロセスをそれぞれ何台のノードに対して配置し、またそれぞれのプロセス幾つを何に対 して配置するのか？という疑問が付きまとわっていました。node, disk, process のそ れぞれの数の関係について知りたいなぁと思っていました。幾つかのドキュメントを読 んでいて、ぼんやり見えてきた事があるのでそれを今回はまとめたいと思います。\nまた、皆さん気になるトコロだと思われる容量設計についても軽く触れたいと思います。\n参考資料  http://ceph.com/docs/master/rados/configuration/mon-config-ref/ http://www.sebastien-han.fr/blog/2013/12/02/ceph-performance-interesting-things-going-on/  各要素の数の関係 ハードウェア要素である node, disk(hdd), ssd そしてソフトウェア要素である mon, osd, mds の数の関係はどのようにするべきか？基本となる関係は\n 1 mds process / node 1 mon process / node 1 osd process / disk n jornal ssd device / disk / node  だと考えられます。僕が今のところ理想かなぁと思っている構成をまとめたいと思いま す。","title":"Ceph のプロセス配置ベストプラクティス"},{"content":"こんにちは。@jedipunkzです。\n昨晩、第17回 OpenStack 勉強会が開催されました\nhttp://connpass.com/event/4545/\nここで発表をしてきましたぁ！発表タイトルは \u0026ldquo;rcbops/chef-cookbooks\u0026rdquo; です。\n何を発表したかと言うと詳しくは上記のスライドを見ていただくとして、簡単に言うと \u0026ldquo;RackSpace 社のエンジニアが管理している Chef Cookbooks でOpenStack 構成を作ろ う\u0026rdquo; ってことです。\n今日知ったのですがどうも昨晩は初心者向けの勉強会という位置付けだったらしく..少 しだけディープな話題を話してしまったかもしれません！すいません！＞＜\nでもとても楽しく発表出来ましたし、逆に質問のコーナーで最新の情報も教えてもらえ たり！なんと Havana 対応の v4.2.0 以降では Swift の Cookbooks が消えてしまった とか！\u0026hellip; 皆 Swift 好きくないの？\u0026hellip;; ;\nrcbops/chef-cookbooks はずっと追っていますが、ものすごいスピードで開発進んでい るので、今後ぜひみなさん使ってみて下さいー。\n最後に詳しい利用方法を記した僕のブログの URL を貼り付けておきます。\n OpenStack Havana を Chef でデプロイ  http://jedipunkz.github.io/blog/2013/11/17/openstack-havana-chef-deploy/\n Swift HA 構成を Chef でデプロイ  http://jedipunkz.github.io/blog/2013/07/26/swift-ha-chef-deploy/\n 実用的な Swift 構成を Chef でデプロイ  http://jedipunkz.github.io/blog/2013/10/27/swift-chef/\n","permalink":"https://jedipunkz.github.io/post/2014/01/21/17th-openstack-study/","summary":"こんにちは。@jedipunkzです。\n昨晩、第17回 OpenStack 勉強会が開催されました\nhttp://connpass.com/event/4545/\nここで発表をしてきましたぁ！発表タイトルは \u0026ldquo;rcbops/chef-cookbooks\u0026rdquo; です。\n何を発表したかと言うと詳しくは上記のスライドを見ていただくとして、簡単に言うと \u0026ldquo;RackSpace 社のエンジニアが管理している Chef Cookbooks でOpenStack 構成を作ろ う\u0026rdquo; ってことです。\n今日知ったのですがどうも昨晩は初心者向けの勉強会という位置付けだったらしく..少 しだけディープな話題を話してしまったかもしれません！すいません！＞＜\nでもとても楽しく発表出来ましたし、逆に質問のコーナーで最新の情報も教えてもらえ たり！なんと Havana 対応の v4.2.0 以降では Swift の Cookbooks が消えてしまった とか！\u0026hellip; 皆 Swift 好きくないの？\u0026hellip;; ;\nrcbops/chef-cookbooks はずっと追っていますが、ものすごいスピードで開発進んでい るので、今後ぜひみなさん使ってみて下さいー。\n最後に詳しい利用方法を記した僕のブログの URL を貼り付けておきます。\n OpenStack Havana を Chef でデプロイ  http://jedipunkz.github.io/blog/2013/11/17/openstack-havana-chef-deploy/\n Swift HA 構成を Chef でデプロイ  http://jedipunkz.github.io/blog/2013/07/26/swift-ha-chef-deploy/\n 実用的な Swift 構成を Chef でデプロイ  http://jedipunkz.github.io/blog/2013/10/27/swift-chef/","title":"第17回 OpenStack 勉強会で話してきました"},{"content":"こんにちは。@jedipunkzです。\nSerf の登場があったり、ここ最近オーケストレーションについて考える人が増えた気 がします。システムをデプロイしてその後各ノード間の連結だったりも同じ Chef, Puppet 等のコンフィギュレーションツールで行うのか？全く別のツールで？..\n最近 Serf というツールの登場がありました。\n僕も Serf を触ってつい先日ブログに書きました。有用なツールだと思います。シ ンプルだからこそ応用が効きますし、リアルタイム性もあり、将来的に異なるネットワー クセグメント上のノードとも連結出来るようになりそうですし、とても期待です。\n話が少し飛びますが..\nいつも Rebuild.fm を楽しく聞いているのですが Immutable Infrastructure の話題の 時にオーケストレーションの話題になって、どうも \u0026lsquo;Chef でも自律的なクラスタを組 むことが認知されていないのでは？\u0026rsquo; と思うようになりました。もちろん Chef でやる べき！とは言い切りませんし、今後どうなるかわかりません。Opscode の中の人も \u0026lsquo;オー ケストレーションは自分でやってね\u0026rsquo; というスタンスだったとずいぶん前ですが聞きま した。Serf を等のオーケストレーションツールを使う使わないの話は今回はしないの ですが Chef でも自律的クラスタを組むことは出来ますよ〜というのが今回の話題。\nまえがきが長くなりました。\n今回は Chef で自律的クラスタを構成する方法を記したいと思います。\nhaproxy 等を利用すれば尚良いと思いますが、よりクラスタを組むのが簡単な nginx を今回は利用したいと思います。\nhttps://github.com/opscode-cookbooks/nginx\n構成 \u0026lsquo;web\u0026rsquo; という Role 名と \u0026lsquo;lb\u0026rsquo; という Role 名で単純な HTTP サーバとしての nginx ノードを複数台と、ロードバランサとしての nginx ノードを1台でクラスタを構成しま す。また共に environment 名は同じものを利用します。別の environment 名の場合は 別クラスタという区切りです。\n \u0026lsquo;lb\u0026rsquo; node x 1 + \u0026lsquo;web\u0026rsquo; node x n (\u0026lsquo;foo\u0026rsquo; environment) \u0026lsquo;lb\u0026rsquo; node x 1 + \u0026lsquo;web\u0026rsquo; node x n (\u0026lsquo;bar\u0026rsquo; environment)  \u0026lsquo;lb\u0026rsquo; nginx ロードバランサのレシピ 下記が \u0026lsquo;lb\u0026rsquo; Role の recipes/cmomnos_conf.rb の修正した内容です。\nenvironment = node.chef_environment webservers = search(:node, \u0026#34;role:web AND chef_environment:#{environment}\u0026#34;) template \u0026#34;#{node[\u0026#39;nginx\u0026#39;][\u0026#39;dir\u0026#39;]}/sites-available/default\u0026#34; do source \u0026#34;default-site.erb\u0026#34; owner \u0026#34;root\u0026#34; group \u0026#34;root\u0026#34; mode 00644 notifies :reload, \u0026#39;service[nginx]\u0026#39; variables ({ :webservers =\u0026gt; webservers }) end 何をやっているかと言うと、environment という変数に自ノードの environment 名を。 webservers という変数に role 名が \u0026lsquo;web\u0026rsquo; で尚且つ自ノードと同じ environment 名 が付いたノード名を入れています。これで自分と同じ environment に所属している \u0026lsquo;web\u0026rsquo; Role なノードを Chef サーバに対して検索しています。また、template 内で webservers という変数をそのまま利用できるように variables で渡しています。\n\u0026lsquo;lb\u0026rsquo; nginx ロードバランサのテンプレート 下記が webservers 変数を受け取った後の template 内の処理です。\n\u0026lt;% if  @webservers and ( @webservers != [] ) %\u0026gt; upstream backend { \u0026lt;% @webservers.each do |hostname| -%\u0026gt; server \u0026lt;%= hostname[\u0026#39;ipaddr\u0026#39;] -%\u0026gt;; \u0026lt;% end -%\u0026gt; } \u0026lt;% end %\u0026gt; server { listen 80; server_name \u0026lt;%= node[\u0026#39;hostname\u0026#39;] %\u0026gt;; access_log \u0026lt;%= node[\u0026#39;nginx\u0026#39;][\u0026#39;log_dir\u0026#39;] %\u0026gt;/localhost.access.log; location / { \u0026lt;% if  @webservers and ( @webservers != [] ) %\u0026gt; proxy_pass http://backend; \u0026lt;% else %\u0026gt; root /var/www/nginx-default; index index.html index.htm; \u0026lt;% end  %\u0026gt; } } upstream backend { \u0026hellip; は皆さん見慣れた記述だと思うのですが、バックエンドの HTTP サーバの IP アドレスを一覧化します。each で回しているので台数分だけ server \u0026lt;ip_addr\u0026gt;; の記述が入ります。\nchef-client をデーモン稼働しておけば、新規に Chef サーバに登録された \u0026lsquo;web\u0026rsquo; Role の HTTP サーバを自動で \u0026lsquo;lb\u0026rsquo; Role のロードバランサが組み込む、つまり自律的 なクラスタが組めることになります。もちろんこの間の手作業は一切ありません。\nちなみに chef-client をデーモン稼働するには\nrecipe[chef-client::service]  というレシピをノードに割り当てることで可能です。\nまとめ Chef でも自律的なクラスタが組めました。もちろん chef-client の稼働間隔があるの でリアルタイム性はありません。chef-client の稼働間隔は \u0026lsquo;chef-client\u0026rsquo; レシピの attributes で調整出来ます。その点は serf のほうが確実に勝っていると見るべきで しょう。冒頭に記したようにこの辺りの操作を Chef で行うのか別のツールを使うのか はまだまだ模索が必要そう。ただ、私がいつも使っている \u0026lsquo;OpenStack を Chef で構成 する Cookbooks\u0026rsquo; 等は複数台構成を Chef で構成しています。なので僕にとってはこの 辺りの話は当たり前だと思っていたのだけど、どうも勉強会に出たりすると \u0026ldquo;Chef は 複数台構成を作るのが苦手だ\u0026rdquo; って話があがってくるので気になっていました。\n","permalink":"https://jedipunkz.github.io/post/2013/12/09/chef-autonoumous-cluster/","summary":"こんにちは。@jedipunkzです。\nSerf の登場があったり、ここ最近オーケストレーションについて考える人が増えた気 がします。システムをデプロイしてその後各ノード間の連結だったりも同じ Chef, Puppet 等のコンフィギュレーションツールで行うのか？全く別のツールで？..\n最近 Serf というツールの登場がありました。\n僕も Serf を触ってつい先日ブログに書きました。有用なツールだと思います。シ ンプルだからこそ応用が効きますし、リアルタイム性もあり、将来的に異なるネットワー クセグメント上のノードとも連結出来るようになりそうですし、とても期待です。\n話が少し飛びますが..\nいつも Rebuild.fm を楽しく聞いているのですが Immutable Infrastructure の話題の 時にオーケストレーションの話題になって、どうも \u0026lsquo;Chef でも自律的なクラスタを組 むことが認知されていないのでは？\u0026rsquo; と思うようになりました。もちろん Chef でやる べき！とは言い切りませんし、今後どうなるかわかりません。Opscode の中の人も \u0026lsquo;オー ケストレーションは自分でやってね\u0026rsquo; というスタンスだったとずいぶん前ですが聞きま した。Serf を等のオーケストレーションツールを使う使わないの話は今回はしないの ですが Chef でも自律的クラスタを組むことは出来ますよ〜というのが今回の話題。\nまえがきが長くなりました。\n今回は Chef で自律的クラスタを構成する方法を記したいと思います。\nhaproxy 等を利用すれば尚良いと思いますが、よりクラスタを組むのが簡単な nginx を今回は利用したいと思います。\nhttps://github.com/opscode-cookbooks/nginx\n構成 \u0026lsquo;web\u0026rsquo; という Role 名と \u0026lsquo;lb\u0026rsquo; という Role 名で単純な HTTP サーバとしての nginx ノードを複数台と、ロードバランサとしての nginx ノードを1台でクラスタを構成しま す。また共に environment 名は同じものを利用します。別の environment 名の場合は 別クラスタという区切りです。\n \u0026lsquo;lb\u0026rsquo; node x 1 + \u0026lsquo;web\u0026rsquo; node x n (\u0026lsquo;foo\u0026rsquo; environment) \u0026lsquo;lb\u0026rsquo; node x 1 + \u0026lsquo;web\u0026rsquo; node x n (\u0026lsquo;bar\u0026rsquo; environment)  \u0026lsquo;lb\u0026rsquo; nginx ロードバランサのレシピ 下記が \u0026lsquo;lb\u0026rsquo; Role の recipes/cmomnos_conf.","title":"Chef で自律的クラスタを考える"},{"content":"こんにちは。@jedipunkzです。\n皆さん CoreOS は利用されたことありますか？CoreOS は軽量な docker との相性の良 い OS です。下記が公式サイト。\nhttp://coreos.com/\n特徴としては下記の3つがあります。\n etcd systemd docker  ここではこの中の etcd について注目していきたいと思います。etcd はクラスタエイ ブルな KVS データベースです。コンフィギュレーションをクラスタ間で共有すること がなので、オーケストレーションの分野でも期待出来るのでは？と個人的に感じていま す。今回は etcd のクラスタ構成構築の手順とその基本動作の確認、またどう応用出来 るのか？について記していきたいと思います。\n参考 URL  http://coreos.com/using-coreos/etcd/ https://github.com/coreos/etcd  ビルド go 1.1 or later をインストールして etcd のコンパイル準備を行います。Ubuntu Saucy のパッケージを用いると容易に行えます。\n% apt-get -y install golang coreos/etcd を取得しビルド\n% git clone https://github.com/coreos/etcd % cd coreos % ./build % ./etcd --version v0.2.0-rc1-60-g73f04d5 CoreOS の用意 ここではたまたま手元にあった OpenStack を用いて CoreOS のイメージを登録してい みます。ベアメタルでも可能ですのでその場合は手順を読み替えて作業してみてくださ い。OpenStack 等クラウドプラットフォームを利用する場合は metadata サービスが必 須となるので注意してください。\n% wget http://storage.core-os.net/coreos/amd64-generic/dev-channel/coreos_production_openstack_image.img.bz2 % bunzip2 coreos_production_openstack_image.img.bz2 % glance image-create --name coreos-image --container-format ovf \\  --disk-format qcow2 --file coreos_production_openstack_image.img nova boot にて CoreOS を起動します。(下記は例)\n% nova keypair-add testkey01 \u0026gt; testkey01.pem % nova boot --nic net-id .... --image coreos-image --flavor 1 --key_name testkey01 coreos01 CoreOS 上での etcd クラスタ起動 上記でコンパイルした etcd のバイナリを起動したインスタンス (CoreOS) に転送しま す。scp 等で転送してください。\nここでは 1 node 上で複数のポート番号を用いて 3 つの etcd を稼働することでクラ スタを構築します。\n7002 番ポートを peer addr として master を起動。listen ポートは 4002\n% ./etcd -peer-addr 127.0.0.1:7002 -addr 127.0.0.1:4002 -data-dir machines/machine1 -name machine1 上記の master を参照する slaves (残り2台) を起動。\n% ./etcd -peer-addr 127.0.0.1:7003 -addr 127.0.0.1:4003 -peers 127.0.0.1:7002 -data-dir machines/machine2 -name machine2 % ./etcd -peer-addr 127.0.0.1:7004 -addr 127.0.0.1:4004 -peers 127.0.0.1:7002 -data-dir machines/machine3 -name machine3 クラスタ構成内のノード情報を確認する。\n% curl -L http://127.0.0.1:4002/v2/machines [etcd] Dec 4 03:46:44.153 INFO | URLs: machine1 / machine1 (http://127.0.0.1:4002,http://127.0.0.1:4003,http://127.0.0.1:4004) http://127.0.0.1:4002, http://127.0.0.1:4003, http://127.0.0.1:4004 leader (master) 情報を確認する。\n% curl -L http://127.0.0.1:4002/v2/leader http://127.0.0.1:7002 上記で起動した master プロセスが leader (master) になっていることを確認出来る と思います。\nキーの投入と参照 テストでキーと値を入力してみましょう。\u0026lsquo;foo\u0026rsquo; キーに \u0026lsquo;bar\u0026rsquo; という値を投入てくだ さい。\n% curl -L http://127.0.0.1:4002/v2/keys/foo -XPUT -d value=bar クラスタ内全てのプロセスから上記のキーの取得できることを確認します。\n% curl -L http://127.0.0.1:4002/v2/keys/foo {\u0026#34;action\u0026#34;:\u0026#34;get\u0026#34;,\u0026#34;node\u0026#34;:{\u0026#34;key\u0026#34;:\u0026#34;/foo\u0026#34;,\u0026#34;value\u0026#34;:\u0026#34;bar\u0026#34;,\u0026#34;modifiedIndex\u0026#34;:4,\u0026#34;createdIndex\u0026#34;:4}} % curl -L http://127.0.0.1:4003/v2/keys/foo {\u0026#34;action\u0026#34;:\u0026#34;get\u0026#34;,\u0026#34;node\u0026#34;:{\u0026#34;key\u0026#34;:\u0026#34;/foo\u0026#34;,\u0026#34;value\u0026#34;:\u0026#34;bar\u0026#34;,\u0026#34;modifiedIndex\u0026#34;:4,\u0026#34;createdIndex\u0026#34;:4}} % curl -L http://127.0.0.1:4004/v2/keys/foo {\u0026#34;action\u0026#34;:\u0026#34;get\u0026#34;,\u0026#34;node\u0026#34;:{\u0026#34;key\u0026#34;:\u0026#34;/foo\u0026#34;,\u0026#34;value\u0026#34;:\u0026#34;bar\u0026#34;,\u0026#34;modifiedIndex\u0026#34;:4,\u0026#34;createdIndex\u0026#34;:4}} master のシャットダウンと master 選挙後の動作確認 テストで master のプロセスをシャットダウンしてみます。\nmaster プロセスのシャットダウン\n% kill \u0026lt;master プロセスの ID\u0026gt; その他 2 つのプロセスから \u0026lsquo;foo\u0026rsquo; キーの確認を行う。\n% curl -L http://127.0.0.1:4004/v2/keys/foo {\u0026#34;action\u0026#34;:\u0026#34;get\u0026#34;,\u0026#34;node\u0026#34;:{\u0026#34;key\u0026#34;:\u0026#34;/foo\u0026#34;,\u0026#34;value\u0026#34;:\u0026#34;bar\u0026#34;,\u0026#34;modifiedIndex\u0026#34;:4,\u0026#34;createdIndex\u0026#34;:4}} % curl -L http://127.0.0.1:4003/v2/keys/foo {\u0026#34;action\u0026#34;:\u0026#34;get\u0026#34;,\u0026#34;node\u0026#34;:{\u0026#34;key\u0026#34;:\u0026#34;/foo\u0026#34;,\u0026#34;value\u0026#34;:\u0026#34;bar\u0026#34;,\u0026#34;modifiedIndex\u0026#34;:4,\u0026#34;createdIndex\u0026#34;:4}} 勿論、旧 master からは確認出来ない。\n% curl -L http://127.0.0.1:4002/v2/keys/foo curl: (7) Failed connect to 127.0.0.1:4002; Connection refused 新 master の確認を行う。選挙の結果 3 つ目のプロセスが master に昇格しているこ とが確認出来る。\n% curl -L http://127.0.0.1:4003/v2/leader http://127.0.0.1:7004 考察とその応用性について とてもシンプルな KVS ではあるけど大きな可能性を秘めていると思っています。オー ケストレーション等への応用です。お互いのノード (今回はプロセス) 間で情報をやり とりできるので自律的なクラスタの構築も可能になるのでは？と思っています。\n\u0026lsquo;etcenv\u0026rsquo; という @mattn さんが開発したツールを見てみましょう。\nhttps://github.com/mattn/etcdenv\n下記、README から引用。\n$ curl http://127.0.0.1:4001/v1/keys/app/db -d value=\u0026#34;newdb\u0026#34; $ curl http://127.0.0.1:4001/v1/keys/app/cache -d value=\u0026#34;new cache\u0026#34; $ curl http://localhost:4001/v1/keys/app [{\u0026#34;action\u0026#34;:\u0026#34;GET\u0026#34;,\u0026#34;key\u0026#34;:\u0026#34;/app/db\u0026#34;,\u0026#34;value\u0026#34;:\u0026#34;newdb\u0026#34;,\u0026#34;index\u0026#34;:4},{\u0026#34;action\u0026#34;:\u0026#34;GET\u0026#34;,\u0026#34;key\u0026#34;:\u0026#34;/app/cache\u0026#34;,\u0026#34;value\u0026#34;:\u0026#34;new cache\u0026#34;,\u0026#34;index\u0026#34;:4}] $ etcdenv -key=/app/ DB=newdb CACHE=new cache $ etcdenv -key=/app/ ruby web.rb クラスタ間の情報を環境変数に落としこむツールです。自ノードの環境変数まで落ちれ ば、クラスタ構築も色々想像出来るのではないでしょうか？\n軽量で docker との相性も良くて etcd 等の仕組みも持っている CoreOS にはこれから も期待です。\n","permalink":"https://jedipunkz.github.io/post/2013/12/09/coreos-etcd-cluster/","summary":"こんにちは。@jedipunkzです。\n皆さん CoreOS は利用されたことありますか？CoreOS は軽量な docker との相性の良 い OS です。下記が公式サイト。\nhttp://coreos.com/\n特徴としては下記の3つがあります。\n etcd systemd docker  ここではこの中の etcd について注目していきたいと思います。etcd はクラスタエイ ブルな KVS データベースです。コンフィギュレーションをクラスタ間で共有すること がなので、オーケストレーションの分野でも期待出来るのでは？と個人的に感じていま す。今回は etcd のクラスタ構成構築の手順とその基本動作の確認、またどう応用出来 るのか？について記していきたいと思います。\n参考 URL  http://coreos.com/using-coreos/etcd/ https://github.com/coreos/etcd  ビルド go 1.1 or later をインストールして etcd のコンパイル準備を行います。Ubuntu Saucy のパッケージを用いると容易に行えます。\n% apt-get -y install golang coreos/etcd を取得しビルド\n% git clone https://github.com/coreos/etcd % cd coreos % ./build % ./etcd --version v0.2.0-rc1-60-g73f04d5 CoreOS の用意 ここではたまたま手元にあった OpenStack を用いて CoreOS のイメージを登録してい みます。ベアメタルでも可能ですのでその場合は手順を読み替えて作業してみてくださ い。OpenStack 等クラウドプラットフォームを利用する場合は metadata サービスが必 須となるので注意してください。","title":"CoreOS etcd のクラスタとその応用性"},{"content":"こんにちは。@jedipunkzです。\nアドベントカレンダーの季節がやって参りました。\nIronic を使って OpenStack でベアメタルサーバを扱いたい！ということで色々とやっ ている最中 (今週から始めました..) なのですが、まだまだ incubator プロジェクト ということもあって実装が追い付いていなかったりドキュメントも揃っていなかったり とシンドい状況ｗ ここ2日程で集めた情報を整理するためにも 2013年 OpenStack アド ベントカレンダーに参加させてもらいますー。\n参考資料のまとめ まずは公式 wiki ページ。逆に言うとここに記されている以上の情報は無いんじゃ？あ とはコード読め！の世界かも..。\nhttps://wiki.openstack.org/wiki/Ironic\ndevtest_undercloud です。上の資料の中でも手順の中で度々こちらにジャンプしている。 同じく incubator プロジェクトの TrippleO のデベロッパ用ドキュメントになっている。 上記の公式 wiki の情報を合わせ読むことで Ironic を使ったデプロイの手順に仕上がります。\nhttp://docs.openstack.org/developer/tripleo-incubator/devtest_undercloud.html\nソースコードとドキュメント。あとでドキュメント作成方法を記しますが、こちらを取 得して作成します。\nhttps://github.com/openstack/ironic\nドキュメントサイト。まだ情報が揃っていません。よって上の github から取得したモ ノからドキュメントを作る方法を後で書きます。\nhttp://docs.openstack.org/developer/ironic/\nlaunchpad サイト。全てのバグ情報やブループリント等が閲覧出来ます。まだ絶賛開発 中なので読む必要があると思います。\nhttps://launchpad.net/ironic\nドキュメントを作る +++\n公式 ドキュメントサイトは一応、上記の通りあるのですが、ドキュメントも絶賛執筆 中ということで所々抜けがあります。また公式ドキュメントサイトがどのスパンで更新 されているか分からないので、いち早く情報をゲットしたい場合ドキュメントを作る必 要があると思います。ということで、その作り方を記していきます。尚、公式 wiki サ イトにも手順が載っていますが Vagrant と Apache を用いた方法になっているので、 普通に Ubuntu サーバが手元にある環境を想定して読み替えながら説明していきます。\n必要なパッケージのインストールを行います。\n% sudo apt-get update % sudo apt-get install -y git python-dev swig libssl-dev python-pip \\  libmysqlclient-dev libxml2-dev libxslt-dev libxslt1-dev python-mysqldb \\  libpq-dev % sudo pip install virtualenv setuptools-git flake8 tox % sudo easy_install nose ソースコード・ドキュメントを取得します。\n% git clone git://github.com/openstack/ironic.git Sphinx で構成されているのでビルドします。\n% cd ironic % tox -evenv -- echo \u0026#39;done\u0026#39; % source .tox/venv/bin/activate \u0026gt; python setup.ph build_sphinx \u0026gt; deactivate ironic/doc/build/html ディレクトリ配下に HTML のドキュメントが生成されたはずで す。これを手元の端末に持ってきて開けばブラウザで最新のドキュメントが閲覧出来ま す。\nironic を有効にした devstack による構築 devstack を使って ironic を機能させていきます。私は下記の localrc を用いて ironic の試験をしていました。またブランチは \u0026lsquo;master\u0026rsquo; を使います。\n% git clone https://github.com/openstack-dev/devstack.git % cd devstack % ${EDITOR} localrc # 下記の通り HOST_IP=\u0026lt;your_machine_ip_addr\u0026gt; LOGFILE=stack.sh.log ADMIN_PASSWORD=nomoresecrete MYSQL_PASSWORD=$ADMIN_PASSWORD RABBIT_PASSWORD=$ADMIN_PASSWORD SERVICE_PASSWORD=$ADMIN_PASSWORD SERVICE_TOKEN=admintoken disable_service n-obj # ironic enable_service ir-api enable_service ir-cond # use neutron disable_service n-net enable_service q-svc enable_service q-agt enable_service q-dhcp enable_service q-l3 enable_service q-meta enable_service q-lbaas enable_service neutron ENABLE_TENANT_TUNNELS=True # heat ENABLED_SERVICES+=,heat,h-api,h-api-cfn,h-api-cw,h-eng ## It would also be useful to automatically download and register VM images that Heat can launch. # 64bit image (~660MB) IMAGE_URLS+=\u0026#34;,http://fedorapeople.org/groups/heat/prebuilt-jeos-images/F17-x86_64-cfntools.qcow2\u0026#34; # 32bit image (~640MB) IMAGE_URLS+=\u0026#34;,http://fedorapeople.org/groups/heat/prebuilt-jeos-images/F17-i386-cfntools.qcow2\u0026#34; # syslog SYSLOG=True SYSLOG_HOST=$HOST_IP SYSLOG_PORT=514 stack.sh を実行します。\n% ./stack.sh Ironic を有効にした OpenStack 環境が出来上がったはずです。\ndiskimage-builder を使ったイメージ作成 ベアメタルサーバをデプロイするためのイメージを作成します。元々は TrippleO のプ ロジェクト内に存在していましたが、現在は git レポジトリが別れています。\n先ほど devstack を導入したホストでイメージを作ります。作成には結構時間が掛かります。\n% cd ~ % git clone https://github.com/openstack/diskimage-builder.git % git clone https://github.com/openstack/tripleo-incubator.git % git clone https://github.com/openstack/tripleo-image-elements.git % git clone https://github.com/openstack/tripleo-heat-templates.git % export UNDERCLOUD_DIB_EXTRA_ARGS=\u0026#39;ironic-api ironic-conductor\u0026#39; % export ELEMENTS_PATH=/home/thirai/tripleo-image-elements/elements % ./diskimage-builder/bin/disk-image-create -a amd64 -o ~/undercloud boot-stack \\  nova-baremetal os-collect-config stackuser dhcp-all-interfaces \\  neutron-dhcp-agent ${UNDERCLOUD_DIB_EXTRA_ARGS:-} ubuntu 2\u0026gt;\u0026amp;1 | tee /tmp/undercloud.log イメージが ~/undercloud.qcow2 が生成されたはずです。作成したイメージを Glance に登録します。\n% ~/tripleo-incubator/scripts/load-image undercloud.qcow2 undercloud.yaml と ironic.yaml をマージします。\n% cd ~/tripleo-heat-templates % make undercloud-vm-ironic.yaml パスワードの生成と環境変数への読み込みを行います。\n% cd ~/tripleo-incubator/scripts/ % export PATH=$PATH:. % ./setup-undercloud-passwords % source tripleo-undercloud-passwords UNDERCLOUD_IRONIC_PASSWORD 環境変数にも読み込みます。\n% export UNDERCLOUD_IRONIC_PASSWORD=$(~/tripleo-incubator/scripts/os-make-password) さて、イメージを利用したベアメタルへの稼働ですが、\u0026hellip;\nif [ \u0026#34;$DHCP_DRIVER\u0026#34; = \u0026#34;bm-dnsmasq\u0026#34; ]; then UNDERCLOUD_NATIVE_PXE=\u0026#34;\u0026#34; else UNDERCLOUD_NATIVE_PXE=\u0026#34;;NeutronNativePXE=True\u0026#34; fi heat stack-create -f ./tripleo-heat-templates/undercloud-vm-ironic.yaml \\ -P \u0026#34;PowerUserName=$(whoami);\\ AdminToken=${UNDERCLOUD_ADMIN_TOKEN};\\ AdminPassword=${UNDERCLOUD_ADMIN_PASSWORD};\\ GlancePassword=${UNDERCLOUD_GLANCE_PASSWORD};\\ HeatPassword=${UNDERCLOUD_HEAT_PASSWORD};\\ NeutronPassword=${UNDERCLOUD_NEUTRON_PASSWORD};\\ NovaPassword=${UNDERCLOUD_NOVA_PASSWORD};\\ BaremetalArch=${NODE_ARCH}$UNDERCLOUD_NATIVE_PXE\u0026#34; \\ IronicPassword=${UNDERCLOUD_IRONIC_PASSWORD}\u0026#34; \\ undercloud コケました。\u0026hellip;エラーは下記の通り。\nTRACE heat.engine.resource Error: Creation of server teststack01-WikiDatabase-5nyiqluilnxn failed: No valid host was found. Exceeded max scheduling attempts 3 for instance 733b69df-2b54-44ae-9d61-de766746f21a (500)#0122013-12-03 16:04:14.513 10022 TRACE heat.engine.resource No Valid Host とな\u0026hellip;。うーん。確かに IPMI を積んだベアメタルマシンの情報って どこにも記していないんだよなぁ。しかも heat のテンプレート (underclound-vm-ironic.yaml) 見てもよく理解していない自分がいる\u0026hellip;(´・ω・`)\nというか手順にはその周りのこと何も書いていないのでぇすがぁ！\u0026hellip;\nということで、まだまだコントリビュートするチャンス満載の状態なので、よかったら 皆さん参加されませんか !?!?\nhttps://wiki.openstack.org/wiki/HowToContribute\n","permalink":"https://jedipunkz.github.io/post/2013/12/05/ironic-openstack-beremetal/","summary":"こんにちは。@jedipunkzです。\nアドベントカレンダーの季節がやって参りました。\nIronic を使って OpenStack でベアメタルサーバを扱いたい！ということで色々とやっ ている最中 (今週から始めました..) なのですが、まだまだ incubator プロジェクト ということもあって実装が追い付いていなかったりドキュメントも揃っていなかったり とシンドい状況ｗ ここ2日程で集めた情報を整理するためにも 2013年 OpenStack アド ベントカレンダーに参加させてもらいますー。\n参考資料のまとめ まずは公式 wiki ページ。逆に言うとここに記されている以上の情報は無いんじゃ？あ とはコード読め！の世界かも..。\nhttps://wiki.openstack.org/wiki/Ironic\ndevtest_undercloud です。上の資料の中でも手順の中で度々こちらにジャンプしている。 同じく incubator プロジェクトの TrippleO のデベロッパ用ドキュメントになっている。 上記の公式 wiki の情報を合わせ読むことで Ironic を使ったデプロイの手順に仕上がります。\nhttp://docs.openstack.org/developer/tripleo-incubator/devtest_undercloud.html\nソースコードとドキュメント。あとでドキュメント作成方法を記しますが、こちらを取 得して作成します。\nhttps://github.com/openstack/ironic\nドキュメントサイト。まだ情報が揃っていません。よって上の github から取得したモ ノからドキュメントを作る方法を後で書きます。\nhttp://docs.openstack.org/developer/ironic/\nlaunchpad サイト。全てのバグ情報やブループリント等が閲覧出来ます。まだ絶賛開発 中なので読む必要があると思います。\nhttps://launchpad.net/ironic\nドキュメントを作る +++\n公式 ドキュメントサイトは一応、上記の通りあるのですが、ドキュメントも絶賛執筆 中ということで所々抜けがあります。また公式ドキュメントサイトがどのスパンで更新 されているか分からないので、いち早く情報をゲットしたい場合ドキュメントを作る必 要があると思います。ということで、その作り方を記していきます。尚、公式 wiki サ イトにも手順が載っていますが Vagrant と Apache を用いた方法になっているので、 普通に Ubuntu サーバが手元にある環境を想定して読み替えながら説明していきます。\n必要なパッケージのインストールを行います。\n% sudo apt-get update % sudo apt-get install -y git python-dev swig libssl-dev python-pip \\  libmysqlclient-dev libxml2-dev libxslt-dev libxslt1-dev python-mysqldb \\  libpq-dev % sudo pip install virtualenv setuptools-git flake8 tox % sudo easy_install nose ソースコード・ドキュメントを取得します。","title":"Ironic でベアメタル OpenStack ！..の一歩手前"},{"content":"こんにちは。@jedipunkzです。\n以前、Sensu を Chef で管理する方法について書きました。\nhttp://jedipunkz.github.io/blog/2013/06/20/sensu-chef-controll/\nこれは今年(2013)の6月頃の記事ですが、この時はまだ sensu-chef を include して使う別の Chef Cookbook が必要でした。また Redis 周りの Cookbooks が完成度あまく、またこれも 公式とは別の Cookbooks を改修して再利用する形でした。この作業は結構しんどかっ た記憶があるのですが、最近 GlideNote さんのブログを読んで( ﾟдﾟ)ﾊｯ!と思い、 sensu-chef を再確認したのですが、だいぶ更新されていました。\n下記が sensu-chef です。\nhttps://github.com/sensu/sensu-chef\nこの Chef Cookbook 単体で利用できる形に更新されていて、plugins, checks 等は Recipe に追記することで対応可能になっていました。早速利用してみたので簡単に使 い方を書いていきます。\n下記が Sensu の管理画面です。最終的にこの画面に監視対象のアラートが上がってきます。\n{% img /pix/sensu.png %}\n使い方 sensu-chef を取得する。chef-repo になっています。\n% git clone https://github.com/sensu/sensu-chef.git ~/chef-repo-sensu bundle にて Gemfile に記述の在る gem パッケージをインストールします。\n% cd ~/chef-repo-sensu % bundle install .chef/ 配下の設定は割愛します。chef サーバの情報に合わせて設定します。\nssl 鍵を生成して data bags に投入します。\n% cd examples/ssl % ./ssl_certs.sh generate % cd ../../ % knife data bag create sensu % knife data bag from file sensu examples/ssl/ssl.json % ./examples/ssl/ssl_certs.sh clean Roles を作成します。chef-repo なのに何も入っていませんでした\u0026hellip;汗 ここで面白いのは \u0026lsquo;sensu-client\u0026rsquo; は sensu で言う subscribers の名前にそのまま利 用されるところです。つまり \u0026lsquo;sensu-client\u0026rsquo; の名前が記された sensu サーバ上の監 視項目 (checks) がこの sensu クライアントに割り当てられます。\n% mkdir  roles % ${EDITOR}  roles/sensu-server.rb name \u0026#34;sensu-server\u0026#34; description \u0026#34;role applied to sensu server.\u0026#34; run_list \u0026#39;recipe[sensu::default]\u0026#39;, \u0026#39;recipe[sensu::rabbitmq]\u0026#39;, \u0026#39;recipe[sensu::redis]\u0026#39;, \u0026#39;recipe[sensu::server_service]\u0026#39;, \u0026#39;recipe[sensu::api_service]\u0026#39;, \u0026#39;recipe[sensu::dashboard_service]\u0026#39;, \u0026#39;recipe[chef-client::service]\u0026#39; % ${EDITOR}  roles/sensu-client.rb name \u0026#34;sensu-client\u0026#34; description \u0026#34;role applied to sensu client.\u0026#34; run_list \u0026#39;recipe[sensu::default]\u0026#39;, \u0026#39;recipe[sensu::client_service]\u0026#39;, \u0026#39;recipe[chef-client::service]\u0026#39; Cheffile に \u0026lsquo;chef-client\u0026rsquo; の Cookbook 名を追記します。\nsite \u0026#39;http://community.opscode.com/api/v1\u0026#39; cookbook \u0026#39;sensu\u0026#39;, path: \u0026#39;./\u0026#39; cookbook \u0026#39;sensu-test\u0026#39;, path: \u0026#39;./test/cookbooks/sensu-test\u0026#39; cookbook \u0026#39;chef-client\u0026#39; # \u0026lt;---- 追記 librarian-chef を実行して Cookbooks を取得します。\n% librarian-chef install Rabbitmq, Redis, API の IP アドレスを設定します。IP アドレスは例です。\n% ${EDITOR} cookbooks/sensu/attributes/default.rb default.sensu.rabbitmq.host = \u0026#34;172.24.19.11\u0026#34; default.sensu.redis.host = \u0026#34;172.24.19.11\u0026#34; default.sensu.api.host = \u0026#34;172.24.19.11\u0026#34; sensu-server 用の Recipe に監視項目を追記します。ここでは cron デーモンの稼働 状況を監視してみました。\n% ${EDITOR} cookbooks/sensu/recipes/server_service.rb # 下記を追記 sensu_check \u0026#34;cron_process\u0026#34; do command \u0026#34;check-procs.rb -p cron -C 1\u0026#34; handlers [\u0026#34;default\u0026#34;] subscribers [\u0026#34;sensu-client\u0026#34;] interval 30 additional(:notification =\u0026gt; \u0026#34;Cron is not running\u0026#34;, :occurrences =\u0026gt; 5) end sensu-client 用の Recipe に client.json (クライアント用設定ファイル) の記述と 上記監視項目に合った plugins 設定の追記を行います。\n% ${EDITOR} cookbooks/sensu/recipes/client_service.rb # 下記を追記 sensu_client node.name do address node.ipaddress subscriptions node.roles + [\u0026#34;all\u0026#34;] end cookbook_file \u0026#39;check-procs.rb\u0026#39; do source \u0026#39;check-procs.rb\u0026#39; mode 0755 path \u0026#39;/etc/sensu/plugins/check-procs.rb\u0026#39; end 上記 check-procs.rb は community サイトからダウンロードする必要があるのですが cookbook_file で対応したので files/ ディレクトリ配下に置いておきます。\n% mkdir -p cookbooks/sensu/files/default % wget -o cookbooks/sensu/files/default/check-procs.rb \\  https://github.com/sensu/sensu-community-plugins/raw/master/plugins/processes/check-procs.rb 上記の check-procs.rb は行頭に \u0026lsquo;#!/usr/bin/env ruby\u0026rsquo; が記されているのですが Sensu インストール時に入る ruby は /opt/sensu/embedded/bin/ruby にあるので行頭 の1行を書き換えます。\n% diff cookbooks/sensu/files/default/check-procs.rb.org cookbooks/sensu/files/default/check-procs.rb - #!/usr/bin/env ruby + #!/opt/sensu/embedded/bin/ruby Roles, Cookbooks を Chef サーバにアップロードします。\n% knife cookbook upload -o ./cookbooks -a % knife role from file roles/*.rb いよいよブートストラップします!\nまずは sensu-server を。\n% knife bootstrap \u0026lt;server_ip_addr\u0026gt; -N sensu-server -r \u0026#39;role[sensu-server]\u0026#39; \\  -x ubuntu --sudo 次に監視対象である sensu-client を。\n% knife bootstrap \u0026lt;client_ip_addr\u0026gt; -N sensu-client01 -r \u0026#39;role[sensu-client]\u0026#39; \\  -x ubuntu --sudo http://\u0026lt;sensu-server の IP アドレス\u0026gt;:8080/ にアクセスすると Sensu の管理画面が 表示されます。認証アカウントは cookbooks/sensu/attributes/default.rb に記述が ありますので確認して下さい。\nまとめ インフラリソースのフルオートメーション化について情報リサーチしていますが監視も 重要なインフラリソースの一部です。Sensu サーバ・クライアントの自動デプロイが出 来たのでこれで一つパーツが揃ったことに。Sensu は API を持っていますのでアプリ から検知することも簡単に出来ますよ。API については下記を参照してください。\nhttp://sensuapp.org/docs/0.12/api\nまた以前抱えていた問題もスッキリクリアになって、これでまた前進できた感があります。\n参考サイト  http://sensuapp.org/docs/0.12 http://blog.glidenote.com/blog/2013/11/26/sensu/ https://github.com/sensu/sensu-chef  ","permalink":"https://jedipunkz.github.io/post/2013/11/27/sensu-chef-deploy-2/","summary":"こんにちは。@jedipunkzです。\n以前、Sensu を Chef で管理する方法について書きました。\nhttp://jedipunkz.github.io/blog/2013/06/20/sensu-chef-controll/\nこれは今年(2013)の6月頃の記事ですが、この時はまだ sensu-chef を include して使う別の Chef Cookbook が必要でした。また Redis 周りの Cookbooks が完成度あまく、またこれも 公式とは別の Cookbooks を改修して再利用する形でした。この作業は結構しんどかっ た記憶があるのですが、最近 GlideNote さんのブログを読んで( ﾟдﾟ)ﾊｯ!と思い、 sensu-chef を再確認したのですが、だいぶ更新されていました。\n下記が sensu-chef です。\nhttps://github.com/sensu/sensu-chef\nこの Chef Cookbook 単体で利用できる形に更新されていて、plugins, checks 等は Recipe に追記することで対応可能になっていました。早速利用してみたので簡単に使 い方を書いていきます。\n下記が Sensu の管理画面です。最終的にこの画面に監視対象のアラートが上がってきます。\n{% img /pix/sensu.png %}\n使い方 sensu-chef を取得する。chef-repo になっています。\n% git clone https://github.com/sensu/sensu-chef.git ~/chef-repo-sensu bundle にて Gemfile に記述の在る gem パッケージをインストールします。\n% cd ~/chef-repo-sensu % bundle install .chef/ 配下の設定は割愛します。chef サーバの情報に合わせて設定します。","title":"sensu-chef で監視システム Sensu を管理 #2"},{"content":"こんにちは。@jedipunkzです。\n自宅ルータを Vyatta で運用しているのですが、諸電力な筐体に交換した際に HDD ス ロットが余っていたので HDD を一本さしてみました。もったいないので Netatalk を インストールして Mac 用の TimeMachine サーバにするか！そんでもってファイルサー バ兼務にしよう！と思い立って作業したら簡単に出来たので共有します。\nVyatta はご存知の通り Debian Gnu/Linux がベースになっているのでパッケージレポ ジトリを追加してちょちょいで設定出来ます。\n手順 電源を通して Disk を追加します。その後起動。私は 3TB Disk が余っていたのでそれ を挿しました。\ndebian wheezy のパッケージレポジトリを Vyatta に追記します。\n% configure # set system package repository debian url http://ftp.jp.debian.org/debian # set system package repository debian distribution wheezy # set system package repository debian components \u0026#34;main contrib\u0026#34; # commit # save # exit  netatalk, avahi をインストールする。その際に libgcrypt11 のバージョン 1.5.0 が 必要になるのでインストールすること。\n% sudo apt-get update % sudo apt-get install netatalk avahi-daemon avahi-utils libgcrypt11 ディスクのパーティショニング・フォーマットを行うために e2fsprogs, gdisk を入れ る。2TB オーバーな disk が一般的になったので fdisk ではなく gdisk を使う。\n% sudo apt-get install e2fsprogs gdisk パーティショニングを行う。私は1つの大きなパーティションを作りました。その後ファ イルシステム ext4 にてフォーマットを行います。\n% sudo gdisk /dev/sdb # パーティショニング方法は割愛 % sudo mkfs.ext4 /dev/sdb1 /mnt ディレクトリにマウントします。/mnt/storage をファイルサーバ用, /mnt/timemachine を Mac の TimeMachine 用として稼働させるためにディレクトリを 作成します。\n% sudo mount -t ext4 /dev/sdb1 /mnt % sudo vi /etc/fstab # /mnt の記述追加 % sudo mkdir /mnt/storage ; sudo chown \u0026lt;userid\u0026gt;:users /mnt/storage % sudo mkdir /mnt/timemachine ; sudo chown \u0026lt;userid\u0026gt;:users /mnt/timemachine /etc/netatalk/apfd.conf を修正します。\n% diff /etc/netatalk/apfd.conf.org /etc/netatalk/afpd.conf - # - -tcp -noddp -uamlist uams_dhx.so,uams_dhx2.so -nosavepassword + - -tcp -noddp -uamlist uams_guest.so,uams_dhx.so,uams_dhx2.so -nosavepassword TimeMachine 用のディレクトリパスに下記のファイルを touch します。中身は空で OK です。\n% touch /mnt/timemachine/.com.apple.timemachine.supported /etc/netatalk/AppleVolumes.default ファイルにファイルサーバ, TimeMachine 用の 設定を投入します。TimeMachine 用は下記の通りオプションが必要になります。\n% sudo vi /etc/netatalk/AppleVolumes.default # 下記を追記 /mnt/storage \u0026#34;Storage\u0026#34; /mnt/timemachine \u0026#34;TimeMachine\u0026#34; cnidscheme:dbd options:usedots,upriv,tm netatalk を再起動します。\n% sudo /etc/init.d/netatalk restart TimeMachine を設定する Mac 側の設定 今回の様に Apple 公式の TimeCupsule 以外のマシンを TimeMachine のバックアップ 先として利用する場合、Mac 側で下記の設定が必要になります。\n% defaults write com.apple.systempreferences TMShowUnsupportedNetworkVolumes 1 あとは \u0026lsquo;環境設定\u0026rsquo; -\u0026gt; \u0026lsquo;TimeMachine\u0026rsquo; にて Vyatta を TimeMachine のバックアップ先 に指定すれば OK です。\nまとめ パッケージレポジトリは Wheezy の main, contrib を設定しましたが他のモノを入れ ることも勿論可能。最近の Netatalk は TimeMachine 用の設定が簡単にできるように なっていました。\n","permalink":"https://jedipunkz.github.io/post/2013/11/26/vyatta-timemachine-netatalk/","summary":"こんにちは。@jedipunkzです。\n自宅ルータを Vyatta で運用しているのですが、諸電力な筐体に交換した際に HDD ス ロットが余っていたので HDD を一本さしてみました。もったいないので Netatalk を インストールして Mac 用の TimeMachine サーバにするか！そんでもってファイルサー バ兼務にしよう！と思い立って作業したら簡単に出来たので共有します。\nVyatta はご存知の通り Debian Gnu/Linux がベースになっているのでパッケージレポ ジトリを追加してちょちょいで設定出来ます。\n手順 電源を通して Disk を追加します。その後起動。私は 3TB Disk が余っていたのでそれ を挿しました。\ndebian wheezy のパッケージレポジトリを Vyatta に追記します。\n% configure # set system package repository debian url http://ftp.jp.debian.org/debian # set system package repository debian distribution wheezy # set system package repository debian components \u0026#34;main contrib\u0026#34; # commit # save # exit  netatalk, avahi をインストールする。その際に libgcrypt11 のバージョン 1.","title":"Vyatta で Mac 用 TimeMachine サーバ兼ファイルサーバを構築！"},{"content":"こんにちは。@jedipunkzです。\n毎度お馴染みになった OpenStack の Chef によるデプロイですが、今回は OpenStack Havana 構成を Chef でデプロイする方法についてです。使用するのは今回も rcbops/chef-cookbooks です。ブランチは \u0026lsquo;havana\u0026rsquo; を用います。\n早速ですが構成について。4.1.2 辺りからだと思うのですが構成の前提が物理ネットワー ク4つを前提にし始めました。public, external (VM) を別ける必要が出てきました。 通信の特性も異なるので (public は public API を。external は VM 用) 、別けるの が得策かもしれません。\n構成  +--------------+------------------------------------------------------- external | | +--------------+--(-----------+--(-----------+--------------+---------------------------- public | | | | | | | +------------+ +------------+ +------------+ +------------+ +------------+ +------------+ | controller | | network | | network | | compute | | compute | | workstation| +------------+ +------------+ +------------+ +------------+ +------------+ +------------+ | | | | | | | | | | +--------------+--(-----------+--(-----------+--(-----------+--(-----------+------------- management | | | | +--------------+--------------+--------------+------------------------- guest 上記の構成の特徴  4つの物理ネットワークを前提 public ネットワーク : 外部 API 用ネットワーク external ネットワーク : インスタンス外部接続用ネットワーク guest ネットワーク : インスタンス内部用ネットワーク management ネットワーク : 各コンポーネント接続用ネットワーク public, external のみグローバルネットワーク controller : 2 nics, network : 4 nics, compute : 3nics の構成 controller はシングル構成 network ノードは台数拡張可能, agent 単位でノード間移動可能 compute ノードも台数拡張可能 workstation は chef-repo の所在地, management ネットワークに所属  各ノードの準備 OS インストール後、各ノードのネットワークインターフェースの設定を下記の通り行っ てください。また LVM を使うのであれば cinder ボリュームの設定も必要になってきます。\ncontroller ノード 2 nics を下記の通り設定します。\n eth0 を public ネットワークに。gateway をこのインターフェースに設定 eth1 を guest ネットワークに。gateway はなし  /etc/network/interfaces の例\nauto eth0 iface eth0 inet static address \u0026lt;ip_addr\u0026gt; netmask \u0026lt;netmask\u0026gt; gateway \u0026lt;gateway\u0026gt; dns-nameservers \u0026lt;dns_cache_server\u0026gt; dns-search \u0026lt;domain\u0026gt; auto eth1 iface eth1 inet static address \u0026lt;ip_addr\u0026gt; netmask \u0026lt;netmask\u0026gt; /dev/sdb を cinder 用ディスクデバイスとします。\n% sudo pvcreate /dev/sdb % sudo vgcreate cinder-volumes /dev/sdb network ノード 4 nics を下記の通り設定します。up route で仮想ネットワークへのルーティングを書 いてあげると、network ノードから直接仮想ネットワーク上のインスタンスへ通信する ことが可能です。最初は書かなくても OK です。\nauto eth0 iface eth0 inet static address \u0026lt;ip_addr\u0026gt; netmask \u0026lt;netmask\u0026gt; dns-nameservers \u0026lt;dns_cache_server\u0026gt; dns-search \u0026lt;domain\u0026gt; auto eth1 iface eth1 inet static address \u0026lt;ip_addr\u0026gt; netmask \u0026lt;netmask\u0026gt; auto eth2 iface eth2 inet static address \u0026lt;ip_addr\u0026gt; netmask \u0026lt;netmask\u0026gt; gateway 10.200.10.1 dns-nameservers 8.8.8.8 8.8.4.4 dns-search cpi.ad.jp auto eth3 iface eth3 inet static address \u0026lt;ip_addr\u0026gt; netmask \u0026lt;netmask\u0026gt; # 仮想ネットワークへのルーティング # up route add -net \u0026lt;virtual_net_cidr\u0026gt; gw \u0026lt;neutron_gw\u0026gt; # up route add -net \u0026lt;virtual_net_cidr\u0026gt; gw \u0026lt;neutron_gw\u0026gt; compute ノード 3 nics を下記の通り設定します。\nauto eth0 iface eth0 inet static address \u0026lt;ip_addr\u0026gt; netmask \u0026lt;netmask\u0026gt; dns-nameservers \u0026lt;dns_cache_server\u0026gt; dns-search \u0026lt;domain\u0026gt; auto eth1 iface eth1 inet static address \u0026lt;ip_addr\u0026gt; netmask \u0026lt;netmask\u0026gt; auto eth2 iface eth2 inet static address \u0026lt;ip_addr\u0026gt; netmask \u0026lt;netmask\u0026gt; gateway \u0026lt;gateway\u0026gt; dns-nameservers \u0026lt;dns_cache_server\u0026gt; dns-search \u0026lt;domain\u0026gt; Cookbooks, Roles, Environments 等の準備 下記の操作は全て workstation ノードからの操作です。 また chef のインストールや chef サーバの構築方法については割愛します。\nrcbops/chef-cookbooks を取得します。\n% git clone https://github.com/rcbops/chef-cookbooks.git % cd chef-cookbooks % git checkout -b havana remotes/origin/havana % # .chef 配下の準備割愛。各 Chef サーバ環境に合わせる % git submodule init % git submodule sync % git submodule update % knife cookbook upload -o cookbooks -a % knife role from file roles/*.rb 今回の構成用の environment を生成します。それぞれの環境に合わせて作成してくだ さい。生成のコツは各 Cookbooks の attributes を見ながら設定することです。 生成した json ファイルを environments ディレクトリ配下に配置します。\n{ \u0026#34;name\u0026#34;: \u0026#34;havana-neutron\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;cookbook_versions\u0026#34;: { }, \u0026#34;json_class\u0026#34;: \u0026#34;Chef::Environment\u0026#34;, \u0026#34;chef_type\u0026#34;: \u0026#34;environment\u0026#34;, \u0026#34;default_attributes\u0026#34;: { }, \u0026#34;override_attributes\u0026#34;: { \u0026#34;package_component\u0026#34;: \u0026#34;havana\u0026#34;, \u0026#34;osops_networks\u0026#34;: { \u0026#34;management\u0026#34;: \u0026#34;10.200.10.0/24\u0026#34;, \u0026#34;public\u0026#34;: \u0026#34;10.200.9.0/24\u0026#34;, \u0026#34;nova\u0026#34;: \u0026#34;10.200.10.0/24\u0026#34; }, \u0026#34;nova\u0026#34;: { \u0026#34;config\u0026#34;: { \u0026#34;use_single_default_gateway\u0026#34;: false, \u0026#34;ram_allocation_ratio\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;cpu_allocation_ratio\u0026#34;: \u0026#34;16\u0026#34; }, \u0026#34;network\u0026#34;: { \u0026#34;provider\u0026#34;: \u0026#34;neutron\u0026#34;, \u0026#34;network_type\u0026#34;: \u0026#34;vlan\u0026#34; }, \u0026#34;apply_patches\u0026#34;: true, \u0026#34;libvirt\u0026#34;: { \u0026#34;vncserver_listen\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;virt_type\u0026#34;: \u0026#34;kvm\u0026#34; }, \u0026#34;db\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;nova\u0026#34; }, \u0026#34;services\u0026#34;: { \u0026#34;novnc-proxy\u0026#34;: { \u0026#34;network\u0026#34;: \u0026#34;public\u0026#34; }, \u0026#34;ec2-admin\u0026#34;: { \u0026#34;network\u0026#34;: \u0026#34;management\u0026#34; } } }, \u0026#34;cinder\u0026#34;: { \u0026#34;db\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;cinder\u0026#34; } }, \u0026#34;keystone\u0026#34;: { \u0026#34;admin_user\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;tenants\u0026#34;: [ \u0026#34;admin\u0026#34;, \u0026#34;service\u0026#34; ], \u0026#34;users\u0026#34;: { \u0026#34;admin\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;secrete\u0026#34;, \u0026#34;roles\u0026#34;: { \u0026#34;admin\u0026#34;: [ \u0026#34;admin\u0026#34; ] } }, \u0026#34;demo\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;demo\u0026#34;, \u0026#34;default_tenant\u0026#34; : \u0026#34;service\u0026#34;, \u0026#34;roles\u0026#34;: { \u0026#34;service\u0026#34;: [ \u0026#34;service\u0026#34; ] } } }, \u0026#34;db\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;keystone\u0026#34; } }, \u0026#34;horizon\u0026#34;: { \u0026#34;theme\u0026#34;: \u0026#34;Rackspace\u0026#34;, \u0026#34;db\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;horizon\u0026#34; }, \u0026#34;endpoint_type\u0026#34; : \u0026#34;publicURL\u0026#34;, \u0026#34;endpoint_scheme\u0026#34; : \u0026#34;http\u0026#34; }, \u0026#34;mysql\u0026#34;: { \u0026#34;root_network_acl\u0026#34;: \u0026#34;%\u0026#34;, \u0026#34;allow_remote_root\u0026#34;: true, \u0026#34;server_root_password\u0026#34;: \u0026#34;secrete\u0026#34;, \u0026#34;server_repl_password\u0026#34;: \u0026#34;secrete\u0026#34;, \u0026#34;server_debian_password\u0026#34;: \u0026#34;secrete\u0026#34; }, \u0026#34;monitoring\u0026#34;: { \u0026#34;procmon_provider\u0026#34;: \u0026#34;monit\u0026#34;, \u0026#34;metric_provider\u0026#34;: \u0026#34;collectd\u0026#34; }, \u0026#34;glance\u0026#34;: { \u0026#34;images\u0026#34;: [ \u0026#34;precise\u0026#34;, \u0026#34;cirros\u0026#34; ], \u0026#34;image\u0026#34;: { }, \u0026#34;image_upload\u0026#34;: false, \u0026#34;db\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;glance\u0026#34; } }, \u0026#34;neutron\u0026#34;: { \u0026#34;service_pass\u0026#34;: \u0026#34;neutron\u0026#34;, \u0026#34;db\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;neutron\u0026#34; } }, \u0026#34;developer_mode\u0026#34;: false } } environment ファイルの chef サーバへのアップロード。\n% knife environment from file environments/havana-neutron.json knife bootstrap によるデプロイ 下記の通り knife bootstrap することで各ノードをデプロイします。\ncontroller ノードのデプロイ。\n% knife bootstrap \u0026lt;controller_ipaddr\u0026gt; -N \u0026lt;controller_name\u0026gt; \\  -r \u0026#39;role[single-controller]\u0026#39;,\u0026#39;role[cinder-volume]\u0026#39; \\  -E havana-neutron -x \u0026lt;username\u0026gt; --sudo network ノードのデプロイ。台数分デプロイしてください。\n% knife bootstrap \u0026lt;network_ipaddr\u0026gt; -N \u0026lt;network_name\u0026gt; \\  -r \u0026#39;role[single-network-node]\u0026#39;,\u0026#39;recipe[nova-network::neutron-l3-agent]\u0026#39; \\  -E neutron-havana -x \u0026lt;username\u0026gt; --sudo compute ノードのデプロイ。台数分デプロイしてください。\n% knife bootstrap \u0026lt;compute_ipaddr\u0026gt; -N \u0026lt;compute_name\u0026gt; \\  -r \u0026#39;role[single-compute]\u0026#39; \\  -E havana-neutron -x \u0026lt;username\u0026gt; --sudo openvswitch の物理 NIC とブリッジインタフェースマッピング作業 各ノードで下記の通り物理 NIC とブリッジのマッピング作業を行ってください。操作 は必ず management ネットワークを介して行うようにしましょう。その他のネットワー クから操作すると通信が途絶える可能性があります。\nnetwork ノード\n% sudo ovs-vsctl add-port br-eth1 eth1 % sudo ovs-vsctl add-port br-ex eth3 compute ノード\n% sudo ovs-vsctl add-port br-eth1 eth1 まとめと考察 havana の roles にはコアプロジェクトのコンポーネントが入っています。つまり ceilometer と heat も上記のデプロイで入ってきます。ceilometer に関して公式のド キュメントではデータ格納用 DB として mongodb が記されていますが、ここでは mysqld 上に格納する構成になっていました。これは個人的には非常に助かります。 また、compute ノードから controller ノードの public api を叩く必要が出てきて、 compute ノードにも public ネットワーク用の NIC を足しましたが、こうするべきな のかどうかは今後考えてみます。理想は management ネットワークを介することですが attributes の調整でけではうまくいきませんでした。\u0026lsquo;havana\u0026rsquo; ブランチはまだ開発が 進んでいるので改善されるかもしれません。コントリビュートするのも手だと思います。\n","permalink":"https://jedipunkz.github.io/post/2013/11/17/openstack-havana-chef-deploy/","summary":"こんにちは。@jedipunkzです。\n毎度お馴染みになった OpenStack の Chef によるデプロイですが、今回は OpenStack Havana 構成を Chef でデプロイする方法についてです。使用するのは今回も rcbops/chef-cookbooks です。ブランチは \u0026lsquo;havana\u0026rsquo; を用います。\n早速ですが構成について。4.1.2 辺りからだと思うのですが構成の前提が物理ネットワー ク4つを前提にし始めました。public, external (VM) を別ける必要が出てきました。 通信の特性も異なるので (public は public API を。external は VM 用) 、別けるの が得策かもしれません。\n構成  +--------------+------------------------------------------------------- external | | +--------------+--(-----------+--(-----------+--------------+---------------------------- public | | | | | | | +------------+ +------------+ +------------+ +------------+ +------------+ +------------+ | controller | | network | | network | | compute | | compute | | workstation| +------------+ +------------+ +------------+ +------------+ +------------+ +------------+ | | | | | | | | | | +--------------+--(-----------+--(-----------+--(-----------+--(-----------+------------- management | | | | +--------------+--------------+--------------+------------------------- guest 上記の構成の特徴  4つの物理ネットワークを前提 public ネットワーク : 外部 API 用ネットワーク external ネットワーク : インスタンス外部接続用ネットワーク guest ネットワーク : インスタンス内部用ネットワーク management ネットワーク : 各コンポーネント接続用ネットワーク public, external のみグローバルネットワーク controller : 2 nics, network : 4 nics, compute : 3nics の構成 controller はシングル構成 network ノードは台数拡張可能, agent 単位でノード間移動可能 compute ノードも台数拡張可能 workstation は chef-repo の所在地, management ネットワークに所属  各ノードの準備 OS インストール後、各ノードのネットワークインターフェースの設定を下記の通り行っ てください。また LVM を使うのであれば cinder ボリュームの設定も必要になってきます。","title":"OpenStack Havana を Chef でデプロイ"},{"content":"こんにちは。@jedipunkzです。\n第2回 Elasticsearch 勉強会に参加してきました。箇条書きですが参加レポートを記し ておきます。\n開催日 : 2013/11/12 場所 : 東京駅 グラントウキョウサウスタワー リクルートテクノロジーズさま URL : http://elasticsearch.doorkeeper.jp/events/6532  Routing 周りの話 株式会社シーマーク　大谷純さん (@johtani)  Index 構成  cluster の中に index -\u0026gt; type が作成される index は shard という部分的な index の集まり shard 数は生成時のみ指定可能 node ごとに replica, primary を別ける replica 数は後に変えられる doc -\u0026gt; hash 値を shard 数で割って replica, primary に登録 doc の id の ハッシュ値を利用 type も含める場合はかの設定を true に クライアントはどのノードに対してクエリを投げても OK  routing  id の代わりに routing (URL パラメータ) で登録 url リクエストパラメータとして登録時にルーティングパラメータを登録 id の代わりにパラメータで指定された値のハッシュ値を計算して利用 検索時 routing 指定で関係のある shard のみを指定出来る  スケールアウト  sharding によるスケールアウト数 = インデックス作成時に指定 shard によるインデックスの分割以外にインデックス自体を複数持つことによるスケール 複数のドキュメントをエイリアス書けることが可能  所感 個人的には非常に興味のあるところでした。mongodb のような sharding をイメージし てよいのか？そうでないのか？すら理解出来ていなかったので。sharding を理解する 前提知識の話もあって非常に参考になりました。\nElasticSearchを使ったBaaS基盤の開発 株式会社富士通ソフトウェアテクノロジーズ 滝田聖己さん（@pisatoshi）  運用の話。これは貴重\u0026hellip;。\n shard 数 : 10 , replica : 1 で運用している データは業務データ , トラッキングデータ マルチテナント, 更新の即時反映, ルーティングによる性能向上 が要件 登録更新 -\u0026gt; 検索結果反映までタイムラグがある -\u0026gt; replica 完了まで待つので高コスト replica 数を減らすことで性能向上  routing id を指定することで\u0026hellip;  doc id の has から shard 自動選択がデフォルト -\u0026gt; hash key を指定して格納シャードを制御出来る -\u0026gt; doc 登録性能向上 -\u0026gt; 検索対象の shard を絞りこみえる -\u0026gt; 不可を軽減  バックアップ/リストア  mysql にもデータ格納 -\u0026gt; backup elasticsearch の index はバックアップ取らず -\u0026gt; bug を踏みたくないのが理由  dynamic mapping の問題  入力データから型を推測 -\u0026gt; 自動マッピング登録 -\u0026gt; マッピング定義が肥大化 -\u0026gt; データ型のコンフリクト  mapping の肥大化  mapping 定義は type, field の数に応じてサイズが増加 -\u0026gt; index 性能低下 -\u0026gt; mapping を伴う doc 登録 3sec (mapping size 80MB) -\u0026gt; 各 node への mapping 定義の同期 大量のデータを一気に登録するときは 1 node が速い -\u0026gt; その後シャード再配置したことがある  =\u0026gt; dynamic mapping を使うのをやめた -\u0026gt; app で指定\nよく利用するツール  bigdesk elasticsearch-head sense (chrome extention)  unit test unit test はどうする？\n nodeClient テスト開始時に起動 テストデータを配置して起動, 終了時に削除 memoryIndex で高速に実行 elasticsearch-test が便利  所感 運用の話はどの技術でも重要!!! 実際に困った話、トラブル等聞けて貴重な時間でした。 よく使うツールの話も意外と参考になるので聞けてよかった。\nKibana Cookpad 水戸祐介さん (@y_310)   作った dashboard は elasticsearch に保存される db 要らず , web サーバのみ, クライアントサイドの技術のみで実装されている term, trends, map, table, column\u0026hellip; それぞれ図形式がある title = \u0026ldquo;test\u0026rdquo;, -title 1つの index に異なるスキーマを持つデータを入れられる 1つの index に入れることでグラフを重ねて比較出来る やはり dynamic mapping は使わないほうが良い m1.large x 1 : 1日のインデックスサイズが10GB 超えるあたりで ES が詰まる  下記が当日の資料です。\n所感 Cookpad さんで実際に利用されているとか。運用の話と同じくやはり dynamic routing でのトラブルの話があった。また Cookpad さんでのトラブル例の話もあってよかった。\nElasticSearch＋Kibana v3を利用する際の運用ノウハウ 株式会社リブセンス Y.Kentaro さん (@yoshi_ken)  下記が発表資料。\n nginx の basic 認証等でアクセス制限を掛ける例  等など\u0026hellip; ごめんなさい！メモが取れてなかった！\u0026hellip;汗\nFluentd as a Kibana @repeatedly さん  fluentd-plugin-kibana-server というkibana をダウンロードしてくるのがめんどい ので fluentd の中で kibana を動かすプラグインを作った話。下記が発表資料。\nhttps://gist.github.com/repeatedly/7427856\n fluentd input plugin として実装 logstash 版もあるらしい  所感 さすが\u0026hellip;。確かにサーバをいちいち作るのめんどいです。fluentd plugin の中で kibana を動かすって発想が\u0026hellip;。\nauth プラグインでアクセスコントロール 株式会社エヌツーエスエム 菅谷信介さん (@shinsuke_sugaya)   elasticsearch のアクセス制御をしたい rest api をアクセス制御する ユーザ管理, REST API のアクセス管理, ログイン・ログアウト・トーケン ユーザ管理は elasticsearch 内に格納 path, http method, role の組み合わせで制御  インストールは下記の通り elasticsearch plugin コマンドで。\n% ./bin/plugin --install org.codelibs/elasticsearch-auth/1.0.0   ユーザ作成は POST で可能  所感とまとめ 箇条書きのざっくりしたまとめでしたが\u0026hellip;。個人的には sharding の話はとても聞き たかったので貴重な時間でした。また、運用の話もとてもおもしろかったし plugin 作っ てみた話も「すごい..」って感じでした。僕はこれからアプリも書くけどインフラエン ジニアなのでアーキテクチャをまず知りたいかなぁという印象。と言うかスケールさせ るときにも横に並べて shard するだけの様な話に聞こえましたが、まだぼんやり感が あります。特に shard 周り。mongo の sharding と同じ様なものを想像していますが。 またモチベーション上がったので週末やってみますー！\nkibana3 は一回使いました。javascript, html で実装されているのでクライアントか ら elasticsearch まで直接アクセス出来なければならないとなって、構成的にどうな のかな？と思っていたら菅谷さんの \u0026lsquo;auth プラグイン\u0026rsquo; の話があったりしたので、良 かったです。使ってみます！\n主催の方々、発表者の方々、当日はありがとうございましたー。また参加させてもらい ます。\n","permalink":"https://jedipunkz.github.io/post/2013/11/13/elasticsearch-second-study-report/","summary":"こんにちは。@jedipunkzです。\n第2回 Elasticsearch 勉強会に参加してきました。箇条書きですが参加レポートを記し ておきます。\n開催日 : 2013/11/12 場所 : 東京駅 グラントウキョウサウスタワー リクルートテクノロジーズさま URL : http://elasticsearch.doorkeeper.jp/events/6532  Routing 周りの話 株式会社シーマーク　大谷純さん (@johtani)  Index 構成  cluster の中に index -\u0026gt; type が作成される index は shard という部分的な index の集まり shard 数は生成時のみ指定可能 node ごとに replica, primary を別ける replica 数は後に変えられる doc -\u0026gt; hash 値を shard 数で割って replica, primary に登録 doc の id の ハッシュ値を利用 type も含める場合はかの設定を true に クライアントはどのノードに対してクエリを投げても OK  routing  id の代わりに routing (URL パラメータ) で登録 url リクエストパラメータとして登録時にルーティングパラメータを登録 id の代わりにパラメータで指定された値のハッシュ値を計算して利用 検索時 routing 指定で関係のある shard のみを指定出来る  スケールアウト  sharding によるスケールアウト数 = インデックス作成時に指定 shard によるインデックスの分割以外にインデックス自体を複数持つことによるスケール 複数のドキュメントをエイリアス書けることが可能  所感 個人的には非常に興味のあるところでした。mongodb のような sharding をイメージし てよいのか？そうでないのか？すら理解出来ていなかったので。sharding を理解する 前提知識の話もあって非常に参考になりました。","title":"第2回 Elasticsearch 勉強会参加レポート"},{"content":"こんにちは。@jedipunkzです。\n僕は Chef 使いなのですが、Chef はオーケストレーションまで踏み込んだツールでは ないように思います。せいぜいインテグレーションが出来る程度なのかなぁと。 しかもインテグレーションするにも Cookbooks の工夫が必要です。以前聞いたことの ある話ですが Opscode 社のエンジニア曰く「オーケストレーション等へのアプローチ はそれぞれ好きにやってね」だそうです。\n個人的にオーケストレーションをテーマに色々調べようかと考えているのですが、 Serf という面白いツールが出てきました。\u0026lsquo;Serf\u0026rsquo; はオーケストレーションを手助けし てくれるシンプルなツールになっています。\nもう既にいろんな方が Serf について調べていますが、どのような動きをするのかを自 分なりに理解した点を記しておこうと思います。\n参考にしたサイト  公式サイト http://www.serfdom.io/ クラスメソッド開発者ブログ http://dev.classmethod.jp/cloud/aws/serf_on_ec2/ Glidenote さん http://blog.glidenote.com/blog/2013/10/30/serf-haproxy/  Serf とは Serf は gossip protocol をクラスタにブロードキャストする。gossip protocol は SWIM : Scalable Weakly-consistent Infecton-style process Group Membership Protocol” をベースとして形成されている。\nSWIM Protocol 概略 serf は新しいクラスタとして稼働するか、既存のクラスタに ‘join’ する形で稼働 するかのどちらかで起動する。\n新しいメンバは TCP で状態を \u0026lsquo;full state sync\u0026rsquo; され既存のクラスタ内にて ‘gossipin (噂)される。この ’gosiping’ は UDP で通信されこれはネットワーク使 用量はノード数に比例することになる。\nランダムなノードとの \u0026lsquo;full state sync\u0026rsquo; は TCP で行われるけどこれは ‘gossiping’ に比べて少い\nある一定期間、あるノードが fails 状態の場合、迂回経路を使ってそのノードに対し てチェックを行う。両方の経路にて fails な場合ノードが ‘suspiciou' (容疑者) 状態になる。もし迂回経路のチェックが fails しなかった場合ネットワーク障害として 判断される。\nSerf の SWIM Protocol からの改修点 Serf は SWIM Protocol をベースにしていると記しましたが、どのような点を改修した のかまとめました。\n \u0026lsquo;full state sync\u0026rsquo; の TCP 化  serf は ‘full state sync’ を TCP にて行う。これにより serf はネットワーセグ メントが分離されている状態でも通信出来る。\n gossip レイヤの分離  serf は failure detection protocol から完全に gossip レイヤを分離。これでより 高速にデータ増殖が行える。\n dead node の扱い  serf は デッドノードに対して回数を記憶している。これにより full state sync が 実施された場合、逆に死んだノードの情報を受取る。SWIM では full state sync を行 わないので死んだノードにを削除してしまう。これはクラスタに対する情報一点集中化 に役立つ。\n使ってみる 早速使ってみます。下記の URL にある demo 用スクリプトを少し改修して使ってみま した。\nhttps://github.com/hashicorp/serf/tree/master/demo/web-load-balancer\n 2つノードを立ち上げる  AWS でも OpenStack でもベアメタルでも何でも良いのでノードを2台用意する。\n 下記のスクリプトを置いて両ノードで実行する。  #!/bin/sh set -e export SERF_ROLE=role01 sudo apt-get install -y unzip # Download and install Serf cd /tmp until wget -O serf.zip https://dl.bintray.com/mitchellh/serf/0.2.1_linux_amd64.zip; do sleep 1 done unzip serf.zip sudo mv serf /usr/local/bin/serf # The member join script is invoked when a member joins the Serf cluster. # Our join script simply adds the node to the load balancer. cat \u0026lt;\u0026lt;EOF \u0026gt;/tmp/join.sh #!/bin/sh echo \u0026#39;member joined\u0026#39; \u0026gt;\u0026gt; /tmp/serf_join.log EOF sudo mv /tmp/join.sh /usr/local/bin/serf_member_join.sh chmod +x /usr/local/bin/serf_member_join.sh # Configure the agent cat \u0026lt;\u0026lt;EOF \u0026gt;/tmp/agent.conf description \u0026#34;Serf agent\u0026#34; start on runlevel [2345] stop on runlevel [!2345] exec /usr/local/bin/serf agent \\\\ -event-handler \u0026#34;member-join=/usr/local/bin/serf_member_join.sh\u0026#34; \\\\ -role=${SERF_ROLE} \u0026gt;\u0026gt;/var/log/serf.log 2\u0026gt;\u0026amp;1 EOF sudo mv /tmp/agent.conf /etc/init/serf.conf # Start the agent! sudo start serf 実行すると \u0026lsquo;role01\u0026rsquo; という Role 名で serf agent が稼働しているはず。またスクリ プトを見てわかると思うが \u0026ndash;event-handler \u0026ldquo;member-join=\u0026lt;スクリプト\u0026gt;\u0026rdquo; としている。 これでクラスタに新しいメンバが join すると /tmp/serf_join.log に \u0026lsquo;member joined\u0026rsquo; というメッセージが出力されるはずだ。実際に実行してみる。\n% sudo serf join \u0026lt;もう片系ノードの IP\u0026gt; % sudo serf members vm01 10.0.2.1 alive role01 vm02 10.0.2.3 alive role01 % tail /tmp/serf_join.log ‘member joined ‘member joined イベントハンドラにはユーザ指定のものも扱える。\nuser:deploy=foo.sh この場合のハンドラは下記のコマンドで発生出来る。\n% serf event deploy コンフィギュレーションファイル 今回は init 起動スクリプト内でイベントハンドラ発生時のスクリプト指定等を行った が、json 形式のコンフィギュレーションファイルにて記述することも可能。\n{ \u0026#34;role\u0026#34;: \u0026#34;load-balancer\u0026#34;, \u0026#34;event_handlers\u0026#34;: [ \u0026#34;member_join.sh\u0026#34;, \u0026#34;user:deploy=foo.sh\u0026#34; ] } 上記ファイルを serf.conf とした場合、このコンフィグの指定は \u0026ndash;config-file=serf.conf で行える。\nロードマップ 最後に Serf の今後のロードマップについて記してあったので、まとめてみた。\n コンフィギュレーションファイル  よりカスタマイズ可能なコンフィギュレーションファイルを扱えるようにする。\n SIGHUP  SIGHUP 信号でリロード出来ないので、これに対応する。\n イベントハンドラライブラリ  イベントハンドラを自作・シェアを容易に行えるようプラグイン化を進める。\nまとめと考察 冒頭にオーケストレーションについて触れましたが、このツールを使うだけでは自分の 考えているオーケストレーションにはならないと思いました。当たり前ですね。。複数 ノードを束ねて構成を形成する、またそれをトリガするのが人間であればそれはインテ グレーションの範囲かなぁと。その先にオーケストレーションがあるとすればトリガ自 身もソフトウェアにさせる必要があるのでそのアルゴリズムを人間が書く必要があるの かなぁと。もちろんそのためにこのSerf は貴重なパーツとなってくれそう。\nあと、ロードマップにも記しましたが開発が盛んにされているようなので今後が楽しみ です。\n","permalink":"https://jedipunkz.github.io/post/2013/11/10/serf/","summary":"こんにちは。@jedipunkzです。\n僕は Chef 使いなのですが、Chef はオーケストレーションまで踏み込んだツールでは ないように思います。せいぜいインテグレーションが出来る程度なのかなぁと。 しかもインテグレーションするにも Cookbooks の工夫が必要です。以前聞いたことの ある話ですが Opscode 社のエンジニア曰く「オーケストレーション等へのアプローチ はそれぞれ好きにやってね」だそうです。\n個人的にオーケストレーションをテーマに色々調べようかと考えているのですが、 Serf という面白いツールが出てきました。\u0026lsquo;Serf\u0026rsquo; はオーケストレーションを手助けし てくれるシンプルなツールになっています。\nもう既にいろんな方が Serf について調べていますが、どのような動きをするのかを自 分なりに理解した点を記しておこうと思います。\n参考にしたサイト  公式サイト http://www.serfdom.io/ クラスメソッド開発者ブログ http://dev.classmethod.jp/cloud/aws/serf_on_ec2/ Glidenote さん http://blog.glidenote.com/blog/2013/10/30/serf-haproxy/  Serf とは Serf は gossip protocol をクラスタにブロードキャストする。gossip protocol は SWIM : Scalable Weakly-consistent Infecton-style process Group Membership Protocol” をベースとして形成されている。\nSWIM Protocol 概略 serf は新しいクラスタとして稼働するか、既存のクラスタに ‘join’ する形で稼働 するかのどちらかで起動する。\n新しいメンバは TCP で状態を \u0026lsquo;full state sync\u0026rsquo; され既存のクラスタ内にて ‘gossipin (噂)される。この ’gosiping’ は UDP で通信されこれはネットワーク使 用量はノード数に比例することになる。","title":"Serf を使ってみた"},{"content":"こんにちは。@jedipunkzです。\n以前、\u0026ldquo;Swift HA 構成を Chef でデプロイ\u0026rdquo; というタイトルで記事を書きました。\nhttp://jedipunkz.github.io/blog/2013/07/26/swift-ha-chef-deploy/\nこちらですが、Swift-Proxy, MySQL, Keystone をそれぞれ haproxy, keepalived で HA 組みました。ですが、これは実用的なのかどうか自分でずっと考えていました。\nMySQL と KeepAlived はできればシングル構成にしたいのと、Swift-Proxy は HA で組 みたい。MySQL は Master/Master レプリケーション構成になり、どちらかのノードが 障害を起こし万が一復旧が難しくなった時、構築し直しがしんどくなります。かと言っ て Swift-Proxy をシングル構成にすると今度はノード追加・削除の作業時にサービス 断が発生します。Swift-Proxy を再起動書ける必要があるからです。なので Swift-Proxy は引き続き HA 構成にしたい。\nもう一点、見直したいと思っていました。\n日経コンピュータから出版されている \u0026ldquo;仮想化大全 2014\u0026rdquo; の記事を読んでいて 気がついたのですが。Swift には下記の通りそれぞれのサーバがあります。\n swift-proxy-server swift-account-server swift-container-server swift-object-server  Swift には下記のような特徴がある事がわかりました。\n swift-object  swift-object は swift-accout, swift-container とは物理リソースの扱いに全く異な る特性を持っています。swift-account, swift-container はクライアントからのリクエ ストに対して \u0026ldquo;アカウントの存在を確認\u0026rdquo;, \u0026ldquo;ACL 情報の確認\u0026rdquo; 等を行うサーバであるの に対して swift-object はストレージ上のオブジェクトをクライアントに提供、または 逆に格納するサーバです。よって、Disk I/O の利用特性として swift-account, container は SSD 等、高スループットの Disk を利用するケースが推奨されるのに対 して swift-object はオブジェクトの実体を格納する必要があるため Disk 容量の大き なストレージを要する。\nよって今回は下記の構成変更をしてより実用的な構成を Chef でデプロイする方法を記 そうと思います。\n swift-object を swift-account, swift-container とノードを分離する MySQL/Keystone の HA を行わず Swift-Proxy のみ HA 化する  参考資料  http://www.rackspace.com/knowledge_center/article/managing-openstack-object-storage-with-chef 仮想化大全  構成 +-----------------+ | load balancer | +-----------------+ | +-------------------+-------------------+-------------------+-------------------+---------------------- proxy network | | | | | +-----------------+ +-----------------+ +-----------------+ +-----------------+ +-----------------+ | chef server | | chef workstation| | swift-mange | | swift-proxy01 | | swift-proxy02 | +-----------------+ +-----------------+ +-----------------+ +-----------------+ +-----------------+ ...\u0026gt; scaling | | | | | +-------------------+-------------------+-------------------+-------------------+-------------------+-- storage network | | | | | | +-----------------+ +-----------------+ +-----------------+ +-----------------+ +-----------------+ +-----------------+ | swift-storage01 | | swift-storage02 | | swift-storage03 | | swift-account01 | | swift-account02 | | swift-account03 | +-----------------+ +-----------------+ +-----------------+ +-----------------+ +-----------------+ +-----------------+ ..\u0026gt; scaling 特徴  load balancer は一般的なハードウェア、ソフトウェアで用意 (今回は割愛) swift-proxy は同等の機能のモノを並列に swift-storage と \u0026lsquo;swift-account, container\u0026rsquo; はそれぞれの zone 毎に並べるため同数 swift-account は swift-account, swift-container が稼働 swift-manage は mysql, keystone, git server を搭載 chef server は外部に配置しても構わないが到達出来る箇所に chef workstation は全ての操作を行う端末。全てのノードに到達出来る箇所に  それぞれのノードでの disk の準備 object, account, container 共に OS 領域とは別の disk を必要とするため /dev/sdb 等のディスクを追加し下記の通り gdisk でパーティションを切る。\n% sudo apt-get update; sudo apt-get -y install gdisk % sudo gdisk /dev/sdb # デバイス名は例 対象ノード : swift-object0[1-3], swift-account0[1-3]\nデプロイ準備 下記は全て chef-workstation での操作\nrcpops 管理の chef-repo を取得する。\n% git clone https://github.com/rcbops/chef-cookbooks.git % cd chef-cookbooks 現時点で v4.1.2 がリリースされているので tag を利用して checkout する。\n% git tag -l 1.0.0 release-2011.3-d5 release-v2.0 v2.9.0 v2.9.1 v2.9.2 v2.9.3 v2.9.4 v2.9.5 v2.9.6 v2.9.7 v2.9.8 v3.0.0 v3.0.1 v3.1.0 v4.1.0 v4.1.1 v4.1.2 % git checkout -b v4.1.2 refs/tags/v4.1.2 git submodule 化されている cookbooks を取得する。\n% git submodule init % git submodule sync % git submodule upate cookbooks, roles の chef-server へのアップロードを行う。\n% knife cookbook upload -o cookbook -a % knife role from file role/*.rb 今回の構成一式を収容する chef environment を作成する。\n{ \u0026#34;name\u0026#34;: \u0026#34;swift\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;cookbook_versions\u0026#34;: { }, \u0026#34;json_class\u0026#34;: \u0026#34;Chef::Environment\u0026#34;, \u0026#34;chef_type\u0026#34;: \u0026#34;environment\u0026#34;, \u0026#34;default_attributes\u0026#34;: { }, \u0026#34;override_attributes\u0026#34;: { \u0026#34;package_component\u0026#34;: \u0026#34;grizzly\u0026#34;, \u0026#34;osops_networks\u0026#34;: { \u0026#34;management\u0026#34;: \u0026#34;10.200.9.0/24\u0026#34;, \u0026#34;public\u0026#34;: \u0026#34;10.200.9.0/24\u0026#34;, \u0026#34;nova\u0026#34;: \u0026#34;10.200.9.0/24\u0026#34;, \u0026#34;swift\u0026#34;: \u0026#34;10.200.9.0/24\u0026#34; }, \u0026#34;keystone\u0026#34;: { \u0026#34;admin_user\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;tenants\u0026#34;: [ \u0026#34;admin\u0026#34;, \u0026#34;service\u0026#34; ], \u0026#34;users\u0026#34;: { \u0026#34;admin\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;secrete\u0026#34;, \u0026#34;roles\u0026#34;: { \u0026#34;admin\u0026#34;: [ \u0026#34;admin\u0026#34; ] } }, \u0026#34;demo\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;demo\u0026#34;, \u0026#34;default_tenant\u0026#34; : \u0026#34;service\u0026#34;, \u0026#34;roles\u0026#34;: { \u0026#34;admin\u0026#34;: [ \u0026#34;admin\u0026#34; ] } } }, \u0026#34;db\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;keystone\u0026#34; } }, \u0026#34;mysql\u0026#34;: { \u0026#34;root_network_acl\u0026#34;: \u0026#34;%\u0026#34;, \u0026#34;allow_remote_root\u0026#34;: true, \u0026#34;server_root_password\u0026#34;: \u0026#34;secrete\u0026#34;, \u0026#34;server_repl_password\u0026#34;: \u0026#34;secrete\u0026#34;, \u0026#34;server_debian_password\u0026#34;: \u0026#34;secrete\u0026#34; }, \u0026#34;monitoring\u0026#34;: { \u0026#34;procmon_provider\u0026#34;: \u0026#34;monit\u0026#34;, \u0026#34;metric_provider\u0026#34;: \u0026#34;collectd\u0026#34; }, \u0026#34;vips\u0026#34;: { \u0026#34;keystone-admin-api\u0026#34;: \u0026#34;10.200.9.112\u0026#34;, \u0026#34;keystone-service-api\u0026#34;: \u0026#34;10.200.9.112\u0026#34;, \u0026#34;keystone-internal-api\u0026#34;: \u0026#34;10.200.9.112\u0026#34;, \u0026#34;swift-proxy\u0026#34;: \u0026#34;10.200.9.112\u0026#34;, \u0026#34;config\u0026#34;: { \u0026#34;10.200.9.112\u0026#34;: { \u0026#34;vrid\u0026#34;: 12, \u0026#34;network\u0026#34;: \u0026#34;management\u0026#34; } } }, \u0026#34;developer_mode\u0026#34;: false, \u0026#34;swift\u0026#34;: { \u0026#34;swift_hash\u0026#34;: \u0026#34;107c0568ea84\u0026#34;, \u0026#34;authmode\u0026#34;: \u0026#34;keystone\u0026#34;, \u0026#34;authkey\u0026#34;: \u0026#34;3f281b71-ce89-4b27-a2ad-ad873d3f2760\u0026#34; } } } 作成した environment ファイル environments/swift.json を chef-server へアップ ロードする。\n% knife environment from file environments/swift.json デプロイ かきのとおり knife bootstrap する。\n% knife bootstrap \u0026lt;manage_ip_addr\u0026gt; -N swift-manage -r \u0026#39;role[base]\u0026#39;,\u0026#39;role[mysql-master]\u0026#39;,\u0026#39;role[keystone]\u0026#39;,\u0026#39;role[swift-management-server]\u0026#39; -E swift --sudo -x thirai % knife bootstrap \u0026lt;proxy01_ip_addr\u0026gt; -N swift-proxy01 -r \u0026#34;role[base]\u0026#34;,\u0026#34;role[swift-proxy-server]\u0026#34;,\u0026#39;role[swift-setup]\u0026#39;,\u0026#39;role[openstack-ha]\u0026#39; -E swift --sudo -x thirai % knife bootstrap \u0026lt;proxy02_ip_addr\u0026gt; -N swift-proxy02 -r \u0026#34;role[base]\u0026#34;,\u0026#34;role[swift-proxy-server]\u0026#34;,\u0026#39;role[openstack-ha]\u0026#39; -E swift --sudo -x thirai % knife bootstrap \u0026lt;storage01_ip_addr\u0026gt; -N swift-storage01 -r \u0026#39;role[base]\u0026#39;,\u0026#39;role[swift-object-server]\u0026#39; -E swift --sudo -x thirai % knife bootstrap \u0026lt;storage02_ip_addr\u0026gt; -N swift-storage02 -r \u0026#39;role[base]\u0026#39;,\u0026#39;role[swift-object-server]\u0026#39; -E swift --sudo -x thirai % knife bootstrap \u0026lt;storage03_ip_addr\u0026gt; -N swift-storage03 -r \u0026#39;role[base]\u0026#39;,\u0026#39;role[swift-object-server]\u0026#39; -E swift --sudo -x thirai % knife bootstrap \u0026lt;account01_ip_addr\u0026gt; -N swift-account01 -r \u0026#39;role[base]\u0026#39;,\u0026#39;role[swift-account-server]\u0026#39;,\u0026#39;role[swift-container-server]\u0026#39; -E swift --sudo -x thirai % knife bootstrap \u0026lt;account02_ip_addr\u0026gt; -N swift-account02 -r \u0026#39;role[base]\u0026#39;,\u0026#39;role[swift-account-server]\u0026#39;,\u0026#39;role[swift-container-server]\u0026#39; -E swift --sudo -x thirai % knife bootstrap \u0026lt;account03_ip_addr\u0026gt; -N swift-account03 -r \u0026#39;role[base]\u0026#39;,\u0026#39;role[swift-account-server]\u0026#39;,\u0026#39;role[swift-container-server]\u0026#39; -E swift --sudo -x thirai zone 番号を付与する。\n% knife exec -E \u0026#34;nodes.find(:name =\u0026gt; \u0026#39;swift-storage01\u0026#39;) {|n| n.set[\u0026#39;swift\u0026#39;][\u0026#39;zone\u0026#39;] = \u0026#39;1\u0026#39;; n.save }\u0026#34; % knife exec -E \u0026#34;nodes.find(:name =\u0026gt; \u0026#39;swift-account01\u0026#39;) {|n| n.set[\u0026#39;swift\u0026#39;][\u0026#39;zone\u0026#39;] = \u0026#39;1\u0026#39;; n.save }\u0026#34; % knife exec -E \u0026#34;nodes.find(:name =\u0026gt; \u0026#39;swift-storage02\u0026#39;) {|n| n.set[\u0026#39;swift\u0026#39;][\u0026#39;zone\u0026#39;] = \u0026#39;2\u0026#39;; n.save }\u0026#34; % knife exec -E \u0026#34;nodes.find(:name =\u0026gt; \u0026#39;swift-account02\u0026#39;) {|n| n.set[\u0026#39;swift\u0026#39;][\u0026#39;zone\u0026#39;] = \u0026#39;2\u0026#39;; n.save }\u0026#34; % knife exec -E \u0026#34;nodes.find(:name =\u0026gt; \u0026#39;swift-storage03\u0026#39;) {|n| n.set[\u0026#39;swift\u0026#39;][\u0026#39;zone\u0026#39;] = \u0026#39;3\u0026#39;; n.save }\u0026#34; % knife exec -E \u0026#34;nodes.find(:name =\u0026gt; \u0026#39;swift-account03\u0026#39;) {|n| n.set[\u0026#39;swift\u0026#39;][\u0026#39;zone\u0026#39;] = \u0026#39;3\u0026#39;; n.save }\u0026#34; zone 番号が付与されたことを確認する。\naccount-server の確認\n% knife exec -E \u0026#39;search(:node,\u0026#34;role:swift-account-server\u0026#34;) \\ { |n| z=n[:swift][:zone]||\u0026#34;not defined\u0026#34;; puts \u0026#34;#{n.name} has the role \\ [swift-account-server] and is in swift zone #{z}\u0026#34;; }\u0026#39; swift-account01 has the role [swift-account-server] and is in swift zone 1 swift-account02 has the role [swift-account-server] and is in swift zone 2 swift-account03 has the role [swift-account-server] and is in swift zone 3 container-server の確認\n% knife exec -E \u0026#39;search(:node,\u0026#34;role:swift-container-server\u0026#34;) \\ { |n| z=n[:swift][:zone]||\u0026#34;not defined\u0026#34;; puts \u0026#34;#{n.name} has the role \\ [swift-container-server] and is in swift zone #{z}\u0026#34;; }\u0026#39; swift-account01 has the role [swift-container-server] and is in swift zone 1 swift-account02 has the role [swift-container-server] and is in swift zone 2 swift-account03 has the role [swift-container-server] and is in swift zone 3 object-server の確認\n% knife exec -E \u0026#39;search(:node,\u0026#34;role:swift-object-server\u0026#34;) \\ { |n| z=n[:swift][:zone]||\u0026#34;not defined\u0026#34;; puts \u0026#34;#{n.name} has the role \\ [swift-object-server] and is in swift zone #{z}\u0026#34;; }\u0026#39; swift-storage01 has the role [swift-object-server] and is in swift zone 1 swift-storage02 has the role [swift-object-server] and is in swift zone 2 swift-storage03 has the role [swift-object-server] and is in swift zone 3 Chef が各々のノードに搭載された Disk を検知出来るか否かを確認する。\n% knife exec -E \\  \u0026#39;search(:node,\u0026#34;role:swift-object-server OR \\ role:swift-account-server \\ OR role:swift-container-server\u0026#34;) \\ { |n| puts \u0026#34;#{n.name}\u0026#34;; \\ begin; n[:swift][:state][:devs].each do |d| \\ puts \u0026#34;\\tdevice #{d[1][\u0026#34;device\u0026#34;]}\u0026#34;; \\ end; rescue; puts \\ \u0026#34;no candidate drives found\u0026#34;; end; }\u0026#39; swift-storage02 device sdb1 swift-storage03 device sdb1 swift-account01 device sdb1 swift-account02 device sdb1 swift-account03 device sdb1 swift-storage01 device sdb1 swift-manage ノードにて chef-client を実行し /etc/swift/ring-workspace/generate-rings.sh を更新する。\nswift-manage% sudo chef-client generate-rings.sh の \u0026lsquo;exit 0\u0026rsquo; 行をコメントアウトし実行\nswift-manage% sudo ${EDITOR} /etc/swift/ring-workspace/generage-rings.sh swift-manage% sudo /etc/swift/ring-workspace/generate-rings.sh この操作で /etc/swift/ring-workspace/rings 配下に account, container, object 用の Rings ファイル群が生成されたことを確認出来るはずである。これらを swift-manage 上で既に稼働している git サーバに push し管理する。\nswift-manage# cd /etc/swift/ring-workspace/rings swift-manage# git add account.builder container.builder object.builder swift-manage# git add account.ring.gz container.ring.gz object.ring.gz swift-manage# git commit -m \u0026#34;initial commit\u0026#34; swift-manage# git push 各々のノードにて chef-client を実行することで git サーバ上の Rings ファイル群 を取得し、swift プロセスを稼働させる。\nswift-proxy01# chef-client swift-proxy02# chef-client swift-storage01# chef-client swift-storage02# chef-client swift-storage03# chef-client swift-account01# chef-client swift-account02# chef-client swift-account03# chef-client 3台のノードが登録されたかどうかを下記の通り確認行う。\nswift-proxy01% sudo swift-recon --md5 [sudo] password for thirai: =============================================================================== --\u0026gt; Starting reconnaissance on 3 hosts =============================================================================== [2013-10-18 11:14:43] Checking ring md5sums 3/3 hosts matched, 0 error[s] while checking hosts. =============================================================================== 動作確認 swift-storage01# source swift-openrc swift-storage01# swift post container01 swift-storage01# echo \u0026#34;test\u0026#34; \u0026gt; test swift-storage01# swift upload container01 test swift-storage01# swift list swift-storage01# swift list container01 ノードの追加方法 次にノードの追加方法を記します。\ngdisk にて /dev/sdb1 等のパーティションを作成する。\n下記の通り knife bootstrap する。\n% knife bootstrap \u0026lt;storage04_ip_addr\u0026gt; -N swift-storage04 -r \u0026#39;role[base]\u0026#39;,\u0026#39;role[swift-object-server]\u0026#39; -E swift --sudo -x thirai % knife bootstrap \u0026lt;account04_ip_addr\u0026gt; -N swift-account04 -r \u0026#39;role[base]\u0026#39;,\u0026#39;role[swift-account-server]\u0026#39;,\u0026#39;role[swift-container-server]\u0026#39; -E swift --sudo -x thirai zone 番号を付与する。\n% knife exec -E \u0026#34;nodes.find(:name =\u0026gt; \u0026#39;swift-storage04\u0026#39;) {|n| n.set[\u0026#39;swift\u0026#39;][\u0026#39;zone\u0026#39;] = \u0026#39;4\u0026#39;; n.save }\u0026#34; % knife exec -E \u0026#34;nodes.find(:name =\u0026gt; \u0026#39;swift-account04\u0026#39;) {|n| n.set[\u0026#39;swift\u0026#39;][\u0026#39;zone\u0026#39;] = \u0026#39;4\u0026#39;; n.save }\u0026#34; zone 番号が付与されたことを確認する。\naccount-server の確認\n% knife exec -E \u0026#39;search(:node,\u0026#34;role:swift-account-server\u0026#34;) \\ { |n| z=n[:swift][:zone]||\u0026#34;not defined\u0026#34;; puts \u0026#34;#{n.name} has the role \\ [swift-account-server] and is in swift zone #{z}\u0026#34;; }\u0026#39; swift-account01 has the role [swift-account-server] and is in swift zone 1 swift-account02 has the role [swift-account-server] and is in swift zone 2 swift-account03 has the role [swift-account-server] and is in swift zone 3 swift-account04 has the role [swift-account-server] and is in swift zone 4 container-server の確認\n% knife exec -E \u0026#39;search(:node,\u0026#34;role:swift-container-server\u0026#34;) \\ { |n| z=n[:swift][:zone]||\u0026#34;not defined\u0026#34;; puts \u0026#34;#{n.name} has the role \\ [swift-container-server] and is in swift zone #{z}\u0026#34;; }\u0026#39; swift-account01 has the role [swift-container-server] and is in swift zone 1 swift-account02 has the role [swift-container-server] and is in swift zone 2 swift-account03 has the role [swift-container-server] and is in swift zone 3 swift-account04 has the role [swift-container-server] and is in swift zone 4 object-server の確認\n% knife exec -E \u0026#39;search(:node,\u0026#34;role:swift-object-server\u0026#34;) \\ { |n| z=n[:swift][:zone]||\u0026#34;not defined\u0026#34;; puts \u0026#34;#{n.name} has the role \\ [swift-object-server] and is in swift zone #{z}\u0026#34;; }\u0026#39; swift-storage01 has the role [swift-object-server] and is in swift zone 1 swift-storage02 has the role [swift-object-server] and is in swift zone 2 swift-storage03 has the role [swift-object-server] and is in swift zone 3 swift-storage04 has the role [swift-object-server] and is in swift zone 4 swift-manage ノードにて chef-client を実行し /etc/swift/ring-workspace/generate-rings.sh を更新する。\nswift-manage% sudo chef-client generate-rings.sh の \u0026lsquo;exit 0\u0026rsquo; 行をコメントアウトし実行\nswift-manage% sudo ${EDITOR} /etc/swift/ring-workspace/generage-rings.sh swift-manage% sudo /etc/swift/ring-workspace/generate-rings.sh この操作で /etc/swift/ring-workspace/rings 配下に account, container, object 用の Rings ファイル群が生成されたことを確認出来るはずである。これらを swift-manage 上で既に稼働している git サーバに push し管理する。\nswift-manage# cd /etc/swift/ring-workspace/rings swift-manage# git add account.builder container.builder object.builder swift-manage# git add account.ring.gz container.ring.gz object.ring.gz swift-manage# git commit -m \u0026#34;added zone 4 nodes\u0026#34; swift-manage# git push 各々のノードにて chef-client を実行することで git サーバ上の Rings ファイル群 を取得し、swift プロセスを稼働させる。\nswift-proxy01# chef-client swift-proxy02# chef-client swift-storage01# chef-client swift-storage02# chef-client swift-storage03# chef-client swift-storage04# chef-client swift-account01# chef-client swift-account02# chef-client swift-account03# chef-client swift-account04# chef-client 4台のノードが登録されたかどうかを下記の通り確認行う。\nswift-proxy01% sudo swift-recon --md5 [sudo] password for thirai: =============================================================================== --\u0026gt; Starting reconnaissance on 4 hosts =============================================================================== [2013-10-18 11:14:43] Checking ring md5sums 4/4 hosts matched, 0 error[s] while checking hosts. =============================================================================== proxy ノードの swift プロセスを停止・起動する\nswift-proxy01# swift-init all stop swift-proxy01# swift-init all start swift-proxy02# swift-init all stop swift-proxy02# swift-init all start ノード削除方法 次にノードの削除方法を記します。\ndisk 障害等のzone4 のノード swift-storage04, swift-account04 を削除する前提で記す。\nswift-manage# swift-ring-builder /etc/swift/ring-workspace/rings/account.builder remove 10.200.9.109 swift-manage# swift-ring-builder /etc/swift/ring-workspace/rings/container.builder remove 10.200.9.109 swift-manage# swift-ring-builder /etc/swift/ring-workspace/rings/object.builder remove 10.200.9.110 swift-manage 上で既に稼働している git サーバに push し管理する。\nswift-manage# cd /etc/swift/ring-workspace/rings swift-manage# git add account.builder container.builder object.builder swift-manage# git add account.ring.gz container.ring.gz object.ring.gz swift-manage# git commit -m \u0026#34;added zone 4 nodes\u0026#34; swift-manage# git push 全てのノードで chef-client を実行する。この操作で rings ファイルの配布が行える。\nswift-proxy01# chef-client swift-proxy02# chef-client swift-storage01# chef-client swift-storage02# chef-client swift-storage03# chef-client swift-storage04# chef-client swift-account01# chef-client swift-account02# chef-client swift-account03# chef-client swift-account04# chef-client proxy ノードの swift プロセスを停止・起動する\nswift-proxy01# swift-init all stop swift-proxy01# swift-init all start swift-proxy02# swift-init all stop swift-proxy02# swift-init all start まとめと考察 MySQL のリカバリは単純にダンプからのリストアが良かったのでシングル構成に出来て よかった。また、通常のシングルスター・マルチスレーブの構成に変えても OK かと思 いますが、Cookbooks を修正する必要がありそう。まぁ Keystone のデータのみを管理 している MySQL なので負荷は無いですしシングル構成が良いと個人的には思います。 また Swift-Proxy は 2 台構成の HA になります。VRRP プロトコルの仕組み、また haproxy の構成上 3 台以上でも OK でありますが、Cookbooks 的に NG でした。これ は修正する価値が大いにありそう。\nまたこの構成を Chef で管理し始めると大量のノードで Chef-Client を実行すること になるので Gnu Prallel や pssh を使った方が良さそう\u0026hellip;。\n","permalink":"https://jedipunkz.github.io/post/2013/10/27/swift-chef/","summary":"こんにちは。@jedipunkzです。\n以前、\u0026ldquo;Swift HA 構成を Chef でデプロイ\u0026rdquo; というタイトルで記事を書きました。\nhttp://jedipunkz.github.io/blog/2013/07/26/swift-ha-chef-deploy/\nこちらですが、Swift-Proxy, MySQL, Keystone をそれぞれ haproxy, keepalived で HA 組みました。ですが、これは実用的なのかどうか自分でずっと考えていました。\nMySQL と KeepAlived はできればシングル構成にしたいのと、Swift-Proxy は HA で組 みたい。MySQL は Master/Master レプリケーション構成になり、どちらかのノードが 障害を起こし万が一復旧が難しくなった時、構築し直しがしんどくなります。かと言っ て Swift-Proxy をシングル構成にすると今度はノード追加・削除の作業時にサービス 断が発生します。Swift-Proxy を再起動書ける必要があるからです。なので Swift-Proxy は引き続き HA 構成にしたい。\nもう一点、見直したいと思っていました。\n日経コンピュータから出版されている \u0026ldquo;仮想化大全 2014\u0026rdquo; の記事を読んでいて 気がついたのですが。Swift には下記の通りそれぞれのサーバがあります。\n swift-proxy-server swift-account-server swift-container-server swift-object-server  Swift には下記のような特徴がある事がわかりました。\n swift-object  swift-object は swift-accout, swift-container とは物理リソースの扱いに全く異な る特性を持っています。swift-account, swift-container はクライアントからのリクエ ストに対して \u0026ldquo;アカウントの存在を確認\u0026rdquo;, \u0026ldquo;ACL 情報の確認\u0026rdquo; 等を行うサーバであるの に対して swift-object はストレージ上のオブジェクトをクライアントに提供、または 逆に格納するサーバです。よって、Disk I/O の利用特性として swift-account, container は SSD 等、高スループットの Disk を利用するケースが推奨されるのに対 して swift-object はオブジェクトの実体を格納する必要があるため Disk 容量の大き なストレージを要する。","title":"実用的な Swift 構成を Chef でデプロイ"},{"content":"こんにちは。@jedipunkzです。\n前回、OpenStack と test-kitchen を使った環境構築方法を書きました。下記の記事で す。\nhttp://jedipunkz.github.io/blog/2013/10/13/test-kitchn-openstack-chef-cookbooks-test/\n今回は実際にテストを書く方法を記していたい思います。\n今回使用するテストツールは下記の2つです。\n rspec と serverspec busser-bats  参考資料 Creationline lab さんの資料を参考にさせて頂きました。\nhttp://www.creationline.com/lab/2933\n用意するモノ達  OpenStack にアクセスするためのユーザ・パスワード Keystone の AUTH_URL テストに用いる OS イメージの Image ID テナント ID nova 管理のキーペアの作成  これらは OpenStack を普段から利用されている方なら馴染みのモノかと思います。\n.kitchen.yml ファイルの作成 下記の通り .kitchen.yml ファイルを test-kitchen のルートディレクトリで作成しま す。今後の操作は全てこのディレクトリで作業行います。\n\u0026ldquo;\u0026lt;\u0026gt;\u0026rdquo; で括った箇所が環境に合わせた設定になります。\nまた、ここでは前回同様に \u0026lsquo;ntp\u0026rsquo; の Cookbook をテストする前提で記します。\n+++ driver_plugin: openstack suites: - name: default run_list: - recipe[ntp::default] attributes: {} platforms: - name: ubuntu-12.04 driver_config: openstack_username: \u0026lt;openstack_username\u0026gt; openstack_api_key: \u0026lt;openstack_password\u0026gt; openstack_auth_url: http://\u0026lt;openstack_ip_addr\u0026gt;:5000/v2.0/tokens image_ref: \u0026lt;image_id\u0026gt; flavor_ref: 1 key_name: \u0026lt;key_name\u0026gt; openstack_tenant: \u0026lt;tenant_name\u0026gt; username: \u0026lt;ssh_username\u0026gt; private_key_path: \u0026lt;path_to_secretkey\u0026gt; - name: centos-64 driver_config: openstack_username: \u0026lt;openstack_username\u0026gt; openstack_api_key: \u0026lt;openstack_password\u0026gt; openstack_auth_url: http://\u0026lt;openstack_ip_addr\u0026gt;:5000/v2.0/tokens image_ref: \u0026lt;image_id\u0026gt; flavor_ref: 1 key_name: \u0026lt;key_name\u0026gt; openstack_tenant: \u0026lt;tenant_name\u0026gt; username: \u0026lt;ssh_username\u0026gt; private_key_path: \u0026lt;path_to_secretkey\u0026gt; busser-bats テスト +++\nまずはじめに busser-bats のテストを記します。\nディレクトリ作成 kitchen init を行うことでもこの操作は可能なのですが kitchen-openstack を利用すること を想定しない形で成形されてしまうため、下記の通り実行する。\n% cd $TEST-KITCHEN-ROOT/ % mkdir -p test/integration/default/bats ディレクトリ構成 ディレクトリ構成は\ntest/integration/${SUITE_NAME}/${BUSSER_NAME}/${TEST_NAME} となっています。\nテスト作成 test/integration/default/bats/test.bats として下記のファイルを作成します。\n@test \u0026#34;ntp must be installed\u0026#34; { which ntpd } @test \u0026#34;ntp.conf must be exist\u0026#34; { cat /etc/ntp.conf | grep \u0026#34;server 0.pool.ntp.org iburst\u0026#34; } 一項目と二項目の説明を書いておきます。\n 一項目の @test  ntpd コマンドが存在するか否かで package \u0026lsquo;ntp\u0026rsquo; がインストール されていることを確認。\n 二項目の @test  特定の文字列が /etc/ntp.conf に記述あるか否かでファイルの存在を確認。\nrspec serverspec によるテスト実施 次に rspec + serverspec のテストの記述方法を記します。\nディレクトリ作成 ディレクトリを作成します。\n% mkdir test/integration/default/rspec Gemfile 追記 下記の通り test/integration/default/rspec/Gemfile を作成します。\nsource \u0026#39;https://rubygems.org\u0026#39; gem \u0026#39;serverspec\u0026#39; serverspec-init の実行 serverspec-init コマンドにより初期化を行います。\n% cd test/integration/default/rspec % serverspec-init Select OS type: 1) UN*X 2) Windows Select number: 1 Select a backend type: 1) SSH 2) Exec (local) Select number: 2 + spec/ + spec/localhost/ + spec/localhost/httpd_spec.rb + spec/spec_helper.rb + Rakefile 上記の通り spec ディレクトリ・ファイルが作成されることを確認します。\nntp_spec.rb を作成 下記の通り test/integration/default/rspec/spec/localhost/ntp_spec.rb を作成し ます。\nrequire \u0026#39;spec_helper\u0026#39; describe package(\u0026#39;ntp\u0026#39;) do it { should be_installed } end describe service(\u0026#39;ntp\u0026#39;) do it { should be_enabled } it { should be_running } end describe port(123) do it { should be_listening } end describe file(\u0026#39;/etc/ntp.conf\u0026#39;) do it { should be_file } it { should contain \u0026#34;server 0.pool.ntp.org iburst\u0026#34; } end テスト内容の解説は\u0026hellip;\n   \u0026lsquo;ntp\u0026rsquo; パッケージがインストールされていることを確認    \u0026lsquo;ntp\u0026rsquo; サービスが再起動時有効になっていること・起動していることを確認    123 番ポートで Listen していることを確認    /etc/ntp.conf に特定の文字列が存在することを確認    テスト実行 下記の通りテストを実行する。\n% cd $TEST-KITCHEN-ROOT % bundle exec kitchen create # \u0026lt;---- OpenStack 上にインスタンス作成 % bundle exec kitchen setup # \u0026lt;---- Chef Cookbooks の実行 % bundle exec kitchen verify # \u0026lt;---- テストの実施 またこれらの操作は下記の一つのコマンドで実施出来る。\n% bundle exec kitchen test テスト結果 busser-bats 下記の通り2つのテストが実施され \u0026lsquo;ok\u0026rsquo; ステータスが帰って来たことを確認する。\n-----\u0026gt; Running bats test suite 1..2 ok 1 ntp must be installed ok 2 ntp.conf must be exist serverspec 下記の通り 6 個のテスト全てが通り failure 0 個であることを確認する。\n-----\u0026gt; Running rspec test suite [2013-10-17T02:27:19+00:00] INFO: Run List is [] [2013-10-17T02:27:19+00:00] INFO: Run List expands to [] Recipe: (chef-apply cookbook)::(chef-apply recipe) * execute[bundle install --local || bundle install] action run [2013-10-17T02:27:19+00:00] INFO: Processing execute[bundle install --local || bundle install] action run ((chef-apply cookbook)::(chef-apply recipe) line 42) Resolving dependencies... Using diff-lcs (1.2.4) Using highline (1.6.20) Using net-ssh (2.7.0) Using rspec-core (2.14.6) Using rspec-expectations (2.14.3) Using rspec-mocks (2.14.4) Using rspec (2.14.1) Using serverspec (0.10.6) Using bundler (1.3.5) Your bundle is complete! Use `bundle show [gemname]` to see where a bundled gem is installed. [2013-10-17T02:27:19+00:00] INFO: execute[bundle install --local || bundle install] ran successfully - execute bundle install --local || bundle install \u0026hellip;\u0026hellip;\n Finished in 0.0567 seconds  6 examples, 0 failures Finished verifying (0m7.44s).\n まとめ ----- busser-bats はまだ地道なテストの記述が必要らしい。それに比べて serverspec は既 に実用的と言えるかもしれない。どちらのテストツールも Cookbook でデプロイされた 環境の *状態* をテストするモノであって Cookbook, Recipe のテストとは違う。これ はある意味都合が良い。Cookbooks のテストは ChefSpec 等のテストツールでテストを 行い、完成された Cookbooks を実際に複数の OS 上にデプロイしてテストするのが今 回紹介したモノとなる。 ","permalink":"https://jedipunkz.github.io/post/2013/10/20/test-kitchen-openstack-chef-cookbooks-test-2/","summary":"こんにちは。@jedipunkzです。\n前回、OpenStack と test-kitchen を使った環境構築方法を書きました。下記の記事で す。\nhttp://jedipunkz.github.io/blog/2013/10/13/test-kitchn-openstack-chef-cookbooks-test/\n今回は実際にテストを書く方法を記していたい思います。\n今回使用するテストツールは下記の2つです。\n rspec と serverspec busser-bats  参考資料 Creationline lab さんの資料を参考にさせて頂きました。\nhttp://www.creationline.com/lab/2933\n用意するモノ達  OpenStack にアクセスするためのユーザ・パスワード Keystone の AUTH_URL テストに用いる OS イメージの Image ID テナント ID nova 管理のキーペアの作成  これらは OpenStack を普段から利用されている方なら馴染みのモノかと思います。\n.kitchen.yml ファイルの作成 下記の通り .kitchen.yml ファイルを test-kitchen のルートディレクトリで作成しま す。今後の操作は全てこのディレクトリで作業行います。\n\u0026ldquo;\u0026lt;\u0026gt;\u0026rdquo; で括った箇所が環境に合わせた設定になります。\nまた、ここでは前回同様に \u0026lsquo;ntp\u0026rsquo; の Cookbook をテストする前提で記します。\n+++ driver_plugin: openstack suites: - name: default run_list: - recipe[ntp::default] attributes: {} platforms: - name: ubuntu-12.","title":"test-kitchen と OpenStack で Chef Cookbooks テスト (後篇)"},{"content":"こんにちは。@jedipunkzです。\ntest-kitchen + Vagrant を利用して複数環境で Chef Cookbooks のテストを行う方法は 結構皆さん利用されていると思うのですが Vagrant だと手元のマシンに仮想マシンが バシバシ立ち上げるので僕はあまり好きではないです。そこで、OpenStack のインスタ ンスをその代替で使えればいいなぁと結構前から思っていたのですが、今回うまくいっ たのでその方法を記します。\n用意するモノ  OpenStack 環境一式 Chef がインストールされた OS イメージとその ID test-kitchen を実行するワークステーション (お手持ちの Macbook 等)  OS イメージの作成ですが Veewee などで自動構築できますし、インスタンス上で Chef のインストールを行った後にスナップショットを作成してそれを利用しても構いません。\ntest-kitchen のインストール test-kitchen をインストールします。versoin 1.0.0 はまだリリースされていないの で github から master ブランチを取得してビルドします。直近で OpenStack に関連 する不具合の修正等が入っているのでこの方法を取ります。\n% git clone https://github.com/opscode/test-kitchen.git % cd test-kitchen % bundle install % rake build # \u0026lt;--- gem をビルド % gen install ./pkg/test-kitchen-1.0.0.dev.gem 現時点 (2013/10/13) で berkshelf の利用しているソフトウェアと衝突を起こす問題 があるので bundle で解決します。下記のように Gemfile に gem \u0026lsquo;kitchen-openstack\u0026rsquo; と記述します。\nsource \u0026#39;https://rubygems.org\u0026#39; gemspec gem \u0026#39;kitchen-openstack\u0026#39; # \u0026lt;--- 追記 group :guard do gem \u0026#39;rb-inotify\u0026#39;, :require =\u0026gt; false gem \u0026#39;rb-fsevent\u0026#39;, :require =\u0026gt; false gem \u0026#39;rb-fchange\u0026#39;, :require =\u0026gt; false gem \u0026#39;guard-minitest\u0026#39;, \u0026#39;~\u0026gt; 1.3\u0026#39; gem \u0026#39;guard-cucumber\u0026#39;, \u0026#39;~\u0026gt; 1.4\u0026#39; end kitchen-openstack のインストール kitchen-openstack をインストールします。こちらも gem をビルドしてインストール します。\n% git clone https://github.com/RoboticCheese/kitchen-openstack.git % cd kitchen-openstack % bundle insatll #\u0026lt;---- 関連ソフトウェアインストール % rake build #\u0026lt;---- gem をビルド % gem install ./pkg/kitchen-openstack-0.5.1.gem .kitchen.yml の作成 .kitchen.yml を用意します。test-kitchen のディレクトリに移動し .kitchen.yml を 下記の例に従って作成します。今回は Ubuntu OS, CentOS にてテストを実行します。\n% cd /path/to/test-kitchen % ${EDITOR} .kitchen.yml +++ driver_plugin: openstack suites: - name: default run_list: - recipe[ntp::default] attributes: {} platforms: - name: ubuntu-12.04 driver_config: openstack_username: \u0026lt;openstac_username\u0026gt; openstack_api_key: \u0026lt;openstack_password\u0026gt; openstack_auth_url: http://\u0026lt;openstack_ip_addr\u0026gt;:5000/v2.0/tokens image_ref: \u0026lt;image_id\u0026gt; flavor_ref: 1 key_name: \u0026lt;key_name\u0026gt; openstack_tenant: service username: ubuntu private_key_path: \u0026lt;path_to_secretkey\u0026gt; - name: centos-64 driver_config: openstack_username: \u0026lt;openstac_username\u0026gt; openstack_api_key: \u0026lt;openstack_password\u0026gt; openstack_auth_url: http://\u0026lt;openstack_ip_addr\u0026gt;:5000/v2.0/tokens image_ref: \u0026lt;image_id\u0026gt; flavor_ref: 1 key_name: \u0026lt;key_name\u0026gt; openstack_tenant: service username: root private_key_path: \u0026lt;path_to_secretkey\u0026gt; ファイルの内容について解説します。\n suites:  実行したい Chef Cookbooks のレシピ名を指定します。attriutes などをここで上書き することも出来ます。\n platforms:  テストに用いたい OS を列挙していけます。ここでは例として Ubuntu, CentOS を記し ました。\n openstack_username, openstack_api_key  OpenStack にログインするためのユーザ名とパスワードです。keystone で作成します。\n openstack_auth_url  Keystone の URL です。最後に /tokens と付けるのを忘れずに。\n image_ref  それぞれの OS イメージの ID を記します。前述したとおりインスタンスでオペレーショ ン後にスナップショットを作成しそれを記すことも可能です。\n flavor_ref  Flavor ID を記します。ここでは一番小さい Flavor である m1.tiny を記しました。\n key_name  インスタンス作成時に選択する Nova のキーペア名です。OpenStack コマンドラインで 言う \u0026ndash;key_name です。\n openstack_tenant  どのテナントにインスタンスを作成するか？を記します。\n username  インスタンスにログインする際のユーザ名を記します。\n private_key_path  インスタンスにログインするための SSH 秘密鍵のパスを記します。ここではノンパス ワードでログイン出来るよう鍵を生成してあげる必要があります。\nCookbooks の配置 カレントディレクトリ配下に \u0026lsquo;cookbooks\u0026rsquo; という名前のディレクトリを用意し テストで用いたい Cookbooks を配置します。Berkshelf を用いれば簡単です。が、 現時点で Berkshelf の用いているソフトウェア test-kitchen のそれが衝突を起こす のでテスト実行前には Berkshelf ファイルを退避してください。\n% ${EDITOR} Berksfile site :opscode cookbook \u0026#39;ntp; % berks install --path=./cookbooks % mv Berksfile Berksfile.old テスト実行 いよいよテストを実行します。上記の例では Ubuntu OS, Debian OS に対して ntp の Chef Cookbooks を実際にデプロイしテストを行います。\n% bundle exec kitchen test \u0026lsquo;test\u0026rsquo; を引数で渡すと\n インスタンス作成 Chef のインストール Cookbooks のアップロード chef-solo の実行 Cookbooks 中に \u0026lsquo;test\u0026rsquo; ディレクトリがある場合はテスト実行  を行ってくれます。それぞれ別々に実行したい場合は\n% bundle exec kitchen create # \u0026lt;---- インスタンスの作成 % bundle exec kitchen setup # \u0026lt;---- chef のインストールと初回の converge % bundle exec kitchen converge # \u0026lt;---- chef-solo を再度実行 % bundle exec kitchen verify # \u0026lt;---- \u0026#39;test\u0026#39; ディレクトリに従いテスト実行 % bundle exec kitchen destroy # \u0026lt;---- インスタンスの削除 と行えば良いです。converge, verify は何度でも繰り返し実行が可能。\nまとめ 前述したとおり Vagrant を使うと手元のマシンのリソースを大量に消費してしまうの で OpenStack を利用する価値は結構あるのかなぁと思っています。バージョン 1.0.0 がリリースされる時期も近いと思うので今のうちに知っておくと良いかと思います。\nまた、テストと言っても test-kitchen の場合2つの意味があると思います。実際に Cookbooks をインスタンスにインストールするテスト、と Cookbooks 自体のテストと いう意味です。後者についてはまた後ほど記したいと思います。\n","permalink":"https://jedipunkz.github.io/post/2013/10/13/test-kitchn-openstack-chef-cookbooks-test/","summary":"こんにちは。@jedipunkzです。\ntest-kitchen + Vagrant を利用して複数環境で Chef Cookbooks のテストを行う方法は 結構皆さん利用されていると思うのですが Vagrant だと手元のマシンに仮想マシンが バシバシ立ち上げるので僕はあまり好きではないです。そこで、OpenStack のインスタ ンスをその代替で使えればいいなぁと結構前から思っていたのですが、今回うまくいっ たのでその方法を記します。\n用意するモノ  OpenStack 環境一式 Chef がインストールされた OS イメージとその ID test-kitchen を実行するワークステーション (お手持ちの Macbook 等)  OS イメージの作成ですが Veewee などで自動構築できますし、インスタンス上で Chef のインストールを行った後にスナップショットを作成してそれを利用しても構いません。\ntest-kitchen のインストール test-kitchen をインストールします。versoin 1.0.0 はまだリリースされていないの で github から master ブランチを取得してビルドします。直近で OpenStack に関連 する不具合の修正等が入っているのでこの方法を取ります。\n% git clone https://github.com/opscode/test-kitchen.git % cd test-kitchen % bundle install % rake build # \u0026lt;--- gem をビルド % gen install .","title":"test-kitchen と OpenStack で Chef Cookbooks テスト(前篇)"},{"content":"こんにちは。@jedipunkz です。\nGlusterFS をちょっと前に調べてました。何故かと言うと OpenStack Havana がもうす ぐリリースされるのですが、Havana から GlusterFS がサポートされる予定だからです。\nこの辺りに色々情報が載っています。\nhttp://www.gluster.org/category/openstack/\nその前に GlusterFS を構築出来ないといけないので、今回はその方法を書いていきま す。各クラスタタイプ毎に特徴や構築方法が異なるのでその辺りを重点的に。\n環境  Ubuntu Server 12.04.3 LTS PPA レポジトリ利用 /dev/sdb を OS 領域とは別の disk としてサーバに追加する  用いる PPA レポジトリ Ubuntu 12.04.3 LTS の GlusterFS バージョンは 3.2 です。3.4 系が今回使いたかっ たので下記の PPA レポジトリを利用させてもらいます。ちゃんと構築するなら自分で パッケージを作成することをオススメします。\nhttps://launchpad.net/~semiosis/+archive/ubuntu-glusterfs-3.4\n準備 ここからの手順は全てのサーバで操作します。\nレポジトリの利用方法 % sudo aptitude install python-software-properties % sudo add-apt-repository ppa:semiosis/ubuntu-glusterfs-3.4 % sudo apt-get update GlusterFS3.4 のインストール % sudo apt-get install glusterfs-server gluserfs-client xfsprogs のインストール glusterfs は xfs を扱うため xfsprogs をインストールする。\n% sudo apt-get install xfsprogs ディスクの準備 % sudo mkfs.xfs -i size=512 /dev/sdb % sudo mkdir -p /export/brick1 % sudo vim /etc/fstab /dev/sdb /export/brick1 xfs defaults 1 2 # \u0026lt;- 追記 % sudo mount -a % mount 各クラスタタイプでのマウント方法 ここからの手順はどこか一台のノードで行えば OK です。\ndistributed タイプ まずはデフォルトとなる distributed のマウント方法。\n% sudo gluster peer probe \u0026lt;other_node_ip_addr\u0026gt; ... % sudo gluster volume create gv0 \u0026lt;mine_ip_addr\u0026gt;:/export/brick1/sdb \u0026lt;another_node_ip_addr\u0026gt;:/export/brick1/sdb % sudo gluster volume start gv0 % sudo gluster volume info Volume Name: gv0 Type: Distribute Volume ID: 803fdd46-4735-444a-99c8-83d1cee172e6 Status: Started Number of Bricks: 2 Transport-type: tcp Bricks: Brick1: \u0026lt;mine_ip_addr\u0026gt;:/export/brick1/sdb Brick2: \u0026lt;other_ip_addr\u0026gt;:/export/brick1/sdb マウント行う。どちらのノードに対して行うことが出来る。\n% sudo mount -t glusterfs \u0026lt;mine_ip_addr\u0026gt;:/gv0 /mnt その他のクラスタタイプのマウント方法 replicated の場合\u0026hellip;\n% sudo gluster volume create gv0 replica 2 \u0026lt;mine_ip_addr\u0026gt;:/export/brick1/sdb \\  \u0026lt;other_node_ip_addr\u0026gt;:/export/brick1/sdb striped の場合\u0026hellip;\n% sudo gluster volume create gv0 stripe 2 \u0026lt;mine_ip_addr\u0026gt;:/export/brick1/sdb \\  \u0026lt;other_node_ip_addr\u0026gt;:/export/brick1/sdb distributed replicated の場合\u0026hellip;\n% sudo gluster volume create dist-repl replica 2 \\  \u0026lt;mine_ip_addr\u0026gt;:/export/brick1/sdb \\  \u0026lt;other_ip_addr\u0026gt;:/export/brick1/sdb \\  \u0026lt;other_ip_addr\u0026gt;:/export/brick1/sdb \\  \u0026lt;other_ip_addr\u0026gt;:/export/brick1/sdb striped replicated の場合\u0026hellip;\n% sudo gluster volume create strip-repl stripe 2 replica 2 \\  \u0026lt;mine_ip_addr\u0026gt;:/export/brick1/sdb \\  \u0026lt;other_ip_addr\u0026gt;:/export/brick1/sdb \\  \u0026lt;other_ip_addr\u0026gt;:/export/brick1/sdb \\  \u0026lt;other_ip_addr\u0026gt;:/export/brick1/sdb distributed striped replicated の場合\u0026hellip;\n% sudo gluster volume create dist-strip-repl stripe 2 replica 2 \\  \u0026lt;mine_ip_addr\u0026gt;:/export/brick1/sdb \\  \u0026lt;other_ip_addr\u0026gt;:/export/brick1/sdb \\  \u0026lt;other_ip_addr\u0026gt;:/export/brick1/sdb \\  \u0026lt;other_ip_addr\u0026gt;:/export/brick1/sdb \u0026lt;other_ip_addr\u0026gt;:/export/brick1/sdb \\  \u0026lt;other_ip_addr\u0026gt;:/export/brick1/sdb \\  \u0026lt;other_ip_addr\u0026gt;:/export/brick1/sdb \\  \u0026lt;other_ip_addr\u0026gt;:/export/brick1/sdb 各クラスタタイプの特徴  distributed  1つのオブジェクトを GlusterFS 上に保管するとオブジェクト単位でどれかのノードに 対して保管する。よって \u0026lsquo;ノード上の disk x ノード数\u0026rsquo; が合計容量として扱うことが 出来る。ノード障害の場合にはその該当ノード上の disk に保管されているオブジェク トの読み書きは NG 。その他のノード上 disk に保管されているオブジェクトに対して は読み書きが正常に行える。\n stripe  1つのオブジェクトをブロック単位で分割し各ノードに保管する。扱える合計容量は distributed と同じく \u0026lsquo;ノード上の disk x ノード数\u0026rsquo; となる。障害がどこかのノード で発生した場合、全てのオブジェクトの読み書きが行えなくなる。\n replicated  1つのオブジェクトを 2 台のノードに対してミラーリングを行い保管する。障害系の対 応が可能になる。その分、扱える合計容量は distrubuted/stripe に比べ半分となる。\n distributed replicated  構成イメージ図\u0026hellip;\n+--------+ | client | +--------+ | +---------------------+ | | +----------+ +----------+ | | | | +--------+ +--------+ +--------+ +--------+ | node1 | | node2 | | node3 | | node4 | +--------+ +--------+ +--------+ +--------+ node1, 2 と 3, 4 にて replicated しそれぞれに対して distributed を組む。扱える disk 容量は node の持っている disk 容量 x ノード数 / 2 となる。distributed は 実質、障害系の対応が良くないので distributed を扱うのであれば、この volume type が推奨されるものと思われる。\n striped replicated  distributed replicated と同等の構成だがブロック単位でのオブジェクトの保管とな る。\nまとめ GlusterFS には一貫性の問題 (disk 容量の一貫性を保つ必要がある) と思っていたが、 昔の話しらしい。容量のことなる disk をノードに追加しても、それをうまく合計容量 に合算されるのを確認した。また分散ファイルシステムの美味しいところと冗長性を兼 ねた構成を組むのが良いと思うので distributed replicated もしくは striped replicated を選択するのがオススメ。今回は TCP で扱ったがバージョン 3.4 からは Infiniband と組み合わせて RDMA を扱うことが可能。下記の URL が参考になる。\nhttp://gluster.org/community/documentation/index.php/Gluster_3.2:_Configuring_GlusterFS_to_work_over_InfiniBand\n","permalink":"https://jedipunkz.github.io/post/2013/10/12/glusterfs-install/","summary":"こんにちは。@jedipunkz です。\nGlusterFS をちょっと前に調べてました。何故かと言うと OpenStack Havana がもうす ぐリリースされるのですが、Havana から GlusterFS がサポートされる予定だからです。\nこの辺りに色々情報が載っています。\nhttp://www.gluster.org/category/openstack/\nその前に GlusterFS を構築出来ないといけないので、今回はその方法を書いていきま す。各クラスタタイプ毎に特徴や構築方法が異なるのでその辺りを重点的に。\n環境  Ubuntu Server 12.04.3 LTS PPA レポジトリ利用 /dev/sdb を OS 領域とは別の disk としてサーバに追加する  用いる PPA レポジトリ Ubuntu 12.04.3 LTS の GlusterFS バージョンは 3.2 です。3.4 系が今回使いたかっ たので下記の PPA レポジトリを利用させてもらいます。ちゃんと構築するなら自分で パッケージを作成することをオススメします。\nhttps://launchpad.net/~semiosis/+archive/ubuntu-glusterfs-3.4\n準備 ここからの手順は全てのサーバで操作します。\nレポジトリの利用方法 % sudo aptitude install python-software-properties % sudo add-apt-repository ppa:semiosis/ubuntu-glusterfs-3.4 % sudo apt-get update GlusterFS3.4 のインストール % sudo apt-get install glusterfs-server gluserfs-client xfsprogs のインストール glusterfs は xfs を扱うため xfsprogs をインストールする。","title":"GlusterFS の各クラスタタイプ構築"},{"content":"こんにちは。@jedipunkzです。\nMesos アーキテクチャについて2つめの記事です。\nhttp://jedipunkz.github.io/blog/2013/09/28/mesos-architecture-number-1/\n上記の前回の記事で Mesos 自体のアーキテクチャについて触れましたが、今回は Mesos + Marathon + Docker の構成について理解したことを書いていこうと思います。\nmesos クラスタは 幾つかの mesos masters と沢山の mesos slaves から成っており、 mesos slaves の上では docker を操作する executor が稼働している。marathon は mesos master の上で稼働する mesos framework である。init や upstart の様な存在 であることが言え、REST API を持ち container の動作を制御する。marathon には ruby の client 等も存在する。下記がそれ。\nhttps://github.com/mesosphere/marathon_client\n構成 +-----------------+ | docker registry | index.docker.io (もしくは local registry) +-----------------+ | +----------------+ | | +--------------+ +--------------+ | mesos master | | mesos master | +--------------+ +--------------+ | | |----------------+-----------------------------------| +--------------+ +--------------+ +--------------+ | mesos slave | | mesos slave | ... | mesos slave | +--------------+ +--------------+ +--------------+ | | | +--------------+ +--------------+ +--------------+ | mesos slave | | mesos slave | ... | mesos slave | +--------------+ +--------------+ +--------------+ . . . . . . . . . +--------------+ +--------------+ +--------------+ | mesos slave | | mesos slave | ... | mesos slave | +--------------+ +--------------+ +--------------+ オファから docker が稼働するまでの流れ 上記の構成の図を見ながら理解していきましょう。\n HTTP API もしくは web UI で marathon がリクエストを受ける marathon はリソースリクエストを作成しオファが受け付けられるのを待つ オファが受け付けられた後、mesos master は slave に task の仕様を送信する slave では docker コマンドラインツールを実行する mesos docker を mesos slave デーモンが呼び出す docker コマンドラインツールはローカルの docker デーモンと image cache, lxc ツールにより通信する もし image cache が存在すればそれを、無ければ docker registry から pull する。 その時、index.docker.io の代わりにローカルの docker registry を稼働させることも可能 docker デーモンが container を稼働させる  marathon のクラスタとしての動作 marathon は init や upstart のようなモノだと上記で説明しましたが、図を交えて説明して いきましょう。\nmarathon が \u0026lsquo;serarch service\u0026rsquo; と \u0026lsquo;docker\u0026rsquo; を稼働させている状態だとする。\n+----------+ +----------+ +----------+ | | | | | | | |search| | | |docker| | | |docker| | +----------+ +----------+ +----------+ +----------+ +----------+ +----------+ | | | | | | | |search| | | |docker| | | |docker| | +----------+ +----------+ +----------+ +----------+ +----------+ +----------+ | | | | | | | |search| | | |docker| | | |docker| | +----------+ +----------+ +----------+ サービスの状況によりオファが立て込んでくると下記のように docker をスケールアウ トが発生する。\n+----------+ +----------+ +----------+ | |docker| | | |docker| | | | | |search| | | |docker| | | |docker| | +----------+ +----------+ +----------+ +----------+ +----------+ +----------+ | | | |docker | | |docker| | | |search| | | |docker| | | |docker| | +----------+ +----------+ +----------+ +----------+ +----------+ +----------+ | |docker| | | |docker| | | |docker| | | |search| | | |docker| | | |docker| | +----------+ +----------+ +----------+ システムに異常がありノードが落ちた場合、下記のように marathon は serach service と docker をノード間で移動させる処置を行う。\n +----------+ +----------+ | |docker| | | |search| | | |docker| | | |docker| | +----------+ +----------+ +----------+ +----------+ +----------+ | |docker| | |docker| | | |docker| | | |search| | | |docker| | | |docker| | +----------+ +----------+ +----------+ +----------+ +----------+ +----------+ | |docker| | | |docker| | | |docker| | | |search| | | |docker| | | |docker| | +----------+ +----------+ +----------+ まとめ mesos と docker, marathon の関係について記していきました。今度、実際にこの構成 を組んでみて障害系のテストしてみたいです。あとは framework について理解してい く必要がありそう。あとは chronos についても。chronos については下記の URL が公 式らしい。これは cron 代替な仕組みらしいです。\nhttps://github.com/airbnb/chronos\nまだまだ理解できていないことだらけだ\u0026hellip;。\n","permalink":"https://jedipunkz.github.io/post/2013/10/01/methos-architecture-number-2-docker-on-mesos/","summary":"こんにちは。@jedipunkzです。\nMesos アーキテクチャについて2つめの記事です。\nhttp://jedipunkz.github.io/blog/2013/09/28/mesos-architecture-number-1/\n上記の前回の記事で Mesos 自体のアーキテクチャについて触れましたが、今回は Mesos + Marathon + Docker の構成について理解したことを書いていこうと思います。\nmesos クラスタは 幾つかの mesos masters と沢山の mesos slaves から成っており、 mesos slaves の上では docker を操作する executor が稼働している。marathon は mesos master の上で稼働する mesos framework である。init や upstart の様な存在 であることが言え、REST API を持ち container の動作を制御する。marathon には ruby の client 等も存在する。下記がそれ。\nhttps://github.com/mesosphere/marathon_client\n構成 +-----------------+ | docker registry | index.docker.io (もしくは local registry) +-----------------+ | +----------------+ | | +--------------+ +--------------+ | mesos master | | mesos master | +--------------+ +--------------+ | | |----------------+-----------------------------------| +--------------+ +--------------+ +--------------+ | mesos slave | | mesos slave | .","title":"Methos アーキテクチャ #2 (Docker on Mesos)"},{"content":"こんにちは。@jedipunkz です。\nDevOps Day Tokyo 2013 に参加してきました。たくさんの刺激を受けたのでレポート書 いておきます。\n開催日 : 2013年09月28日 場所 : 東京六本木ミッドタウン Yahoo! Japan さま URL : http://www.devopsdays.org/events/2013-tokyo/  Making Operation Visible Nick Galbreath (@ngalbreath) さん  DevOps の拠点 Etsy に努めた経緯のある DevOps リーダ Galbreath さん。DevOps の 概略から何が必要でありどう行動に起こせばよいか説明してくださいました。\nこちら、Galbreath さんの当日の資料。\nDevOps が実行出来ない理由  Tool が足りない 社風の影響 見えないモノが価値がないと事業から考えられている  出来る事は、価値があるモノの社内への説明と、Tool を使った可視化。データの可視 化が重要。Ops の人は結構「データをどこそこの部署に見せても理解してもらえない」 だとか「データを閲覧させると万が一の時にシステムが破損する」等と考えがち。が、 ビジネス寄りの人にとって重要なグラフが含まれていたり、アカウント担当の人に役立 つものも含まれている。ましてシステムが破損することなど決して無い。\n重要なのは \u0026ldquo;運用のメトリクスを公開する\u0026rdquo; こと！\nGraphite  グラフ描画ツール まず完成度が高いわけではない 同類のソフトウェアでは行えないクエリが発行出来る REST API Flexible Input \u0026amp; Output Simple UI \u0026amp; Dashboard 3rd Party Custom Client Side Dashboard あり URL 型なので Dashboard 開発が楽ちん 稼働させるための物理インフラリソースは結構必要 apt-get install graphite できるよ  statd  UDP 使ってる Event Data を Application から statd へ  下記は例。ログイン情報を送るためのコードはこれだけ。\nStatD::increment('logins')  国別にも出来る。\nStatD::increment('logins'); $kuni = geoip2country($ipv4); StatD::increment('logins.$kuni')  後に DataDog の方から詳しく説明がある。\nSensu Sean Porter さん  メモとりにくいプレゼンだったので思い出しながら\u0026hellip;.\nSensu は Nagios のプラグイン等がそのまま再利用できる監視ツール。API を介してア ラートの具合やクライアントのリスト・監視項目のリスト等が取得できます。Nagios を使った場合、ターゲットノードの追加のたびにコンフィギュレーションを生成しなく てはならなかったり不便だったのが Sensu を開発しようとした動機だとか。Chef との 親和性が高く、Chef Cookbook の公開もしている。私も実際に使っていますが、\n Sensu サーバの構築 監視項目の追加 ターゲットノードへのアージェントの追加  等はすべて Chef の Knife コマンドで出来ます。特に Chef からの影響だと思われる が Omnibus 形式のパッケージを採用していることもあり、エージェントのインストー ルは簡単です。パッケージの中に動作に必要な Ruby 一式も含まれています。\nドキュメントは下記のところにあります。\nhttp://docs.sensuapp.org/0.10/index.html\npuppet max martin  for next level +++\n Puppet 3.x Hiera integration PuppetDB Mcollective 2.x Geppetto : IDE Puppet Forge : web site  Puppet が提供するソフトウェアやサービス達。DevOps の次のステップへの必要な技術 要素。\npuppet 3.x  pupput2 に比較して speedup x 3 agents/server : 100% up これらは理論値ではなく実効値！  Puppet2 と比較してかなりの性能アップらしい。処理速度3倍は実効値だそうです。\npuppet3.0 = hiera functions + data bindings  hiera : hierarchical key value store with pluggable backend (json, yaml..) keep site specific data out of puppet code parameter values are now automatically looked up in hiera data bind でコードがシンプルに : include ntp -\u0026gt; yaml, json..  json や yaml でパラメータを記述出来るのでコードがシンプルで綺麗になるよ、との こと。Chef でいう Attributes だと思われる。\npuppetdb  puppet データすべてを格納 (facts, catalogs, reports,..) replaces existing library is much faster and more faster postgreSQL, Clojure, JVM -\u0026gt; JAR ファイルで展開 クエリシンタックスを自ら定義可能 API がシンプルなので dashboard などの開発も簡単  PostgreSQL が最近あちらの国ではイケてるっていう話はよく聞いていたけど Puppet3 は PostgreSQL, Clojur, JVM の組み合わせで構成されているらしい。Chef も Erlang に置き換わったあたりを見ての反応かなぁ？と想像。\nMCollective  orchestration engine ruby を使って独自のエージェント開発可能 mco rpc service restart service=httpd \u0026ndash;nodes=hosts.txt # query a file mco rpc service restart service=httpd -W country=uk -dm=puppetdb # discover mco rpc rpcutil ping -I example.com # direct addressing ruby のライブラリとして利用可能  query a file のサンプル\nc = rpcclient(\u0026quot;service\u0026quot;) c.discover :nodes =\u0026gt; File.readline(\u0026quot;host.txt\u0026quot;).map {|i| i.chomp} printrpc c.restart(:service =\u0026gt; \u0026quot;httpd\u0026quot;)  これはオーケストレーションツール。ホスト名の記述されたファイルから、それらのホ ストに対して一斉に指示を送ったり検索を行ったりコマンドの実行が出来る。これは Ruby ライブラリとして再利用出来るらしい。これは便利。\nGeppetto  IDE for developing puppet modules and code git \u0026amp; svn Linux, Mac OSX, Windows  Linux, Mac, Windows 用の IDE まで提供してるんですね。驚き。Puppet ものすごい勢 い。\nPuppet Forge  module repository 1,500+ modules puppet module install foo/foo module のためのチームが内部にある  コミュニティの作ったモジュールを公開しているレポジトリ WEB サービス。1,500 以 上のモジュールが公開されているらしい。このためのチームも Puppet 社内にいるとか。\n迷ったら健全な方を Cookpad Naruta Issei さん  印象的なプレゼンでした。\n現実に起こった問題を切り口に Devops について語っていました。リリース日当日 Ops が「今日リリースだと初めて知る」という事件。繰り返さないためにどうするか？\n リリース日の決定に Ops の承認が必要なルールにする？ Ops 「ソースコードが fix してからリリース日まで3営業日必要？」  このようなルールを作りがちだけど、これでは Ops が権威になってしまう。Ops の立 場からしてこの「権威」になりやすいという特徴があるが、Ops はそう振る舞ってはい けない！という話。DevOps に必要なコミュニケーションで回避出来ると。DevOps であ る限り仕事は楽しくなくてはならない。承認を取るテクニック・政治が発生するのもこ れまた問題。\nCookpad はこれからもコミュニケーションでこういった問題をクリアしていく！と宣言 がありました。最後は若干 Ops 避難のようなプレゼンを Ops の前でするかのような状 況に Issei さんも声を震わせながらのプレゼンでしたが、逆にそれが僕には響きまし た。ぼくも Ops 出身だし、こういった問題は嫌になる程見てきているので、この訴え かけはガツンときた。プレゼン最高でした！\nDataDog Alexis Lê-Quôc さん  監視ツールにとっての重要な要素\n Timely Correct Comprehensive (包括的)  これらを満たせなければ全く無駄。\n 前提条件を元にメトリクスを決めている -\u0026gt; 不完全なメトリクスが多い -\u0026gt; 最低限の条件でメトリクスに要素を追加できることが重要だと考える  statd types  gauges counters histgrams timers sets  参考資料 statd と graphite の連携\nhttps://github.com/etsy/statsd/blob/master/docs/graphite.md\nサポートされているバックエンド一覧\nhttps://github.com/etsy/statsd/blob/master/docs/backend.md\nまとめ とても有意義だった。丸一日、最後まで聞いているのも辛かったけど、運営されている 側の方々はもっと大変だったでしょう。ありがとうございました。日本ではなかなか話 を聞けないめちゃホットな方々の話を直に聞けるなんて無いのでホント貴重でした。ま たディスカッションには参加出来なかったけど、久しぶりに会えた方がいてめちゃ良かっ たです。同じ方向向いている方々がこれだけ社外には居るんだという気付きもあったり。\n皆さん、ありがとうございました！\n","permalink":"https://jedipunkz.github.io/post/2013/09/29/devops-day-tokyo-2013-report/","summary":"こんにちは。@jedipunkz です。\nDevOps Day Tokyo 2013 に参加してきました。たくさんの刺激を受けたのでレポート書 いておきます。\n開催日 : 2013年09月28日 場所 : 東京六本木ミッドタウン Yahoo! Japan さま URL : http://www.devopsdays.org/events/2013-tokyo/  Making Operation Visible Nick Galbreath (@ngalbreath) さん  DevOps の拠点 Etsy に努めた経緯のある DevOps リーダ Galbreath さん。DevOps の 概略から何が必要でありどう行動に起こせばよいか説明してくださいました。\nこちら、Galbreath さんの当日の資料。\nDevOps が実行出来ない理由  Tool が足りない 社風の影響 見えないモノが価値がないと事業から考えられている  出来る事は、価値があるモノの社内への説明と、Tool を使った可視化。データの可視 化が重要。Ops の人は結構「データをどこそこの部署に見せても理解してもらえない」 だとか「データを閲覧させると万が一の時にシステムが破損する」等と考えがち。が、 ビジネス寄りの人にとって重要なグラフが含まれていたり、アカウント担当の人に役立 つものも含まれている。ましてシステムが破損することなど決して無い。\n重要なのは \u0026ldquo;運用のメトリクスを公開する\u0026rdquo; こと！\nGraphite  グラフ描画ツール まず完成度が高いわけではない 同類のソフトウェアでは行えないクエリが発行出来る REST API Flexible Input \u0026amp; Output Simple UI \u0026amp; Dashboard 3rd Party Custom Client Side Dashboard あり URL 型なので Dashboard 開発が楽ちん 稼働させるための物理インフラリソースは結構必要 apt-get install graphite できるよ  statd  UDP 使ってる Event Data を Application から statd へ  下記は例。ログイン情報を送るためのコードはこれだけ。","title":"DevOps Day Tokyo 2013 参加レポート"},{"content":"こんにちは。@jedipunkzです。\n今回はクラスタマネージャである Mesos について書こうと思います。\nMesos は Apache Software Foundation によって管理されるソフトウェアで分散アプリ ケーションをクラスタ化することが出来るマネージャです。Twitter が採用しているこ とで有名だそうで、開発にも積極的に参加しているそうです。\nhttp://mesos.apache.org/\n@riywo さんが既に Mesos + Marathon + Zookeper + Docker な構成を組む手順をブロ グで紹介されていますので是非試してみると面白いと思います。\nhttp://tech.riywo.com/blog/2013/09/27/mesos-introduction-1/\n私は理解した Mesos のアーキテクチャについて少し書いていきたいと思います。\n全体の構造 +-----------+ | zookeeper | | quorum | +-----------+ | +----------------+----------------+ | | | +--------------+ +--------------+ +--------------+ | mesos-master | | mesos-master | | mesos-master | | active | | hot standby | | hot standby | +--------------+ +--------------+ +--------------+ ... | +----------------+----------------+ | | | +--------------+ +--------------+ +--------------+ | mesos-slave | | mesos-slave | | mesos-slave | | executor | | executor | | executor | | +----++----+ | | +----++----+ | | +----++----+ | | |task||task| | | |task||task| | | |task||task| | | +----++----+ | | +----++----+ | | +----++----+ | +--------------+ +--------------+ +--------------+ ... 基本的に few masters + many slaves の構成です。task は slaves の上で走ります。 master はオファーによりアプリケーション(フレームワーク)に対して CPU, メモリの リソースをシェア出来ます。リソースは slave ID, resource1: amount1, resource2, amount2, \u0026hellip; といった配列を含みます。master はポリシに従いそれぞれ のフレームワークに対してリソースをどれだけ提供するか決定します。プラグイン形式 で様々なポリシを取り込む仕組みになっています。\nmesos は zookeeper により HA を組むことが出来ます。1つの active master と複数 (もしくは1つの) stand-by master(s) の構成です。active を投票するため zookeeper のアルゴリズムが用いられます。\nmesos-master を起動する際に下記のパラメータを渡すとmulti masters の構成が組め ます。\n--zk=zk://host1:port1/path,host2:port2/path,... それに対して multi slaves に対しては下記のパラータを渡します。\n--master=zk://host1:port1/path,host2:port2/path,... ネットワークの分断 ネットワークの分断により様々な事象が発生します。幾つかのパターンによる動きにつ いて見ていきます。\n slave が master と分断された場合  ヘルスチェックが失敗。master は slave のことを \u0026lsquo;deactivated\u0026rsquo; と判断しその slave に分配した task を LOST とします。\n master に再接続出来ない場合  \u0026lsquo;deactivated\u0026rsquo; となった slave が master に再接続出来ない場合、シャットダウンの 指示が送られる\n slave が zookeeper と分断された場合  master がいない状態となり、再び master の投票が行われ active master が現れるま で master からの指示を拒否する\nリソースのオファ +-------------------+ | framework | | +----+----+ | | |job1|job2| | | +----+----+ | | scheduler | +-------------------+ ~(2) |(3) | ~ +-------------------+ | mesos-master | | allocation module | +-------------------+ ~(1) |(4) | ~ +-------------------+ | mesos-slave | | executor | | +----+----+ | | |task|task| | | +----+----+ | +-------------------+ リソースのオファの流れについて。\n (1) mesos-slave が空きの CPU, Mem の状況(4cpus,4g mem)を mesos-mater に伝える。 (2) mesos-master はそのリソース空き状況を framework に対して伝える (3) framework の scheduler は mesos-mater に対して mesos-slave の上で動かすべ き2個のタスクについて伝える。1個めは 2cpus, 1g mem。2個めは 1cpus, 2g memと。 (4) master は task を slave に対して伝える。結果 1cpu, 1gb mem がアロケートさ れない状況だが、それについては別の framework に対してアロケートすることとな る。  まとめ このクラスタ自体が1つの OS かのような動きを取ることが見て取れます。Mesos の上 で Hadoop, Spark などを稼働させることが出来るそうですが、次回、私は docker を動 かしてみたいと思っています。\nhttp://mesosphere.io/2013/09/26/docker-on-mesos/\nここが参考になります。\n駆け足でしたが、以上です。\n","permalink":"https://jedipunkz.github.io/post/2013/09/28/mesos-architecture-number-1/","summary":"こんにちは。@jedipunkzです。\n今回はクラスタマネージャである Mesos について書こうと思います。\nMesos は Apache Software Foundation によって管理されるソフトウェアで分散アプリ ケーションをクラスタ化することが出来るマネージャです。Twitter が採用しているこ とで有名だそうで、開発にも積極的に参加しているそうです。\nhttp://mesos.apache.org/\n@riywo さんが既に Mesos + Marathon + Zookeper + Docker な構成を組む手順をブロ グで紹介されていますので是非試してみると面白いと思います。\nhttp://tech.riywo.com/blog/2013/09/27/mesos-introduction-1/\n私は理解した Mesos のアーキテクチャについて少し書いていきたいと思います。\n全体の構造 +-----------+ | zookeeper | | quorum | +-----------+ | +----------------+----------------+ | | | +--------------+ +--------------+ +--------------+ | mesos-master | | mesos-master | | mesos-master | | active | | hot standby | | hot standby | +--------------+ +--------------+ +--------------+ ... | +----------------+----------------+ | | | +--------------+ +--------------+ +--------------+ | mesos-slave | | mesos-slave | | mesos-slave | | executor | | executor | | executor | | +----++----+ | | +----++----+ | | +----++----+ | | |task||task| | | |task||task| | | |task||task| | | +----++----+ | | +----++----+ | | +----++----+ | +--------------+ +--------------+ +--------------+ .","title":"Mesos アーキテクチャ #1"},{"content":"こんにちは。@jedipunkzです。\n前回 kibana + elasticsearch + fluentd を構築する方法を載せたのだけど手動で構築 したので格好悪いなぁと思っていました。いうことで！ Chef でデプロイする方法を調 べてみました。\n意外と簡単に出来たし、スッキリした形でデプロイ出来たのでオススメです。\n前提の環境は\u0026hellip;  Ubuntu 12.04 LTS precise Chef サーバ構成 入力するログは nginx (例) オールインワン構成  Cookbook が他の OS に対応しているか確認していないので Ubuntu を前提にしていま す。Chef サーバのデプロイや knife の設定は済んでいるものとして説明していきます。 例で nginx のログを入力します。なので nginx も Chef でデプロイします。ここは他 のものに置き換えてもらっても大丈夫です。手順を省略化するためオールインワン構成 で説明します。nginx, fluentd は複数のノードに配置することも手順を読み替えれば もちろん可能です。\nCookbook の準備 お決まり。Cookbooks の取得に Berkshelf を用いる。\n% cd chef-repo % gem install berkshelf % ${EDITOR} Berksfile cookbook \u0026#39;elasticsearch\u0026#39;, git: \u0026#39;https://github.com/elasticsearch/cookbook-elasticsearch.git\u0026#39; cookbook \u0026#39;td-agent\u0026#39;, git: \u0026#39;https://github.com/treasure-data/chef-td-agent.git\u0026#39; cookbook \u0026#39;kibana\u0026#39;, git: \u0026#39;https://github.com/realityforge/chef-kibana.git\u0026#39; cookbook \u0026#39;nginx\u0026#39;, git: \u0026#39;https://github.com/opscode-cookbooks/nginx.git\u0026#39; Cookbooks を取得します。\n% berks install --path=./cookbooks elasticsearch デプロイ準備 elasticsearch の Role を作成する。java, elasticsearch レシピを指定。また Java に openjdk7 を override_attributes にて指定する。openjdk6 だと elasticsearch が起動し なかった。尚、デフォルトは 6 です。Environment で override_attributes を指定し てもらっても構わないです。\n% ${EDITOR} roles/elasticsearch.rb name \u0026#34;elasticsearch\u0026#34; description \u0026#34;Base role applied to elasticsearch nodes.\u0026#34; run_list( \u0026#34;recipe[java]\u0026#34;, \u0026#34;recipe[elasticsearch]\u0026#34; ) override_attributes \u0026#34;java\u0026#34; =\u0026gt; { \u0026#34;install_flavor\u0026#34; =\u0026gt; \u0026#34;openjdk\u0026#34;, \u0026#34;jdk_version\u0026#34; =\u0026gt; \u0026#34;7\u0026#34; } fluentd デプロイの準備 td-agent (fluentd) の Role を作成する。override_attributes にて td-agent のプ ラグインを指定。配列で複数指定可能。こちらも同じく Role で override_attributes を指定したが Environments で指定しても構わないです。あと公式の README には attributes に直接 plugins を指定するよう書いてありましたが、構成毎に plugins は変えたいと思うので、この様に Roles, Environments で指定するのが良いと思います。\n% ${EDITOR}  roles/td-agent.rb name \u0026#34;td-agent\u0026#34; description \u0026#34;Base role applied to td-agent nodes.\u0026#34; run_list( \u0026#34;recipe[td-agent]\u0026#34; ) override_attributes \u0026#34;td_agent\u0026#34; =\u0026gt; { \u0026#34;plugins\u0026#34; =\u0026gt; [ \u0026#34;elasticsearch\u0026#34; ] } td-agent.conf の template を作成。今回は nginx の /var/log/nginx/localhost.access.log を入力する。これは例なので好きなログを指定 可能。\n% ${EDITOR}  cookbooks/td-agent/templates/default/td-agent.conf.erb \u0026lt;source\u0026gt; type tail format nginx path /var/log/nginx/localhost.access.log tag foo01.nginx.access \u0026lt;/source\u0026gt; \u0026lt;match *.nginx.*\u0026gt; index_name adminpack type_name nginx type elasticsearch include_tag_key true tag_key @log_name host localhost port 9200 logstash_format true flush_interval 10s \u0026lt;/match\u0026gt; デプロイ Cookbooks, Roles を Chef サーバにアップロード。\n% knife cookbook upload -o cookbooks -a % knife role from file roles/*.rb knife bootstrap しデプロイ！\n% knife bootstrap \u0026lt;ip_addr\u0026gt; -N \u0026lt;hostname\u0026gt; -r \\  \u0026#39;role[td-agent]\u0026#39;,\u0026#39;role[elasticsearch]\u0026#39;,\u0026#39;recipe[kibana]\u0026#39;,\u0026#39;recipe[nginx]\u0026#39; \\  --sudo -x \u0026lt;username\u0026gt; 下記の URL にブラウザでアクセスし nginx のログを生成。\nhttp://\u0026lt;ip_addr\u0026gt;/ 下記の URL にて Kibana の UI 越しにログ解析結果を確認出来る。\nhttp://\u0026lt;ip_addr\u0026gt;:5601 まとめ fluentd plugins は複数指定可能。環境毎に td-agent.conf を変更したいという要望 があると思うけど fluentd の Recipe を修正する必要がありそう。それとも td-agent.conf 自体でうまくさばけるのかな？調べてない。オールインワン構成で説明 しましたが、elasticsearch + kibana のノードに対して、複数の nginx(なんでも良い)\n td-agent ノードの構成が一般的だと思います。手順を読み替えてやってみてくださ い。その場合 kibana の Cookbook の attributes 中に elasticsearch の IP アドレ ス・ポートを記す箇所があるので Role, Environments で上書きしてあげてください。  default[\u0026#39;kibana\u0026#39;][\u0026#39;elasticsearch\u0026#39;][\u0026#39;hosts\u0026#39;] = [\u0026#39;127.0.0.1\u0026#39;] default[\u0026#39;kibana\u0026#39;][\u0026#39;elasticsearch\u0026#39;][\u0026#39;port\u0026#39;] = 9200 あと td-agent/templates/default/td-agent.conf.erb の中に記した host, port の指 定も elasticsearch のホスト情報に書き換えてあげてください。\nhost localhost port 9200 以上です。意外と簡単に出来ました。スッキリ。\n","permalink":"https://jedipunkz.github.io/post/2013/09/13/chef-kibana-elasticsearch-fluentd/","summary":"こんにちは。@jedipunkzです。\n前回 kibana + elasticsearch + fluentd を構築する方法を載せたのだけど手動で構築 したので格好悪いなぁと思っていました。いうことで！ Chef でデプロイする方法を調 べてみました。\n意外と簡単に出来たし、スッキリした形でデプロイ出来たのでオススメです。\n前提の環境は\u0026hellip;  Ubuntu 12.04 LTS precise Chef サーバ構成 入力するログは nginx (例) オールインワン構成  Cookbook が他の OS に対応しているか確認していないので Ubuntu を前提にしていま す。Chef サーバのデプロイや knife の設定は済んでいるものとして説明していきます。 例で nginx のログを入力します。なので nginx も Chef でデプロイします。ここは他 のものに置き換えてもらっても大丈夫です。手順を省略化するためオールインワン構成 で説明します。nginx, fluentd は複数のノードに配置することも手順を読み替えれば もちろん可能です。\nCookbook の準備 お決まり。Cookbooks の取得に Berkshelf を用いる。\n% cd chef-repo % gem install berkshelf % ${EDITOR} Berksfile cookbook \u0026#39;elasticsearch\u0026#39;, git: \u0026#39;https://github.com/elasticsearch/cookbook-elasticsearch.git\u0026#39; cookbook \u0026#39;td-agent\u0026#39;, git: \u0026#39;https://github.","title":"Chef で kibana + elasticsearch + fluentd デプロイしてみた"},{"content":"こんにちは。@jedipunkz です。\nOpenStack 第14回勉強会 ハッカソンに参加してきました。その時の自分の作業ログを 記しておきます。自分の作業内容は \u0026lsquo;OpenStack + Docker 構築\u0026rsquo; です。\n場所 : CreationLine さま 日時 : 2013年9月8日(土)  当日の atnd。\nhttp://atnd.org/events/42891\n当日発表のあった内容\n Ansible で OpenStack を実際に皆の前でデプロイ！ Yoshiyama さん開発 LogCas お披露目 Havana の機能改善・機能追加内容確認 その他 Horizon の機能についてだったり openstack.jp の運用についてなど  自分が話を聞きながら黙々とやったことは\n OpenStack + Docker 構築  結果\u0026hellip; NG 動かず。時間切れ。公式の wiki の手順がだいぶ変なので手順を修正しながら進めました。\n公式の wiki はこちらにあります。\nhttps://wiki.openstack.org/wiki/Docker\nその修正しながらメモった手順を下記に貼り付けておきます。\n作業環境  ホスト : Ubuntu 12.04.3 Precise OpenStack バージョン : devstack (2013/09/08 master ブランチ) 構成 : オールインワン (with heat, ceilometer, neutron)  普通に動かすとエラーが出力される これは devstack (2013/09/08 時点) での不具合なので直ちに修正されるかも。\nデフォルトのエンコーディングが \u0026lsquo;ascii\u0026rsquo; になっているのが原因らしい.\n% python Python 2.7.3 (default, Apr 10 2013, 06:20:15) [GCC 4.6.3] on linux2 Type \u0026#34;help\u0026#34;, \u0026#34;copyright\u0026#34;, \u0026#34;credits\u0026#34; or \u0026#34;license\u0026#34; for more information. \u0026gt;\u0026gt;\u0026gt; import sys \u0026gt;\u0026gt;\u0026gt; sys.getdefaultencoding() \u0026#39;ascii\u0026#39; \u0026gt;\u0026gt;\u0026gt; エラーの内容は… この状態だと… 下記のようなエラーが nova から出力される。むむむ。\n% nova boot --image \u0026#34;ubuntu:latest\u0026#34; --flavor 1 vm01 ERROR: UnicodeError: \u0026#39;ascii\u0026#39; codec can\u0026#39;t decode byte 0xe5 in position 2: ordinal not in range(128) (HTTP 400) (Request-ID: req-40ebfb9b-d76a-40b8-8f75-facb4dd73db4) とりあえず暫定処置として、下記のように setdefaultencoding(\u0026lsquo;utf-8\u0026rsquo;) を追記。ち なみに devstack デプロイ前にこの作業を済ませました。デプロイ後だと色々めんどい。 当日、何度もめんどい場面に遭遇しました\u0026hellip;\n% ${EDITOR} /usr/lib/python2.7/sitecustomize.py # install the apport exception handler if available try: import apport_python_hook except ImportError: pass else: apport_python_hook.install() # 下記の2行を追記 import sys sys.setdefaultencoding(\u0026#39;utf-8\u0026#39;) Ubuntu12.04 の場合 Linux Kernel 3.8 にアップデート wiki には載っていないが docker が Linux Kernel 3.8 以上を推奨しているため raring から 3.8 を持ってくる。\n% sudo apt-get update % sudo apt-get install linux-image-generic-lts-raring linux-headers-generic-lts-raring % sudo reboot socat インストール 後に実行する install_docker.sh スクリプトが必要とする。実行前に入れないとと痛 い目見る。当日何度も痛い目見ました\u0026hellip;\n% sudo apt-get install socat localrc 追記 localrc に下記を追記。その他のパラメータは各自のモノで良い。\nVIRT_DRIVER=docker install_docker.sh 実行 一般ユーザ権限で OK 。中で sudo している。が、極稀に sudo のタイムアウトが来る ので、色々しんどい。ちなみに僕はここで何度もやりなおしました。\u0026hellip;\n% ./tools/docker/install_docker.sh devstack インストール devstack をデプロイする。\n% ./stack.sh index.docker.io から \u0026lsquo;ubuntu\u0026rsquo; イメージをダウンロード index.docker.io のトップレベルから \u0026lsquo;ubuntu\u0026rsquo; イメージを取得。どのイメージでも良い。\n% sudo docker pull ubuntu Pulling repository ubuntu 8dbd9e392a96: Download complete b750fe79269d: Download complete 27cf78414709: Download complete docker-registry (プライベートレポジトリ) に対して push する -\u0026gt; Glance に登録 この状態で docker-registry.sh というプロセスが起動しているはず。これは docker\nのプライベートレポジトリに相当。5042 tcp で待ち受けているので下記のように tag を打った後、プライベートレポジトリに アップロード。\n% sudo docker tag ubuntu 10.200.9.25:5042/ubuntu % sudo docker push 10.200.9.25:5042/ubuntu The push refers to a repository [10.200.9.25:5042/ubuntu] (len: 1) Sending image list Pushing repository 10.200.9.25:5042/ubuntu (1 tags) Pushing 8dbd9e392a964056420e5d58ca5cc376ef18e2de93b5cc90e868a1bbc8318c1c Pushing tags for rev [8dbd9e392a964056420e5d58ca5cc376ef18e2de93b5cc90e868a1bbc8318c1c] on {http://10.200.9.25:5042/v1/repositories/ubuntu/tags/latest} docker のレポジトリについてはここが参考になる。\nhttp://docs.docker.io/en/latest/use/workingwithrepository/\nglance に登録されていることを確認 \u0026lsquo;ubuntu:latest\u0026rsquo; が表示されるはず。やった。\n% glance image-list +--------------------------------------+---------------------------------+-------------+------------------+----------+--------+ | ID\t| Name\t| Disk Format | Container Format | Size\t| Status | +--------------------------------------+---------------------------------+-------------+------------------+----------+--------+ | f5845be4-1ac0-42c7-9280-a8c316be6beb | cirros-0.3.1-x86_64-uec\t| ami\t| ami\t| 25165824 | active | | aa8bcdff-6eb9-402b-9e27-675648dbe311 | cirros-0.3.1-x86_64-uec-kernel | aki\t| aki\t| 4955792 | active | | f8857736-5613-401a-b28c-02c286271af4 | cirros-0.3.1-x86_64-uec-ramdisk | ari\t| ari\t| 3714968 | active | | ae56cacb-7eb6-413e-bd75-46f3cd63123b | docker-busybox:latest\t| raw\t| docker\t| 2271796 | active | | 613b05c3-30f6-4c53-aa94-103ea516373e | ubuntu:latest\t| raw\t| docker\t| 71497587 | active | +--------------------------------------+---------------------------------+-------------+------------------+----------+--------+ nova boot ぐったり\u0026hellip; nova-scheduler がエラー吐いてるぽいけど、原因つかめず。ハッカソン時間切れ。\n% nova boot --image \u0026#34;ubuntu:latest\u0026#34; --flavor m1.tiny vm01 % nova list +--------------------------------------+------+--------+------------+-------------+----------+ | ID | Name | Status | Task State | Power State | Networks | +--------------------------------------+------+--------+------------+-------------+----------+ | e1563b55-bb5d-43a6-a1fc-c3bc63600ac7 | vm01 | ERROR | None | NOSTATE | | +--------------------------------------+------+--------+------------+-------------+----------+ エラー内容.. 下記のエラーが出力されていた。なんだか OpenStack のメッセージじゃないような。 調べてたら素の Python のメッセージらしく。うーん。devstack なので仕方ない。\n Sep 7 14:53:08 devstack01 2013-09-07 14:53:08.490 ERROR nova.compute.manager [req-d4ff6a45-88bc-4d6e-8f9d-43de79e601c6 demo demo] [instance: fdd0fdfb-aaa2-4e91-98cc-cdb347e9ae09] Instance failed to spawn#0122013-09-07 14:53:08.490 31835 TRACE nova.compute.manager [instance: fdd0fdfb-aaa2-4e91-98cc-cdb347e9ae09] Traceback (most recent call last):#0122013-09-07 14:53:08.490 31835 TRACE nova.compute.manager [instance: fdd0fdfb-aaa2-4e91-98cc-cdb347e9ae09] File \u0026quot;/opt/stack/nova/nova/compute/manager.py\u0026quot;, line 1416, in _spawn#0122013-09-07 14:53:08.490 31835 TRACE nova.compute.manager [instance: fdd0fdfb-aaa2-4e91-98cc-cdb347e9ae09] block_device_info)#0122013-09-07 14:53:08.490 31835 TRACE nova.compute.manager [instance: fdd0fdfb-aaa2-4e91-98cc-cdb347e9ae09] File \u0026quot;/opt/stack/nova/nova/virt/docker/driver.py\u0026quot;, line 305, in spawn#0122013-09-07 14:53:08.490 31835 TRACE nova.compute.manager [instance: fdd0fdfb-aaa2-4e91-98cc-cdb347e9ae09] raise exception.InstanceDeployFailure(msg.format(e),#0122013-09-07 14:53:08.490 31835 TRACE nova.compute.manager [instance: fdd0fdfb-aaa2-4e91-98cc-cdb347e9ae09] File \u0026quot;/opt/stack/nova/nova/openstack/common/gettextutils.py\u0026quot;, line 255, in __getattribute__#0122013-09-07 14:53:08.490 31835 TRACE nova.compute.manager [instance: fdd0fdfb-aaa2-4e91-98cc-cdb347e9ae09] return UserString.UserString.__getattribute__(self, name)#0122013-09-07 14:53:08.490 31835 TRACE nova.compute.manager [instance: fdd0fdfb-aaa2-4e91-98cc-cdb347e9ae09] AttributeError: 'Message' object has no attribute 'format'#0122013-09-07 14:53:08.490 31835 TRACE nova.compute.manager [instance: fdd0fdfb-aaa2-4e91-98cc-cdb347e9ae09] 所感とまとめ 前半の1,2時間、みなさんの話に聞き入ってしまったので時間が…。発表の内容も皆さ んの会話も楽しかった。また参加したい。Netron 周りで聞きたい事とか色々あったの だけどコアデベロッパの方に聞けなかったのが後悔。今度教えてもらおう。あとやっぱ りみなさん詳しい。その情報どこから？ということまでよく知っているし知識が深い。 理解度がまだまだちゃうなぁと実感。次回も参加させていただこうと思います。\n","permalink":"https://jedipunkz.github.io/post/2013/09/09/14th-openstack-study-hackathon/","summary":"こんにちは。@jedipunkz です。\nOpenStack 第14回勉強会 ハッカソンに参加してきました。その時の自分の作業ログを 記しておきます。自分の作業内容は \u0026lsquo;OpenStack + Docker 構築\u0026rsquo; です。\n場所 : CreationLine さま 日時 : 2013年9月8日(土)  当日の atnd。\nhttp://atnd.org/events/42891\n当日発表のあった内容\n Ansible で OpenStack を実際に皆の前でデプロイ！ Yoshiyama さん開発 LogCas お披露目 Havana の機能改善・機能追加内容確認 その他 Horizon の機能についてだったり openstack.jp の運用についてなど  自分が話を聞きながら黙々とやったことは\n OpenStack + Docker 構築  結果\u0026hellip; NG 動かず。時間切れ。公式の wiki の手順がだいぶ変なので手順を修正しながら進めました。\n公式の wiki はこちらにあります。\nhttps://wiki.openstack.org/wiki/Docker\nその修正しながらメモった手順を下記に貼り付けておきます。\n作業環境  ホスト : Ubuntu 12.04.3 Precise OpenStack バージョン : devstack (2013/09/08 master ブランチ) 構成 : オールインワン (with heat, ceilometer, neutron)  普通に動かすとエラーが出力される これは devstack (2013/09/08 時点) での不具合なので直ちに修正されるかも。","title":"第14回 OpenStack 勉強会参加ログ"},{"content":"こんにちは。@jedipunkzです。\n{% img /pix/kibana3.png %}\n前回の記事で Kibana + elasticsearch + fluentd を試しましたが、ツイッターで @nora96o さんに \u0026ldquo;Kibana3 使うと、幸せになれますよ！\u0026rdquo; と教えてもらいました。早 速試してみましたので、メモしておきます。\n前回の記事。\nhttp://jedipunkz.github.io/blog/2013/09/07/kibana-plus-elasticsearch-plus-fluentd/\n前半の手順は前回と同様ですが、念のため書いておきます。\n前提の環境  OS : Ubuntu 12.04 Precise (同じ方法で 13.04 Raring でも出来ました)  必要なパッケージのインストール 下記のパッケージを事前にインストールします。\n% sudo apt-get install git-core build-essential ruby1.9.3 openjdk-7-jdk 手順を省くために Ruby はパッケージで入れました。また Java は他の物を利用しても 構いません。Ruby は fluentd が、Java は elasticsearch が必要とします。\nelasticsearch のインストール 下記のサイトより elasticsearch をダウンロードします。\nhttp://www.elasticsearch.org/download/\n現時点 (2013/09/08) で最新のバージョンは 0.90.3 でした。\n% wget https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-0.90.3.deb % sudo dpkg -i elasticsearch-0.90.3.deb % sudo service elasticsearch start fluentd のインストール fluentd を下記の通りインストールします。\n% sudo -i % curl -L http://toolbelt.treasure-data.com/sh/install-ubuntu-precise.sh | sh /etc/td-agent/td-agent.conf を下記の通りに修正します。サンプルで Apache のログ を elasticsearch に飛ばす設定にします。\n\u0026lt;source\u0026gt; type tail format apache path /var/log/apache2/access.log tag kibana01.apache.access \u0026lt;/source\u0026gt; \u0026lt;match *.apache.*\u0026gt; index_name adminpack type_name apache type elasticsearch include_tag_key true tag_key @log_name host 10.0.1.8 port 9200 logstash_format true flush_interval 3s \u0026lt;/match\u0026gt; host パラメータは環境に合わせて設定します。\nfluentd を起動します。\n% sudo service td-agent start Apache のインストール システムに必要なわけではありませんが、サンプルで Apache のログを fluentd 経由 で飛ばして Kibana で確認したいのでインストールします。\n% sudo apt-get install apache2 % sudo chown -R td-agent /var/log/apache2 ファイルのオーナを td-agent にする必要がありました。\nKibana3 のインストール 下記のサイトから Kibana3 をダウンロードします。\nhttp://three.kibana.org/intro.html\n% wget https://github.com/elasticsearch/kibana/archive/master.tar.gz % tar zxvf master.tar.gz % sudo mv kibana-master /var/www/kibana3\nconfig.js を修正します。elasticsearch の起動している URL を入力します。ここで 注意が必要なのが、ブラウザからアクセス出来る elasticsearch のアドレスにする必 要があるという点です。従来の Kibana の場合は Ruby 製だったため Kibana から elasticsearch に到達できれば良かったのですが、Kibana3 は JavaScript 製ですので ブラウザから elasticsearch の 9200 版ポートに到達出来る必要があります。\n% sudo ${EDITOR} /var/www/kibana3/config.js ...snip... elasticsearch: \u0026#34;http://\u0026lt;ElasticSearch IP addr\u0026gt;:9200\u0026#34;, ...snip... これで完了です。\nブラウザでアクセス あとはブラウザでアクセスするだけです。\nhttp://\u0026lt;kibana IP addr\u0026gt;/kibana3/ まとめ ElasticSearch を外部に公開する必要があるのだけど、その辺りどう制限掛けるかは別 途調べないといけないなぁと。あと ElasticSearch, fluentd を Cookbook でデプロイ するのも出来そう。もちろん Apache も。次回以降の記事に載せます。またブラウザで アクセスした後に様々な形式のグラフ等を追加していけました。もちろん Kibana3 を 使わないでも elasticseach だけで API によるログの検索が出来たりして、僕らエン ジニアにとってはめっちゃくちゃメリットあります。あぁなんでもっと早く使ってなかっ たんだろう。elasticsearch\u0026hellip;。\n","permalink":"https://jedipunkz.github.io/post/2013/09/08/kibana3-plus-elasticsearch-plus-fluentd/","summary":"こんにちは。@jedipunkzです。\n{% img /pix/kibana3.png %}\n前回の記事で Kibana + elasticsearch + fluentd を試しましたが、ツイッターで @nora96o さんに \u0026ldquo;Kibana3 使うと、幸せになれますよ！\u0026rdquo; と教えてもらいました。早 速試してみましたので、メモしておきます。\n前回の記事。\nhttp://jedipunkz.github.io/blog/2013/09/07/kibana-plus-elasticsearch-plus-fluentd/\n前半の手順は前回と同様ですが、念のため書いておきます。\n前提の環境  OS : Ubuntu 12.04 Precise (同じ方法で 13.04 Raring でも出来ました)  必要なパッケージのインストール 下記のパッケージを事前にインストールします。\n% sudo apt-get install git-core build-essential ruby1.9.3 openjdk-7-jdk 手順を省くために Ruby はパッケージで入れました。また Java は他の物を利用しても 構いません。Ruby は fluentd が、Java は elasticsearch が必要とします。\nelasticsearch のインストール 下記のサイトより elasticsearch をダウンロードします。\nhttp://www.elasticsearch.org/download/\n現時点 (2013/09/08) で最新のバージョンは 0.90.3 でした。\n% wget https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-0.90.3.deb % sudo dpkg -i elasticsearch-0.","title":"Kibana3 + elasticsearch + fluentd を試した"},{"content":"こんにちは。@jedipunkzです。\n自動化の流れを検討する中でログ解析も忘れてはいけないということで ElasticSearch を使いたいなぁとぼんやり考えていて Logstash とか Kibana とかいうキーワードも目 に止まるようになってきました。\nElasticSaerch は API で情報を検索出来たりするので自動化にもってこい。バックエ ンドに Logstash を使って\u0026hellip; と思ってたのですが最近よく聞くようになった fluentd をそろそろ真面目に使いたい！ということで、今回は Kibana + ElasticSearch + fluentd の組み合わせでログ解析システムを組む方法をメモしておきます。\n参考にさせて頂いた URL http://memocra.blogspot.jp/2013/04/kibanakibanaelasticsearchfluentd.html\n前提の環境  OS : Ubuntu 12.04 Precise (同じ方法で 13.04 Raring でも出来ました)  必要なパッケージインストール 下記のパッケージを事前にインストールします。\n% sudo apt-get install git-core build-essential ruby1.9.3 openjdk-7-jdk 手順を省くために Ruby はパッケージで入れました。また Java は他の物を利用しても 構いません。Ruby は Kibana, fluentd が、Java は ElasticSearch が必要とします。\nElasticSearch のインストール 下記のサイトより ElasticSearch をダウンロードします。\nhttp://www.elasticsearch.org/download/\n現時点 (2013/09/07) で最新のバージョンは 0.90.3 でした。\n% wget https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-0.90.3.deb % sudo dpkg -i elasticsearch-0.90.3.deb % sudo service elasticsearch start Kibana のインストール Kibana をダウンロードして Gemfile にしたがって必要なモノをインストールします。\n% git clone --branch=kibana-ruby https://github.com/rashidkpc/Kibana.git % sudo gem install bundler % cd Kibana % sudo bundle install % sudo gem install tzinfo-data KibanaConfig.rb の KibanaHost はリスンアドレスになるので 0.0.0.0 に修正sます。\nKibanaHost = \u0026#39;0.0.0.0\u0026#39; Kibana を起動します。\n% ruby kibana.rb == Sinatra/1.4.3 has taken the stage on 5601 for development with backup from Thin \u0026gt;\u0026gt; Thin web server (v1.5.1 codename Straight Razor) \u0026gt;\u0026gt; Maximum connections set to 1024 \u0026gt;\u0026gt; Listening on 0.0.0.0:5601, CTRL+C to stop fluentd のインストール fluentd を下記の通りインストールします。\n% sudo -i % curl -L http://toolbelt.treasure-data.com/sh/install-ubuntu-precise.sh | sh /etc/td-agent/td-agent.conf を下記の通りに修正します。サンプルで Apache のログ を ElasticSearch に飛ばす設定にします。\n\u0026lt;source\u0026gt; type tail format apache path /var/log/apache2/access.log tag kibana01.apache.access \u0026lt;/source\u0026gt; \u0026lt;match *.apache.*\u0026gt; index_name adminpack type_name apache type elasticsearch include_tag_key true tag_key @log_name host 10.0.1.8 port 9200 logstash_format true flush_interval 3s \u0026lt;/match\u0026gt; host パラメータは環境に合わせて設定します。\nfluentd を起動します。\n% sudo service td-agent start Apache のインストール システムに必要なわけではありませんが、サンプルで Apache のログを fluentd 経由 で飛ばして Kibana で確認したいのでインストールします。\n% sudo apt-get install apache2 % sudo chown -R td-agent /var/log/apache2 ファイルのオーナを td-agent にする必要がありました。\nブラウザでアクセス Apache2 のポート 80 にブラウザでアクセスし、その後 ポート 5601 にアクセスする と Kibana の画面が表示されます。ポート 80 のアクセスに応じて結果が出力されます。\n{% img /pix/kibana-cap.png %}\nまとめ LogStash でも今度試してみたい。 あと API をどう叩くかは..\nhttp://www.elasticsearch.org/guide/reference/api/\nにリファレンスがあるので、時間を見つけてやってみる。Apache 以外のログ転送につ いては fluentd を詳細に知る必要があるので、そちらも時間を見つけてやってみる。 kibana.rb を起動するもっと良い方法がないかも調べないと。\n2013.09.08追記\n@nora96o さんに「Kibana3 使うと幸せになれますよ！」と教えて頂いて早速 Kibana3 も試してみました。\nhttp://jedipunkz.github.io/blog/2013/09/08/kibana3-plus-elasticsearch-plus-fluentd/\n","permalink":"https://jedipunkz.github.io/post/2013/09/07/kibana-plus-elasticsearch-plus-fluentd/","summary":"こんにちは。@jedipunkzです。\n自動化の流れを検討する中でログ解析も忘れてはいけないということで ElasticSearch を使いたいなぁとぼんやり考えていて Logstash とか Kibana とかいうキーワードも目 に止まるようになってきました。\nElasticSaerch は API で情報を検索出来たりするので自動化にもってこい。バックエ ンドに Logstash を使って\u0026hellip; と思ってたのですが最近よく聞くようになった fluentd をそろそろ真面目に使いたい！ということで、今回は Kibana + ElasticSearch + fluentd の組み合わせでログ解析システムを組む方法をメモしておきます。\n参考にさせて頂いた URL http://memocra.blogspot.jp/2013/04/kibanakibanaelasticsearchfluentd.html\n前提の環境  OS : Ubuntu 12.04 Precise (同じ方法で 13.04 Raring でも出来ました)  必要なパッケージインストール 下記のパッケージを事前にインストールします。\n% sudo apt-get install git-core build-essential ruby1.9.3 openjdk-7-jdk 手順を省くために Ruby はパッケージで入れました。また Java は他の物を利用しても 構いません。Ruby は Kibana, fluentd が、Java は ElasticSearch が必要とします。\nElasticSearch のインストール 下記のサイトより ElasticSearch をダウンロードします。\nhttp://www.elasticsearch.org/download/\n現時点 (2013/09/07) で最新のバージョンは 0.","title":"Kibana + ElasticSearch + fluentd を試してみた"},{"content":"こんにちは。@jedipunkzです。\n自宅の IPv6 化、したいなぁとぼんやり考えていたのですが、Hurricane Electric Internet Services を見つけました。IPv4 の固定グローバル IP を持っていれば誰で も IPv6 のトンネルサービスを無料で受けられるサービスです。\n1つのユーザで5アカウントまで取得でき (5 エンドポイント)、1アカウントで /64 の アドレスがもらえます。また申請さえすれば (クリックするだけ) /48 も1アカウント 毎にもらえます。つまり /48 x 5 + /64 x 5 \u0026hellip; でか！\n私の宅内は Vyatta で PPPOE してるのですが、各種 OS (Debian, NetBSD\u0026hellip;) や機器 (Cisco, JunOS..)のコンフィギュレーションを自動生成してくれるので、接続するだけ であればそれをターミナルに貼り付けるだけ！です。\nサービス URL Hurricane Electric は下記の URL です。アカウントもここで作成出来ます。\nhttp://tunnelbroker.net\nIPv6 接続性を確保する方法 Vyatta が IPv6 のアドレスを持ち接続性を確保するだけであれば、上に記したように コピペで出来ます。上記の URL でアカウントを作成しログインします。左メニューの \u0026ldquo;Create Regular Tunnel\u0026rdquo; を押して、自分の情報 (IPv4 のエンドポイントアドレス等) を入力します。その後、取得した IPv6 のレンジのリンクをクリックし上記メニュー \u0026ldquo;Example Configuration\u0026rdquo; を選択します。プルダウンメニューが現れるので、自宅の OS や機器に合った名前を選択します。\nすると下記のようなコマンドがテキストエリアに出力されるのでこれをコピペする\u0026hellip;だ けです。\nconfigure edit interfaces tunnel tun0 set encapsulation sit set local-ip xxx.xxx.xxx.xxx set remote-ip xxx.xxx.xxx.xxx set address 2001:470:xx:xxx:2/64 set description \u0026#34;HE.NET IPv6 Tunnel\u0026#34; exit set protocols static interface-route6 ::/0 next-hop-interface tun0 commit コピペしたら接続できたか確認しましょう。\n% ping6 ipv6.google.com -c 3 PING ipv6.google.com(2404:6800:4004:803::1010) 56 data bytes 64 bytes from 2404:6800:4004:803::1010: icmp_seq=1 ttl=59 time=32.6 ms 64 bytes from 2404:6800:4004:803::1010: icmp_seq=2 ttl=59 time=27.4 ms 64 bytes from 2404:6800:4004:803::1010: icmp_seq=3 ttl=59 time=20.5 ms --- ipv6.google.com ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 2002ms rtt min/avg/max/mdev = 20.542/26.901/32.677/4.971 ms Vyatta 配下の宅内 LAN の端末を IPv6 化する ここまでの操作だと Vyatta は IPv6 トンネル接続出来ましたが、その配下の機器はま だ接続できていません。そこで IPv6 RA (Router Advertise) します。\nconfigure edit interfaces ethernet eth0 set ipv6 router-advert send-advert true set ipv6 router-advert cur-hop-limit 64 set ipv6 router-advert max-interval 10 set ipv6 router-advert other-config-flag true set ipv6 router-advert default-preference high set ipv6 router-advert managed-flag true set ipv6 router-advert prefix 2001:470:xx:xxx::/64 set ipv6 router-advert prefix 2001:470:xx:xxx::/64 autonomous-flag true commit ここで RA のみの起動をしていますが、LAN 内に Windows 端末がある場合、IPv6 ネー ムサーバの情報が取得できないため DHCPv6 サーバを別途起動する必要があります。が、 私の LAN 内には Mac, Linux, *BSD しかありませんでしたので起動しませんでした。 必要に応じて起動してください。\nIPv6 ファイアーウォール設定 これで端末も IPv6 トンネル接続できるようになった！のですが、外部から端末に直接 接続が出来てしまいます。ファイアーウォールを設定しましょう。\nconfigure set firewall ipv6-name tun-in description \u0026#34;IPv6 Traffice to Internal\u0026#34; set firewall ipv6-name tun-in default-action drop set firewall ipv6-name tun-in rule 10 action accept set firewall ipv6-name tun-in rule 10 description \u0026#34;Accept Established-Related\u0026#34; set firewall ipv6-name tun-in rule 10 state established enable set firewall ipv6-name tun-in rule 10 state related enable set firewall ipv6-name tun-local default-action drop set firewall ipv6-name tun-local description \u0026#34;IPv6 Traffic to Router\u0026#34; set firewall ipv6-name tun-local rule 10 action accept set firewall ipv6-name tun-local rule 10 description \u0026#34;Accept Established-Related\u0026#34; set firewall ipv6-name tun-local rule 10 state established enable set firewall ipv6-name tun-local rule 10 state related enable set interface tunnel tun0 firewall in ipv6-name tun-in set interface tunnel tun0 firewall local ipv6-name tun-local commit save まとめ ルータ配下のサービスを IPv6 で外部に公開したい場合は tun-in のファイアーウォールに ルールを追加すれば OK です。\nまた自動生成出来るサンプルコンフィギュレーションに対応したOS, 機器の種類が豊富です。 pfSense や Solaris, Windows, ScreenOS, Fortigate 等など、多岐にわたっているの で、大抵の方は問題ないのではないでしょうか。\n","permalink":"https://jedipunkz.github.io/post/2013/09/01/hurricane-electric-vyatta-ipv6/","summary":"こんにちは。@jedipunkzです。\n自宅の IPv6 化、したいなぁとぼんやり考えていたのですが、Hurricane Electric Internet Services を見つけました。IPv4 の固定グローバル IP を持っていれば誰で も IPv6 のトンネルサービスを無料で受けられるサービスです。\n1つのユーザで5アカウントまで取得でき (5 エンドポイント)、1アカウントで /64 の アドレスがもらえます。また申請さえすれば (クリックするだけ) /48 も1アカウント 毎にもらえます。つまり /48 x 5 + /64 x 5 \u0026hellip; でか！\n私の宅内は Vyatta で PPPOE してるのですが、各種 OS (Debian, NetBSD\u0026hellip;) や機器 (Cisco, JunOS..)のコンフィギュレーションを自動生成してくれるので、接続するだけ であればそれをターミナルに貼り付けるだけ！です。\nサービス URL Hurricane Electric は下記の URL です。アカウントもここで作成出来ます。\nhttp://tunnelbroker.net\nIPv6 接続性を確保する方法 Vyatta が IPv6 のアドレスを持ち接続性を確保するだけであれば、上に記したように コピペで出来ます。上記の URL でアカウントを作成しログインします。左メニューの \u0026ldquo;Create Regular Tunnel\u0026rdquo; を押して、自分の情報 (IPv4 のエンドポイントアドレス等) を入力します。その後、取得した IPv6 のレンジのリンクをクリックし上記メニュー \u0026ldquo;Example Configuration\u0026rdquo; を選択します。プルダウンメニューが現れるので、自宅の OS や機器に合った名前を選択します。","title":"Hurricane Electric + Vyatta で宅内 IPv6 化"},{"content":"こんにちは。@jedipunkzです。\n以前、こんな記事をブログに記しました。2012/06 の記事です。\nhttp://jedipunkz.github.io/blog/2012/06/13/vyatta-vpn/\nその後、PPTP で保護されたネットワークの VPN パスワードを奪取出来るツールが公開 されました。2012/07 のことです。よって今では VPN に PPTP を用いることが推奨さ れていません。\nということで L2TP over IPsec による VPN 構築を Vyatta で行う方法を記します。\nfig.1 : home lan と vyatta のアドレス  +--------+ +-----+ home lan ---| vyatta | --- the internet --- | CPE | +--------+ +-----+ X.X.X.X/X(NAT) pppoe0 Y.Y.Y.Y この様に X.X.X.X/X と Y.Y.Y.Y/Y が関係しているとします。CPE は VPN により X.X.X.X/X に接続することが出来ます。\n手順 : IPsec 下記の操作で IPsec を待ち受けるインターフェースの設定します。\n% configure # edit vpn ipsec # set ipsec-interface interface pppoe0 インターフェース名は環境に合わせて設定してください。私の環境では pppoe0 です。\nIPsec パケットが NAT を超えるように設定します。\n# set nat-traversal enable CPE がどの環境にいるか、IPsec 接続を許可するネットワークアドレスを入力します。 渡しの場合はどこからでも接続できるよう 0.0.0.0/0 を入力しました。\n# set nat-networks allowed-network 0.0.0.0/0 # exit 手順 : L2TP 入力を省くために edit します。\n# edit vpn l2tp remote-access IPsec 認証モードに pre-shared-secret を選択し pre-shared-secret (パスワード) を設定します。このパスワードは接続するユーザ全員が知るものです。また認証モード を local に設定します。\n# set ipsec-settings authentication mode pre-shared-secret # set ipsec-settings authentication pre-shared-secret \u0026lt;パスワード\u0026gt; # set authentication mode local 接続ユーザを作成します。パスワードはユーザに合わせたパスワードを入力します。先 ほど設定した pre-shared-secret とは別のパスワードを設定するべきです。\n# set authentication local-users username \u0026lt;ユーザ名\u0026gt; password \u0026lt;パスワード\u0026gt; 接続するネットワーク情報を記します。pool のアドレス範囲は環境に合わせて設定し てください。渡しの場合は第4オクテット 100 - 120 を設定しました。\n# set outside-address Y.Y.Y.Y # set client-ip-pool start X.X.X.100 # set client-ip-pool stop X.X.X.120 最後に commit \u0026amp; save を忘れずに。\n# commit # save まとめ iPhone で接続を確認しました。iOS の場合は L2TP, PPTP, IPsec と3つのタブがあり、 どれかを選択するようになっていますが、L2TP over IPsec の場合には L2TP タブの項 目に情報を入力すれば OK です。\n","permalink":"https://jedipunkz.github.io/post/2013/08/24/vyatta-l2tp-ipsec-vpn/","summary":"こんにちは。@jedipunkzです。\n以前、こんな記事をブログに記しました。2012/06 の記事です。\nhttp://jedipunkz.github.io/blog/2012/06/13/vyatta-vpn/\nその後、PPTP で保護されたネットワークの VPN パスワードを奪取出来るツールが公開 されました。2012/07 のことです。よって今では VPN に PPTP を用いることが推奨さ れていません。\nということで L2TP over IPsec による VPN 構築を Vyatta で行う方法を記します。\nfig.1 : home lan と vyatta のアドレス  +--------+ +-----+ home lan ---| vyatta | --- the internet --- | CPE | +--------+ +-----+ X.X.X.X/X(NAT) pppoe0 Y.Y.Y.Y この様に X.X.X.X/X と Y.Y.Y.Y/Y が関係しているとします。CPE は VPN により X.X.X.X/X に接続することが出来ます。\n手順 : IPsec 下記の操作で IPsec を待ち受けるインターフェースの設定します。\n% configure # edit vpn ipsec # set ipsec-interface interface pppoe0 インターフェース名は環境に合わせて設定してください。私の環境では pppoe0 です。","title":"Vyatta で L2TP over IPsec による VPN 構築"},{"content":"こんにちは。@jedipunkzです\n今更なのかもしれませんが、OpenStack の nova-network を IPv6 対応する方法を調べ てみました。何故 nova-network なのか? 自宅の構成が nova-network だからです..。 以前は Quantum (現 Neutron) 構成で使っていましたが、ノードをコントローラとコン ピュートに別けた時に NIC が足らなくなり\u0026hellip;。\nさて本題です。下記のサイトを参考にしました。ほぼそのままの手順ですが、自分のた めにもメモです。\n参考 URL http://docs.openstack.org/grizzly/openstack-compute/admin/content/configuring-compute-to-use-ipv6-addresses.html\n前提  OpenStack の構成は予め構築されている nova-network を用いている 構成はオールインワンでも複数台構成でも可能  手順 nova がインストールされているすべてのノードで python-netaddr をインストールし ます。私の場合は rcbops の chef cookbooks で構築したのですが、既にインストール されていました。\n% sudo apt-get install python-netaddr nova-network が稼働しているノードで radvd をインストールします。これは IPv6 を Advertise しているルータ等が予め備わっている環境であっても、インストー ルする必要があります。また /etc/radvd.conf が初め無いので radvd 単体では稼働し ませんが、問題ありません。OpenStack の場合 /var/lib/nova 配下のコンフィギュレー ションファイルを読み込んでくれます。\n% sudo apt-get install radvd /etc/sysctl.conf に下記の記述を追記します。RA の受信とフォワーディングを許可し ています。\n% ${EDITOR} /etc/sysctl.conf net.ipv6.conf.default.forwarding=1 net.ipv6.conf.all.forwarding = 1 net.ipv6.conf.all.accept_ra = 1 % sudo sysctl -p nova がインストールされている全てのノードで /etc/nova.conf に下記の行を追記し ます。\n% sudo ${EDITOR} /etc/nova/nova.conf use_ipv6=true # \u0026lt;- 追記 nova プロセスを再起動します。\n% cd /etc/init.d/; for i in $( ls nova-* ); do sudo service $i restart; done 次に IPv6 のレンジの仮想ネットワークを nova-manage コマンドで生成します。環境にあったレンジを 追加してください。\n% sudo nova-manage network create public --fixed_range_v4 x.x.x.x/24 \\  --fixed_range_v6 xxxx:xxx:xx:xxx::/64 --bridge=br300 --bridge_interface=eth0 \\  --dns1=8.8.8.8 --dns2=8.8.4.4 --multi_host=T ここで IPv4 のレンジも追加しなくてはならないようです。IPv6 オンリーで生成したところ、 nova-network が下記のエラーを吐いてハングアップしました。\nTRACE nova Stderr: 'Error: an inet prefix is expected rather than \u0026quot;None\u0026quot;.\\n' また仮想ネットワーク生成時に指定した物理ネットワークインターフェース名は IPv6 が通信出来る セグメントのものを利用してください。(環境に合わせてください)\nあとは nova boot で仮想マシンを生成すると IPv4, IPv6 のデュアルスタックで起動してきます。\nまとめ OpenStack 外の構成についても IPv6 にもちろん対応している必要があります。上記の例だと OpenStack の eth0 側の IPv6 ネットワークを適切にルーティングしてくれるルータが必要です。 自分の場合は自宅で Vyatta を使って行いました。(今度その方法も記そうと思います) また floating ip は IPv6 には対応していないそうです。はじめこの方法を取ろうと思ったのですがダメでした。 上記のように public ネットワーク側のネットワークインターフェースをブリッジインターフェースにして public ネットワークを生成する方法で対応出来ますので問題ないかと。\nまた、Neutron 構成でも今度 IPv6 対応してみないと。:D\n","permalink":"https://jedipunkz.github.io/post/2013/08/18/openstack-nova-network-ipv6/","summary":"こんにちは。@jedipunkzです\n今更なのかもしれませんが、OpenStack の nova-network を IPv6 対応する方法を調べ てみました。何故 nova-network なのか? 自宅の構成が nova-network だからです..。 以前は Quantum (現 Neutron) 構成で使っていましたが、ノードをコントローラとコン ピュートに別けた時に NIC が足らなくなり\u0026hellip;。\nさて本題です。下記のサイトを参考にしました。ほぼそのままの手順ですが、自分のた めにもメモです。\n参考 URL http://docs.openstack.org/grizzly/openstack-compute/admin/content/configuring-compute-to-use-ipv6-addresses.html\n前提  OpenStack の構成は予め構築されている nova-network を用いている 構成はオールインワンでも複数台構成でも可能  手順 nova がインストールされているすべてのノードで python-netaddr をインストールし ます。私の場合は rcbops の chef cookbooks で構築したのですが、既にインストール されていました。\n% sudo apt-get install python-netaddr nova-network が稼働しているノードで radvd をインストールします。これは IPv6 を Advertise しているルータ等が予め備わっている環境であっても、インストー ルする必要があります。また /etc/radvd.conf が初め無いので radvd 単体では稼働し ませんが、問題ありません。OpenStack の場合 /var/lib/nova 配下のコンフィギュレー ションファイルを読み込んでくれます。\n% sudo apt-get install radvd /etc/sysctl.","title":"OpenStack nova-network IPv6 対応"},{"content":"こんにちは。@jedipunkzです。\nrcbops Cookbooks で Neutron 構成の OpenStack をデプロイする方法を書きたいと思 います。先日紹介した openstack-chef-repo にも Neutron のレシピが含まれているの ですが、まだまだ未完成で手作業をおりまぜながらのデプロイになっていまうので、今 現在のところ Neutron 構成を組みたいのであればこの rcbops の Cookbooks を用いる しかないと思います。\n今回は VLAN モードの構築を紹介します。GRE モードも少し手順を修正すれば構成可能 です。最後のまとめに GRE モードの構築について少し触れています。\n構成  public network +----------------+----------------+----------------+----------------+---------------- | | | +--------------+ +--------------+ +--------------+ +--------------+ +--------------+ | controller01 | | network01 | | network02 | | compute01 | | compute02 | +--------------+ +-------+------+ +-------+------+ +-------+------+ +-------+------+ | | | | | | | | | management network +----------------+-------o--------+-------o--------+-------o--------+-------o-------- | | | | vm network +----------------+----------------+----------------+-------- 特徴は\u0026hellip;\n network ノード 2台 (何台でも可) compute ノード 2台 (何台でも可) api ネットワークは management ネットワーク上に展開 (別けても可) controller ノードは1台 (ha な cookbooks を使うので2台構成も可) 図には表現されていませんが management ネットワーク上に Chef ワークステーションと Chef サーバが必要  物理 NIC のマッピング情報  controller : management(eth0), public(eth1) network : management(eth0), vm(eth1), public(eth2) compute : management(eth0), vm(eth1)  前提のネットワークセグメント情報 ここでは仮に下記のネットワークセグメントを前提として手順を記します。\n public ネットワーク : 10.0.0.0/24 vm ネットワーク : 10.0.1.0/24 management(api) ネットワーク : 10.0.2.0/24  デプロイの準備 Chef 的なワークステーション端末から操作を行います。皆さんの使っているノート PC 等で OK です。\n% git clone https://github.com/rcbops/chef-cookbooks.git ~/chef-repo % cd ~/chef-repo v4.0.0 (安定した最新版) を利用します。\n% git checkout v4.0.0 % git submodule init % git submodule sync % git submodule update Chef サーバに Cookbooks をアップロードします。\n% knife cookbook upload -o cookbooks -a Chef サーバに Roles をアップロードします。\n% knife role from file roles/*rb environment を json 形式で生成します。これは各 Cookbooks の attributes を上書きし、一つの構成 を作るために用いられます。\n% ${EDITOR} environments/env-neutron.json { \u0026#34;name\u0026#34;: \u0026#34;env-neutron\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;cookbook_versions\u0026#34;: { }, \u0026#34;json_class\u0026#34;: \u0026#34;Chef::Environment\u0026#34;, \u0026#34;chef_type\u0026#34;: \u0026#34;environment\u0026#34;, \u0026#34;default_attributes\u0026#34;: { }, \u0026#34;override_attributes\u0026#34;: { \u0026#34;package_component\u0026#34;: \u0026#34;grizzly\u0026#34;, \u0026#34;osops_networks\u0026#34;: { \u0026#34;management\u0026#34;: \u0026#34;10.0.2.0/24\u0026#34;, \u0026#34;public\u0026#34;: \u0026#34;10.0.0.0/24\u0026#34;, \u0026#34;nova\u0026#34;: \u0026#34;10.0.2.0/24\u0026#34; }, \u0026#34;nova\u0026#34;: { \u0026#34;config\u0026#34;: { \u0026#34;use_single_default_gateway\u0026#34;: false, \u0026#34;ram_allocation_ratio\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;cpu_allocation_ratio\u0026#34;: \u0026#34;16\u0026#34; }, \u0026#34;network\u0026#34;: { \u0026#34;provider\u0026#34;: \u0026#34;quantum\u0026#34;, \u0026#34;network_type\u0026#34;: \u0026#34;vlan\u0026#34; }, \u0026#34;apply_patches\u0026#34;: true, \u0026#34;libvirt\u0026#34;: { \u0026#34;vncserver_listen\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;virt_type\u0026#34;: \u0026#34;kvm\u0026#34; }, \u0026#34;db\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;nova\u0026#34; }, \u0026#34;services\u0026#34;: { \u0026#34;novnc-proxy\u0026#34;: { \u0026#34;network\u0026#34;: \u0026#34;public\u0026#34; } } }, \u0026#34;cinder\u0026#34;: { \u0026#34;db\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;cinder\u0026#34; } }, \u0026#34;keystone\u0026#34;: { \u0026#34;admin_user\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;tenants\u0026#34;: [ \u0026#34;admin\u0026#34;, \u0026#34;service\u0026#34; ], \u0026#34;users\u0026#34;: { \u0026#34;admin\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;secrete\u0026#34;, \u0026#34;roles\u0026#34;: { \u0026#34;admin\u0026#34;: [ \u0026#34;admin\u0026#34; ] } }, \u0026#34;demo\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;demo\u0026#34;, \u0026#34;default_tenant\u0026#34; : \u0026#34;service\u0026#34;, \u0026#34;roles\u0026#34;: { \u0026#34;service\u0026#34;: [ \u0026#34;service\u0026#34; ] } } }, \u0026#34;db\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;keystone\u0026#34; } }, \u0026#34;horizon\u0026#34;: { \u0026#34;theme\u0026#34;: \u0026#34;Default\u0026#34;, \u0026#34;db\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;horizon\u0026#34; } }, \u0026#34;mysql\u0026#34;: { \u0026#34;root_network_acl\u0026#34;: \u0026#34;%\u0026#34;, \u0026#34;allow_remote_root\u0026#34;: true, \u0026#34;server_root_password\u0026#34;: \u0026#34;secrete\u0026#34;, \u0026#34;server_repl_password\u0026#34;: \u0026#34;secrete\u0026#34;, \u0026#34;server_debian_password\u0026#34;: \u0026#34;secrete\u0026#34; }, \u0026#34;monitoring\u0026#34;: { \u0026#34;procmon_provider\u0026#34;: \u0026#34;monit\u0026#34;, \u0026#34;metric_provider\u0026#34;: \u0026#34;collectd\u0026#34; }, \u0026#34;glance\u0026#34;: { \u0026#34;images\u0026#34;: [ \u0026#34;precise\u0026#34;, \u0026#34;cirros\u0026#34; ], \u0026#34;image\u0026#34;: { }, \u0026#34;image_upload\u0026#34;: false, \u0026#34;db\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;glance\u0026#34; } }, \u0026#34;quantum\u0026#34;: { \u0026#34;service_pass\u0026#34;: \u0026#34;quantum\u0026#34;, \u0026#34;db\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;quantum\u0026#34; } }, \u0026#34;developer_mode\u0026#34;: false } } 生成した environment ファイルを Chef サーバにアップロードします。\n% knife environment from file environments/env-neutron.json デプロイ 各ノード、下記の通りデプロイします。\ncontroller ノード % knife bootstrap \u0026lt;ip_of_controller01\u0026gt; -N controller01 -r \\  \u0026#39;role[ha-controller1]\u0026#39;,\u0026#39;role[cinder-volume]\u0026#39; -E env-neutron --sudo -x \u0026lt;your_accout\u0026gt; cinder-volumes 用のディスクがある場合は下記の手順で Cinder が利用できるように なります。\n% sudo pvcreate /dev/sdb #デバイス名は仮 % sudo vgcreate cinder-volumes /dev/sdb % sudo service cinder-volume restart network ノード % knife bootstrap \u0026lt;ip_of_network01\u0026gt; -N network01 -r \\  \u0026#39;role[single-network-node]\u0026#39;,\u0026#39;recipe[nova-network::quantum-l3-agent]\u0026#39; \\  -E env-neutron --sudo -x \u0026lt;your_account\u0026gt; % knife bootstrap \u0026lt;ip_of_network02\u0026gt; -N network02 -r \\  \u0026#39;role[single-network-node]\u0026#39;,\u0026#39;recipe[nova-network::quantum-l3-agent]\u0026#39; \\  -E env-neutron --sudo -x \u0026lt;your_account\u0026gt; compute ノード % knife bootstrap \u0026lt;ip_of_compute01\u0026gt; -N compute01 -r \u0026#39;role[single-compute]\u0026#39; \\  -E env-neutron --sudo -x \u0026lt;your_account\u0026gt; % knife bootstrap \u0026lt;ip_of_compute02\u0026gt; -N compute02 -r \u0026#39;role[single-compute]\u0026#39; \\  -E env-neutron --sudo -x \u0026lt;your_account\u0026gt; 物理 NIC のブリッジインターフェースへのマッピング操作 Open vSwitch が管理しているブリッジインターフェースと物理インターフェースをマッ ピングしてあげます。\nnetwork ノードの2台にて下記の操作を行います。\n% sudo ovs-vsctl add-port br-eth1 eth1 % sudo ovs-vsctl add-port br-ex eth2 compute ノードの2台にて下記の操作を行います。\n% sudo ovs-vsctl add-port br-eth1 eth1 仮想ネットワークと仮想マシンの生成 環境によって仮想ネットワークの構成が異なりますので手順は割愛しますが、Horizon かコマンドライン・API を使って仮想ネットワークを生成し仮想マシンを接続すれば完 了です。\nまとめ network ノードを2台構成にしましたが何が嬉しいかと言うと、l3-agent, dhcp-agent を冗長取ることが可能になります。障害時、手作業になりますが切り替えが可能です。 l3-agent は仮想ルータ毎に、dhcp-agent は仮想ネットワーク毎に切り替えが可能です。 詳しくは私の以前の記事を参考にしてください。\nhttp://jedipunkz.github.io/blog/2013/04/26/quantum-network-distributing/\nまた、compute ノードを更に増やせば仮想マシンの数を増やすことが出来ます。\n今回は controller ノードを1台構成にしましたが、更に下記の手順で controller02 ノードを増やし controller ノードの HA 化を行うことも出来ます。\n% knife bootstrap \u0026lt;ip_of_controller02\u0026gt; -N controller02 -r \\  \u0026#39;role[ha-controller1]\u0026#39; -E env-neutron --sudo -x \u0026lt;your_accout\u0026gt; HA 構成を各々のノードに構成させるため各ノードで chef-client を1回ずつ実行しま す。\ncontroller01% sudo chef-client controller02% sudo chef-client HA 構成は haproxy, keepalived で形成されています。\nまた、environment ファイルを下記のように修正すると VLAN の代わりに GRE トンネ ルが扱えます。\n\u0026#34;nova\u0026#34;: { \u0026#34;network\u0026#34;: { \u0026#34;provider\u0026#34;: \u0026#34;quantum\u0026#34;, \u0026#34;network_type\u0026#34;: \u0026#34;gre\u0026#34; } }, \u0026lt;snip\u0026gt;... \u0026#34;quantum\u0026#34;: { \u0026#34;ovs\u0026#34;: { \u0026#34;network_type\u0026#34; : \u0026#34;gre\u0026#34;, \u0026#34;tunnnel_range\u0026#34; : \u0026#34;1:1000\u0026#34; } }, 以上です。roles ディレクトリ配下に様々な Roles があるので、各コンポーネント毎 にノードを別けるなどの構成も可能そうです。皆さん試してみてください。\n","permalink":"https://jedipunkz.github.io/post/2013/08/16/rcbops-cookbooks-neutron-openstack/","summary":"こんにちは。@jedipunkzです。\nrcbops Cookbooks で Neutron 構成の OpenStack をデプロイする方法を書きたいと思 います。先日紹介した openstack-chef-repo にも Neutron のレシピが含まれているの ですが、まだまだ未完成で手作業をおりまぜながらのデプロイになっていまうので、今 現在のところ Neutron 構成を組みたいのであればこの rcbops の Cookbooks を用いる しかないと思います。\n今回は VLAN モードの構築を紹介します。GRE モードも少し手順を修正すれば構成可能 です。最後のまとめに GRE モードの構築について少し触れています。\n構成  public network +----------------+----------------+----------------+----------------+---------------- | | | +--------------+ +--------------+ +--------------+ +--------------+ +--------------+ | controller01 | | network01 | | network02 | | compute01 | | compute02 | +--------------+ +-------+------+ +-------+------+ +-------+------+ +-------+------+ | | | | | | | | | management network +----------------+-------o--------+-------o--------+-------o--------+-------o-------- | | | | vm network +----------------+----------------+----------------+-------- 特徴は\u0026hellip;","title":"rcbops Cookbooks で Neutron 構成 OpenStack"},{"content":"こんにちは。@jedipunkzです。\n今日も軽めの話題を。\nGmail を Emacs + Mew で読み書きする方法を何故かいつも忘れてしまうので自分のた めにもメモしておきます。Gmail はブラウザで読み書き出来るのに！と思われるかもし れませんが、Emacs で文章が書けるのは重要なことです。:D\n対象 OS 比較的新しい\u0026hellip;\n Debian Gnu/Linux Ubuntu  を使います。\n手順 Emacs, Mew, stunnel4 をインストールします。Emacs は好きな物を入れてください。\n% sudo apt-get install emacs24-nox stunnel4 mew mew-bin ca-certificates openssl コマンドで mail.pem を生成します。生成したものを /etc/stunnel 配下に設 置します。\n% openssl req -new -out mail.pem -keyout mail.pem -nodes -x509 -days 365 % sudo cp mail.pem /etc/stunnel/ stunnel はインストール直後、起動してくれないので ENABLE=1 に修正します。\n% sudo ${EDITOR} /etc/default/stunnel4 ENABLE=1 # 0 -\u0026gt; 1 へ変更 stunenl.conf のサンプルを /etc/stunnel 配下に設置します。\n% sudo cp /usr/share/doc/stunnel4/examples/stunnel.conf-sample /etc/stunnel/stunnel.conf $HOME/.mew.el ファイルを生成します。自分のアカウント情報などを入力します。\n; Stunnel (setq mew-prog-ssl \u0026#34;/usr/bin/stunnel4\u0026#34;) ; IMAP for Gmail (setq mew-proto \u0026#34;%\u0026#34;) (setq mew-imap-server \u0026#34;imap.gmail.com\u0026#34;) (setq mew-imap-user \u0026#34;example@gmail.com\u0026#34;) (setq mew-imap-auth t) (setq mew-imap-ssl t) (setq mew-imap-ssl-port \u0026#34;993\u0026#34;) (setq mew-smtp-auth t) (setq mew-smtp-ssl t) (setq mew-smtp-ssl-port \u0026#34;465\u0026#34;) (setq mew-smtp-user \u0026#34;example@gmail.com\u0026#34;) (setq mew-smtp-server \u0026#34;smtp.gmail.com\u0026#34;) (setq mew-fcc \u0026#34;%Sent\u0026#34;) ; 送信メイルを保存する (setq mew-imap-trash-folder \u0026#34;%[Gmail]/ゴミ箱\u0026#34;) (setq mew-use-cached-passwd t) (setq mew-ssl-verify-level 0) $HOME/.emacs.d/init.el に Mew の記述を追記します。\n(autoload \u0026#39;mew \u0026#34;mew\u0026#34; nil t) (autoload \u0026#39;mew-send \u0026#34;mew\u0026#34; nil t) (setq mew-fcc \u0026#34;+outbox\u0026#34;) ; 送信メールを保存 (setq exec-path (cons \u0026#34;/usr/bin\u0026#34; exec-path)) Emacs + Mew を起動します。\n% emacs -e mew まとめ 以上です。他の distro だと ca-certificate とか無いので、大変だなぁといつも思っ てしまいます。\n","permalink":"https://jedipunkz.github.io/post/2013/08/12/emacs-mew-gmail/","summary":"こんにちは。@jedipunkzです。\n今日も軽めの話題を。\nGmail を Emacs + Mew で読み書きする方法を何故かいつも忘れてしまうので自分のた めにもメモしておきます。Gmail はブラウザで読み書き出来るのに！と思われるかもし れませんが、Emacs で文章が書けるのは重要なことです。:D\n対象 OS 比較的新しい\u0026hellip;\n Debian Gnu/Linux Ubuntu  を使います。\n手順 Emacs, Mew, stunnel4 をインストールします。Emacs は好きな物を入れてください。\n% sudo apt-get install emacs24-nox stunnel4 mew mew-bin ca-certificates openssl コマンドで mail.pem を生成します。生成したものを /etc/stunnel 配下に設 置します。\n% openssl req -new -out mail.pem -keyout mail.pem -nodes -x509 -days 365 % sudo cp mail.pem /etc/stunnel/ stunnel はインストール直後、起動してくれないので ENABLE=1 に修正します。\n% sudo ${EDITOR} /etc/default/stunnel4 ENABLE=1 # 0 -\u0026gt; 1 へ変更 stunenl.","title":"Emacs + Mew で Gmail を読み書きする"},{"content":"こんにちは。@jedipunkzです。\nLinux のウィンドウマネージャは使い続けて長いのですが、既に1周半しました。twm -\u0026gt; gnome -\u0026gt; enlightenment -\u0026gt; OpenBox -\u0026gt; .. 忘れた .. -\u0026gt; twm -\u0026gt; vtwm -\u0026gt; awesome -\u0026gt; kde -\u0026gt; gnome -\u0026gt; enligtenment \u0026hellip;\n巷では Linux のデスクトップ環境は死んだとか言われているらしいですが、stumpwm というウィンドウマネージャは結構いいなと思いました。タイル型のウィンドウマネー ジャで Emacs 好きの人が開発したらしいです。設定は lisp で書けます。\n見た目は派手では無いのですが、\n グルーピング機能 すべての操作がキーボードで出来る タイル型であるので煩わしいマウスでのウィンドウ操作が不要  という点に惹かれました。\nLinux を使う時、私の場合 Debian Gnu/Linux Unstalble をいつも使うのですが、 Unstable だと apt-get install stumpwm したバイナリがコケる\u0026hellip;ということでビル ドしてあげました。普段慣れないビルド方法だったので、その時の手順を自分のために もメモしておきます。\n前提環境  Debian Gnu/Linux unstable 利用 X の環境は揃っている  ビルド手順 clisp をインストール clisp をインストールします。\n% sudo apt-get install clisp-dev lisp.run というファイルを stumpwm が見つけられないので symlink 張ってあげます。 ちょっと泥臭い。\n% sudo ln -s /usr/lib/clisp-2.49/base /usr/lib/clisp-2.49/full sbcl をインストール sbcl をインストールします。\n% sudo apt-get install sbcl quicklisp をインストール quicklisp をインストールします。また lisp init file に諸々追加します。\n% wget http://beta.quicklisp.org/quicklisp.lisp % sbcl --load quicklisp.lisp * (quicklisp-quickstart:install) * (ql:system-apropos \u0026#34;vecto\u0026#34;) * (ql:quickload \u0026#34;vecto\u0026#34;) * (ql:add-to-init-file) * (ql:quickload \u0026#34;clx\u0026#34;) * (ql:quickload \u0026#34;cl-ppcre\u0026#34;) * quit stumpwm のビルド stumpwm をビルドします。\n% git clone https://github.com/sabetts/stumpwm.git % cd stumpwm % ./configure % make % sudo make install まとめ $HOME/.stumpwmrc を配置して、いろいろカスタマイズ出来ます。ウェブを検索すると 皆さんの rc ファイルが見つかるので参考にすると良いかも。prefix キーは C-t なの ですが、tmux と被りますよね\u0026hellip;。tmux 側は変えたくないので stumpwm 側を C-z 等 に変更して使っています。\n(set-prefix-key (kbd \u0026quot;C-z\u0026quot;)) ","permalink":"https://jedipunkz.github.io/post/2013/08/09/debian-unstable-stumpwm/","summary":"こんにちは。@jedipunkzです。\nLinux のウィンドウマネージャは使い続けて長いのですが、既に1周半しました。twm -\u0026gt; gnome -\u0026gt; enlightenment -\u0026gt; OpenBox -\u0026gt; .. 忘れた .. -\u0026gt; twm -\u0026gt; vtwm -\u0026gt; awesome -\u0026gt; kde -\u0026gt; gnome -\u0026gt; enligtenment \u0026hellip;\n巷では Linux のデスクトップ環境は死んだとか言われているらしいですが、stumpwm というウィンドウマネージャは結構いいなと思いました。タイル型のウィンドウマネー ジャで Emacs 好きの人が開発したらしいです。設定は lisp で書けます。\n見た目は派手では無いのですが、\n グルーピング機能 すべての操作がキーボードで出来る タイル型であるので煩わしいマウスでのウィンドウ操作が不要  という点に惹かれました。\nLinux を使う時、私の場合 Debian Gnu/Linux Unstalble をいつも使うのですが、 Unstable だと apt-get install stumpwm したバイナリがコケる\u0026hellip;ということでビル ドしてあげました。普段慣れないビルド方法だったので、その時の手順を自分のために もメモしておきます。\n前提環境  Debian Gnu/Linux unstable 利用 X の環境は揃っている  ビルド手順 clisp をインストール clisp をインストールします。","title":"Debian Unstable で stumpwm"},{"content":"こんにちは。@jedipunkzです。\n前回、github.com/rcbops の Cookbooks を利用した OpenStack デプロイ方法を紹介し ました。これは RackSpace 社の Private Cloud Service で使われている Cookbooks で Apache ライセンスの元、誰でも利用できるようになっているものです。HA 構成を 組めたり Swift の操作 (Rings 情報管理など) も Chef で出来る優れた Cookbooks な わけですが、運用するにあたり幾つか考えなくてはならないこともありそうです。\n chef-client の実行順番と実行回数が密接に関わっていること HA 構成の手動切替等、運用上必要な操作について考慮する必要性  ※ 後者については OpenStack ユーザ会の方々に意見もらいました。\n特に前項は Chef を利用する一番の意義である \u0026ldquo;冪等性\u0026rdquo; をある程度 (全くという意味 ではありませんが) 犠牲にしていると言えます。また chef-client の実行回数、タイ ミング等 Cookbooks を完全に理解していないと運用は難しいでしょう。自ら管理して いる Cookbooks なら問題ないですが、rcbops が管理しているので常に更新状況を追っ ていく必要もありそうです。\n一方、Opscode, RackSpace, AT\u0026amp;T 等のエンジニアが管理している Cookbooks がありま す。これは以前、日本の OpenStack 勉強会で私が話した \u0026lsquo;openstack-chef-repo\u0026rsquo; を利 用したモノです。github.com/stackforge の元に管理されています。 openstack-chef-repo は Berksfile, Roles, Environments のみの構成で各 Cookbooks は Berksfile を元に取得する形になります。取得先は同じく github.com/stackforge 上で管理されています。\nこちらの Cookbooks を利用するメリットとしては\n 冪等性が保たれる 複数ベンダ管理なのでリスクがある程度軽減される  などです。逆にデメリットは\u0026hellip;\n 容易に HA 構成を組むことが出来ない node, environment, role による search で自律的に構成してくれない  です。一長一短ですが、運用し切れない HA 構成を組むのであれば、冪等性が保たれる こちらの Cookbooks を使う、というのも一つの考えだと思います。\n今回は nova-network を用いた複数台構成 (controller + compute x n) を この Stackforge Cookbooks を使ってデプロイする方法を紹介しようと思います！\n前提条件  Chef サーバの構築は済ませている knife をワークショテーションで扱えるまでの準備は出来ている デプロイする先のノード (controller01 , compute0[12]) は sshd が起動している  構成 +----------------+----------------+------------------------------------------------- public | eth0 | eth0 | eth0 network +--------------+ +--------------+ +--------------+ +--------------+ +--------------+ | controller01 | | compute01 | | compute02 | | chef server | | workstation | +--------------+ +--------------+ +--------------+ +--------------+ +--------------+ | eth1 | eth2 | | eth2 | | eth0 | eth0 +----------------+-------o--------+-------o--------+----------------+--------------- api/management | eth1 | eth1 network +----------------+----------------------------------------- vm network 特徴は\u0026hellip;\n controller01 ノードは OpenStack コントローラノード compute0[12] ノードは OpenStack コンピュートノード Chef サーバ、ワークステーションは api/management ネットワーク上に配置 compute0[12] は 3 本目の NIC (eth1) を出し vm ネットワークをマッピング (bridge デバイス) API endpoint は controller01 の eth1 で受ける 仮想マシンは compute ノードの eth1 にブリッジ接続し eth0 を介してインターネットへ  IP アドレス情報\u0026hellip;\n controller01, eth0 : 10.0.0.10, eth1 : 10.0.1.10 compute01, eth0 : 10.0.0.11, eth1 : 172.24.18.11, eth2 : 10.0.1.11 compute02, eth0 : 10.0.0.12, eth1 : 172.24.18.12, eth2 : 10.0.1.12 chef server, eth0 : 10.0.1.13 workstation, eth0 : 10.0.1.14  デプロイ手順 ここからの操作は全てワークステーション上から行います。\nまず、openstakc-chef-repo を取得します。私のアカウント上のものを利用します。stackforge のモノは 管理が追いついていないため、動作しません\u0026hellip;。私のモノは fork して Roles を主に修正しています。\n% git clone git://github.com/jedipunkz/openstack-chef-repo.git ~/openstack-chef-repo % cd ~/openstack-chef-repo Cookbooks を取得します。Berksfile に従って取得しますが予め berkshelf を gem で インストールしましょう。\n% gem install berkshelf --no-ri --no-rdoc % rbenv rehash # rbenv を利用している場合 % berks install --path=./coobooks 上記の環境に合わせ environment を作成します。この environment は各 Cookbooks の attributes を上書きし 1つの environment で 1つの構成 (controller, compute..) を形成します。knife bootstrap, chef-client を 実行するとこの environment をサーチし、それぞれが連携し合ってくれます。\n% ${EDITOR} environments/openstack-cluster01.rb name \u0026#34;separated\u0026#34; description \u0026#34;separated nodes environment\u0026#34; override_attributes( \u0026#34;release\u0026#34; =\u0026gt; \u0026#34;grizzly\u0026#34;, \u0026#34;osops_networks\u0026#34; =\u0026gt; { \u0026#34;management\u0026#34; =\u0026gt; \u0026#34;10.0.1.0/24\u0026#34;, \u0026#34;public\u0026#34; =\u0026gt; \u0026#34;10.0.1.0/24\u0026#34;, \u0026#34;nova\u0026#34; =\u0026gt; \u0026#34;10.0.1.0/24\u0026#34; }, \u0026#34;mysql\u0026#34; =\u0026gt; { \u0026#34;bind_address\u0026#34; =\u0026gt; \u0026#34;0.0.0.0\u0026#34;, \u0026#34;root_network_acl\u0026#34; =\u0026gt; \u0026#34;%\u0026#34;, \u0026#34;allow_remote_root\u0026#34; =\u0026gt; true, \u0026#34;server_root_password\u0026#34; =\u0026gt; \u0026#34;secrete\u0026#34;, \u0026#34;server_repl_password\u0026#34; =\u0026gt; \u0026#34;secrete\u0026#34;, \u0026#34;server_debian_password\u0026#34; =\u0026gt; \u0026#34;secrete\u0026#34; }, \u0026#34;nova\u0026#34; =\u0026gt; { \u0026#34;network\u0026#34; =\u0026gt; { \u0026#34;fixed_range\u0026#34; =\u0026gt; \u0026#34;172.18.0.0/24\u0026#34;, \u0026#34;public_interface\u0026#34; =\u0026gt; \u0026#34;eth0\u0026#34; } }, \u0026#34;openstack\u0026#34; =\u0026gt; { \u0026#34;developer_mode\u0026#34; =\u0026gt; true, \u0026#34;compute\u0026#34; =\u0026gt; { \u0026#34;rabbit\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.0.1.10\u0026#34; }, \u0026#34;novnc_proxy\u0026#34; =\u0026gt; { \u0026#34;bind_interface\u0026#34; =\u0026gt; \u0026#34;eth1\u0026#34; }, \u0026#34;libvirt\u0026#34; =\u0026gt; { \u0026#34;bind_interface\u0026#34; =\u0026gt; \u0026#34;eth1\u0026#34; }, \u0026#34;network\u0026#34; =\u0026gt; { \u0026#34;fixed_range\u0026#34; =\u0026gt; \u0026#34;172.18.0.0/24\u0026#34; }, \u0026#34;networks\u0026#34; =\u0026gt; [ { \u0026#34;label\u0026#34; =\u0026gt; \u0026#34;private\u0026#34;, \u0026#34;ipv4_cidr\u0026#34; =\u0026gt; \u0026#34;172.18.0.0/24\u0026#34;, \u0026#34;num_networks\u0026#34; =\u0026gt; \u0026#34;1\u0026#34;, \u0026#34;network_size\u0026#34; =\u0026gt; \u0026#34;255\u0026#34;, \u0026#34;bridge\u0026#34; =\u0026gt; \u0026#34;br200\u0026#34;, \u0026#34;bridge_dev\u0026#34; =\u0026gt; \u0026#34;eth1\u0026#34;, \u0026#34;dns1\u0026#34; =\u0026gt; \u0026#34;8.8.8.8\u0026#34;, \u0026#34;dns2\u0026#34; =\u0026gt; \u0026#34;8.8.4.4\u0026#34;, \u0026#34;multi_host\u0026#34; =\u0026gt; \u0026#34;T\u0026#34; } ] }, \u0026#34;identity\u0026#34; =\u0026gt; { \u0026#34;bind_interface\u0026#34; =\u0026gt; \u0026#34;eth1\u0026#34;, \u0026#34;users\u0026#34; =\u0026gt; { \u0026#34;demo\u0026#34; =\u0026gt; { \u0026#34;password\u0026#34; =\u0026gt; \u0026#34;demo\u0026#34;, \u0026#34;default_tenant\u0026#34; =\u0026gt; \u0026#34;service\u0026#34;, \u0026#34;roles\u0026#34; =\u0026gt; { \u0026#34;Member\u0026#34; =\u0026gt; [ \u0026#34;Member\u0026#34; ] } } } }, \u0026#34;image\u0026#34; =\u0026gt; { \u0026#34;bind_interface\u0026#34; =\u0026gt; \u0026#34;eth1\u0026#34;, }, \u0026#34;network\u0026#34; =\u0026gt; { \u0026#34;api\u0026#34; =\u0026gt; { \u0026#34;bind_interface\u0026#34; =\u0026gt; \u0026#34;eth1\u0026#34;, } }, \u0026#34;db\u0026#34; =\u0026gt; { \u0026#34;bind_interface\u0026#34; =\u0026gt; \u0026#34;eth1\u0026#34;, \u0026#34;compute\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.0.1.10\u0026#34; }, \u0026#34;identity\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.0.1.10\u0026#34; }, \u0026#34;image\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.0.1.10\u0026#34; }, \u0026#34;network\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.0.1.10\u0026#34; }, \u0026#34;volume\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.0.1.10\u0026#34; }, \u0026#34;dashboard\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.0.1.10\u0026#34; } }, \u0026#34;mq\u0026#34; =\u0026gt; { \u0026#34;bind_interface\u0026#34; =\u0026gt; \u0026#34;eth1\u0026#34; }, \u0026#34;endpoints\u0026#34; =\u0026gt; { \u0026#34;identity-api\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.0.1.10\u0026#34; }, \u0026#34;identity-admin\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.0.1.10\u0026#34; }, \u0026#34;compute-api\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.0.1.10\u0026#34; }, \u0026#34;compute-ec2-api\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.0.1.10\u0026#34; }, \u0026#34;compute-ec2-admin\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.0.1.10\u0026#34; }, \u0026#34;compute-novnc\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.0.1.10\u0026#34;, }, \u0026#34;network-api\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.0.1.10\u0026#34;, }, \u0026#34;image-api\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.0.1.10\u0026#34;, }, \u0026#34;image-registry\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.0.1.10\u0026#34;, }, \u0026#34;volume-api\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.0.1.10\u0026#34;, } } } ) spiceweasel を gem でインストールします。これは Cookbooks, Roles, Environments 等を Chef サーバへアップロードしたり、knife bootstrap を一気に行なってくれるツー ルです。knife bootstrap コマンドの書式も書く必要がありません。\n% gem install spiceweasel --no-ri --no-rdoc % rbenv rehash # rbenv を使っている場合 infrastracture.yml を修正します。作成した environments 名の修正と node: を修正 します。\n...\u0026lt;snip\u0026gt;... environments: - separated: ...\u0026lt;snip\u0026gt;... nodes: - 10.0.0.10: run_list: role[os-compute-single-controller] options: -N controller01 -E separated --sudo -x \u0026lt;your_accout_name\u0026gt;  - 10.0.0.11: run_list: role[os-compute-worker] options: -N compute01 -E separated --sudo -x \u0026lt;your_accout_name\u0026gt;  - 10.0.0.12: run_list: role[os-compute-worker] options: -N compute02 -E separated --sudo -x \u0026lt;your_accout_name\u0026gt;  spiceweasel を実行します。すると knife コマンドの一覧が出力されますので実行さ れる内容を確認します。ここではまだ実行が行われません。\n% spiceweasel infrastructure.yml -e オプションをつけて実行すると先ほど出力された knife コマンドが順に実行されま す。この操作で3台のデプロイが完了します。\n% spiceweasel -e infrastructure.yml まとめと考察 以上の操作で OpenStack 複数台構成が組める。Neutron 構成についても \u0026lsquo;openstack-network\u0026rsquo; Cookbook に実装が入っている様子なので以後試してみたいです。 今回は public ネットワーク・api/management ネットワーク・vm ネットワークの3つ のネットワーク構成で組みました。運用するのであれば public と api/management が 別れている必要があると思いますので。自宅で作ってみたい！でもネットワークが\u0026hellip;と いう場合、environment ファイルの内容を修正すれば pubcic/api/management ネット ワーク・vm ネットワークの2つのネットワーク構成でも組めるので試してみてください。 もしくは bridge_dev の記述を environment から削除すると仮想ネットワークの物理 ネットワークへのマッピングがされないので、1つのネットワークでも構成出来ます。\nまた、オールインワン構成も組めます。オールインワン構成の場合\n% knife bootstrap \u0026lt;ip_addr\u0026gt; -N allinone01 -E allinone \\  -r \u0026#39;role[allinone-compute]\u0026#39; --sudo --x \u0026lt;your_account\u0026gt; でデプロイ出来ます。こちらもネットワーク構成は1つでも2つでも3つでも組めます。もう少し 詳しい手順をオールインワン構成については別途ブログにまとめるかもしれませんー。\n","permalink":"https://jedipunkz.github.io/post/2013/08/06/opscode-cookbooks-openstack-deploy/","summary":"こんにちは。@jedipunkzです。\n前回、github.com/rcbops の Cookbooks を利用した OpenStack デプロイ方法を紹介し ました。これは RackSpace 社の Private Cloud Service で使われている Cookbooks で Apache ライセンスの元、誰でも利用できるようになっているものです。HA 構成を 組めたり Swift の操作 (Rings 情報管理など) も Chef で出来る優れた Cookbooks な わけですが、運用するにあたり幾つか考えなくてはならないこともありそうです。\n chef-client の実行順番と実行回数が密接に関わっていること HA 構成の手動切替等、運用上必要な操作について考慮する必要性  ※ 後者については OpenStack ユーザ会の方々に意見もらいました。\n特に前項は Chef を利用する一番の意義である \u0026ldquo;冪等性\u0026rdquo; をある程度 (全くという意味 ではありませんが) 犠牲にしていると言えます。また chef-client の実行回数、タイ ミング等 Cookbooks を完全に理解していないと運用は難しいでしょう。自ら管理して いる Cookbooks なら問題ないですが、rcbops が管理しているので常に更新状況を追っ ていく必要もありそうです。\n一方、Opscode, RackSpace, AT\u0026amp;T 等のエンジニアが管理している Cookbooks がありま す。これは以前、日本の OpenStack 勉強会で私が話した \u0026lsquo;openstack-chef-repo\u0026rsquo; を利 用したモノです。github.com/stackforge の元に管理されています。 openstack-chef-repo は Berksfile, Roles, Environments のみの構成で各 Cookbooks は Berksfile を元に取得する形になります。取得先は同じく github.","title":"openstack-chef-repo で OpenStack 複数台構成をデプロイ"},{"content":"こんにちは。@jedipunkzです。\n最近、自動化は正義だと最近思うのですが、その手助けをしてくれるツール Cobbler を試してみました。Cobbler と複数 OS, ディストリビューションを CLI, GUI で管理出 来るツールです。PXE, TFTP, DHCPを組分せれば OS の自動構築が出来るのは古くから ありますが、TFTP サーバに配置するカーネルイメージやマックアドレスの管理を一元 して管理してくれるのがこの Cobbler です。\n今回は Cobbler の構築方法をお伝えします。本当は Chef Cookbooks で構築したかっ たのですが Opscode Community にある Cookbooks はイマイチだったので、今回は手動 で。\n前提環境  OS は CentOSを。Ubuntu を利用すると DHCP のコンフィギュレーションを自動で出 来ません 利用するネットワークの DHCP はオフにします  構築手順 SELINUX を無効にします。石◯さん、ごめんなさい。\n# ${EDITOR} /etc/sysconfig/selinux SELINUX=disabled # setenforce 0 EPEL のレポジトリを追加します。\n# rpm -Uvh http://ftp.iij.ad.jp/pub/linux/fedora/epel/6/x86_64/epel-release-6-8.noarch.rpm cobbler をインストールします。またその他必要なパッケージもここでインストールし ます。\n# yum install cobbler debmirror pykickstart 自分の設定したいパスワードを生成して /etc/cobbler/settings 内の default_password_crypted: に設定します。パスワードの生成は下記のように openssl コマンドを利用します。\n% openssl passwd -1 /etc/cobbler/settings 内の下記のパラメータについて設定します。manage_dhcp: は 1 にしておくと、Cobbler が自動で ISC DHCP の設定も書き換えてくれるので、ほぼ必 須の設定です。\nserver: \u0026lt;自ホストの IP\u0026gt; next_server: \u0026lt;自ホストの IP\u0026gt; manage_dhcp: 1 /etc/cobbler/dhcp.template を設定します。自分のネットワーク環境に合わせて設定 します。\nsubnet 192.168.1.0 netmask 255.255.255.0 { option routers 192.168.1.254; option domain-name-servers 8.8.8.8,8.8.4.4; option subnet-mask 255.255.255.0; range dynamic-bootp 192.168.1.120 192.168.1.150; default-lease-time 21600; max-lease-time 43200; next-server $next_server; class \u0026quot;pxeclients\u0026quot; { match if substring (option vendor-class-identifier, 0, 9) = \u0026quot;PXEClient\u0026quot;; if option pxe-system-type = 00:02 { filename \u0026quot;ia64/elilo.efi\u0026quot;; } else if option pxe-system-type = 00:06 { filename \u0026quot;grub/grub-x86.efi\u0026quot;; } else if option pxe-system-type = 00:07 { filename \u0026quot;grub/grub-x86_64.efi\u0026quot;; } else { filename \u0026quot;pxelinux.0\u0026quot;; } } } /etc/xinetd.d/rsync の disable = yes を no に設定します。\n# ${EDITOR} /etc/xinetd.d/rsync \u0026lt;snip\u0026gt; disable = no \u0026lt;snip\u0026gt; サービスを起動します。\n# service cobbler start # service dhcpd start # service httpd start # service xinetd start OS 起動時に各サービスが起動するよう設定します。\n# chkconfig cobbler on # chkconfig dhcpd on # chkconfig httpd on # chkconfig xinetd on iptables をオフにします。\n# service iptables stop # chkconfig iptables off 構築は完了です。\nディストリビューションのインポート ここではテストで Ubuntu Server 12.04 amd64 をインポートしてみます。\nダウンロードしてきたインストール ISO をマウントします。\n# mount -t iso9660 -o loop,ro /path/to/Image.iso /mnt インポートします。\n# cobbler import --name=ubuntu1204 --arch=x86_64 --path=/mnt Debian 系の Ubuntu は x86_64 ではなく通常 amd64 と記しますが、Cobbler が CentOS, RHEL を前提に開発されているので x86_64 と記します。注意してください。\nインポートされたか確認します。\n# cobbler distro list # cobbler profile list ここでは自分の作成した preseed.cfg を使いたいのでその設定を行います。この操作 は行わなくても構いませんが、preseed.cfg 内で自分の環境に合わせて色々したいと思 うので、行なっておくと便利です。予め preseed.cfg は作成する必要ありますが。\n# cp /path/to/preseed.cfg /var/lib/cobbler/kickstarts/ubuntu1204-preseed.cfg # cobbler profile edit --name=ubuntu1204-x86_64 \\ --kickstart=/var/lib/cobbler/kickstarts/ubuntu1204-preseed.cfg \\  --kopts=\u0026#34;priority=critical locale=en_US\u0026#34; インストールターゲットマシンの登録 インストールターゲットのマシンを登録します。予め NIC の Mac アドレスを用意して ください。\n# cobbler system add --name=foo --profile=ubuntu1204-x86_64 # cobbler system edit --name=foo --interface=eth0 \\ --mac=00:1c:25:72:1f:79 --ip-address=192.168.1.120 --netmask \\  255.255.255.0 --static=0 # cobbler system edit --name=foo -gateway=192.168.1.254 --hostname=foo こちらもネットワーク環境に合わせて gateway や netmask の情報を記してください。\nターゲットマシンのインストール PXE ブートするだけです。\nまとめ \u0026lsquo;cobbler-web\u0026rsquo; の設定は今回は説明しませんでした、慣れている人なら CLI のほうがい いと思うので \u0026lsquo;cobbler\u0026rsquo; パッケージだけでも十分だと思います。\nCobbler は CentOS, RHEL を中心に考えられたツールなので Debian 系への対応がイマ イチでした。あと、import する時にコケることがあります。コケる理由がエラーメッ セージとして表示されないものもあるので、ちょっと苦労します。\nただ管理する・新しいターゲット・ディストリビューションの登録を容易にしてくれる ツールとしてはとても有用です。今、この辺りの操作であれば Chef で出来ないかな？ と考えている最中です。と言うか既に Cookbooks を用意し始めています。Chef でシン プルな PXE インストール環境を作った方が多くのディストリビューションに対応出来 そうですし、やる意味あるかな？と思っています。\n","permalink":"https://jedipunkz.github.io/post/2013/07/28/cobbler-os-automation-install/","summary":"こんにちは。@jedipunkzです。\n最近、自動化は正義だと最近思うのですが、その手助けをしてくれるツール Cobbler を試してみました。Cobbler と複数 OS, ディストリビューションを CLI, GUI で管理出 来るツールです。PXE, TFTP, DHCPを組分せれば OS の自動構築が出来るのは古くから ありますが、TFTP サーバに配置するカーネルイメージやマックアドレスの管理を一元 して管理してくれるのがこの Cobbler です。\n今回は Cobbler の構築方法をお伝えします。本当は Chef Cookbooks で構築したかっ たのですが Opscode Community にある Cookbooks はイマイチだったので、今回は手動 で。\n前提環境  OS は CentOSを。Ubuntu を利用すると DHCP のコンフィギュレーションを自動で出 来ません 利用するネットワークの DHCP はオフにします  構築手順 SELINUX を無効にします。石◯さん、ごめんなさい。\n# ${EDITOR} /etc/sysconfig/selinux SELINUX=disabled # setenforce 0 EPEL のレポジトリを追加します。\n# rpm -Uvh http://ftp.iij.ad.jp/pub/linux/fedora/epel/6/x86_64/epel-release-6-8.noarch.rpm cobbler をインストールします。またその他必要なパッケージもここでインストールし ます。\n# yum install cobbler debmirror pykickstart 自分の設定したいパスワードを生成して /etc/cobbler/settings 内の default_password_crypted: に設定します。パスワードの生成は下記のように openssl コマンドを利用します。","title":"Cobbler で OS 自動インストール"},{"content":"こんにちは。@jedipunkzです。\n最近 Chef で OpenStack をデプロイすることばかりに興味持っちゃって、他のことも やらんとなぁと思っているのですが、せっかくなので Swift HA 構成を Chef でデプロ イする方法を書きます。\nSwift って分散ストレージなのに HA ってなんよ！と思われるかもしれませんが、ご存 知の様に Swift はストレージノード (accout, object, container) とプロキシノード に別れます。今回紹介する方法だとプロキシノードを Keepalived と Haproxy で HA、 また MySQL も KeepAlived で HA の構成に出来ました。いつものように RackSpace 管 理の Cookbooks を使っています。\n参考資料 http://www.rackspace.com/knowledge_center/article/openstack-object-storage-configuration\n構成 構成は簡単に記すと下記のようになります。特徴としては\u0026hellip;\n swift-proxy01, swift-proxy02 で HA。VRRP + LB な構成。 swift-proxy01 で git サーバ稼働。Rings 情報を管理。 swift-storageNN がストレージノード OS は Ubuntu server 12.04  です。\n|--------- VRRP + Load Balancer ------| +-----------------+ +-----------------+ +-----------------+ +-----------------+ +-----------------+ | swift-proxy01 | | swift-proxy02 | | swift-storage01 | | swift-storage02 | | swift-storage03 | +-----------------+ +-----------------+ +-----------------+ +-----------------+ +-----------------+ | | | | | +-------------------+-------------------+-------------------+-------------------+------------------ | | +-----------------+ +-----------------+ | chef workstation| | chef server | +-----------------+ +-----------------+ 絵は書く意味なかったか\u0026hellip;。\n手順 chef server 構築 例によって Omnibus パッケージを使います。\n% wget https://opscode-omnibus-packages.s3.amazonaws.com/ubuntu/12.04/x86_64/chef-server_11.0.8-1.ubuntu.12.04_amd64.deb % sudo dpkg -i chef-server_11.0.8-1.ubuntu.12.04_amd64.deb % sudo chef-server-ctl reconfigure % knife configure -i \u0026lt; 適当に答える \u0026gt; \u0026lsquo;knife configure -i\u0026rsquo; で自分用の秘密鍵が生成出来ます。\nworkstation ノードでの準備 ほぼ全ての操作を workstation ノードで行います。knife.rb や秘密鍵の設置等につい ては方法を割愛します。このへんはモヤモヤと説明しますが、皆さんならご存知かと思 いますので..。\ngithub より rackspace 管理の Chef Cookbooks を取得する。\n% git clone https://github.com/rcbops/chef-cookbooks.git ~/openstack-chef-repo % cd ~/openstack-chef-repo v.4.0.0 とい現在 (2013/07/25) 最新リリース版をチェックアウトする。\n% git checkout v4.0.0 % git submodule init % git submodule sync % git submodule update \u0026lsquo;chef server\u0026rsquo; ノードへ Cookbooks をアップロードする\n% knife cookbook upload -o cookbooks -a \u0026lsquo;chef-server\u0026rsquo; ノードへ Roles をアップロードする\n% knife role from file roles/*rb 今回の構成用の environment \u0026lsquo;swift-ha\u0026rsquo; を用意します。ここでは各 Cookbooks の Attributes を上書きし、一つの構成を組みます。Cookbooks 内でこの environment 名 をキーに検索し自ノードと同環境のノードを見つけ出し、関連付けがされます。\n{ \u0026#34;name\u0026#34;: \u0026#34;swift-ha\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;cookbook_versions\u0026#34;: { }, \u0026#34;json_class\u0026#34;: \u0026#34;Chef::Environment\u0026#34;, \u0026#34;chef_type\u0026#34;: \u0026#34;environment\u0026#34;, \u0026#34;default_attributes\u0026#34;: { }, \u0026#34;override_attributes\u0026#34;: { \u0026#34;package_component\u0026#34;: \u0026#34;grizzly\u0026#34;, \u0026#34;osops_networks\u0026#34;: { \u0026#34;management\u0026#34;: \u0026#34;10.0.0.0/24\u0026#34;, \u0026#34;public\u0026#34;: \u0026#34;10.0.0.0/24\u0026#34;, \u0026#34;nova\u0026#34;: \u0026#34;10.0.0.0/24\u0026#34;, \u0026#34;swift\u0026#34;: \u0026#34;10.0.0.0/24\u0026#34; }, \u0026#34;keystone\u0026#34;: { \u0026#34;admin_user\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;tenants\u0026#34;: [ \u0026#34;admin\u0026#34;, \u0026#34;service\u0026#34; ], \u0026#34;users\u0026#34;: { \u0026#34;admin\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;secrete\u0026#34;, \u0026#34;roles\u0026#34;: { \u0026#34;admin\u0026#34;: [ \u0026#34;admin\u0026#34; ] } }, \u0026#34;demo\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;demo\u0026#34;, \u0026#34;default_tenant\u0026#34; : \u0026#34;service\u0026#34;, \u0026#34;roles\u0026#34;: { \u0026#34;admin\u0026#34;: [ \u0026#34;admin\u0026#34; ] } } }, \u0026#34;db\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;keystone\u0026#34; } }, \u0026#34;mysql\u0026#34;: { \u0026#34;root_network_acl\u0026#34;: \u0026#34;%\u0026#34;, \u0026#34;allow_remote_root\u0026#34;: true, \u0026#34;server_root_password\u0026#34;: \u0026#34;secrete\u0026#34;, \u0026#34;server_repl_password\u0026#34;: \u0026#34;secrete\u0026#34;, \u0026#34;server_debian_password\u0026#34;: \u0026#34;secrete\u0026#34; }, \u0026#34;monitoring\u0026#34;: { \u0026#34;procmon_provider\u0026#34;: \u0026#34;monit\u0026#34;, \u0026#34;metric_provider\u0026#34;: \u0026#34;collectd\u0026#34; }, \u0026#34;vips\u0026#34;: { \u0026#34;keystone-admin-api\u0026#34;: \u0026#34;10.0.0.11\u0026#34;, \u0026#34;keystone-service-api\u0026#34;: \u0026#34;10.0.0.11\u0026#34;, \u0026#34;keystone-internal-api\u0026#34;: \u0026#34;10.0.0.11\u0026#34;, \u0026#34;mysql-db\u0026#34;: \u0026#34;10.0.0.12\u0026#34;, \u0026#34;swift-proxy\u0026#34;: \u0026#34;10.0.0.11\u0026#34; }, \u0026#34;developer_mode\u0026#34;: false, \u0026#34;swift\u0026#34;: { \u0026#34;swift_hash\u0026#34;: \u0026#34;127005c8ea84\u0026#34;, \u0026#34;authmode\u0026#34;: \u0026#34;keystone\u0026#34;, \u0026#34;authkey\u0026#34;: \u0026#34;1f281c71-cf89-5b27-a2ad-ad873d3f2760\u0026#34; } } } 生成した environment ファイル \u0026lsquo;environments/swift-ha.json\u0026rsquo; を Chef サーバへアッ プロードします。\n% knife environment from file environments/swift-ha.json disk デバイスの用意 swift-storageNN で オブジェクト格納用の Disk がある場合はパーティションを作成 します。追加の Disk が無くても構わないと思います。後に Chef が使用可能な Disk デバイスを検知してくれます。ここでは例として /dev/sdb として認識されていること を前提に記します。追加の disk が無い場合は /dev/sda6 等、OS インストール時にオ ブジェクト格納用のパーティションを用意してもらえば大丈夫です。また GPT なパー ティションを切る必要があると思いますので (大きいから) fdisk ではなく gdisk を 用いましょう。\n% swift-storageNN% sudo apt-get update; apt-get -y install gdisk % swift-storageNN% sudo gdisk /dev/sdb 各ノードをブートストラップ いよいよデプロイします。\n% knife bootstrap \u0026lt;ip_swift-proxy01\u0026gt; -N swift-proxy01 -r \u0026#39;role[ha-swift-controller1]\u0026#39; -E swift-ha --sudo -x jedipunkz % knife bootstrap \u0026lt;ip_swift-proxy02\u0026gt; -N swift-proxy02 -r \u0026#39;role[ha-swift-controller2]\u0026#39; -E swift-ha --sudo -x jedipunkz % knife bootstrap \u0026lt;ip_swift-storage01\u0026gt; -N swift-storage01 -r \\ \u0026#39;role[swift-object-server]\u0026#39;,\u0026#39;role[swift-container-server]\u0026#39;,\u0026#39;role[swift-account-server]\u0026#39; -E swift-ha --sudo -x jedipunkz % knife bootstrap \u0026lt;ip_swift-sotrage02\u0026gt; -N swift-storage02 -r \\ \u0026#39;role[swift-object-server]\u0026#39;,\u0026#39;role[swift-container-server]\u0026#39;,\u0026#39;role[swift-account-server]\u0026#39; -E swift-ha --sudo -x jedipunkz % knife bootstrap \u0026lt;ip_swift-storage03\u0026gt; -N swift-storage03 -r \\ \u0026#39;role[swift-object-server]\u0026#39;,\u0026#39;role[swift-container-server]\u0026#39;,\u0026#39;role[swift-account-server]\u0026#39; -E swift-ha --sudo -x jedipunkz Chef サーバにノード情報が登録されているので各ストレージノードの zone 情報にシー ケンシャル番号を付与します。これにより各ノードは自分の zone 情報を Chef サーバ から知ることが出来ます。\n% knife exec -E \u0026#34;nodes.find(:name =\u0026gt; \u0026#39;swift-storage01\u0026#39;) {|n| n.set[\u0026#39;swift\u0026#39;][\u0026#39;zone\u0026#39;] = \u0026#39;1\u0026#39;; n.save }\u0026#34; % knife exec -E \u0026#34;nodes.find(:name =\u0026gt; \u0026#39;swift-storage02\u0026#39;) {|n| n.set[\u0026#39;swift\u0026#39;][\u0026#39;zone\u0026#39;] = \u0026#39;2\u0026#39;; n.save }\u0026#34; % knife exec -E \u0026#34;nodes.find(:name =\u0026gt; \u0026#39;swift-storage03\u0026#39;) {|n| n.set[\u0026#39;swift\u0026#39;][\u0026#39;zone\u0026#39;] = \u0026#39;3\u0026#39;; n.save }\u0026#34; Account, Container, Object に対して適切に割り当てられたか確認を行います。\n% knife exec -E \u0026#39;search(:node,\u0026#34;role:swift-account-server\u0026#34;) \\ { |n| z=n[:swift][:zone]||\u0026#34;not defined\u0026#34;; puts \u0026#34;#{n.name} has the role \\ role [swift-account-server] and is in swift zone #{z}\u0026#34;; }\u0026#39; % knife exec -E \u0026#39;search(:node,\u0026#34;role:swift-container-server\u0026#34;) \\ { |n| z=n[:swift][:zone]||\u0026#34;not defined\u0026#34;; puts \u0026#34;#{n.name} has the role \\ [swift-container-server] and is in swift zone #{z}\u0026#34;; }\u0026#39; % knife exec -E \u0026#39;search(:node,\u0026#34;role:swift-object-server\u0026#34;) \\ { |n| z=n[:swift][:zone]||\u0026#34;not defined\u0026#34;; puts \u0026#34;#{n.name} has the role \\ [swift-object-server] and is in swift zone #{z}\u0026#34;; }\u0026#39; また、先ほど用意した Swift 用ディスクデバイスを Chef がディスカバ出来るか確認 を行います。\n% knife exec -E \\  \u0026#39;search(:node,\u0026#34;role:swift-object-server OR \\ role:swift-account-server \\ OR role:swift-container-server\u0026#34;) \\ { |n| puts \u0026#34;#{n.name}\u0026#34;; \\ begin; n[:swift][:state][:devs].each do |d| \\ puts \u0026#34;\\tdevice #{d[1][\u0026#34;device\u0026#34;]}\u0026#34;; \\ end; rescue; puts \\ \u0026#34;no candidate drives found\u0026#34;; end; }\u0026#39; またディスカバされたディスクデバイス /dev/sdb1 が既にマウントされているかどう か確認します。\nswift-strageNN# df\n\u0026lsquo;swift-proxy01\u0026rsquo; ノードにて再度 chef-client を実行し /etc/swift/ring-workspace/generate-rings.sh を 更新する。上記 zone 情報に従って内容が更新される。\nswift-proxy01# chef-client 生成された generate-rails.sh を実行。\u0026lsquo;exit 0\u0026rsquo; の行をコメントアウトして実行すること。\nswift-proxy01# cd /etc/swift/ring-workspace swift-proxy01# ./generate-rings.sh /etc/swift/ring-workstation/rings 配下に Rings ファイルが生成されたはずです。確 認してみてください。この Rings ファイルを git サーバにプッシュします。git サー バは既に \u0026lsquo;swift-proxy01\u0026rsquo; ノード上に構築されています。\nswift-proxy01# cd /etc/swift/ring-workspace/rings swift-proxy01# git add account.builder container.builder object.builder swift-proxy01# git add account.ring.gz container.ring.gz object.ring.gz swift-proxy01# git commit -m \u0026#34;initial commit\u0026#34; swift-proxy01# git push git サーバにプッシュされた Rings ファイルを各ノードに配布します。配布の方法は 各ノードで chef-client を実行することです。これは knife ssh 等を用いても構いま せん。\nswift-proxy01# chef-client swift-proxy02# chef-client swift-storage01# chef-client swift-storage02# chef-client swift-storage03# chef-client swift-storageNN の計3台が登録されたかどうかを確認します。\nswift-proxy# swift-recon --md5 動作確認 動作確認をしましょう。\nswift-storage01# source swift-openrc swift-storage01# swift post container01 swift-storage01# echo \u0026#34;test\u0026#34; \u0026gt; test swift-storage01# swift upload container01 test swift-storage01# swift list swift-storage01# swift list container01 また、これらの操作が swift-proxy01, swift-proxy02 の片系を落とした状態でも可能 かどうかも確認しましょう。\nまとめ swift-proxy 片系の障害時にも読み込み系・書き込み系の操作が可能なことを確認出来 ました。復旧に関しても MySQL 的な Slave 系 (今回だと swift-proxy02(後からブー トストラップしたノード)) に関しては knife bootstrap で出来ます。Master 系の復 旧は簡単にはいきません。この事態に備えるには Rings 情報のバックアップ (Git レ ポジトリバックアップ) と MySQL のダンプの定期的取得が必要になるでしょう。\nまた、今回 VRRP で HA しているので2台構成になります。Swift プロキシは本来、ノー ドを増やすことでより多くのリクエストを受けられる (何がボトルネックになるかで状 況は変わりますが) 利点があるため、この2台構成がベストかどうかは状況に合わせて 考えた方が良いかもしれません。この構成の場合、ロードバランサが必要ない点がメリッ トだったりします。\n","permalink":"https://jedipunkz.github.io/post/2013/07/26/swift-ha-chef-deploy/","summary":"こんにちは。@jedipunkzです。\n最近 Chef で OpenStack をデプロイすることばかりに興味持っちゃって、他のことも やらんとなぁと思っているのですが、せっかくなので Swift HA 構成を Chef でデプロ イする方法を書きます。\nSwift って分散ストレージなのに HA ってなんよ！と思われるかもしれませんが、ご存 知の様に Swift はストレージノード (accout, object, container) とプロキシノード に別れます。今回紹介する方法だとプロキシノードを Keepalived と Haproxy で HA、 また MySQL も KeepAlived で HA の構成に出来ました。いつものように RackSpace 管 理の Cookbooks を使っています。\n参考資料 http://www.rackspace.com/knowledge_center/article/openstack-object-storage-configuration\n構成 構成は簡単に記すと下記のようになります。特徴としては\u0026hellip;\n swift-proxy01, swift-proxy02 で HA。VRRP + LB な構成。 swift-proxy01 で git サーバ稼働。Rings 情報を管理。 swift-storageNN がストレージノード OS は Ubuntu server 12.04  です。\n|--------- VRRP + Load Balancer ------| +-----------------+ +-----------------+ +-----------------+ +-----------------+ +-----------------+ | swift-proxy01 | | swift-proxy02 | | swift-storage01 | | swift-storage02 | | swift-storage03 | +-----------------+ +-----------------+ +-----------------+ +-----------------+ +-----------------+ | | | | | +-------------------+-------------------+-------------------+-------------------+------------------ | | +-----------------+ +-----------------+ | chef workstation| | chef server | +-----------------+ +-----------------+ 絵は書く意味なかったか\u0026hellip;。","title":"Swift HA 構成を Chef でデプロイ"},{"content":"こんにちは。@jedipunkzです。\nOpenStack を運用する中でコントローラは重要です。コントローラノードが落ちると、 仮想マシンの操作等が利用出来ません。コントローラの冗長構成を取るポイントは公式 wiki サイトに記述あるのですが PaceMaker を使った構成でしんどいです。何より運用 する人が混乱する仕組みは避けたいです。\nRackSpace 社の管理している Chef Cookbooks の Roles に \u0026lsquo;ha-controller1\u0026rsquo;, \u0026lsquo;ha-controller2\u0026rsquo; というモノがあります。今回はこれを使った HA 構成の構築方法に ついて書いていこうかと思います。\n構成 最小構成を作りたいと思います。HA のためのコントローラノード2台, コンピュートノー ド1台, Chef ワークショテーション1台, Chef サーバノード1台。\n+----------------+----------------+----------------+----------------+--------------- public network | | | eth0 | | 10.0.0.0/24 +--------------+ +--------------+ +--------------+ +--------------+ +--------------+ | controller01 | | controller01 | | compute01 | | chef server | | workstation | +--------------+ +--------------+ +--------------+ +--------------+ +--------------+ | eth1 +------------------------------------------------- fixed range network 172.16.0.0/24 特徴  \u0026lsquo;controller01\u0026rsquo;, \u0026lsquo;controller02\u0026rsquo; で HA 化コントローラ \u0026lsquo;compute01\u0026rsquo; はコンピュートノード \u0026lsquo;chef server\u0026rsquo; は Chef サーバ、Chef11 推奨 ほとんどの作業を行う \u0026lsquo;workstaion\u0026rsquo; は Chef ワークステーション nova-network 構成  Chef サーバの構築 Chef サーバの構築方法は本題から外れるので割愛します。今は Omnibus インストーラで一発です。\nwokstation ノードでの準備  workstation で knife を使えるまでの準備についても割愛します。 プロンプトに何もホスト名が記されていないモノは全て workstation ノード上での操作です。  Rackspace 管理の Cookbook を取得します。\n% git clone https://github.com/rcbops/chef-cookbooks.git ~/openstack-chef-repo % cd ~/openstack-chef-repo v4.0.0 というタグ (現在 2013/07/16 最新リリース版) をチェックアウトします。また submodue update で関連 する Cookbooks を全て取得する事ができます。\n% git checkout v4.0.0 % git submodule init % git submodule sync % git submodule update 取得した Cookbooks を Chef サーバにアップロードします。\n% knife cookbook upload -o cookbooks -a 同様に Roles も Chef サーバにアップロードします。\n% knife role from file roles/*rb environment ファイルを生成します。ここで指定するパラメターは各々の Cookbooks の Attributes を上書き出来ます。 また、Recipe 内でノードサーチする際に同じ environment 名が指定されているノードをクラスタ構成の一部と判断出来る といった仕組みです。\n% ${EDITOR} environments/Cluster01.json { \u0026#34;name\u0026#34;: \u0026#34;Cluster01\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;cookbook_versions\u0026#34;: { }, \u0026#34;json_class\u0026#34;: \u0026#34;Chef::Environment\u0026#34;, \u0026#34;chef_type\u0026#34;: \u0026#34;environment\u0026#34;, \u0026#34;default_attributes\u0026#34;: { }, \u0026#34;override_attributes\u0026#34;: { \u0026#34;package_component\u0026#34;: \u0026#34;grizzly\u0026#34;, \u0026#34;osops_networks\u0026#34;: { \u0026#34;management\u0026#34;: \u0026#34;10.0.0.0/24\u0026#34;, \u0026#34;public\u0026#34;: \u0026#34;10.0.0.0/24\u0026#34;, \u0026#34;nova\u0026#34;: \u0026#34;10.0.0.0/24\u0026#34; }, \u0026#34;nova\u0026#34;: { \u0026#34;networks\u0026#34;: [ { \u0026#34;bridge\u0026#34;: \u0026#34;br100\u0026#34;, \u0026#34;num_networks\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;dns2\u0026#34;: \u0026#34;8.8.4.4\u0026#34;, \u0026#34;dns1\u0026#34;: \u0026#34;8.8.8.8\u0026#34;, \u0026#34;ipv4_cidr\u0026#34;: \u0026#34;172.16.0.0/24\u0026#34;, \u0026#34;network_size\u0026#34;: \u0026#34;256\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;private\u0026#34;, \u0026#34;bridge_dev\u0026#34;: \u0026#34;eth1\u0026#34; } ], \u0026#34;config\u0026#34;: { \u0026#34;use_single_default_gateway\u0026#34;: false, \u0026#34;ram_allocation_ratio\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;cpu_allocation_ratio\u0026#34;: \u0026#34;16\u0026#34; }, \u0026#34;network\u0026#34;: { \u0026#34;dmz_cidr\u0026#34;: \u0026#34;172.18.0.0/24\u0026#34;, \u0026#34;public_interface\u0026#34;: \u0026#34;eth0\u0026#34;, \u0026#34;multi_host\u0026#34;: true, \u0026#34;fixed_range\u0026#34;: \u0026#34;172.16.0.0/24\u0026#34; }, \u0026#34;apply_patches\u0026#34;: true, \u0026#34;libvirt\u0026#34;: { \u0026#34;vncserver_listen\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;virt_type\u0026#34;: \u0026#34;kvm\u0026#34; }, \u0026#34;db\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;nova\u0026#34; } }, \u0026#34;cinder\u0026#34;: { \u0026#34;db\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;cinder\u0026#34; } }, \u0026#34;keystone\u0026#34;: { \u0026#34;admin_user\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;tenants\u0026#34;: [ \u0026#34;admin\u0026#34;, \u0026#34;service\u0026#34; ], \u0026#34;users\u0026#34;: { \u0026#34;admin\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;secrete\u0026#34;, \u0026#34;roles\u0026#34;: { \u0026#34;admin\u0026#34;: [ \u0026#34;admin\u0026#34; ] } }, \u0026#34;demo\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;demo\u0026#34;, \u0026#34;default_tenant\u0026#34; : \u0026#34;service\u0026#34;, \u0026#34;roles\u0026#34;: { \u0026#34;service\u0026#34;: [ \u0026#34;service\u0026#34; ] } } }, \u0026#34;db\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;keystone\u0026#34; } }, \u0026#34;horizon\u0026#34;: { \u0026#34;theme\u0026#34;: \u0026#34;Rackspace\u0026#34;, \u0026#34;db\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;horizon\u0026#34; } }, \u0026#34;mysql\u0026#34;: { \u0026#34;root_network_acl\u0026#34;: \u0026#34;%\u0026#34;, \u0026#34;allow_remote_root\u0026#34;: true }, \u0026#34;monitoring\u0026#34;: { \u0026#34;procmon_provider\u0026#34;: \u0026#34;monit\u0026#34;, \u0026#34;metric_provider\u0026#34;: \u0026#34;collectd\u0026#34; }, \u0026#34;glance\u0026#34;: { \u0026#34;images\u0026#34;: [ \u0026#34;precise\u0026#34;, \u0026#34;cirros\u0026#34; ], \u0026#34;image\u0026#34;: { }, \u0026#34;image_upload\u0026#34;: true, \u0026#34;db\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;glance\u0026#34; } }, \u0026#34;vips\u0026#34;: { \u0026#34;cinder-api\u0026#34;: \u0026#34;10.0.0.22\u0026#34;, \u0026#34;glance-api\u0026#34;: \u0026#34;10.0.0.22\u0026#34;, \u0026#34;glance-registry\u0026#34;: \u0026#34;10.0.0.22\u0026#34;, \u0026#34;horizon-dash\u0026#34;: \u0026#34;10.0.0.22\u0026#34;, \u0026#34;horizon-dash_ssl\u0026#34;: \u0026#34;10.0.0.22\u0026#34;, \u0026#34;keystone-admin-api\u0026#34;: \u0026#34;10.0.0.22\u0026#34;, \u0026#34;keystone-service-api\u0026#34;: \u0026#34;10.0.0.22\u0026#34;, \u0026#34;keystone-internal-api\u0026#34;: \u0026#34;10.0.0.22\u0026#34;, \u0026#34;mysql-db\u0026#34;: \u0026#34;10.0.0.21\u0026#34;, \u0026#34;nova-api\u0026#34;: \u0026#34;10.0.0.22\u0026#34;, \u0026#34;nova-ec2-public\u0026#34;: \u0026#34;10.0.0.22\u0026#34;, \u0026#34;nova-novnc-proxy\u0026#34;: \u0026#34;10.0.0.22\u0026#34;, \u0026#34;nova-xvpvnc-proxy\u0026#34;: \u0026#34;10.0.0.22\u0026#34;, \u0026#34;rabbitmq-queue\u0026#34;: \u0026#34;10.0.0.23\u0026#34; }, \u0026#34;developer_mode\u0026#34;: false } } 作成した environment を Chef サーバにアップロードします。\n% knife environment from file environments/Cluster01.json HA コントローラデプロイ controller01, controller02 ノードを HA 化するわけですが、この操作を全て Chef で行います。\n% knife bootstrap \u0026lt;ip_controller01\u0026gt; -N controller01 -r \u0026#39;role[ha-controller1]\u0026#39; -E Cluster01 --sudo -x jedipunkz % knife bootstrap \u0026lt;ip_controller02\u0026gt; -N controller02 -r \u0026#39;role[ha-controller2]\u0026#39; -E Cluster01 --sudo -x jedipunkz chef サーバに node の情報が登録されました。ここから environment で指定した VIP を利用した HA 構成を組むため再度 chef-client を実行します。chef-client をデーモン化しておいた場合は 時間と共に自動で構築されるはずです。\ncontroller01# chef-client controller02# chef-client Cinder の利用に関しては片系のコントローラノードに寄せる必要があります。\u0026lsquo;cinder-volumes\u0026rsquo; という名前の Volume Group を controller01 ノードに作成し下記の操作を行います。\n% knife node run_list add controller01 \u0026#39;role[cinder-volume]\u0026#39; controller01# chef-client controller01# service cinder-volume restart Compute ノードのデプロイ compute ノードのデプロイも同様に Chef を用います。\n% knife bootstrap \u0026lt;ip_compute01\u0026gt; -N compute01 -r \u0026#39;role[single-compute]\u0026#39; -E Cluster01 --sudo -x jedipunkz compute ノードの追加に関しても同様に行なっていくことで仮想マシンの数を拡張出来ます。\nデプロイされた HA 構成の内部 PaceMaker は今回使われていないようです。デプロイされた構成の内部を覗いてみると下記のような特徴がありました。\n各 OpenStack APIs Keepalived で VRRP 構成。active/passive 構成で明示的な Master は存在しない。その都度、評価の高いノードが VIP を引き継ぎサービスリクエストに応じる。また、リクエストを受けたコントローラノードは haproxy により HA の2台に対して それぞれリクエストを分散。ラウンドロビンの冗長が取られている。\nRabbitMQ Keepalived で VRRP 構成。APIs と同様に active/passive 構成。その後は HA 内片系のコントローラの RabbitMQ が全てのリクエストに 応え処理を行う。\nMySQL Keepalived で VRRP 構成。APIs と同様に active/passive 構成。MySQL はミドルウェアベースで master/master, active/passive レプリケーションが組まれている。\n障害系の対処 ここまで来ると、HA 構成が壊れた際に Chef で復旧を自動化してみたくなります。試したところ HA 構成の MySQL 的な passive 側 が壊れた際には knife bootstrap で直ちに復旧することが出来ました。ただし active (master) 側が壊れた際にはバックアップした データからのリストア作業が必要でした。つまり Chef での自動復旧はできませんでした。\nまとめ keepalived と haproxy によるシンプルな HA 構成のため運用する人間が理解・対処する事も容易になると考えられます。 コントローラノード障害といっても、大抵は disk 交換や筐体交換で復旧出来るわけですから、HA さえ組まれていればかなり 安心して運用が出来そうです。今回は nova-network を試してみましたが、neutron 構成も組める Cookbooks になっているので 時間を見つけてやってみようかなと考えています。\n","permalink":"https://jedipunkz.github.io/post/2013/07/17/openstach-ha-chef-deploy/","summary":"こんにちは。@jedipunkzです。\nOpenStack を運用する中でコントローラは重要です。コントローラノードが落ちると、 仮想マシンの操作等が利用出来ません。コントローラの冗長構成を取るポイントは公式 wiki サイトに記述あるのですが PaceMaker を使った構成でしんどいです。何より運用 する人が混乱する仕組みは避けたいです。\nRackSpace 社の管理している Chef Cookbooks の Roles に \u0026lsquo;ha-controller1\u0026rsquo;, \u0026lsquo;ha-controller2\u0026rsquo; というモノがあります。今回はこれを使った HA 構成の構築方法に ついて書いていこうかと思います。\n構成 最小構成を作りたいと思います。HA のためのコントローラノード2台, コンピュートノー ド1台, Chef ワークショテーション1台, Chef サーバノード1台。\n+----------------+----------------+----------------+----------------+--------------- public network | | | eth0 | | 10.0.0.0/24 +--------------+ +--------------+ +--------------+ +--------------+ +--------------+ | controller01 | | controller01 | | compute01 | | chef server | | workstation | +--------------+ +--------------+ +--------------+ +--------------+ +--------------+ | eth1 +------------------------------------------------- fixed range network 172.","title":"OpenStack HA 構成を Chef でデプロイ"},{"content":"こんにちは。@jedipunkzです。\n前回の記事で OpenCenter を使った OpenStack デプロイを行いましたが、デプロイの 仕組みの実体は Opscode Chef です。慣れている人であれば Chef を単独で使った方が よさそうです。僕もこの方法を今後取ろうと思っています。\n幾つかの構成を試している最中ですが、今回 nova-network を使ったオールインワン構 成を作ってみたいと思います。NIC の数は1つです。ノート PC や VPS サービス上にも 構築できると思いますので試してみてください。\n今回は Chef サーバの構築や Knife の環境構築に関しては割愛します。\nまた全ての操作は workstation ノードで行います。皆さんお手持ちの Macbook 等です。 デプロイする先は OpenStack をデプロイするサーバです。\n手順 Chef Cookbook を取得 RackSpace 社のエンジニアがメンテナンスしている Chef Cookbook を使います。各 Cookbook が git submodule 化されているので \u0026ndash;recursive オプションを付けます。\n% git clone https://github.com/rcbops/chef-cookbooks.git ~/openstack-chef-repo % cd openstack-chef-repo \u0026lsquo;v4.0.0\u0026rsquo; ブランチをチェックアウト master ブランチは今現在 (2013/07/08) folsom ベースの構成になっているので \u0026lsquo;grizzly\u0026rsquo; のためのブランチ \u0026lsquo;v4.0.0\u0026rsquo; をローカルにチェックアウトします。\n% git checkout v4.0.0 submodule である Cookbooks を初期化 -\u0026gt; 更新を行います。\n% git submodule init % git submodule sync % git submodule update Environment の作成 Chef の中に Environment という情報があります。これは環境に合わせ各 Cookbooks 内の Attributes 等を上書きすることが可能です。この Environment を作成すること でそれぞれ独立している Cookbooks を用いて一つの環境を作ることが可能になっています。 Environement は clone してきた環境の environments ディレクトリ配下に格納します。 下記の情報をコピペして environments/AllinOne.json として保存してください。\n{ \u0026#34;name\u0026#34;: \u0026#34;AllinOne\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;cookbook_versions\u0026#34;: { }, \u0026#34;json_class\u0026#34;: \u0026#34;Chef::Environment\u0026#34;, \u0026#34;chef_type\u0026#34;: \u0026#34;environment\u0026#34;, \u0026#34;default_attributes\u0026#34;: { }, \u0026#34;override_attributes\u0026#34;: { \u0026#34;package_component\u0026#34;: \u0026#34;grizzly\u0026#34;, \u0026#34;osops_networks\u0026#34;: { \u0026#34;management\u0026#34;: \u0026#34;10.0.0.0/24\u0026#34;, \u0026#34;public\u0026#34;: \u0026#34;10.0.0.0/24\u0026#34;, \u0026#34;nova\u0026#34;: \u0026#34;10.0.0.0/24\u0026#34; }, \u0026#34;nova\u0026#34;: { \u0026#34;networks\u0026#34;: [ { \u0026#34;bridge\u0026#34;: \u0026#34;br100\u0026#34;, \u0026#34;num_networks\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;dns2\u0026#34;: \u0026#34;8.8.4.4\u0026#34;, \u0026#34;dns1\u0026#34;: \u0026#34;8.8.8.8\u0026#34;, \u0026#34;ipv4_cidr\u0026#34;: \u0026#34;172.16.0.0/24\u0026#34;, \u0026#34;network_size\u0026#34;: \u0026#34;256\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;private\u0026#34; } ], \u0026#34;config\u0026#34;: { \u0026#34;use_single_default_gateway\u0026#34;: false, \u0026#34;ram_allocation_ratio\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;cpu_allocation_ratio\u0026#34;: \u0026#34;16\u0026#34; }, \u0026#34;network\u0026#34;: { \u0026#34;dmz_cidr\u0026#34;: \u0026#34;172.18.0.0/24\u0026#34;, \u0026#34;public_interface\u0026#34;: \u0026#34;eth0\u0026#34;, \u0026#34;multi_host\u0026#34;: true, \u0026#34;fixed_range\u0026#34;: \u0026#34;172.16.0.0/24\u0026#34; }, \u0026#34;apply_patches\u0026#34;: true, \u0026#34;libvirt\u0026#34;: { \u0026#34;vncserver_listen\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;virt_type\u0026#34;: \u0026#34;qemu\u0026#34; } }, \u0026#34;keystone\u0026#34;: { \u0026#34;admin_user\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;tenants\u0026#34;: [ \u0026#34;admin\u0026#34;, \u0026#34;service\u0026#34; ], \u0026#34;users\u0026#34;: { \u0026#34;admin\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;secrete\u0026#34;, \u0026#34;roles\u0026#34;: { \u0026#34;admin\u0026#34;: [ \u0026#34;admin\u0026#34; ] } }, \u0026#34;demo\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;demo\u0026#34;, \u0026#34;default_tenant\u0026#34; : \u0026#34;service\u0026#34;, \u0026#34;roles\u0026#34;: { \u0026#34;service\u0026#34;: [ \u0026#34;service\u0026#34; ] } } } }, \u0026#34;horizon\u0026#34;: { \u0026#34;theme\u0026#34;: \u0026#34;Rackspace\u0026#34; }, \u0026#34;mysql\u0026#34;: { \u0026#34;root_network_acl\u0026#34;: \u0026#34;%\u0026#34;, \u0026#34;allow_remote_root\u0026#34;: true }, \u0026#34;monitoring\u0026#34;: { \u0026#34;procmon_provider\u0026#34;: \u0026#34;monit\u0026#34;, \u0026#34;metric_provider\u0026#34;: \u0026#34;collectd\u0026#34; }, \u0026#34;glance\u0026#34;: { \u0026#34;images\u0026#34;: [ \u0026#34;precise\u0026#34;, \u0026#34;cirros\u0026#34; ], \u0026#34;image\u0026#34;: { }, \u0026#34;image_upload\u0026#34;: true }, \u0026#34;developer_mode\u0026#34;: false } } 環境に合わせ修正する必要があるのは\u0026hellip;\n\u0026quot;osops_networks\u0026quot;: { \u0026quot;management\u0026quot;: \u0026quot;10.0.0.0/24\u0026quot;, \u0026quot;public\u0026quot;: \u0026quot;10.0.0.0/24\u0026quot;, \u0026quot;nova\u0026quot;: \u0026quot;10.0.0.0/24\u0026quot; }, だけです。私の環境は 10.0.0.0/24 にあるので上記のように記しましたが、これはみ なさんのネットワーク環境に合わせ修正してください。\nRoles の修正 デフォルトの \u0026lsquo;allinone\u0026rsquo; Role では Cinder が使えない状態です。これは別ストレー ジを扱うことを前提にしているためと思われます。今回はオールインワンでコミコミの 構成を組みたいので roles/allinone.json を下記のように追記します。\nrun_list( \u0026#34;role[single-controller]\u0026#34;, \u0026#34;role[single-compute]\u0026#34;, \u0026#34;role[cinder-volume]\u0026#34; # \u0026lt;- 追加 ) Cookbooks, Roles, Environments を Chef サーバへアップロード チェックアウトした Cookbooks, Roles, あと今作成した Environements を Chef サー バへアップロードします。\n% knife cookbook upload -o cookbooks -a % knife role from file roles/*.rb % knife environment from file environments/AllinOne.json OpenStack をデプロイ いよいよ、次のコマンドでデプロイを行います。\n% knife bootstrap \u0026lt;ip_address\u0026gt; -N \u0026lt;hostname\u0026gt; -r \u0026#39;role[allinone]\u0026#39; -E AllinOne --sudo -x \u0026lt;username\u0026gt; 数分経つと OpenStack デプロイが完了します。ブラウザで https://\u0026lt;ip_address\u0026gt; へ アクセスして確認してみてください。ログインアカウントを上記の environments 内に 記してあります。(user : demo, pass : demo, もしくは user : admin, pass : secrete)\nCinder の利用 この状態で VM を作成してアクセスすることは出来るのですが、Block Storage as a Service (Cinder) は利用できない状態です。下記の操作で使えるようになります。\n% sudo dd if=/dev/zero of=/var/lib/cinder/volumes-disk bs=2 count=0 seek=7G % sudo modprobe loop % sudo losetup /dev/loop3 /var/lib/cinder/volumes-disk % sudo pvcreate /dev/loop3 % sudo vgcreate cinder-volumes /dev/loop3 % sudo service cinder-volume restart まとめと考察 操作して気がついたと思いますが、Roles が数多く存在します。これは色んな構成が組 めることを明示していると言えます。また、Environments を OpenStack 上の一つの構 成に見立てる (例 : controller + compute x n ) あたりが、とても Chef との親和性 が高いと言えます。今まで構築のために bash スクリプトを書いてきましたが、もうそ の必要が無くなりました。また Git のブランチに \u0026lsquo;folsom\u0026rsquo; 等がある通り、ブランチ をチェックアウトし直すことで folsom ベースの構成も組むことが出来ます。私は今、 コントローラノードの HA 化をこの Cookbooks でデプロイしてみています。とても完 成度が高いなぁと感じるのは environments である json ファイルに vips という項目 を追記するだけで HA 構成が組める点です。これにより MySQL, Rabbitmq, API が VIP を持ち、コントローラノードの片系に障害が発生しても OpenStack 全体の構成 (今回 で言う一つの Envorinments ですね) がサービス継続できる！ということです。次回、 機会がありましたら、その構成のデプロイ方法についても紹介したいと思います。\nまた、今回は VPS サービス上等に構築出来るように\n\u0026quot;virt_type\u0026quot;: \u0026quot;qemu\u0026quot; と environments に記しましたが、KVM リソースが扱える環境であればここを \u0026lsquo;kvm\u0026rsquo; とすることでパフォーマンスは若干上がると思います。\n","permalink":"https://jedipunkz.github.io/post/2013/07/08/chef-openstack-deploy/","summary":"こんにちは。@jedipunkzです。\n前回の記事で OpenCenter を使った OpenStack デプロイを行いましたが、デプロイの 仕組みの実体は Opscode Chef です。慣れている人であれば Chef を単独で使った方が よさそうです。僕もこの方法を今後取ろうと思っています。\n幾つかの構成を試している最中ですが、今回 nova-network を使ったオールインワン構 成を作ってみたいと思います。NIC の数は1つです。ノート PC や VPS サービス上にも 構築できると思いますので試してみてください。\n今回は Chef サーバの構築や Knife の環境構築に関しては割愛します。\nまた全ての操作は workstation ノードで行います。皆さんお手持ちの Macbook 等です。 デプロイする先は OpenStack をデプロイするサーバです。\n手順 Chef Cookbook を取得 RackSpace 社のエンジニアがメンテナンスしている Chef Cookbook を使います。各 Cookbook が git submodule 化されているので \u0026ndash;recursive オプションを付けます。\n% git clone https://github.com/rcbops/chef-cookbooks.git ~/openstack-chef-repo % cd openstack-chef-repo \u0026lsquo;v4.0.0\u0026rsquo; ブランチをチェックアウト master ブランチは今現在 (2013/07/08) folsom ベースの構成になっているので \u0026lsquo;grizzly\u0026rsquo; のためのブランチ \u0026lsquo;v4.0.0\u0026rsquo; をローカルにチェックアウトします。","title":"Chef で OpenStack デプロイ"},{"content":"こんにちは。@jedipunkzです。\n第13回 OpenStack 勉強会に参加してきました。内容の濃い収穫のある勉強会でした。 参加してよかった。特にえぐちさんの OpenCenter に関するプレゼン (下記のスライド 参照のこと) には驚きました。ちょうど当日 RackSpace のエンジニアが管理している github 上の Chef Cookbooks を使って OpenStack 構築できたぁ！と感動していたのに、 その晩のうちに Chef を使って GUI で！ OpenStack が自動構築出来るだなんて\u0026hellip;。\nえぐちさんのスライド資料はこちら。\nhttp://www.slideshare.net/guchi_hiro/open-centeropenstack\n早速、私も手元で OpenCenter 使ってみました。えぐちさん、情報ありがとうございましたー。\n実は私 としては Chef 単体で OpenStack を構築したいのですが、OpenCenter がどう Cookbook や Roles, Environment を割り当てているのか知りたかったので、 OpenCenter を使って構築してみました。今回はその準備。今日は OpenCenter で皆さ んも OpenStack を構築できるようキャプチャ付きで方法を紹介しますが、次の機会に Chef 単体での OpenStack の構築方法を紹介出来ればいいなぁと思っています。\n構成 +---------------+---------------+---------------+---------------+-------------- public network |eth0 |eth0 |eth1 |eth1 | eth1 |10.200.10.11 |10.200.10.12 |10.200.10.13 |10.200.10.14 |10.200.10.15 +-------------+ +-------------+ +-------------+ +-------------+ +-------------+ | opencenter | | oc-chef | |oc-controller| | oc-compute01| | oc-compute02| +-------------+ +-------------+ +-------------+ +-------------+ +-------------+ |10.200.9.14 |10.200.9.15 | eth0 |eth0 +---------------+-------------- vm network  5台使ってみた 全てのノードでインターネットへの経路を public network に向ける VM が接続する br100 ブリッジは eth0 にバインドした VM からインターネットへのアクセスは oc-computeNN が NAT する oc-chef は Chef Server !  OpenCenter の構築 構築は\u0026hellip;\n% curl -s -L http://sh.opencenter.rackspace.com/install.sh | sudo bash -s - --role=server % curl -s -L http://sh.opencenter.rackspace.com/install.sh | \\  sudo bash -s - --role=dashboard --ip=10.200.10.11 これだけ！\nOpenCenter で管理するノードへエージェントをインストール oc-chef, oc-controller, oc-computeNN は全て OpenCenter で管理する必要があるの で OpenCeter エージェントをインストールします。\n% curl -s -L http://sh.opencenter.rackspace.com/install.sh | \\  sudo bash -s - --role=agent --ip=10.200.10.11 GUI へアクセス https://10.200.10.11 ブラウザで上記の URL にアクセスするとデフォルトアカウント情報 (ユーザ名 : admin, パスワード : password) でログイン出来ます。\n{% img /pix/oc01.png 600 %}\nこんな感じ。\nChef Server のインストール oc-chef をクリック -\u0026gt; \u0026lsquo;Install Chef Server\u0026rsquo; を選択\n暫くするとタスクがグリーンになり完了。\n{% img /pix/oc02.png 600 %}\nこうなります。この Chef Server 上の Cookbooks, Roles 等を利用して knife bootstrap しコントローラ・コンピュートノードを構築する仕組みです。\nNovaCluster の作成 NovaCluster という controller x n, compute x n のセットを意味するモノを作りま す。入力フォームが表示されるので下記の画像のように入力します。\n{% img /pix/oc03.png 600 %}\nNAT Exclusion CIDRs が DMZ, VM Network CIDR が Fixed Range だと理解すれば、他 は問題ないように思います。\n{% img /pix/oc04.png 600 %}\n結果、こうなります。\nコントローラノード構築 available nodes にあるノード \u0026lsquo;oc-controller\u0026rsquo; を \u0026lsquo;Infrastructure\u0026rsquo; へドラッグア ンドドロップします。\n{% img /pix/oc05.png 600 %}\nコンピュートノード構築 available nodes にあるノード \u0026lsquo;oc-computeNN\u0026rsquo; を \u0026lsquo;AZ Nova\u0026rsquo; へドラッグアンドドロッ プします。\n{% img /pix/oc07.png 600 %}\n2台ともドラッグアンドドロップすると、こうなります。\nHorizon へアクセスし操作 あとはいつものように OpenStack を操作するだけです。Horizon へは下記の URL でア クセス出来ます。admin ユーザのパスワードは先ほどの NovaCluster 作成の際に入力 したモノです。\nhttps://10.200.10.13 まとめ 完成された OpenStack を覗いたのですが monit が利用されていたり、Horizon が HTTPS 化されていたり、完成度が高かったです。さすが RackSpace 製という感じ。も う OpenStack 構築スクリプトを作成する必要がなくなったなぁと。Chef で構築するべ きです。\n内部で使われている Chef Cookbooks は下記の URL に同じものがありました。\nhttps://github.com/rcbops/chef-cookbooks\nこの Chef Cookbooks を単体で使って構築してみましたが同様に1コマンドで構築が出 来ました。(オール・イン・ワン構成しか試していません) 僕らとしては GUI は仕事で は操作しにくいのでこのCookbooks を利用した形で改修し使っていきたいなぁと感じて います。GUI であってもドラッグアンドドロップで元には戻せませんし、後々 Cookbook に手を入れ構成を変更するといったことも面倒になりそうです。\nとは言っても、ドラッグアンドドロップで構築できるなんて\u0026hellip; 。誰でも構築できる時 期にあるんですね。運用するには Cookbook を理解している必要がありそうですが、 RackSpace としては Private Cloud Service に運用もセットで付加して売っているの で問題ないのでしょう。\nちなみに Chef の情報は os-chef ノードの root ユーザホームディレクトリ配下で探 れます。\noc-chef# cd ~ oc-chef# knife cookbook list oc-chef# knife role list oc-chef# knife environment list oc-chef# knife environment show NovaCluster01 ","permalink":"https://jedipunkz.github.io/post/2013/07/02/chef-opencenter-openstack/","summary":"こんにちは。@jedipunkzです。\n第13回 OpenStack 勉強会に参加してきました。内容の濃い収穫のある勉強会でした。 参加してよかった。特にえぐちさんの OpenCenter に関するプレゼン (下記のスライド 参照のこと) には驚きました。ちょうど当日 RackSpace のエンジニアが管理している github 上の Chef Cookbooks を使って OpenStack 構築できたぁ！と感動していたのに、 その晩のうちに Chef を使って GUI で！ OpenStack が自動構築出来るだなんて\u0026hellip;。\nえぐちさんのスライド資料はこちら。\nhttp://www.slideshare.net/guchi_hiro/open-centeropenstack\n早速、私も手元で OpenCenter 使ってみました。えぐちさん、情報ありがとうございましたー。\n実は私 としては Chef 単体で OpenStack を構築したいのですが、OpenCenter がどう Cookbook や Roles, Environment を割り当てているのか知りたかったので、 OpenCenter を使って構築してみました。今回はその準備。今日は OpenCenter で皆さ んも OpenStack を構築できるようキャプチャ付きで方法を紹介しますが、次の機会に Chef 単体での OpenStack の構築方法を紹介出来ればいいなぁと思っています。\n構成 +---------------+---------------+---------------+---------------+-------------- public network |eth0 |eth0 |eth1 |eth1 | eth1 |10.200.10.11 |10.200.10.12 |10.200.10.13 |10.200.10.14 |10.200.10.15 +-------------+ +-------------+ +-------------+ +-------------+ +-------------+ | opencenter | | oc-chef | |oc-controller| | oc-compute01| | oc-compute02| +-------------+ +-------------+ +-------------+ +-------------+ +-------------+ |10.","title":"内部で Chef を使っている OpenCenter で OpenStack 構築"},{"content":"こんにちは。@jedipunkzです。\n自動化の基盤を導入するために色々調べているのですが、監視も自動化しなくちゃ！と いうことで Sensu を調べてたのですが Chef との相性バッチリな感じで、自分的にイ ケてるなと思いました。\n 公式サイト http://www.sonian.com/cloud-monitoring-sensu/ ドキュメント http://docs.sensuapp.org/0.9/index.html  開発元が予め Chef の Cookbook (正確にはラッパー Cookbook 開発のための Cookbook で Include して使う) を用意してくれていたり、インストールを容易にする ための Omnibus 形式のパッケージの提供だったり。Omnibus なのでインストールと共 に Sensu が推奨する Ruby 一式も一緒にインストールされます。Chef と同じですね。\n今回紹介したいのは、Chef で Sensu を構築・制御する方法です。\n+--------------+ +--------------+ | chef-server | | workstation | +--------------+ +--------------+ | | +----------------+ | +--------------+ | sensu-server | +--------------+ | +----------------+----------------+----------------+ | | | | +--------------+ +--------------+ +--------------+ +--------------+ | sensu-client | | sensu-client | | sensu-client | | sensu-client | ..\u0026gt; +--------------+ +--------------+ +--------------+ +--------------+ | service node | | service node | | service node | | service node | +--------------+ +--------------+ +--------------+ +--------------+ この構成の処理の流れとしては\u0026hellip;\nsensu-server, sensu-client の構築の流れ    workstation から cookbook, role, data_bag を chef-server へアップ    workstation から sensu-server を bootstrap で構築    workstation から sensu-client を boostrrap で構築    監視項目の追加・無効化の流れ   workstation から監視項目 data bag の chef-server へのアップ    sensu-server 上の chef-client が chef-server から data bag の取得    sensu-server に新たな(削除された)監視項目が追加    sensu-client が新たな(削除された)監視項目を検知し、監視開始    つまり\u0026hellip; 運用で必要な操作 (監視対象の追加・監視項目の追加・無効化)を workstation から knife を使って全て行えるっていうことです。しかも Chef も Sensu も API を持っているので自動化を形成するプログラムの開発も容易です。実際に API を叩くちょっとしたダッシュボードを Ruby on Rails で作ってみましたが簡単に出来 ました。\n今回はこの構成の構築と操作方法について書いていきます。予め私のほうで作っておい た sensu のための chef-repo を使って環境を作っていきます。\nhttps://github.com/jedipunkz/sensu-chef-repo\nこの chef-repo には\n Berksfile サンプルの監視項目 data bag SSL 鍵生成の仕組み (後に data bag に収める)  のみが入っています。公式の sensu-chef レポジトリ内にあったサンプルを利用して作っ ています。また、Berksfile 内で、これまた\n chef-redis (redis の cookbook) sensu-chef (公式の sensu cookbook, ラッパー cookbook 内で用いる) chef-monitor (ラッパー cookbook の例)  を取得するようにしています。それぞれ fork して私のレポジトリに置いています。動 く状態を保ちたかったためです。redis に関しては結構手を加えました。そのままでは 全く構築出来ない状態でしたので。chef-monitor は内部で sensu-chef (公式 cookbook) を Include しているラッパー cookbook です。公式 cookbook 内で例とし て挙げられていたモノです。こちらは手を加えていません。\nでは手順を\u0026hellip;\n(chef 環境の構築方法は割愛します)\nsensu-server のデプロイ sensu-chef-repo の取得を行います。ここからの操作は全て上図の workstation 上で の操作になります。\n% git clone https://github.com/jedipunkz/sensu-chef-repo SSL 鍵ペアを生成し data bag に投入します。\n% cd ~/sensu-chef-repo/data_bags/ssl % ./ssl_certs.sh generate % knife data bag create sensu % knife data bag from file sensu ./ssl.json サンプル監視項目 proc_cron.json を data bags に投入します。\n% cd ~/sensu-chef-repo/ % knife data bag create sensu_checks % knife data bag from file sensu_checks data_bags/sensu_checks/proc_cron.json proc_cron.json の内容は下記の通り\n{ \u0026#34;id\u0026#34;: \u0026#34;proc_cron\u0026#34;, \u0026#34;command\u0026#34;: \u0026#34;check-procs.rb -p cron -C 1\u0026#34;, \u0026#34;subscribers\u0026#34;: [ \u0026#34;sensu-client\u0026#34; ], \u0026#34;interval\u0026#34;: 10 } json の構成を説明すると..\n id : 監視項目名 command : agent が実行するコマンド subscribers : 監視対象のグループ名, chef role 名が自動で監視対象に割り当てられる interval : 監視間隔 (秒)  となります。\nBerkshelf を使って cookbook の取得を行います。\n% gem install berksfile --no-ri --no-rdoc % berks install --path ./cookbooks/ roles を chef server へアップロードします。\n% knife role from file roles/sensu-client.rb # 後の sensu-client デプロイのためついでに準備します % knife role from file roles/sensu-server.rb \u0026ldquo;master_address\u0026rdquo; を sensu-server の IP アドレスに書き換えます。書き換える箇所 は \u0026lsquo;monitor\u0026rsquo; cookbook の attributes です。\n% ${EDITOR} cookbooks/monitor/attributes/default.rb default[\u0026#34;monitor\u0026#34;][\u0026#34;master_address\u0026#34;] = \u0026#34;XXX.XXX.XXX.XXX\u0026#34; cookbooks を chef server へアップロードします。\nworkstation% knife cookbook upload -a sensu-server を knife を用いてブートストラップします。\n% knife bootstrap \u0026lt;server-ip\u0026gt; -N \u0026lt;server-name\u0026gt; -r \u0026#39;role[sensu-server]\u0026#39; -x root -i \u0026lt;secret-key\u0026gt; sensu ダッシュボード URL : http://:8080 にアクセスし動作確認, アカ ウント情報は下記の attributes に記載してあります。\ncookbooks/sensu/attributes/default.rb sensu-client デプロイ方法 knife を用いて sensu-client をデプロイします。\n% knife bootstrap \u0026lt;client-ip\u0026gt; -N \u0026lt;client-name\u0026gt; -r \u0026#39;role[sensu-client]\u0026#39; -x root -i \u0026lt;secret-key\u0026gt; この状態で先ほどの \u0026lsquo;proc_cron\u0026rsquo; が監視開始されます。\n監視項目の追加方法 下記のような json ファイルを生成します。ここでは例として nginx のプロセス監視 のための項目を追加してみます。\n% ${EDITOR} data_bags/sensu_checks/proc_nginx.json { \u0026#34;id\u0026#34;: \u0026#34;proc_nginx\u0026#34;, \u0026#34;command\u0026#34;: \u0026#34;check-procs.rb -p nginx -w 5 -c 10\u0026#34;, \u0026#34;subscribers\u0026#34;: [ \u0026#34;sensu-client\u0026#34; ], \u0026#34;interval\u0026#34;: 10 } 先ほども書きましたが subscribers は chef 的な role 名と一致しています。なので sensu の監視をグルーピングしたいときは role 名を変えて knife bootstrap すると 良いと思います。ここでは例として \u0026lsquo;sensu-client\u0026rsquo; という先ほど利用した role 名を 用います。\ndata bags に監視項目情報を投入します。\n% knife data bag from file sensu_checks data_bags/sensu_checks/proc_nginx.json 暫くすると下記のプロセスを経て監視が開始される\n sensu-server 上の chef-client が interval 間隔後実行 sensu-server に proc_nginx.json が配置 sensu-server が chef により再起動 sensu-client 上の sensu agent が自らの subscribers が属している proc_nginx.json を検知 sensu-client が nginx のプロセス監視開始  まとめ 監視対象追加 (agent 仕込み), 監視項目追加を workstation 上から knife を使って 操作出来ました。監視項目の削除についてはダミーの json (command 項に \u0026lsquo;echo ok\u0026rsquo; など設定) を投入することで、私は対処していますが、本来は data bag が存在しない ことを検知して sensu-server 上から監視項目を削除する cookbook に仕上げなければ いけないと思います。\n","permalink":"https://jedipunkz.github.io/post/2013/06/20/sensu-chef-controll/","summary":"こんにちは。@jedipunkzです。\n自動化の基盤を導入するために色々調べているのですが、監視も自動化しなくちゃ！と いうことで Sensu を調べてたのですが Chef との相性バッチリな感じで、自分的にイ ケてるなと思いました。\n 公式サイト http://www.sonian.com/cloud-monitoring-sensu/ ドキュメント http://docs.sensuapp.org/0.9/index.html  開発元が予め Chef の Cookbook (正確にはラッパー Cookbook 開発のための Cookbook で Include して使う) を用意してくれていたり、インストールを容易にする ための Omnibus 形式のパッケージの提供だったり。Omnibus なのでインストールと共 に Sensu が推奨する Ruby 一式も一緒にインストールされます。Chef と同じですね。\n今回紹介したいのは、Chef で Sensu を構築・制御する方法です。\n+--------------+ +--------------+ | chef-server | | workstation | +--------------+ +--------------+ | | +----------------+ | +--------------+ | sensu-server | +--------------+ | +----------------+----------------+----------------+ | | | | +--------------+ +--------------+ +--------------+ +--------------+ | sensu-client | | sensu-client | | sensu-client | | sensu-client | .","title":"Sensu 監視システムを Chef で制御"},{"content":"こんにちは。@jedipunkzです。\nrequire \u0026lsquo;chef\u0026rsquo; して Ruby コードの中で chef を利用したいと思って色々調べていた のですが、そもそもリファレンスが無くサンプルコードもごくわずかしかネット上に見 つけられない状態でした。結局ソースコードを読んで理解していく世界なわけですが、 サンプルコードが幾つかあると他の人に役立つかなぁと思い、ブログに載せていこうか なぁと。\nまず Chef サーバへアクセスするためには下記の情報が必要です。\n ユーザ名 ユーザ用のクライアント鍵 Chef サーバの URL  これらは Chef::Config で記していきます。\nでは早速サンプルコードです。まずは data bags 内データの一覧を取得するコードで す。data bags 内のデータを全で取得し配列で表示します。\n#!/usr/bin/env ruby require \u0026#39;rubygems\u0026#39; require \u0026#39;chef/rest\u0026#39; require \u0026#39;chef/search/query\u0026#39; Chef::Config[:node_name]=\u0026#39;user01\u0026#39; Chef::Config[:client_key]=\u0026#39;/home/user01/user01.pem\u0026#39; Chef::Config[:chef_server_url]=\u0026#34;https://10.200.9.22\u0026#34; Chef::DataBag::list.each do |bag_name, url| Chef::DataBag::load(bag_name).each do |item_name, url| item = Chef::DataBagItem.load(bag_name, item_name).to_hash puts item end end 次は data bags にデータを入力するコードです。json_data という JSON 形式のデー タを test_data という data bag に放り込んでいます。\n#!/usr/bin/env ruby require \u0026#39;rubygems\u0026#39; require \u0026#39;chef/rest\u0026#39; require \u0026#39;chef/search/query\u0026#39; Chef::Config[:node_name]=\u0026#39;user01\u0026#39; Chef::Config[:client_key]=\u0026#39;/home/user01/user01.pem\u0026#39; Chef::Config[:chef_server_url]=\u0026#34;https://10.0.0.10\u0026#34; json_data = { \u0026#34;id\u0026#34; =\u0026gt; \u0026#34;test\u0026#34;, \u0026#34;command\u0026#34; =\u0026gt; \u0026#34;echo test\u0026#34; } databag_item = Chef::DataBagItem.new databag_item.data_bag(\u0026#39;test_data\u0026#39;) databag_item.raw_data = proc_nginx databag_item.save 次は nodes 一覧の取得です。\n#!/usr/bin/env ruby require \u0026#39;rubygems\u0026#39; require \u0026#39;chef/rest\u0026#39; require \u0026#39;chef/search/query\u0026#39; Chef::Config[:node_name]=\u0026#39;user01\u0026#39; Chef::Config[:client_key]=\u0026#39;/home/user01/user01.pem\u0026#39; Chef::Config[:chef_server_url]=\u0026#34;https://10.0.0.10\u0026#34; Chef::Node.list.each do |node| puts node end 次は bootstrap するコード。\n#!/usr/bin/env ruby require \u0026#39;rubygems\u0026#39; require \u0026#34;chef\u0026#34; require \u0026#34;chef/knife/core/bootstrap_context\u0026#34; require \u0026#39;chef/knife\u0026#39; require \u0026#39;chef/knife/ssh\u0026#39; require \u0026#39;net/ssh\u0026#39; require \u0026#39;net/ssh/multi\u0026#39; require \u0026#39;chef/knife/bootstrap\u0026#39; Chef::Config[:node_name]=\u0026#39;user01\u0026#39; Chef::Config[:client_key]=\u0026#39;/home/user01/user01.pem\u0026#39; Chef::Config[:validation_key]=\u0026#39;/home/user01/chef-validator.pem\u0026#39; Chef::Config[:chef_server_url]=\u0026#34;https://10.0.0.10\u0026#34; kb = Chef::Knife::Bootstrap.new kb.name_args = [\u0026#34;sensu-client04.deathstar.jp\u0026#34;, \u0026#34;10.0.0.20\u0026#34;] kb.config[:ssh_user] = \u0026#34;root\u0026#34; kb.config[:identity_file] = \u0026#34;~/novakey01\u0026#34; kb.config[:ssh_port] = \u0026#34;22\u0026#34; kb.config[:run_list ] = \u0026#34;role[sensu-client]\u0026#34; kb.config[:template_file] = \u0026#34;/home/thirai/chef-full.erb\u0026#34; kb.run 以上です。他のサンプルもこれから探していこうかと思ってます。knife のソース見る のが一番はやいかなぁと。もしくは Chef のテストコード見るか。皆さんもご存知であ れば共有してくださーい。\n","permalink":"https://jedipunkz.github.io/post/2013/06/12/chef-ruby-code/","summary":"こんにちは。@jedipunkzです。\nrequire \u0026lsquo;chef\u0026rsquo; して Ruby コードの中で chef を利用したいと思って色々調べていた のですが、そもそもリファレンスが無くサンプルコードもごくわずかしかネット上に見 つけられない状態でした。結局ソースコードを読んで理解していく世界なわけですが、 サンプルコードが幾つかあると他の人に役立つかなぁと思い、ブログに載せていこうか なぁと。\nまず Chef サーバへアクセスするためには下記の情報が必要です。\n ユーザ名 ユーザ用のクライアント鍵 Chef サーバの URL  これらは Chef::Config で記していきます。\nでは早速サンプルコードです。まずは data bags 内データの一覧を取得するコードで す。data bags 内のデータを全で取得し配列で表示します。\n#!/usr/bin/env ruby require \u0026#39;rubygems\u0026#39; require \u0026#39;chef/rest\u0026#39; require \u0026#39;chef/search/query\u0026#39; Chef::Config[:node_name]=\u0026#39;user01\u0026#39; Chef::Config[:client_key]=\u0026#39;/home/user01/user01.pem\u0026#39; Chef::Config[:chef_server_url]=\u0026#34;https://10.200.9.22\u0026#34; Chef::DataBag::list.each do |bag_name, url| Chef::DataBag::load(bag_name).each do |item_name, url| item = Chef::DataBagItem.load(bag_name, item_name).to_hash puts item end end 次は data bags にデータを入力するコードです。json_data という JSON 形式のデー タを test_data という data bag に放り込んでいます。","title":"Chef を Ruby コード内で利用する"},{"content":"こんにちは。@jedipunkzです。\nCeph を運用する上で考慮しなければいけないのがトラフィックの負荷です。特に OSD 同士のレプリケーション・ハートビートには相当トラフィックの負荷が掛かることが想 像出来ます。\nこのため MDS, MON の通信に影響を与えないよう、OSD レプリケーション・ハートビー トのためのネットワークを別に設けるのがベストプラクティスな構成の様です。このネッ トワークのことをクラスターネットワークと Ceph 的に言うそうです。\nこんな接続になります。\n +------+ |Client| +------+ | +-------+-------+-------+-------+------ public network | | | | | +-----+ +-----+ +-----+ +-----+ +-----+ | MON | | MDS | | OSD | | OSD | | OSD | +-----+ +-----+ +-----+ +-----+ +-----+ | | | ----------------+-------+-------+------ cluster network  上図の様に MON, MDS は public ネットワークを介し OSD のレプリケーション・ハー トビートのみ cluster ネットワークを介します。Client と MDS との通信に影響を与 えない構成になります。\n今回はその様な構成を ceph-deploy を使って構築する方法を書いていきます。前提と なるホストとプロセスとネットワークの関係は下記の図の通りです。\n+----------+ | 'client' | +----------+ | +---------------+---------------+------------- public network | | | |10.0.0.11 | 10.0.0.12 | 10.0.0.13 +----------+ +----------+ +----------+ | 'ceph01' | | 'ceph02' | | 'ceph03' | | osd | | osd | | osd | | mon | | mon | | mon | | mds | | mds | | mds | +----------+ +----------+ +----------+ |172.18.0.11 | 172.18.0.12 | 172.18.0.13 | | | +---------------+---------------+------------- cluster network  特徴としては..\n ceph01-03 には NIC を2本出します。 ceph01-03 の全てに MDS, MON, OSD を稼働させます。 ceph01-03:/dev/sdb を Ceph 用のディスクとして利用  となります。\nCeph-Deply を利用するまでの準備 今回は ceph-deploy を利用し Ceph を構築する。そのための準備として下記の操作を 行う。\nceph サーバ (ceph01-03) の準備 \u0026lsquo;ceph\u0026rsquo; ユーザの作成を行う。\n% ssh user@ceph-server % sudo useradd -d /home/ceph -m ceph % sudo passwd ceph  sudoers の設定を行う。\n% echo \u0026quot;ceph ALL = (root) NOPASSWD:ALL\u0026quot; | sudo tee /etc/sudoers.d/ceph % sudo chmod 0440 /etc/sudoers.d/ceph  \u0026lsquo;client\u0026rsquo; の準備 ホスト \u0026lsquo;client\u0026rsquo; で準備をします。この準備によって ceph-deploy をそれぞれのホス トに対して実行できるようになります。\nまず、ノンパスフレーズの SSH 公開鍵・秘密鍵を生成します。\nclient% ssh-keygen Generating public/private key pair. Enter file in which to save the key (/ceph-client/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /ceph-client/.ssh/id_rsa. Your public key has been saved in /ceph-client/.ssh/id_rsa.pub.  公開鍵をターゲットホスト (ceph01-03) に配置します。\nclient% ssh-copy-id ceph@ceph01 client% ssh-copy-id ceph@ceph02 client% ssh-copy-id ceph@ceph03  ceph-deploy を取得します。\nclient% git clone https://github.com/ceph/ceph-deploy.git ~/ceph-deploy  \u0026lsquo;python-virtualenv\u0026rsquo; パッケージをインストールする。\nclient% sudo apt-get update ; sudo apt-get -y install python-virtualenv  ceph-deploy をブートストラップする\nclient% cd ~/ceph-deploy client% ./bootstrap  PATH を通す。下記は例。\nclient% ${EDITOR} ~/.zshrc export PATH=$HOME/ceph-deploy:$PATH  ホスト名の解決を行う。\ncephclient% sudo ${EDITOR} /etc/hosts 10.0.0.11 ceph01 10.0.0.12 ceph02 10.0.0.13 ceph03  これで準備は OK です。ceph-deploy が使える状態になりました。\nCeph 構築手順 今回は ceph01-03 の3台構成を構築しますが、すべての操作はホスト \u0026lsquo;client\u0026rsquo; から行 います。先ほど配置した公開鍵によってそれぞれのホストに対して操作が行えます。\nceph サーバ・クライアント間通信のための鍵の生成とコンフィギュレーションの生成 を下記の操作にて行う。\nclient% ceph-deploy new ceph01 ceph02 ceph03  上記の操作で生成されたカレントディレクトリ上の ceph.conf に対して下記の記述を 追記する。\npublic network = 10.0.0.0/24 cluster network = 172.18.0.0/24 [mon.a] host = ceph01 mon addr = 10.0.0.11:6789 [mon.b] host = ceph02 mon addr = 10.0.0.12:6789 [mon.c] host = ceph03 mon addr = 10.0.0.13:6789 [osd.0] public addr = 10.0.0.11 cluster addr = 172.18.0.11 [osd.1 public addr = 10.0.0.12 cluster addr = 172.18.0.12 [osd.2] public addr = 10.0.0.13 cluster addr = 172.18.0.13 [mds.a] host = ceph01 [mds.a] host = ceph02 [mds.a] host = ceph03  次の操作でそれぞれのホストに対して ceph の公開しているレポジトリを参照させ ceph をインストールしていきます。\ncephclient% ceph-deploy install ceph01 ceph02 ceph03  MON daemon のデプロイを行う。\ncephclient% ceph-deploy mon create ceph01 ceph02 ceph03  鍵のデプロイを行う。Ceph サーバ間・クライアント間での共有鍵です。1 Cluster に対して1つの鍵を保有することになります。\ncephclient% ceph-deploy gatherkeys ceph01 ceph02 ceph03  OSD daemon のデプロイを行う。下記の様にパーティションを指定しなければツールが 自動でパーティショニングを行なってくれます。\ncephclient% ceph-deploy osd create create ceph01:/dev/sdb ceph02:/dev/sdb ceph03:/dev/sdb  MDS deamon のデプロイを行う。\ncephcleint% ceph-deploy mds create ceph01 ceph02 ceph03  完成です。\n全てのホスト ceph01-03 にて MON, MDS, OSD のプロセスが稼働しているのが確認出来 ると思います。実際にどれかのホストの MDS に対して client から ceph ストレージ をマウントしてみてください。\nまとめ 通常のフラットなネットワーク上にデプロイする方法とほぼ同じ操作で構築出来ます。 異なるところは ceph.conf に対して設定を追加した点です。また、それぞれのホスト への ceph のインストールの時にオプションが渡せます。\u0026ndash;testing と渡せば RC 版の ceph が利用できます。今回の様に何も記さなければ stable 版の利用ということにな ります。\n","permalink":"https://jedipunkz.github.io/post/2013/05/25/ceph-cluster-network/","summary":"こんにちは。@jedipunkzです。\nCeph を運用する上で考慮しなければいけないのがトラフィックの負荷です。特に OSD 同士のレプリケーション・ハートビートには相当トラフィックの負荷が掛かることが想 像出来ます。\nこのため MDS, MON の通信に影響を与えないよう、OSD レプリケーション・ハートビー トのためのネットワークを別に設けるのがベストプラクティスな構成の様です。このネッ トワークのことをクラスターネットワークと Ceph 的に言うそうです。\nこんな接続になります。\n +------+ |Client| +------+ | +-------+-------+-------+-------+------ public network | | | | | +-----+ +-----+ +-----+ +-----+ +-----+ | MON | | MDS | | OSD | | OSD | | OSD | +-----+ +-----+ +-----+ +-----+ +-----+ | | | ----------------+-------+-------+------ cluster network  上図の様に MON, MDS は public ネットワークを介し OSD のレプリケーション・ハー トビートのみ cluster ネットワークを介します。Client と MDS との通信に影響を与 えない構成になります。","title":"Ceph クラスターネットワーク構成"},{"content":"こんにちは。最近 OpenStack の導入に向けて保守性や可用性について調査している @jedipunkzです。\nOpenStack は MySQL のダンプや OS イメージ・スナップショットのバックアップをとっ ておけばコントローラの復旧も出来ますし、Grizzly 版の Quantum では冗長や分散が 取れるので障害時に耐えられます。また Quantum の復旧は手動もで可能です。最後の 悩みだった Cinder の接続先ストレージですが、OpenStack のスタンスとしては高価な ストレージの機能を使ってバックアップ取るか、Ceph, SheepDog のようなオープンソー スを使うか、でした。で、今回は Ceph を OpenStack に連携させようと思いました。\nこの作業により Cinder の接続先ストレージが Ceph になるのと Glance の OS イメー ジ・スナップショットの保管先が Ceph になります。\n下記の参考資料が完成度高く、ほぼ内容はそのままです。若干付け足していますが。\n参考資料 http://ceph.com/docs/master/rbd/rbd-openstack/\n前提の構成 +-------------+-------------+--------------------------------------------- Public/API Network | | | +-----------+ +-----------+ +-----------+ +-----------+ +-----------+ +-----------+ | | | | |vm|vm|.. | | | | | | | | controller| | network | +-----------+ | ceph01 | | ceph01 | | ceph01 | | | | | | compute | | | | | | | | | | | | | | | | | | | +-----------+ +-----------+ +-----------+ +-----------+ +-----------+ +-----------+ | | | | | | | | +-------------+-----)-------+-----)-------+-------------+-------------+-- Management/API Network | | +-------------+-----------------------------------+-- Data Network   Ceph は OpenStack の Management Network 上に配置 Ceph は3台構成 (何台でも可) OpenStack も3台構成 (何台でも可) 連携処理するのは controller, compute ノード  では早速手順ですが、OpenStack と Ceph の構築手順は割愛します。私の他の記事を参 考にしていただければと思います。\n 構築スクリプト ceph-deploy で Ceph 構築  Ceph + OpenStack 連携手順 OpenStack 用に Ceph Pool を作成する ceph01% sudo ceph pool create volumes 128 ceph01% sudo ceph pool create images 128  sudoers の設定 controller, compute ノードにて sudoers の設定\njedipunkz ALL = (root) NOPASSWD:ALL  ceph パッケージのインストール controller, compute ノードに ceph をインストールする。\ncontroller% wget -q -O- 'https://ceph.com/git/?p=ceph.git;a=blob_plain;f=keys/release.asc' | sudo apt-key add - controller% echo deb http://ceph.com/debian/ $(lsb_release -sc) main | sudo tee /etc/apt/sources.list.d/ceph.list controller% sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y python-ceph ceph-common  /etc/ceph 作成 controller% sudo mkdir /etc/ceph compute % sudo mkdir /etc/ceph  ceph コンフィギュレーションのコピー controller, compute ノードに ceph コンフィギュレーションをコピーする。尚、接続 先の OpenStack ノードでの sudoers 設定は予め済ませること。\nceph01% sudo -i ceph01# ssh \u0026lt;controller\u0026gt; sudo tee /etc/ceph/ceph.conf \u0026lt;/etc/ceph/ceph.conf ceph01# ssh \u0026lt;compute\u0026gt; sudo tee /etc/ceph/ceph.conf \u0026lt;/etc/ceph/ceph.conf  認証設定 nova, cinder, glance 用にユーザを作成する。\nceph01% sudo ceph auth get-or-create client.volumes mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool=volumes, allow rx pool=images' ceph01% sudo ceph auth get-or-create client.images mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool=images'  キーリングの作成 Ceph キーリングの作成を行う。Glance, Cinder が起動しているホスト controller ノードに キーリングを配置する。\nceph01% sudo ceph auth get-or-create client.images | ssh {your-glance-api-server} sudo tee /etc/ceph/ceph.client.images.keyring ceph01% ssh {your-glance-api-server} sudo chown glance:glance /etc/ceph/ceph.client.images.keyring ceph01% sudo ceph auth get-or-create client.volumes | ssh {your-volume-server} sudo tee /etc/ceph/ceph.client.volumes.keyring ceph01% ssh {your-volume-server} sudo chown cinder:cinder /etc/ceph/ceph.client.volumes.keyring  compute ノードにて libvirt に secret key を格納する。ここで登場する uuid は後 に利用するためメモをとっておくこと。\nceph01 % sudo ceph auth get-key client.volumes | ssh 10.200.10.59 tee client.volumes.key compute% cat \u0026gt; secret.xml \u0026lt;\u0026lt;EOF \u0026lt;secret ephemeral='no' private='no'\u0026gt; \u0026lt;usage type='ceph'\u0026gt; \u0026lt;name\u0026gt;client.volumes secret\u0026lt;/name\u0026gt; \u0026lt;/usage\u0026gt; \u0026lt;/secret\u0026gt; EOF comupte% sudo virsh secret-define --file secret.xml \u0026lt;uuid of secret is output here\u0026gt; compute% sudo virsh secret-set-value --secret {uuid of secret} --base64 $(cat client.volumes.key) \u0026amp;\u0026amp; rm client.volumes.key secret.xml  OpenStack 連携のための設定 controller:/etc/glance/glance-api.conf に下記を追記。\ndefault_store=rbd rbd_store_user=images rbd_store_pool=images show_image_direct_url=True  controller:/etc/cinder/cinder.conf に下記を追記。先ほど登場した uuid を入力す る。\nvolume_driver=cinder.volume.driver.RBDDriver rbd_pool=volumes rbd_user=volumes rbd_secret_uuid={uuid of secret}  controller:/etc/init/cinder-volume.conf の冒頭に下記の記述を追記する。\nenv CEPH_ARGS=\u0026quot;--id volumes\u0026quot;  OpenStack の各サービスを再起動もしくはホストの再起動を行う。\nsudo service glance-api restart sudo service nova-compute restart sudo service cinder-volume restart  確認 実際にインスタンスを作成して Volume をアタッチしディスクを消費していくと Ceph のディスク使用量が増えていきます。\n% cinder create --display-name test 5 % nova volumeattach \u0026lt;instance_id\u0026gt; \u0026lt;volume_id\u0026gt; auto  まとめ Cinder は分散ストレージですので各ファイルのレプリカが全て失われない限りデータ はロストしません。ただし Ceph 自体の完成度は以前に比べ高くはなったものの、運用 に耐えられるかどうかまだ私にも分かりません。先日の OpenStack Day に来日してい たファウンデーションの方が「ベンダロックインするな」と言っていました。僕もオー プンソースでなんとかしたいと思っています。OpenStack を導入するためには今、Ceph は欠かすことが出来ないコンポーネントな気がしています。皆で Ceph も盛り上げて行 きたいです。\nまた、この構成の際のOpenStack 全体の保全について考えると\u0026hellip;\n MySQL のデータさえダンプの取得すれば OK OS イメージ・スナップショットは Ceph 上にあるのでバックアップ不要 Ceph はなんとしても守る。バックアップ取るのは難しい Network ノードは分散・冗長可能, データのバックアップは不要 Compute ノード上のインスタンスデータは Ceph のスナップショットから復旧  といったことが考えられます。つまり MySQL のデータさえダンプしておけば OpenStack 全体が復旧できることになります。実際にやってみましたが可能でした。\n","permalink":"https://jedipunkz.github.io/post/2013/05/19/openstack-ceph/","summary":"こんにちは。最近 OpenStack の導入に向けて保守性や可用性について調査している @jedipunkzです。\nOpenStack は MySQL のダンプや OS イメージ・スナップショットのバックアップをとっ ておけばコントローラの復旧も出来ますし、Grizzly 版の Quantum では冗長や分散が 取れるので障害時に耐えられます。また Quantum の復旧は手動もで可能です。最後の 悩みだった Cinder の接続先ストレージですが、OpenStack のスタンスとしては高価な ストレージの機能を使ってバックアップ取るか、Ceph, SheepDog のようなオープンソー スを使うか、でした。で、今回は Ceph を OpenStack に連携させようと思いました。\nこの作業により Cinder の接続先ストレージが Ceph になるのと Glance の OS イメー ジ・スナップショットの保管先が Ceph になります。\n下記の参考資料が完成度高く、ほぼ内容はそのままです。若干付け足していますが。\n参考資料 http://ceph.com/docs/master/rbd/rbd-openstack/\n前提の構成 +-------------+-------------+--------------------------------------------- Public/API Network | | | +-----------+ +-----------+ +-----------+ +-----------+ +-----------+ +-----------+ | | | | |vm|vm|.. | | | | | | | | controller| | network | +-----------+ | ceph01 | | ceph01 | | ceph01 | | | | | | compute | | | | | | | | | | | | | | | | | | | +-----------+ +-----------+ +-----------+ +-----------+ +-----------+ +-----------+ | | | | | | | | +-------------+-----)-------+-----)-------+-------------+-------------+-- Management/API Network | | +-------------+-----------------------------------+-- Data Network   Ceph は OpenStack の Management Network 上に配置 Ceph は3台構成 (何台でも可) OpenStack も3台構成 (何台でも可) 連携処理するのは controller, compute ノード  では早速手順ですが、OpenStack と Ceph の構築手順は割愛します。私の他の記事を参 考にしていただければと思います。","title":"OpenStack + Ceph 連携"},{"content":"こんにちは。@jedipunkzです。 今回は Opscode Chef でユーザ・グループを作成する方法をまとめます。\n\u0026lsquo;users\u0026rsquo; Cookbook を使います。\n% cd ${YOUR_CHEF_REPO} % ${EDITOR} Berksfile cookbook 'users' % berks install --path ./cookbooks  data_bag を使ってユーザ・グループの管理をしたいので管理ディレクトリを作成しま す。\n% mkdir -p data_bags/users  data_bags/users/jedipunkz.json ファイルを作成します。必要に応じて内容を書き換えてください。\n{ \u0026quot;id\u0026quot;: \u0026quot;jedipunkz\u0026quot;, \u0026quot;ssh_keys\u0026quot;: \u0026quot;ssh-rsa AAAABx92tstses jedipunkz@somewhere\u0026quot;, \u0026quot;groups\u0026quot;: [ \u0026quot;sysadmin\u0026quot;, \u0026quot;sudo\u0026quot; ], \u0026quot;uid\u0026quot;: 2001, \u0026quot;shell\u0026quot;: \u0026quot;\\/usr\\/bin\\/zsh\u0026quot;, \u0026quot;comment\u0026quot;: \u0026quot;jedipunkz sysadmin\u0026quot;, \u0026quot;password\u0026quot;: \u0026quot;$1$s%H8BMHlB$7s3h30y9IB1SklftZXYhvssJ\u0026quot; }  json ファイルの説明です。\n id : ユーザ名 ssh_keys : SSH 公開鍵 groups : 所属させるグループ uid : unix id sheell : ログインシェル comment : コメント passwd : ハッシュ化したパスワード  特にハッシュ化したパスワードは下記のコマンドで生成出来ます。\n% openssl passwd -1 'yourPassword'  data_bag を作成し json ファイルを読み込みます。\n% knife data bag create users % knife data bag from file users data_bags/users/jedipunkz.json  現在 (2013/05/18 現在) 、\u0026lsquo;users\u0026rsquo; Cookbook に不具合があるらしく groups に記した グループにユーザが所属してくれませんでした。なので下記の対処をします。 sysadmins.rb を今回は利用します。このファイルに下記の行を追記します。僕は sudo グループに所属させたかったので (先ほど groups: に記した) こうしましたが、他の グループが良ければ変更してください。また、Ubuntu Server を扱うことがメインの僕 なので group_id は 27 にしています。適宜変更してください。\n% ${EDITOR} cookbooks/users/recipes/sysadmins.rb # 下記の行を追記 users_manage \u0026quot;sudo\u0026quot; do group_id 27 end  cookbook を Chef サーバにアップロードします。\n% knife cookbook upload users  適用したいノードの run_list に Recipe \u0026lsquo;users::sysadmins\u0026rsquo; を追加します。\n% knife node run_list add ${YOUR_NODE_NAME} users::sysadmins  chef-client の次回実行時にユーザ \u0026lsquo;jedipunkz\u0026rsquo; が作成されているはずです。SSH で ログインして確認してみてください。待ちきれなかったら knife ssh して chef-client を実行してください。\nこの \u0026lsquo;users\u0026rsquo; cookbook は他の Cookbook からも呼び出して利用することが出来るので 応用が利きますね。\n","permalink":"https://jedipunkz.github.io/post/2013/05/18/chef-cookbook-adding-users/","summary":"こんにちは。@jedipunkzです。 今回は Opscode Chef でユーザ・グループを作成する方法をまとめます。\n\u0026lsquo;users\u0026rsquo; Cookbook を使います。\n% cd ${YOUR_CHEF_REPO} % ${EDITOR} Berksfile cookbook 'users' % berks install --path ./cookbooks  data_bag を使ってユーザ・グループの管理をしたいので管理ディレクトリを作成しま す。\n% mkdir -p data_bags/users  data_bags/users/jedipunkz.json ファイルを作成します。必要に応じて内容を書き換えてください。\n{ \u0026quot;id\u0026quot;: \u0026quot;jedipunkz\u0026quot;, \u0026quot;ssh_keys\u0026quot;: \u0026quot;ssh-rsa AAAABx92tstses jedipunkz@somewhere\u0026quot;, \u0026quot;groups\u0026quot;: [ \u0026quot;sysadmin\u0026quot;, \u0026quot;sudo\u0026quot; ], \u0026quot;uid\u0026quot;: 2001, \u0026quot;shell\u0026quot;: \u0026quot;\\/usr\\/bin\\/zsh\u0026quot;, \u0026quot;comment\u0026quot;: \u0026quot;jedipunkz sysadmin\u0026quot;, \u0026quot;password\u0026quot;: \u0026quot;$1$s%H8BMHlB$7s3h30y9IB1SklftZXYhvssJ\u0026quot; }  json ファイルの説明です。\n id : ユーザ名 ssh_keys : SSH 公開鍵 groups : 所属させるグループ uid : unix id sheell : ログインシェル comment : コメント passwd : ハッシュ化したパスワード  特にハッシュ化したパスワードは下記のコマンドで生成出来ます。","title":"Chef Cookbook でユーザ・グループ追加"},{"content":"今回は ceph-deploy というツールを使って Ceph ストレージを簡単に構築することが 出来るので紹介します。Ceph は分散ストレージでオブジェクトストレージとしてもブ ロックストレージとしても動作します。今回の構築ではブロックストレージとしてのみ の動作です。\nCeph が公開しているのが ceph-deploy なわけですが、マニュアル操作に代わる構築方 法として公開しているようです。その他にも Chef Cookbook も公開されているようで す。\nそれでは早速。\n今回の構成 +--------+ +--------+ +--------+ | ceph01 | | ceph02 | | ceph03 | | osd | | osd | | osd | | mon | | mon | | mon | | mds | | mds | | mds | +--------+ +--------+ +--------+ | 10.0.0.1 | 10.0.0.2 | 10.0.0.3 | | | +----------+----------+ | | 10.0.0.10 +-------------+ | workstation | +-------------+  特徴は\n すべてのホストで osd, mon, mds を動作 ceph データ格納用ディスクデバイスを /dev/sdb として利用 workstation は ceph-deploy を実行するホスト  です。osd は object store daemon で実際にファイルを格納していくデーモン。mon はモニタリング用デーモン, mds は metadata server で POSIX 互換のファイルシステ ムをクライアントに提供するためのデーモンです。\nceph-deploy を使うまでの準備 ceph-deploy を使うまでのターゲットのホスト ceph01-03 と workstation と共に準備 が必要です。\nceph01-03 の準備 \u0026lsquo;ceph\u0026rsquo; ユーザの作成を行う。\n% ssh user@ceph-server % sudo useradd -d /home/ceph -m ceph % sudo passwd ceph  sudoers の設定を行う。\n% echo \u0026quot;ceph ALL = (root) NOPASSWD:ALL\u0026quot; | sudo tee /etc/sudoers.d/ceph % sudo chmod 0440 /etc/sudoers.d/ceph  workstation の準備 ノンパスフレーズの SSH 公開鍵・秘密鍵を生成する。\nworkstation% ssh-keygen Generating public/private key pair. Enter file in which to save the key (/ceph-client/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /ceph-client/.ssh/id_rsa. Your public key has been saved in /ceph-client/.ssh/id_rsa.pub.  公開鍵をターゲットホスト (ceph01-03) に配置\nworkstation% ssh-copy-id ceph@ceph01 workstation% ssh-copy-id ceph@ceph02 workstation% ssh-copy-id ceph@ceph03  ceph-deploy の取得を行う。\nworkstation% git clone https://github.com/ceph/ceph-deploy.git ~/ceph-deploy  \u0026lsquo;python-virtualenv\u0026rsquo; パッケージをインストールする。\nworkstation% sudo apt-get update ; sudo apt-get -y install python-virtualenv  ceph-deploy をブートストラップする\nworkstation% cd ~/ceph-deploy workstation% ./bootstrap  PATH を通す。自分の shell に合わせて登録してください。\nworkstation% ${EDITOR} ~/.zshrc export PATH=$HOME/ceph-deploy:$PATH  ホスト名の解決を行う。\nworkstation% sudo ${EDITOR} /etc/hosts 10.0.0.1 ceph01 10.0.0.2 ceph02 10.0.0.3 ceph03  これで準備は終わり。\n3台構成構築 3台 (ceph01-03) を新規に構築する方法を書きます。すべて workstaiton 上からの操 作です。\nceph サーバ・クライアント間通信のための鍵の生成とコンフィギュレーションの生成 を下記の操作で行う。\nworkstation% ceph-deploy new ceph01 ceph02 ceph03  下記の操作で ceph パッケージのインストールを各 Ceph サーバにて行う。\u0026ndash;testing 等と引数を渡せば RC 版の利用が行える。何も渡さなければ stable 版。\nworkstation% ceph-deploy install ceph01 ceph02 ceph03  MON daemon のデプロイを行う。\nworkstation% ceph-deploy mon create ceph01 ceph02 ceph03  鍵のデプロイを行う。Ceph サーバ間・クライアント間での共有鍵である。1 Cluster に対して1つの鍵を保有する。\nworkstation% ceph-deploy gatherkeys create ceph01 ceph02 ceph03  OSD daemon のデプロイを行う。下記の様にパーティションを指定しなければツールが 自動でパーティショニングを行なってくれる。\nworkstation% ceph-deploy osd create create ceph01:/dev/sdb ceph02:/dev/sdb ceph03:/dev/sdb  MDS deamon のデプロイを行う。\ncephcleint% ceph-deploy mds create ceph01 ceph02 ceph03  これで終わりです。これらの操作が終わるとすべてのホスト ceph01-03 で mon, osd, mds の各デーモンが起動していることが分かると思います。超カンタン！\nマウントしてみよう！ さぁ～、クライアントからマウントしてみましょう。ここでは workstaion ホストを利 用します。Linux 系のマシンで同じネットワークセグメントに属していれば大抵マウン ト出来ると思います。mds が稼働しているホストに対してであればどこにでもマウント 出来ます。\nBlock Device としてマウントする方法 ストレージ上に block device を生成しそれをマウントする。\nworkstation% rbd create foo --size 4096 workstation% sudo modprobe rbd workstation% sudo rbd map foo --pool rbd --name client.admin workstation% sudo mkfs.ext4 -m0 /dev/rbd/rbd/foo workstation% sudo mkdir /mnt/myrbd workstation% sudo mount /dev/rbd/rbd/foo /mnt/myrbd  Kernel Driver を用いてマウントする方法 kernel Driver を用いてストレージをマウントする。\nworkstation% sudo mkdir /mnt/mycephfs workstation% sudo mount -t ceph 10.0.0.1:6789:/ /mnt/mycephfs -o \\ name=admin,secret=`sudo ceph-authtool -p /etc/ceph/ceph.keyring`  Fuse Driver (ユーザランド) を用いてマウントする方法 ユーザランドソフトウェア FUSE を用いてマウントする。\nworkstation% sudo mkdir /home/\u0026lt;username\u0026gt;/cephfs workstation% sudo ceph-fuse -m 10.0.0.1:6789 /home/\u0026lt;username\u0026gt;/cephfs  まとめ もし導入するのであればマニュアルでの構築も一度体験した方が良いかもしれません。 ツールを使うと一体どんな作業がされているのか理解出来ないので。ただ今ではマニュ アル操作で構築している途中に \u0026lsquo;ceph-deploy を使ってください\u0026rsquo; と warning が出る ので、開発元としてもこちらの構築方法を薦めたいのでしょう。あと Ceph はドキュメ ントが非常に充実しています。ドキュメントの全てに大事なことが書いてあるので一度 読むことをオススメします。また Ceph が Chef Cookbook も公開しているようで、そ ちらの方法もドキュメントにチラっと書いてありました。私はまだ試していませんが時 間があればやってみたいです。あとあと！ceph-deploy はまだ未完成な域を脱していま せん。上記の通り新規構築系の操作はひと通り出来るのですが、ホストの削除系の実装 がまだされていませんでした。ホスト追加系の操作に関しても削除系程ではないのです が完成度が上がっていません。手作業で少しカバーしてあげる必要があります。\nOpenStack の Cinder の先のストレージについて最近考えていました。LVM 管理のロー カルディスクでもいいのですが運用のことを考えるとバックアップを取らなくちゃい けないのだけど logcal volume が存在しないのでスナップショットバックアップが出 来なそう。Cinder は比較的高価なストレージも扱えるのでそちらの機能でバックアッ プ取るのもいいけど、ここはオープンソースでなんとかしたい！と思って Ceph を検討 してみました。\nCeph は分散ストレージでオブジェクトストレージとしてもブロックストレージとして も動作が可能。OpenStack と組み合わせると Cinder の先のストレージとしても Glance のイメージ置き場としても利用可能らしい。Cinder の接続先ストレージとして の動作方法はまた別の機会にブログに書きます。\n","permalink":"https://jedipunkz.github.io/post/2013/05/11/ceph-deploy/","summary":"今回は ceph-deploy というツールを使って Ceph ストレージを簡単に構築することが 出来るので紹介します。Ceph は分散ストレージでオブジェクトストレージとしてもブ ロックストレージとしても動作します。今回の構築ではブロックストレージとしてのみ の動作です。\nCeph が公開しているのが ceph-deploy なわけですが、マニュアル操作に代わる構築方 法として公開しているようです。その他にも Chef Cookbook も公開されているようで す。\nそれでは早速。\n今回の構成 +--------+ +--------+ +--------+ | ceph01 | | ceph02 | | ceph03 | | osd | | osd | | osd | | mon | | mon | | mon | | mds | | mds | | mds | +--------+ +--------+ +--------+ | 10.0.0.1 | 10.0.0.2 | 10.0.0.3 | | | +----------+----------+ | | 10.","title":"Ceph-Deploy で Ceph 分散ストレージ構築"},{"content":"こんにちは。Grizzly がリリースされてから暫く経ちました。今回は Folsom リリース まであった Quantum ノードのボトルネックと単一障害点を解決する新しい機能につい て評価した結果をお伝えします。\nFolsom までは\n Quantum L3-agent が落ちると、その OpenStack 一式の上にある仮想マシン全ての通 信が途絶える Quantum L3-agent に仮想マシンの全てのトラフィックが集まりボトルネックとなる。  という問題がありました。Folsom リリース時代にもし僕が職場で OpenStack を導入す るのであればこれらを理由に nova-network を選択していたかもしれません。 nova-network は compute ノードが落ちればその上の仮想マシンも同時に落ちるが、他 の compute ノード上の仮想マシンの通信には影響を与えないからです。もちろん仮想 ルータ・仮想ネットワークの生成等を API でユーザに提供したいなどの要望があれば Quantum を選択するしかありませんが。これに対して Grizzly リリースの Quantum は 改善に向けて大きな機能を提供してくれています。L3-agent, DHCP-agent の分散・冗 長機能です。\n下記の構成が想定出来ます。ここでは Network ノードを2台用意しました。それ以上の 台数に増やすことも出来ます。\n+-------------+-------------+-------------------------- Public/API Network | | | +-----------+ +-----------+ +-----------+ +-----------+ | | | | | | |vm|vm|.. | | controller| | network | | network | +-----------+ | | | | | | | compute | +-----------+ +-----------+ +-----------+ +-----------+ | | | | | | | +-------------+-----)-------+-----)-------+-----)------ Management/API Network | | | +-------------+-------------+------ Data Network  L3-agent の分散は仮想ルータ単位で行います。それに対し DHCP-agent は仮想 ネットワーク単位で行います。\nagent 一覧の取得 上記の構成を構築すると下記のように agent 一覧が取得出来ます。\n% quantum agent-list # 'admin' ユーザでアクセス +--------------------------------------+--------------------+-----------------------+-------+----------------+ | id | agent_type | host | alive | admin_state_up | +--------------------------------------+--------------------+-----------------------+-------+----------------+ | 44795822-2d9f-434e-ba98-748f7411442f | DHCP agent | grizzly03.example.com | :-) | True | | a5150a40-0405-4399-ac1a-be012f55d9f5 | DHCP agent | grizzly02.example.com | :-) | True | | b7bf4e59-06ac-475c-84ab-413d8d29f293 | Open vSwitch agent | grizzly04.example.com | :-) | True | | cc5a6b94-6ddd-4109-8f2f-1b28c6aaf5e6 | L3 agent | grizzly03.example.com | :-) | True | | d39803cf-19d3-47d7-8205-cf9a143dd0ea | Open vSwitch agent | grizzly02.example.com | :-) | True | | d8e59803-9aad-4c62-a47a-519bc788e0fb | Open vSwitch agent | grizzly03.example.com | :-) | True | | f6f747cf-ffb0-446c-a455-2947fd3e87e8 | L3 agent | grizzly02.example.com | :-) | True | +--------------------------------------+--------------------+-----------------------+-------+----------------+  ホスト名は下記。\n controller : grizzly01.exmaple.com network01 : grizzly02.exmaple.com network02 : grizzly03.exmaple.com compute : grizzly04.exmaple.com  L3-agent の分散方法 (ノード移動) 仮想ルータ (ここでは \u0026lsquo;router-test01\u0026rsquo; とする) がどの L3-agent に属しているか確 認を取る。\n% quantum l3-agent-list-hosting-router router-demo +--------------------------------------+-----------------------+----------------+-------+ | id | host | admin_state_up | alive | +--------------------------------------+-----------------------+----------------+-------+ | f6f747cf-ffb0-446c-a455-2947fd3e87e8 | grizzly02.example.com | True | :-) | +--------------------------------------+-----------------------+----------------+-------+  1台目の Network ノード (grizzly02.example.com) 上の L3-agent に属していること が確認取れた。次にこの親子関係を削除する。\n% quantum l3-agent-router-remove f6f747cf-ffb0-446c-a455-2947fd3e87e8 router-test01 Removed Router router-demo to L3 agent  最後に仮想ルータ \u0026lsquo;router-test01\u0026rsquo; を2台目の Network ノード上の L3-agent の管理 下に設定する。\n% quantum l3-agent-router-add cc5a6b94-6ddd-4109-8f2f-1b28c6aaf5e6 router-demo Added router router-demo to L3 agent % quantum l3-agent-list-hosting-router router-demo +--------------------------------------+-----------------------+----------------+-------+ | id | host | admin_state_up | alive | +--------------------------------------+-----------------------+----------------+-------+ | cc5a6b94-6ddd-4109-8f2f-1b28c6aaf5e6 | grizzly0404.cpi.ad.jp | True | xxx | +--------------------------------------+-----------------------+----------------+-------+  DHCP-Agent の分散方法 (ノード移動) 仮想マシンが所属しているネットワーク (ここでは \u0026lsquo;int_net\u0026rsquo;) がどの DHCP-agent に所属しているか確認する。\n% quantum dhcp-agent-list-hosting-net int_net +--------------------------------------+-----------------------+----------------+-------+ | id | host | admin_state_up | alive | +--------------------------------------+-----------------------+----------------+-------+ | a5150a40-0405-4399-ac1a-be012f55d9f5 | grizzly02.example.com | True | :-) | +--------------------------------------+-----------------------+----------------+-------+  1台目のノードに所属しているのが確認できる。次に \u0026lsquo;int_net\u0026rsquo; が所属する DHCP-agent を削除行う。\n% quantum dhcp-agent-network-remove a5150a40-0405-4399-ac1a-be012f55d9f5 int_net Removed network int_net to DHCP agent  2台目のノードの DHCP-agent を仮想ネットワーク \u0026lsquo;int_net\u0026rsquo; に紐付ける。\n% quantum dhcp-agent-network-add 44795822-2d9f-434e-ba98-748f7411442f int_net Added network int_net to DHCP agent % quantum dhcp-agent-list-hosting-net int_net +--------------------------------------+-----------------------+----------------+-------+ | id | host | admin_state_up | alive | +--------------------------------------+-----------------------+----------------+-------+ | 44795822-2d9f-434e-ba98-748f7411442f | grizzly0404.cpi.ad.jp | True | :-) | +--------------------------------------+-----------------------+----------------+-------+  まとめ この様に仮想ルータ, 仮想ネットワーク単位で Network ノードの agent の分散が行え る。上記のように仮想ルータ・ネットワークが1つずつでは分散という意味では無いが 運用の過程で仮想ルータ・ネットワークは増えることが想定出来るのでその際にはトラ フィック・DHCP 機能を分散することが可能になる、と言える。また片系の Network ノー ドに寄せておいてからの障害テスト -\u0026gt; もう片系への移動も行なってみたが作業ととも に仮想マシンの通信が復旧した。このテストを行う前まで \u0026lsquo;agent の移動だけ行えるの であって仮想ルータ自体が移動するわけではないので冗長という意味はない\u0026rsquo; と考えて いたのだが、実際には上記の操作で namespace が移動していることが判り (Quantum の仮想ルータの実体は Linux Namespace) 障害テストの結果、うまくいった。 OpenStack を導入するという意味で、この機能は非常に大きな前進だと僕は思っていま す。\n","permalink":"https://jedipunkz.github.io/post/2013/04/26/quantum-network-distributing/","summary":"こんにちは。Grizzly がリリースされてから暫く経ちました。今回は Folsom リリース まであった Quantum ノードのボトルネックと単一障害点を解決する新しい機能につい て評価した結果をお伝えします。\nFolsom までは\n Quantum L3-agent が落ちると、その OpenStack 一式の上にある仮想マシン全ての通 信が途絶える Quantum L3-agent に仮想マシンの全てのトラフィックが集まりボトルネックとなる。  という問題がありました。Folsom リリース時代にもし僕が職場で OpenStack を導入す るのであればこれらを理由に nova-network を選択していたかもしれません。 nova-network は compute ノードが落ちればその上の仮想マシンも同時に落ちるが、他 の compute ノード上の仮想マシンの通信には影響を与えないからです。もちろん仮想 ルータ・仮想ネットワークの生成等を API でユーザに提供したいなどの要望があれば Quantum を選択するしかありませんが。これに対して Grizzly リリースの Quantum は 改善に向けて大きな機能を提供してくれています。L3-agent, DHCP-agent の分散・冗 長機能です。\n下記の構成が想定出来ます。ここでは Network ノードを2台用意しました。それ以上の 台数に増やすことも出来ます。\n+-------------+-------------+-------------------------- Public/API Network | | | +-----------+ +-----------+ +-----------+ +-----------+ | | | | | | |vm|vm|.. | | controller| | network | | network | +-----------+ | | | | | | | compute | +-----------+ +-----------+ +-----------+ +-----------+ | | | | | | | +-------------+-----)-------+-----)-------+-----)------ Management/API Network | | | +-------------+-------------+------ Data Network  L3-agent の分散は仮想ルータ単位で行います。それに対し DHCP-agent は仮想 ネットワーク単位で行います。","title":"Quantum Network ノードの分散・冗長"},{"content":"以前にも話題にしたことがある Chef For OpenStack ですが今週新しい情報が入って来 ました。#ChefConf 2013 というイベントがあったのですがここで Opscode の Matt Ray さんらが集まり OpenStack を Chef で構築する \u0026lsquo;Chef for OpenStack\u0026rsquo; について 語られた模様です。その時の資料が SlideShare に上がっていたので見てみました。\n気にあった点を幾つか挙げていきます。\n https://github.com/osops で管理される 各コンポーネントの cookbook の名前には \u0026lsquo;-cookbook\u0026rsquo; を最後に付ける quantum, cinder, ceilometer, heat 等、比較的新しいコンポーネントも加わる gerrit でコードレビューされ CI も提供される Chef11 が用いられる Ruby 1.9.x に対応した chef-client が用いられる Foodcritic で可能な限りテストされる chef-solo はサポートされない 5月に \u0026lsquo;2013.1.0\u0026rsquo; がリリースされる (openstack 2013.1 対応と思われる) chef-repo の形で提供される Ubuntu 12.04 が前提 HyperVisor は KVM, LXC がサポートされる  以上です。恐らく chef-repo で提供されるということは spiceweasel を使った構成構 築が出来るような形になるでしょう。楽しみです。またコントリビュートする方法も掲 載されているので興味が有る方は協力してみるのも楽しいかもしれません。\n","permalink":"https://jedipunkz.github.io/post/2013/04/21/chef-for-openstack-grizzly-roadmap/","summary":"以前にも話題にしたことがある Chef For OpenStack ですが今週新しい情報が入って来 ました。#ChefConf 2013 というイベントがあったのですがここで Opscode の Matt Ray さんらが集まり OpenStack を Chef で構築する \u0026lsquo;Chef for OpenStack\u0026rsquo; について 語られた模様です。その時の資料が SlideShare に上がっていたので見てみました。\n気にあった点を幾つか挙げていきます。\n https://github.com/osops で管理される 各コンポーネントの cookbook の名前には \u0026lsquo;-cookbook\u0026rsquo; を最後に付ける quantum, cinder, ceilometer, heat 等、比較的新しいコンポーネントも加わる gerrit でコードレビューされ CI も提供される Chef11 が用いられる Ruby 1.9.x に対応した chef-client が用いられる Foodcritic で可能な限りテストされる chef-solo はサポートされない 5月に \u0026lsquo;2013.1.0\u0026rsquo; がリリースされる (openstack 2013.1 対応と思われる) chef-repo の形で提供される Ubuntu 12.04 が前提 HyperVisor は KVM, LXC がサポートされる  以上です。恐らく chef-repo で提供されるということは spiceweasel を使った構成構 築が出来るような形になるでしょう。楽しみです。またコントリビュートする方法も掲 載されているので興味が有る方は協力してみるのも楽しいかもしれません。","title":"Chef for OpenStack"},{"content":"こんにちは jedipunkz です。\nVirtio に対応していない OS を OpenStack で稼働させることが今まで出来なかったの ですが Grizzly から非 Virtio な OS イメージが扱えるようになった。今まで NetBSD やら古い FreeBSD やら virtio ドライバを OS イメージに入れることに苦労していたの だけど、これで問題無くなった。\n最初、この機能のこと調べるのに「どうせ libvirt が生成する xml を書き換えるのだ から nova 周りの設定なんだろうー」と思っていたら全く方法が見つからず\u0026hellip;。結局 OS イメージを格納している Glance の設定にありました。\nここでは FreeBSD7.4 Release を例に挙げて説明していきます。\n前提とする環境  OpenStack Grizzly が稼働していること ホスト OS に Ubuntu 12.04.2 LTS が稼働していること ゲスト OS に FreeBSD 7.4 Release を用いる  とします。OS のバージョンはホスト・ゲスト共に、上記以外でも構いません。Grizzly さえ動いていれば OK です。\nOS イメージ作成 KVM で OS イメージを作成します。もちろん virtio なインターフェースは指定せず\n IDE ディスクインターフェース e1000 (intel) ネットワークインターフェース  を指定してあげてください。\n% kvm-img create -f qcow2 \u0026lt;IMAGE_NAME\u0026gt; 5G % sudo kvm -m 1024 --cdrom FreeBSD-7.4-RELEASE-amd64-disc1.iso --drive \\ file=./\u0026lt;IMAGE_NAME\u0026gt; -boot d -net nic,model=e1000 -net user -nographic \\ -vnc :9  VNC クライアントソフトを用いてホスト :9 番に接続し OS をインストールする。\nGlance への登録 OpenStack API に接続する環境変数等を合わせ下記のコマンドを実行します。\n% glance image-create --name=\u0026quot;FreeBSD7.4\u0026quot; --is-public \\ true --container-format bare --disk-format qcow2 \u0026lt; \u0026lt;IMAGE_NAME\u0026gt; % glance image-update --property hw_vif_model=e1000 \u0026quot;FreeBSD7.4\u0026quot; % glance image-update --property hw_disk_bus=ide \u0026quot;FreeBSD7.4\u0026quot;  \u0026ndash;property オプションでディスク・ネットワークインターフェースの指定を変更して います。\nVM の稼働 あとは普段通り nova boot コマンドで VM を稼働させるだけです。\n% nova boot --nic net-id=\u0026lt;network_id\u0026gt; --image \u0026lt;image_id\u0026gt; --flavor \u0026lt;flavor_number\u0026gt; \u0026lt;vm_name\u0026gt;  ","permalink":"https://jedipunkz.github.io/post/2013/04/21/openstack-non-virtio/","summary":"こんにちは jedipunkz です。\nVirtio に対応していない OS を OpenStack で稼働させることが今まで出来なかったの ですが Grizzly から非 Virtio な OS イメージが扱えるようになった。今まで NetBSD やら古い FreeBSD やら virtio ドライバを OS イメージに入れることに苦労していたの だけど、これで問題無くなった。\n最初、この機能のこと調べるのに「どうせ libvirt が生成する xml を書き換えるのだ から nova 周りの設定なんだろうー」と思っていたら全く方法が見つからず\u0026hellip;。結局 OS イメージを格納している Glance の設定にありました。\nここでは FreeBSD7.4 Release を例に挙げて説明していきます。\n前提とする環境  OpenStack Grizzly が稼働していること ホスト OS に Ubuntu 12.04.2 LTS が稼働していること ゲスト OS に FreeBSD 7.4 Release を用いる  とします。OS のバージョンはホスト・ゲスト共に、上記以外でも構いません。Grizzly さえ動いていれば OK です。\nOS イメージ作成 KVM で OS イメージを作成します。もちろん virtio なインターフェースは指定せず","title":"OpenStack Grizzy で非 Virtio OS 稼働"},{"content":"OpenStack Grizzly がリリースされて2週間ほど経過しました。皆さん動かしてみまし たか？今回、毎度の構築 Bash スクリプトを開発したので公開します。\n下記のサイトで公開しています。\nhttps://github.com/jedipunkz/openstack_grizzly_install\nこのスクリプト、複数台構成とオールインワン構成の両方が構成出来るようなっていま すが、今回は簡単なオールインワン構成の組み方をを簡単に説明したいと思います。\n前提の環境  Ubuntu 12.04 LTS が稼働している Cinder のためのディスクを OS 領域と別に用意 (/dev/sdb1 など) オールインワン構成の場合は 2 NICs 準備  Ubuntu 13.04 の daily build も完成度上がっている時期ですが OVS 側の対応が OpenStack 構成に問題を生じさせるため 12.04 LTS + Ubuntu Cloud Archive の組み合 わせで構築するのが主流になっているようです。また、Cinder 用のディスクは OS 領 域を保持しているディスクとは別 (もしくはパーティションを切ってディスクデバイス を別けても可) が必要です。オールインワン構成の場合は NIC を2つ用意する必要があ ります。通常 OpenStack を複数台構成する場合は\n コントローラノード x 1 台 ネットワークノード x 1 台 コンピュートノード x n 台  で組み、VM はコンピュートノードからネットワークノードを介してインターネットに 接続します。よってそのため更に NIC が必要になるのですが、オールインワン構成の 場合は\n マネージメントネットワーク, API ネットワーク(内部通信用) パブリックネットワーク (VM のためのブリッジインターフェース)  の計2つを用意してください。\n実行前の準備 OS のインストール OS のインストール方法は割愛しますが\n \u0026lsquo;openssh-server\u0026rsquo; のみをインストール ディスクが1つしかない場合は cinder 用のパーティションを用意  の条件が満たされていれば OK です。\nCinder 用のディスクデバイスパーティショニング Cinder 用に信頼性のあるディスクを用意している場合は fdisk 等を用いてパーティショ ニングしてください。近々 loopback デバイスでも構築できるようスクリプトの改修を する予定です。ディスクが一つしかない場合は先程述べたとおり、OS インストール時 にパーティショニングしたディスクデバイスを使います。\n% sudo fdisk /dev/sdb  ネットワークインターフェースの設定 下記のように2つのネットワークインターフェースを設定してください。\n% sudo ${EDITOR} /etc/network/interfaces auto lo iface lo inet loopback # this NIC will be used for VM traffic to the internet auto eth0 iface eth0 inet static up ifconfig $IFACE 0.0.0.0 up up ip link set $IFACE promisc on down ip link set $IFACE promisc off down ifconfig $IFACE down address 10.200.9.10 netmask 255.255.255.0 dns-nameservers \u0026lt;DNS_RESOLVER1\u0026gt; \u0026lt;DNS_RESOLVER\u0026gt; dns-search example.com # this NIC must be on management network auto eth1 iface eth1 inet static address 10.200.10.10 netmask 255.255.255.0 gateway 10.200.10.1 dns-nameservers \u0026lt;DNS_RESOLVER1\u0026gt; \u0026lt;DNS_RESOLVER\u0026gt;  eth0 が VM のためのブリッジインターフェースになります。eth1 はマネージメントネッ トワーク用・内部 API 通信用の兼務です。\nスクリプトの取得とパラメータ設定 スクリプトの取得を行います。\n% git clone git://github.com/jedipunkz/openstack_grizlly_install.git % cd openstack_grizzly_install  パラメータを設定するため setup.conf 内の各パラメータを設定変更します。数多くの パラメータがありますが、最低限のパラメータということで\u0026hellip;\nHOST_IP='10.200.10.10' HOST_PUB_IP='10.200.9.10' PUBLIC_NIC='eth0'  を設定してください。HOST_IP は eth1 の IP アドレス、HOST_PUB_IP は eth0 の IP アドレス、PUBLIC_NIC は eth0 (HOST_PUB_IP のインターフェース名) を指定します。\nスクリプトの実行 いよいよスクリプトを実行します。\n% sudo ./setup.sh allinone  しばらくすると構築が完了します。あとは\nhttp://${HOST_IP}/horizon/  にブラウザでアクセスすると WEB I/F である Horizon のログイン画面が表示されます。 パラメータをいじっていなければユーザ : demo, パスワード : demo でアクセス出来 ます。\n各 API にコマンドでアクセスする API にアクセスするためにコマンドを用いることも出来ます。スクリプトを実行した結 果、下記のファイルが生成されているはずです。\n~/openstackrc-demo # 'demo' ユーザで API にアクセス ~/openstackrc # 'admin' ユーザで API にアクセス  \u0026lsquo;demo\u0026rsquo; ユーザでアクセスするためには\n% source ~/openstackrc-demo  を実行してください。環境変数が設定され API にアクセス出来るようになります。例 として下記のコマンドを実行してみてください。\n% glnace image-list +--------------------------------------+---------------------+-------------+------------------+------------+--------+ | ID | Name | Disk Format | Container Format | Size | Status | +--------------------------------------+---------------------+-------------+------------------+------------+--------+ | 1a7943a5-8f8f-4c02-9763-5a6d519c31bb | Cirros 0.3.0 x86_64 | qcow2 | bare | 9761280 | active | +--------------------------------------+---------------------+-------------+------------------+------------+--------+  OS イメージ一覧が取得出来ます。スクリプトで予め Glance に登録した Cirros とい う小さな OS イメージが確認出来るはずです。\nまとめ 本格的な構成を組むのであれば上記の URL にも知る指定ある複数台構成を組んでみて ください。同じくスクリプトで構築出来ます。また今回から Quantum に実装された LBaaS も組めるようになっています。構築出来た OpenStack で LB を組んでみてくだ さい。LBaaS の説明については OpenStack ユーザ会の中島さんのブログが参考になり ます。\nhttp://aikotobaha.blogspot.jp/2013/04/use-full-function-of-openstack-grizzly.html\nLBaaS で組める負荷分散方式が \u0026lsquo;ROUND_ROBIN\u0026rsquo; 以外にも選択出来るぽいのでもう少し 調べたら、僕のブログでも紹介しようかと思います。また Grizzly になって数多くの 機能が新たに実装されているので引き続き紹介していこうかと思います。\nOpenStack は多くの機能がありますし構成の仕方も様々。予め理解しなければいけない 技術も多岐にわたるのでブログだけではなかなか説明し切れないところです。 OpenStack のコミュニティが書いた \u0026lsquo;OpenStack Operations Guide\u0026rsquo; なるドキュメント が最近リリースされました。\nhttp://docs.openstack.org/ops/\n日本のユーザ会でもこのドキュメントを翻訳しようという活動がされている最中です。 興味があるかたは一度読むことをオススメしますし、もし更に興味が有る方は翻訳活動 に協力するのはいかがでしょうか。ユーザ会の ML で現在話が進んでいます。\n引き続き、OpenStack ネタはアップしていきますー。\n","permalink":"https://jedipunkz.github.io/post/2013/04/20/openstack-grizzly-installation-script/","summary":"OpenStack Grizzly がリリースされて2週間ほど経過しました。皆さん動かしてみまし たか？今回、毎度の構築 Bash スクリプトを開発したので公開します。\n下記のサイトで公開しています。\nhttps://github.com/jedipunkz/openstack_grizzly_install\nこのスクリプト、複数台構成とオールインワン構成の両方が構成出来るようなっていま すが、今回は簡単なオールインワン構成の組み方をを簡単に説明したいと思います。\n前提の環境  Ubuntu 12.04 LTS が稼働している Cinder のためのディスクを OS 領域と別に用意 (/dev/sdb1 など) オールインワン構成の場合は 2 NICs 準備  Ubuntu 13.04 の daily build も完成度上がっている時期ですが OVS 側の対応が OpenStack 構成に問題を生じさせるため 12.04 LTS + Ubuntu Cloud Archive の組み合 わせで構築するのが主流になっているようです。また、Cinder 用のディスクは OS 領 域を保持しているディスクとは別 (もしくはパーティションを切ってディスクデバイス を別けても可) が必要です。オールインワン構成の場合は NIC を2つ用意する必要があ ります。通常 OpenStack を複数台構成する場合は\n コントローラノード x 1 台 ネットワークノード x 1 台 コンピュートノード x n 台  で組み、VM はコンピュートノードからネットワークノードを介してインターネットに 接続します。よってそのため更に NIC が必要になるのですが、オールインワン構成の 場合は","title":"OpenStack Grizzly 構築スクリプト"},{"content":"chef-solo を使うの？Chef サーバを使うの？という議論は結構前からあるし、答えは 「それぞれの環境次第」だと思うのだが、僕は個人的に Chef サーバを使ってます。ホ ステッド Chef を使いたいけどお金ないし。会社で導入する時はホステッド Chef を契 約してもらうことを企んでます。(・∀・) 何故なら cookbooks を開発することがエン ジニアの仕事の本質であって Chef サーバを運用管理することは本質ではないから。そ れこそクラウド使えという話だと思う。\nでも！Chef に慣れるには無料で使いたいし、継続的に Cookbooks をターゲットノード で実行したい。ということで Chef サーバを構築して使っています。\nChef 10 の時代は Chef サーバの構築方法は下記の通り3つありました。\n 手作業！ Bootstrap 構築 Opscode レポジトリの Debian, Ubuntu, CentOS パッケージ構築  それが Chef 11 では\n Ubuntu, RHEL のパッケージ (パッケージインストールですべて環境が揃う)  http://www.opscode.com/chef/install/\nこの方法1つだけ。でも簡単になりました。\n\u0026lsquo;Chef Server\u0026rsquo; タブを選択するとダウンロード出来る。じっくりは deb ファイルの中 身を見たことがないけど、チラ見した時に chef を deb の中で実行しているように見 えた。徹底してるｗ\nChef 10 時代のパッケージと違って行う操作は下記の2つのコマンドだけ。\n% sudo dpkg -i chef-server_11.0.6-1.ubuntu.12.04_amd64.deb # ダウンロードしたもの % sudo chef-server-ctl reconfigure  簡単。でも\u0026hellip; この状態だと https://\u0026lt;サーバの FQDN\u0026gt; でサーバが Listen している。 IP アドレスでアクセスしてもリダイレクトされる。つまり、ローカルネットワーク上 に構築することが出来ない。安易に hosts で解決も出来ない。何故ならターゲットノー ドは通常まっさらな状態なので bootstrap するたびに hosts を書くなんてアホらしい しやってはいけない。\n今日の本題。ローカルネットワーク上に Chef 11 サーバを構築する方法を調べました。\n修正箇所  /var/opt/chef-server/chef-pedant/etc/pedant_config.rb  URL を IP アドレスに変更する。\nchef_server \u0026quot;https://chef.example.com\u0026quot;   /var/opt/chef-server/erchef/etc/app.config  URL を IP アドレスに変更する。\n{s3_url, \u0026quot;https://chef.example.com\u0026quot;},   /var/opt/chef-server/nginx/etc/nginx.conf  URL を IP アドレスに変更する。2箇所あるので注意。鍵ファイルにも FQDN が記され ているがそれらは変更しない。\nserver_name chef.example.com;   /etc/chef-server/chef-server-running.json  URL を IP アドレスに変更する。鍵ファイルにも FQDN が記されているがそれらは変更 しない。\n\u0026quot;vip\u0026quot;: \u0026quot;chef.example.com\u0026quot;, \u0026quot;api_fqdn\u0026quot;: \u0026quot;chef.example.com\u0026quot;, \u0026quot;web_ui_fqdn\u0026quot;: \u0026quot;chef.example.com\u0026quot;, \u0026quot;server_name\u0026quot;: \u0026quot;chef.example.com\u0026quot;, \u0026quot;url\u0026quot;: \u0026quot;https://chef.example.com\u0026quot;,  最後に Chef サーバを再起動する。\n% sudo chef-server-ctl restart  まとめ 会社で、この環境を使って色々試しています。今のところ不具合なく動作している。強 引ワザなのでもっと綺麗な方法を知っている方がいましたら教えて下さい。 m ( _ _ ) m\n","permalink":"https://jedipunkz.github.io/post/2013/04/06/chef-11-private-network/","summary":"chef-solo を使うの？Chef サーバを使うの？という議論は結構前からあるし、答えは 「それぞれの環境次第」だと思うのだが、僕は個人的に Chef サーバを使ってます。ホ ステッド Chef を使いたいけどお金ないし。会社で導入する時はホステッド Chef を契 約してもらうことを企んでます。(・∀・) 何故なら cookbooks を開発することがエン ジニアの仕事の本質であって Chef サーバを運用管理することは本質ではないから。そ れこそクラウド使えという話だと思う。\nでも！Chef に慣れるには無料で使いたいし、継続的に Cookbooks をターゲットノード で実行したい。ということで Chef サーバを構築して使っています。\nChef 10 の時代は Chef サーバの構築方法は下記の通り3つありました。\n 手作業！ Bootstrap 構築 Opscode レポジトリの Debian, Ubuntu, CentOS パッケージ構築  それが Chef 11 では\n Ubuntu, RHEL のパッケージ (パッケージインストールですべて環境が揃う)  http://www.opscode.com/chef/install/\nこの方法1つだけ。でも簡単になりました。\n\u0026lsquo;Chef Server\u0026rsquo; タブを選択するとダウンロード出来る。じっくりは deb ファイルの中 身を見たことがないけど、チラ見した時に chef を deb の中で実行しているように見 えた。徹底してるｗ\nChef 10 時代のパッケージと違って行う操作は下記の2つのコマンドだけ。\n% sudo dpkg -i chef-server_11.","title":"Chef 11 サーバのローカルネットワーク上構築"},{"content":"クラウドマネジメント勉強会に参加してきた。今が旬なのか定員140名が埋まっていま した。クラウドフェデレーションサービス各種の話が聞ける貴重な勉強会の場でした。\n場所 : スクエアエニックスさん 日程 : 2013年4月5日 19:00 -  少し長くなるので、早速。\nクラウド運用管理研究会 クラウド利用推進機構が運営するクラウド運用管理研究会は下記の3つに分別されるそ うです。今回は一項目の \u0026lsquo;クラウドマネジメントツール研究会\u0026rsquo; にあたるそう。別の研 究会も既に勉強会を実施しているそうです。\n クラウドマネジメントツール研究会 デザインパターン研究会 運用管理・監視研究会  AWS OpsWorks アマゾンデータサービスジャパン AWS 片山さん, 船崎さん  OpsWorks は最近話題になった AWS 利用者に無料で提供されるクラウドフェデレーショ ンサービス。Web UI で操作し簡単デプロイを実現するサービスです。\nOpsWorks が自動化するモノ  サーバ設定 ミドルウェア構築  特徴  Chef フレームワークを利用 (chef-solo を内部的に利用) 任意の cookbooks を利用可能 LB, AP, DB などをレイヤ化, 任意のレイヤも作成可能  OpsWorks の流れ  Stack 作成 レイヤ作成 (LB, AP, DB, 任意) レシピの作成 レイヤにインスタンス作成  下記をレイヤ化で区別する  Package インストール OS 設定 アプリデプロイ  所感 AWS OpsWorks の登場で他のクラウドフェデレーションサービスがどうなるの？とさえ 思った。AWS はインターネット・ホスティング業界のあらゆるサービスを押さえようと している感がある。もう隙間がない！ｗ OpsWorks に関してまだ問題は残っているそう だ。VPC, micro 現在未対応など。が解決に向けて作業しているそう。\nAeolus Conductor RedHat 中井さん  概要 複数クラウドに対応したイメージ作成・アプリケーション環境構築の自動化ツール\n アプリのデプロイ機能にフォーカス Red Hat CoudForms が商用版 マルチクラウド (ユーザにクラウドが割り当てられる, Hybrid, EC2, RHEV)  自動化について中井さんの案 (手作り)  libvirt キック kickstart 実行 post script にて puppet 実行 manifest は github で管理  これらは単一のサーバのみで実施できて、複数台構成等を前提に出来ない等の問題があ る。それらを解決するのが Aeolus Conductor。\nAeolus Conductor の要素  システムテンプレート用意 (XML) : OS 構成内容が記されている マシンイメージ JEOS アプリケーションブループリント (アプリデプロイ設計書) shell script である。puppet, chef を呼び出しても OK. Config サーバを介して VM 間の構成を管理している : インテグレート！ DB, Web  Aeolus Conductor の不便な点  特定のクラウド特有の機能には未対応 複数 VM デプロイ時のワークフロー処理が不十分  所感 画面を見させてもらったが AWS EC2, RHEV (RedHat の仮想化ソフト) とマルチクラウ ドに対応していた。ユーザにどのクラウドを割り当てるか？等の権限委譲が出来るもの ユニーク。\nScalr Scalr ユーザ会 梶川さん (IDC フロンティア)  概要, 特徴  オープンソースのマルチクラウド管理ツール 利用出来るクラウド : AWS, Eucalyptus, RackSpace, nimbula, OpenStack, \u0026hellip; 冗長化・オートスケール可能 モニタリングも自動で開始 DNS 管理, オートスケール時、自動的に修正が行われる スクリプト実行 (任意のタイミングで可能、またタイミングを作成可能) 各サービスのコンフィグプリセット管理 (ミドルウェアのパラメータ？)  所感 Scalr ユーザ会のメンバ募集中だそうだ。個人的にオープンソースの Scalr を試そう と思ったことがあるのだが、手順の wiki が解りづらかった。商用サービスを使わせる ためにわざと解りづらくしているのか？と思うほど。ユーザ拡大のために是非ドキュメ ントの整備をお願いしたい。\nRightScale の利用効果と苦労話 So-net エンタテインメント 成田さん  利用効果  1つのスクリプトを複数台に対して実行可能 手作業が自動化へ ベストプラクティスの利用が可能に モニタリングの自動化へ サーバ台数のスケジューリング化 セキュリティグループはマクロで作成 権限分離による開発者・プロデューサに役割移譲 履歴管理の自動記録 chef recipe が right スクリプトとして走らせられる  苦労話  Alert 設定のミスでメール大量受信 自動化スクリプトのエラー対応 計画メンテナンスの後は要注意 (仕様変更) RightScale 上の表示を過信しない, 詳しくはクラウドサービス側を確認 LANG=ja_JP.UTF-8 するとコケる メンテナンスは金曜日日中 (月に一回)  所感 実際に運用している方の話はとても貴重。特に苦労ネタはなかなか知ることが出来ない ので。自動化のためにスクリプトを書くのがインフラ系エンジニアの仕事になると知ら せてくれた。Right スクリプトには Chef のレシピも走らせることが出来る、というも が魅力。またインフラ系エンジニア以外の職種の人にも権限委譲し UI を操作してもら える辺りは、業務の最適化のために大いに利用できると感じた。\nChef の話 Engine Yard @yando さん  Engine Yard とは  PaaS AWS + Chef + サポート, 監視 chef-solo をキック chef recipe の管理は Engine Yard が行う  Chef へのモチベーション  冪等性 シェルスクリプトだと構築直後の状態しか保証されない  chef-solo の話  knife-solo でノードに SSH せずに実行  利用するにあたって直面する課題  レシピの実装 -\u0026gt; github 上のレシピを参照・利用 Vagrant の利用でレシピの複数プラットフォーム上でのテスト レシピの配布方法 -\u0026gt; github, berkshelf, knife solo, nfs, chef-server レシピの反映 -\u0026gt; capistrano, chef-client, cron  Engine Yard Local  クラウドと同じレシピでローカルに開発環境を構築出来るツール クラウドはコストが掛かるし遅いので出来る事ならローカルで、という発想  所感 Chef 流行ってますね。うんうん、(・∀・)ｲｲ!! 個人的には Chef の Cookbooks 開発 はインフラエンジニアにしてもらいたい。Engine Yard のような PaaS 使うならアレだ けどクラウド使うなら運用は引き続き必要だし、運用を意識した Cookbooks 開発は絶 対に必要になってくるからだ。Chef の関連技術がものすごいスピードで進化している のも魅力。より便利で旬な技術をすぐに利用し貢献する、という良いサイクルをうちの 会社でも実現したい。だって楽しいから。chef-solo 使うの？chef サーバ使うの？と いう話はここでも挙がってた。どこかでブログにしようかな。僕は chef サーバ使わな い理由がないと思ってる。\n","permalink":"https://jedipunkz.github.io/post/2013/04/06/cloudmanagement/","summary":"クラウドマネジメント勉強会に参加してきた。今が旬なのか定員140名が埋まっていま した。クラウドフェデレーションサービス各種の話が聞ける貴重な勉強会の場でした。\n場所 : スクエアエニックスさん 日程 : 2013年4月5日 19:00 -  少し長くなるので、早速。\nクラウド運用管理研究会 クラウド利用推進機構が運営するクラウド運用管理研究会は下記の3つに分別されるそ うです。今回は一項目の \u0026lsquo;クラウドマネジメントツール研究会\u0026rsquo; にあたるそう。別の研 究会も既に勉強会を実施しているそうです。\n クラウドマネジメントツール研究会 デザインパターン研究会 運用管理・監視研究会  AWS OpsWorks アマゾンデータサービスジャパン AWS 片山さん, 船崎さん  OpsWorks は最近話題になった AWS 利用者に無料で提供されるクラウドフェデレーショ ンサービス。Web UI で操作し簡単デプロイを実現するサービスです。\nOpsWorks が自動化するモノ  サーバ設定 ミドルウェア構築  特徴  Chef フレームワークを利用 (chef-solo を内部的に利用) 任意の cookbooks を利用可能 LB, AP, DB などをレイヤ化, 任意のレイヤも作成可能  OpsWorks の流れ  Stack 作成 レイヤ作成 (LB, AP, DB, 任意) レシピの作成 レイヤにインスタンス作成  下記をレイヤ化で区別する  Package インストール OS 設定 アプリデプロイ  所感 AWS OpsWorks の登場で他のクラウドフェデレーションサービスがどうなるの？とさえ 思った。AWS はインターネット・ホスティング業界のあらゆるサービスを押さえようと している感がある。もう隙間がない！ｗ OpsWorks に関してまだ問題は残っているそう だ。VPC, micro 現在未対応など。が解決に向けて作業しているそう。","title":"クラウドマネジメント勉強会レポ"},{"content":"もう数日で OpenStack の次期バージョン版 Grizzly がリリースされるタイミングだが 現行バージョン Folsom の OpenStack の上に NetBSD を載せてみた。完全にお遊び だけど\u0026hellip;。\n結局、ほとんど何も特別な対応取ることなく NetBSD が動いた。もちろんハイパーバイ ザは KVM です。だけど少し条件がある。\nqemu の不具合があり Ubuntu 12.04 LTS + Ubuntu Cloud Archives の組み合わせでは NetBSD が動作しなかった。下記のようなカーネルパニックが発生。\npanic: pci_make_tag: bad request  この不具合に相当するんじゃないかと思ってる。\nhttps://bugs.launchpad.net/qemu/+bug/897771\nよって下記の組み合わせで動作を確認した。\n Ubuntu 12.10 + OpenStack (Native Packages) qemu, kvm : 1.2.0+noroms-0ubuntu2.12.10.3 NetBSD 6.1 RC2 amd64  前提条件 OpenStack Folsom が動作していること。\nNetBSD OS イメージ作成 nova-compute が動作しているホストの qemu-kvm を利用する。OpenStack 上に何でも 良いので OS を動作させこの kvm プロセスのパラメータを参考に kvm コマンドを実行 し NetBSD をインストールさせた。一番確実な方法。\n% cd ~/ % wget http://ftp.netbsd.org/pub/NetBSD/iso/6.1_RC2/NetBSD-6.1_RC2-amd64.iso % kvm-img create -f qcow2 netbsd.img 5G % /usr/bin/kvm -M pc-1.0 -cpu \\ core2duo,+lahf_lm,+rdtscp,+hypervisor,+avx,+osxsave,+save,+aes,+popcnt,+sse4.2,+sse4.1,+cx16,+vmx,+pclmuldq,+ht,+ss,+ds \\ -enable-kvm -m 512 -smp 1,sockets=1,cores=1,threads=1 -device piix3-usb-uhci,id=usb,bus=pci.0,addr=0x1.0x2 \\ -drive file=~/netbsd.img,if=none,id=drive-virtio-disk0,format=qcow2,cache=none \\ -device virtio-blk-pci,scsi=off,bus=pci.0,addr=0x4,drive=drive-virtio-disk0,id=virtio-disk0,bootindex=1 \\ -net nic,model=virtio -vnc :9 -cdrom ~/NetBSD-6.1_RC2-amd64.iso  VNC :9 に接続して NetBSD を普通ににインストールする。\nインストールが終わったら CDROM デバイスを外して起動。\n% core2duo,+lahf_lm,+rdtscp,+hypervisor,+avx,+osxsave,+save,+aes,+popcnt,+sse4.2,+sse4.1,+cx16,+vmx,+pclmuldq,+ht,+ss,+ds \\ -enable-kvm -m 512 -smp 1,sockets=1,cores=1,threads=1 -device piix3-usb-uhci,id=usb,bus=pci.0,addr=0x1.0x2 \\ -drive file=~/netbsd.img,if=none,id=drive-virtio-disk0,format=qcow2,cache=none \\ -device virtio-blk-pci,scsi=off,bus=pci.0,addr=0x4,drive=drive-virtio-disk0,id=virtio-disk0,bootindex=1 \\ -net nic,model=virtio -vnc :9  VNC :9 に接続し下記の操作を行う。\nvioif0 という virtio なネットワークインターフェースが起動するので下記のように 追記する。\n# vi /etc/rc.conf # 下記を追記 ifconfig_vioif0=dhcp sshd=YES  OpenStack の metadata server から nova の管理するキーペア鍵を取得し authorized_keys に配置する様、/etc/rc.local に追記する。curl とか便利なツール はもちろん！入っていないので ftp コマンドでなんとかする。\n# vi /etc/rc.local # 下記を追記 if [ ! -d /root/.ssh ]; then mkdir -p /root/.ssh fi echo \u0026gt;\u0026gt; /root/.ssh/authorized_keys cd /tmp ftp http://169.254.169.254/latest/meta-data/public-keys/0/openssh-key cat openssh-key | grep \u0026#39;ssh-rsa\u0026#39; \u0026gt;\u0026gt; /root/.ssh/authorized_keys echo \u0026#34;AUTHORIZED_KEYS:\u0026#34; echo \u0026#34;*********************\u0026#34; cat /root/.ssh/authorized_keys echo \u0026#34;*********************\u0026#34; nova のキーペアでログインしたいので sshd は root ユーザでログイン出来るように 修正行う。\n# vi /etc/ssh/sshd_config PermitRootLogin yes  OS をシャットダウンしてイメージ作成は終わり。\nGlance へ登録 netbsd.img を Glance に接続できるホストへ移動しイメージ登録を行う。\n% glance add name=\u0026quot;NetBSD 6.1 RC2 amd64\u0026quot; is_public=true \\ container_format=ovf disk_format=qcow2 \u0026lt; ~/netbsd.img  まとめ 何も手を加えていない\u0026hellip;。NetBSD 6.1 は最初から Virtio が有効になっているので何 も考えることなくイメージ作成が出来た。コツは nova-compute が実際に動作している ホストでイメージを作ること。ハイパーバイザの OS が若干古いホストでも作業してみ たのだが、OpenStack に載せた途端カーネルパニックに陥った。Qemu はものすごいス ピードで進化しているのでバージョンの差異は致命的であると共に、日に日に快適な環 境が整ってきているとも言える。\n","permalink":"https://jedipunkz.github.io/post/2013/03/28/netbsd-on-openstack/","summary":"もう数日で OpenStack の次期バージョン版 Grizzly がリリースされるタイミングだが 現行バージョン Folsom の OpenStack の上に NetBSD を載せてみた。完全にお遊び だけど\u0026hellip;。\n結局、ほとんど何も特別な対応取ることなく NetBSD が動いた。もちろんハイパーバイ ザは KVM です。だけど少し条件がある。\nqemu の不具合があり Ubuntu 12.04 LTS + Ubuntu Cloud Archives の組み合わせでは NetBSD が動作しなかった。下記のようなカーネルパニックが発生。\npanic: pci_make_tag: bad request  この不具合に相当するんじゃないかと思ってる。\nhttps://bugs.launchpad.net/qemu/+bug/897771\nよって下記の組み合わせで動作を確認した。\n Ubuntu 12.10 + OpenStack (Native Packages) qemu, kvm : 1.2.0+noroms-0ubuntu2.12.10.3 NetBSD 6.1 RC2 amd64  前提条件 OpenStack Folsom が動作していること。\nNetBSD OS イメージ作成 nova-compute が動作しているホストの qemu-kvm を利用する。OpenStack 上に何でも 良いので OS を動作させこの kvm プロセスのパラメータを参考に kvm コマンドを実行 し NetBSD をインストールさせた。一番確実な方法。","title":"NetBSD on OpenStack"},{"content":"こんにちは。@jedipunkzです。\n今日は Chef Cookbook の管理をしてくれる Berkshelf について。\nBerkshelf は Librarian-Chef と同じく Cookbook の管理をしてくれるツールです。依 存関係のクリアもしてくれます。Opscode の中の人 @someara さんにこんなこと言われ て、\nLibrarian-chef じゃなくて Berkshelf 使えってことだろうなぁと思ったので僕は Bekshelf を使うようにしてます。先日ブログ記事にした openstack-chef-repo も以前 は Librarian-chef を使っていたのですが最近 Berkshelf に置き換わりました。 openstack-chef-repo は Opscode の中の人の @mattray さん達が管理しています。\nでは早速。\nインストール インストールは簡単。gem install するだけです。\n% gem install berkshelf  使い方 chef-repo 配下で Berksfile を下記のように書きます。\nsite :opscode cookbook 'chef-client' cookbook 'nginx', '= 0.101.2'  berks コマンドを実行して Cookbooks をダウンロードします。\n% export BERKSHELF_PATH=/Users/jedipunkz/chef-repo % berks install  それぞれ依存関係のある Cookbook 達もダウンロードされます。これでいちいち依存を 解決しつつダウンロードなんてこともしなくて済む。\nここで言えるのは chef-repo とこの Berksfile だけ git 等で管理すれば良いという こと。Cookbooks 達はそれぞれ別のレポジトリに git で管理する形が良いと思う。 プロジェクトで chef を使っていて chef-repo ごと管理しがちだと思うけど、この方 が Cookbook を他のプロジェクトでも使いまわせるメリットがある。\nopscode コミュニティ管理外の Cookbooks 自分で管理している cookbook 等も Berkshelf で管理出来ます。下記のように書くと、\nsite : opscode cookbook 'chef-client' cookbook 'nginx', '= 0.101.2' cookbook \u0026quot;pxe_install_server\u0026quot;, git: 'https://github.com/jedipunkz/pxe_install_server'  github から直接 git clone をしてくれます。\nCookbooks のアップデート Cookbook は独立した git レポジトリで管理すると良いと書きましたが、それぞれの Cookbooks は自分が若しくは他人が開発が進むので berks を使って最新の Cookbooks にアップデートすることも出来る。\n% berks update  バージョン指定 Cookbook のバージョン指定ももちろん出来る。先の例では\ncookbook 'nginx', '= 0.101.2'  では 0.101.2 と言うバージョンを指定した。その他\n Equal to (=) Greater than (\u0026gt;) Greater than equal to (\u0026lt;) Less than (\u0026lt;) Less than equal to (\u0026lt;=) Pessimistic (~\u0026gt;)  が指定出来るようだ。\nまとめ Cookbooks の管理は今のところ Berkshelf だけで OK。紹介したもの以外にも幾つか機 能があるので公式のサイトで確認してみて欲しい。個人的には path: 指定が Berksfile で出来ないのが不便。公式のサイトには一応記述あるのだが動かなかった。 そのうち改善されるだろう。また Chef サーバに Cookbooks をアップロードする機能 もあるが、まぁこれは knife でも出来るしいいか。一番欲しかったのは \u0026lsquo;依存関係の クリア\u0026rsquo; だったので。また Berksfile, Berksfile.lock のファイルさえプロジェクト で管理すれば良いと言うのが個人的には綺麗になるなぁという印象。\n昔、Linux の rpm コマンドでパッケージを入れてて依存関係にあるパッケージをコマ ンド打つたびにダウンロードして\u0026hellip;と言う問題を yum が解決してくれて。それに似て るなぁと見ていて思った。\n","permalink":"https://jedipunkz.github.io/post/2013/03/17/berkshelf-chef-cookbook-manage/","summary":"こんにちは。@jedipunkzです。\n今日は Chef Cookbook の管理をしてくれる Berkshelf について。\nBerkshelf は Librarian-Chef と同じく Cookbook の管理をしてくれるツールです。依 存関係のクリアもしてくれます。Opscode の中の人 @someara さんにこんなこと言われ て、\nLibrarian-chef じゃなくて Berkshelf 使えってことだろうなぁと思ったので僕は Bekshelf を使うようにしてます。先日ブログ記事にした openstack-chef-repo も以前 は Librarian-chef を使っていたのですが最近 Berkshelf に置き換わりました。 openstack-chef-repo は Opscode の中の人の @mattray さん達が管理しています。\nでは早速。\nインストール インストールは簡単。gem install するだけです。\n% gem install berkshelf  使い方 chef-repo 配下で Berksfile を下記のように書きます。\nsite :opscode cookbook 'chef-client' cookbook 'nginx', '= 0.101.2'  berks コマンドを実行して Cookbooks をダウンロードします。\n% export BERKSHELF_PATH=/Users/jedipunkz/chef-repo % berks install  それぞれ依存関係のある Cookbook 達もダウンロードされます。これでいちいち依存を 解決しつつダウンロードなんてこともしなくて済む。","title":"Berkshelf で Chef Cookbook の管理"},{"content":"こんにちは。@jedipunkzです。\n久々に Chef の話題。\n有名な方々が最近 Chef について記事書いたりで Chef が再び盛り上がってきましたね。 僕のブログにも chef-solo で検索してアクセスしてくる方が増えているようです。\nちょうど今、仕事で試験的なサービスを立ち上げてそこで Chef を使っているのですが Server, Workstation, Target Node(s) の構成で使っていて、僕らは最初から chef-solo と capistrano でってことは考えていませんでした。もちろん chef-solo + capistrano の環境も調査しましたが、今の Server 構成が便利なのでもう戻れない。\n今日は Chef サーバ構成の良さについての記事ではないですが、それについては次回、 時間見つけて書こうかと思ってます。\n今日は \u0026lsquo;chef-client をどうアップデートするか\u0026rsquo; について。せっかく Chef でサーバ構成を継続的にデプロイ出来ても Chef 自身をアップデート出来ないと悲しい ですよね。chef-client が稼働しているインスタンスなんて起動して利用してすぐに破 棄だって時代ですが、なかなかそうもいなかい気がしています。\n「ほら、だから chef-solo 使えばいいんだよ！」って思ってるあなた！違うんですよー。 そのデメリットを上回るメリットが Chef サーバ構成にあるんです。次回書きますｗ\nChef10 から Chef11 と試験してみるにはちょうど良い時期でした。今回の構成は\u0026hellip;\n旧環境 +------------------+ | chef server | | version 10.18 | +------------------+ ^ | +--------------------+ | | | | +------------------+ +------------------+ | chef workstation | | target node | | version 10.24 | | version 10.24 | +------------------+ +------------------+   chef server は apt.opscode.com レポジトリを利用した Chef 10.18 なもの chef workstaion は version 10.24 (2013年3月15日現在 10.x 系最新) target node は chef workstaion から knife bootstrap を行い構成  knife bootstrap の際に下記のオプションを指定します。\n% knife bootstrap -N vmtest01 -r 'role[base]' \\ -i -x root -d chef-full  distro は chef-full を選択。chef-full については下記にコードがあります。\nhttps://github.com/opscode/chef/blob/master/lib/chef/knife/bootstrap/chef-full.erb\nコードを抜粋すると、omnibus インストーラをダンロードして実行しているのがわかり ます。つまり Chef11 環境で再度 knife bootstrap すれば新しい Chef がインストー ルされるはず。\ninstall_sh=\u0026#34;http://opscode.com/chef/install.sh\u0026#34; version_string=\u0026#34;-v \u0026lt;%= chef_version %\u0026gt;\u0026#34; if ! exists /usr/bin/chef-client; then if exists wget; then bash \u0026lt;(wget \u0026lt;%= \u0026#34;--proxy=on \u0026#34; if knife_config[:bootstrap_proxy] %\u0026gt; ${install_sh} -O -) ${version_string} elif exists curl; then bash \u0026lt;(curl -L \u0026lt;%= \u0026#34;--proxy \\\u0026#34;#{knife_config[:bootstrap_proxy]}\\\u0026#34; \u0026#34; if knife_config[:bootstrap_proxy] %\u0026gt; ${install_sh}) ${version_string} else echo \u0026#34;Neither wget nor curl found. Please install one and try again.\u0026#34; \u0026gt;\u0026amp;2 exit 1 fi fi このように Chef10 で運用している状態を Chef11 に移行します。新しい環境は\u0026hellip;\n新環境 +------------------+ +------------------+ | chef server | | chef server | | version 10.18 | | version 11.0.6 | +------------------+ +------------------+ ^ | +--------------------+ | | | | +------------------+ +------------------+ | chef workstation | | target node | | version 11.4 | | version 11.4 | +------------------+ +------------------+   chef server は omnibus インストールから構築 chef workstation は version 11.4 (2013年3月15日現在最新)  移行手順 Chef11 サーバを用意する Omnibus インストーラでコマンド3つ打つだけで Chef サーバは構築出来ます。Chef11 からはこの方法が推奨されているようです。bootstrap を使った方法より簡単ですし。\n% wget https://opscode-omnitruck-release.s3.amazonaws.com/ubuntu/12.04/x86_64/chef-server_11.0.6-1.ubuntu.12.04_amd64.deb % sudo dpkg -i chef-server_11.0.6-1.ubuntu.12.04_amd64.deb % sudo chef-server-ctl reconfigure  Chef11 Workstation を用意する 詳細は割愛します。chef10 と同じです。手順としては\n chef10 の chef-repo をコピー pem ファイル群を chef11 server からコピー knife configure -i にて knife.rb の生成 cookbooks, roles, data_bags, environments 等を chef11 にアップロード。若干修正が必要な場合がある。  です。\nTarget Node における旧 chef-client の停止と削除 稼働している Chef10 の chef-client を停止し削除、そして pem ファイル達を退避し てあげます。ここでは ssh で消す例を書きますが、これこそ Cookbook を書いて実行 すれば良いと思います。\n% ssh -i \u0026lt;ssh_secret_key\u0026gt; -l root \u0026lt;ip_address\u0026gt; \\ 'service chef-client stop; apt-get -y remove chef; mv /etc/chef /etc/chef.old'  再 knife bootstrap 実行 Chef11 の Workstation から knife bootstrap を実行します。これによって Target Node に Chef11 の環境がインストールされ Chef11 Server と接続出来ます。\n% knife bootstrap \u0026lt;ip_address\u0026gt; -N vmtest01 -r 'role[base]' -i \u0026lt;ssh_secret_key\u0026gt; \\ -x root -d chef-full  role[base] 中の run_list に\u0026rsquo;chef-client::service' を追加すると chef-client が 常時稼動してくれて定期的に Chef Server と接続、更新してくれます。\nまとめと考察 chef-client の更新は簡単に出来た！\n今回はデフォルトの distro \u0026lsquo;chef-full\u0026rsquo; の例で書いたのだけど、distro には他にも 下記がある。\nhttps://github.com/opscode/chef/tree/master/lib/chef/knife/bootstrap\nLinux のディストリビューション名がついた distro は ruby をパッケージで, Chef を gem でインストールしている。下記は distro \u0026lsquo;ubuntu12.04-gems\u0026rsquo; のコードの抜粋 です。\nif [ ! -f /usr/bin/chef-client ]; then aptitude update aptitude install -y ruby ruby1.8-dev build-essential wget libruby1.8 rubygems fi gem update --no-rdoc --no-ri gem install ohai --no-rdoc --no-ri --verbose gem install chef --no-rdoc --no-ri --verbose \u0026lt;%= bootstrap_version_string %\u0026gt; ruby はディストリビューションが用意しているパッケージを。Chef は bootstrap_version_string を指定し gem でインストールしている。\nつまり \u0026lsquo;ubuntu12.04-gems\u0026rsquo; でも chef-client の更新は出来る。ちなみに試してみま した。chef-client の停止・削除を下記の通り行うと、全く同じ knife bootstrap コ マンドで chef-client のアップデートが行えた。\n% ssh -i \u0026lt;ssh_secret_key\u0026gt; -l root \u0026lt;ip_address\u0026gt; 'service chef-client stop; mv \\ /usr/local/bin/chef-client /usr/local/bin/chef-client.old; mv /etc/chef /etc/chef.old'  以上です。\nなんか簡単なこと書くのに長くなっちゃったけど\u0026hellip;。運用を僕らはまだ出来ていない のだけど、その時のことを考えると今回の試験はしてみたかったし、いい結果が出てよ かった。Chef がメジャーバージョンアップされて Chef Server の構成が大きく代わっ ても対応した cookbook, role, .. があればスムースに移行出来る。\nChef 無しにはインフラエンジニアやってられない時代が来たって感じｗ Chef 周りた のしー！\n追伸: 今回の方法より良いベストプラクティス的な方法があれば教えて下さい。\n","permalink":"https://jedipunkz.github.io/post/2013/03/15/chef-contenuously-deploy/","summary":"こんにちは。@jedipunkzです。\n久々に Chef の話題。\n有名な方々が最近 Chef について記事書いたりで Chef が再び盛り上がってきましたね。 僕のブログにも chef-solo で検索してアクセスしてくる方が増えているようです。\nちょうど今、仕事で試験的なサービスを立ち上げてそこで Chef を使っているのですが Server, Workstation, Target Node(s) の構成で使っていて、僕らは最初から chef-solo と capistrano でってことは考えていませんでした。もちろん chef-solo + capistrano の環境も調査しましたが、今の Server 構成が便利なのでもう戻れない。\n今日は Chef サーバ構成の良さについての記事ではないですが、それについては次回、 時間見つけて書こうかと思ってます。\n今日は \u0026lsquo;chef-client をどうアップデートするか\u0026rsquo; について。せっかく Chef でサーバ構成を継続的にデプロイ出来ても Chef 自身をアップデート出来ないと悲しい ですよね。chef-client が稼働しているインスタンスなんて起動して利用してすぐに破 棄だって時代ですが、なかなかそうもいなかい気がしています。\n「ほら、だから chef-solo 使えばいいんだよ！」って思ってるあなた！違うんですよー。 そのデメリットを上回るメリットが Chef サーバ構成にあるんです。次回書きますｗ\nChef10 から Chef11 と試験してみるにはちょうど良い時期でした。今回の構成は\u0026hellip;\n旧環境 +------------------+ | chef server | | version 10.18 | +------------------+ ^ | +--------------------+ | | | | +------------------+ +------------------+ | chef workstation | | target node | | version 10.","title":"chef-client の継続的デリバリ"},{"content":"OpenStack をコードで管理するためのフレームワークは幾つか存在するのだけど Ruby で記述出来る Fog が良い！と隣に座ってるアプリエンジニアが言うので僕も最近少し 触ってます。\nFog を使った OpenStack を管理するコードを書くことも大事なのだけど、Fog のコン トリビュートってことで幾つかの機能を付け足して (Quantum Router 周り) ってこと をやってました。まだ取り込まれてないけど。\nその開発の中で pry の存在を教えてもらいその便利さに驚いたので少し説明します。 バリバリ開発系の人は既に知っているだろうけど、インフラ系エンジニアの僕にとって は感激モノでした。\npry は irb 代替な Ruby のインタラクティブシェルです。下記の URL から持ってこれ ます。\nhttps://github.com/pry/pry\nシンタックスハイライトされたり json のレスポンスが綺麗に成形されたり irb 的に 使うだけでも便利なのだけど \u0026lsquo;?\u0026rsquo; や \u0026lsquo;$\u0026rsquo; でコードのシンタックスを確認したりコード 内容を確認したり出来るのがアツい！\nちょうど今回追加した Fog の機能を使って説明していみます。\nFog のコードを require して OpenStack に接続するための情報を設定し OpenStack Quantum に接続します。これで準備完了。\n[38] pry(main)\u0026gt; require \u0026#39;/home/jedipunkz/fog/lib/fog.rb\u0026#39; [49] pry(main)\u0026gt; @connection_hash = { [49] pry(main)* :openstack_username =\u0026gt; \u0026#39;demo\u0026#39;, [49] pry(main)* :openstack_api_key =\u0026gt; \u0026#39;demo\u0026#39;, [49] pry(main)* :openstack_tenant =\u0026gt; \u0026#39;service\u0026#39;, [49] pry(main)* :openstack_auth_url =\u0026gt; \u0026#39;http://172.16.1.11:5000/v2.0/tokens\u0026#39;, [49] pry(main)* :provider =\u0026gt; \u0026#39;OpenStack\u0026#39;, [49] pry(main)* } [50] pry(main)\u0026gt; @quantum = Fog::Network.new(@connection_hash) 試しに Router 一覧を取得します。list_routers メソッドです。\n[54] pry(main)\u0026gt; @quantum.list_routers() =\u0026gt; #\u0026lt;Excon::Response:0x00000003da3560 @body= \u0026#34;{\\\u0026#34;routers\\\u0026#34;: [{\\\u0026#34;status\\\u0026#34;: \\\u0026#34;ACTIVE\\\u0026#34;, \\\u0026#34;external_gateway_info\\\u0026#34;: {\\\u0026#34;network_id\\\u0026#34;: \\\u0026#34;b8ef37a9-9ed1-4b6d-862d-fe9e381a2f2a\\\u0026#34;}, \\\u0026#34;name\\\u0026#34;: \\\u0026#34;router-admin\\\u0026#34;, \\\u0026#34;admin_state_up\\\u0026#34;: true, \\\u0026#34;tenant_id\\\u0026#34;: \\\u0026#34;5e9544d4823a44d59f3591144049f691\\\u0026#34;, \\\u0026#34;id\\\u0026#34;: \\\u0026#34;35c65e2c-5cd8-4eb5-87a8-c370988c101a\\\u0026#34;}]}\u0026#34;, @data= {:body=\u0026gt; {\u0026#34;routers\u0026#34;=\u0026gt; [{\u0026#34;status\u0026#34;=\u0026gt;\u0026#34;ACTIVE\u0026#34;, \u0026#34;external_gateway_info\u0026#34;=\u0026gt; {\u0026#34;network_id\u0026#34;=\u0026gt;\u0026#34;b8ef37a9-9ed1-4b6d-862d-fe9e381a2f2a\u0026#34;}, \u0026#34;name\u0026#34;=\u0026gt;\u0026#34;router-admin\u0026#34;, \u0026#34;admin_state_up\u0026#34;=\u0026gt;true, \u0026#34;tenant_id\u0026#34;=\u0026gt;\u0026#34;5e9544d4823a44d59f3591144049f691\u0026#34;, \u0026#34;id\u0026#34;=\u0026gt;\u0026#34;35c65e2c-5cd8-4eb5-87a8-c370988c101a\u0026#34;}]}, :headers=\u0026gt; {\u0026#34;Content-Type\u0026#34;=\u0026gt;\u0026#34;application/json\u0026#34;, \u0026#34;Content-Length\u0026#34;=\u0026gt;\u0026#34;259\u0026#34;, \u0026#34;Date\u0026#34;=\u0026gt;\u0026#34;Wed, 06 Mar 2013 06:53:22 GMT\u0026#34;}, :status=\u0026gt;200, :remote_ip=\u0026gt;\u0026#34;172.16.1.11\u0026#34;}, @headers= {\u0026#34;Content-Type\u0026#34;=\u0026gt;\u0026#34;application/json\u0026#34;, \u0026#34;Content-Length\u0026#34;=\u0026gt;\u0026#34;259\u0026#34;, \u0026#34;Date\u0026#34;=\u0026gt;\u0026#34;Wed, 06 Mar 2013 06:53:22 GMT\u0026#34;}, @remote_ip=\u0026#34;172.16.1.11\u0026#34;, @status=200\u0026gt; 綺麗に色付けされてレスポンスがあります。\n次に \u0026lsquo;cd @quantum\u0026rsquo; して cd します。そして \u0026lsquo;? メソッド名\u0026rsquo; するとメソッドのシン タックスを確認出来ます。試しに Router を生成する create_router メソッドを見て みます。\n[56] pry(main)\u0026gt; cd @quantum [59] pry(#\u0026lt;Fog::Network::OpenStack::Real\u0026gt;):1\u0026gt; ? create_router From: /home/jedipunkz/fog/lib/fog/openstack/requests/network/create_router.rb @ line 6: Owner: Fog::Network::OpenStack::Real Visibility: public Signature: create_router(name, options=?) Number of lines: 1 そして \u0026lsquo;$ メソッド名\u0026rsquo; するとコードが確認出来ます。\n[64] pry(#\u0026lt;Fog::Network::OpenStack::Real\u0026gt;):1\u0026gt; $ create_router From: /home/jedipunkz/fog/lib/fog/openstack/requests/network/create_router.rb @ line 6: Owner: Fog::Network::OpenStack::Real Visibility: public Number of lines: 27 def create_router(name, options = {}) data = { \u0026#39;router\u0026#39; =\u0026gt; { \u0026#39;name\u0026#39; =\u0026gt; name, } } vanilla_options = [ :admin_state_up, :tenant_id, :network_id, :external_gateway_info, :status, :subnet_id ] vanilla_options.reject{ |o| options[o].nil? }.each do |key| data[\u0026#39;router\u0026#39;][key] = options[key] end request( :body =\u0026gt; Fog::JSON.encode(data), :expects =\u0026gt; [201], :method =\u0026gt; \u0026#39;POST\u0026#39;, :path =\u0026gt; \u0026#39;routers\u0026#39; ) end あとは \u0026lsquo;puts @quantum\u0026rsquo; 等するとオブジェクトの内容が確認出来たり、\u0026lsquo;ls @quantum\u0026rsquo; すると @quantum オブジェクトのメソッド一覧が確認出来たり。\n開発の効率が上がるなぁと感激。\n春なので OpenStack もそろろろ次期リリースの時期。それぞれのコンポーネントの機 能が拡張されているようなので Fog 等のフレームワークにコントリビュートする機会 もますます増えそう。Fog やその他のクラウドフレームワークはなんだかんだ言って AWS のフューチャがメインなので OpenStack の機能追加に追いついていない感がある。 もし興味持っている人が居たら是非一緒に OpenStack 界隈を盛り上げましょう。\n","permalink":"https://jedipunkz.github.io/post/2013/03/06/pry/","summary":"OpenStack をコードで管理するためのフレームワークは幾つか存在するのだけど Ruby で記述出来る Fog が良い！と隣に座ってるアプリエンジニアが言うので僕も最近少し 触ってます。\nFog を使った OpenStack を管理するコードを書くことも大事なのだけど、Fog のコン トリビュートってことで幾つかの機能を付け足して (Quantum Router 周り) ってこと をやってました。まだ取り込まれてないけど。\nその開発の中で pry の存在を教えてもらいその便利さに驚いたので少し説明します。 バリバリ開発系の人は既に知っているだろうけど、インフラ系エンジニアの僕にとって は感激モノでした。\npry は irb 代替な Ruby のインタラクティブシェルです。下記の URL から持ってこれ ます。\nhttps://github.com/pry/pry\nシンタックスハイライトされたり json のレスポンスが綺麗に成形されたり irb 的に 使うだけでも便利なのだけど \u0026lsquo;?\u0026rsquo; や \u0026lsquo;$\u0026rsquo; でコードのシンタックスを確認したりコード 内容を確認したり出来るのがアツい！\nちょうど今回追加した Fog の機能を使って説明していみます。\nFog のコードを require して OpenStack に接続するための情報を設定し OpenStack Quantum に接続します。これで準備完了。\n[38] pry(main)\u0026gt; require \u0026#39;/home/jedipunkz/fog/lib/fog.rb\u0026#39; [49] pry(main)\u0026gt; @connection_hash = { [49] pry(main)* :openstack_username =\u0026gt; \u0026#39;demo\u0026#39;, [49] pry(main)* :openstack_api_key =\u0026gt; \u0026#39;demo\u0026#39;, [49] pry(main)* :openstack_tenant =\u0026gt; \u0026#39;service\u0026#39;, [49] pry(main)* :openstack_auth_url =\u0026gt; \u0026#39;http://172.","title":"pry のススメ"},{"content":"2013年2月9日に行われた OpenStack 勉強会第11回で話してきました。\nopenstack-chef-repo と言う、Opscode Chef で OpenStack を構築する内容を話して きました。その時に説明出来なかった詳細についてブログに書いておきます。\n説明で使ったスライドです。\nまえがき Essex ベースで構築することしか今は出来ません。Folsom に関しては \u0026lsquo;直ちに開発が スタートする\u0026rsquo; と記されていました。今回は Opscode と RackSpace のエンジニアが共 同で開発を進めているので期待しています。今まで個人で OpenStack の各コンポーネ ントの cookbook を開発されていた方がいらっしゃるのだけど汎用性を持たせるという 意味で非常に難しく、またどの方の開発に追従していけばよいか判断困っていました。 よって今回こそ期待。\n前提の構成 +-------------+ | chef-server | +-------------+ 10.0.0.10 | +---------------+ 10.0.0.0/24 | | +-------------+ +-------------+ | workstation | | node | +-------------+ +-------------+ 10.0.0.11 10.0.0.12   chef-server : chef API を持つ chef-server 。cookbook, role..などのデータを持つ workstation : openstack-chef-repo を使うノード。knife が使える必要がある。 node : OpenStack を構築するターゲットノード  目次  chef-server の構築 (BootStrap 使う) openstack-chef-repo を使用する準備 openstack-chef-repo 実行  chef-server の構築 Opscode の wiki に記されている通りなのですが、簡単に書いておきます。今回は bootstrap 方式で用意します。\nchef-server ノードに chef をインストールします。chef-solo を用いるからです。\nchef-server# gem install chef  chef-solo を使うための設定情報を /etc/chef/solo.rb に記します。\nchef-server# mkdir /etc/chef chef-server# ${EDITOR} /etc/chef/solo.rb file_cache_path \u0026quot;/tmp/chef-solo\u0026quot; cookbook_path \u0026quot;/tmp/chef-solo/cookbooks\u0026quot;  chef-solo を実行する際に使う json ファイルを用意します。attribute を上書きする ことが出来ます。今回は webui を有効にした状態にします。\nchef-server# ${EDITOR} ~/chef.json { \u0026quot;chef_server\u0026quot;: { \u0026quot;server_url\u0026quot;: \u0026quot;http://localhost:4000\u0026quot;, \u0026quot;init_style\u0026quot;: \u0026quot;runit\u0026quot; }, \u0026quot;run_list\u0026quot;: [ \u0026quot;recipe[chef-server::rubygems-install]\u0026quot; ] }  bootstrap して chef-server を構築します。\nchef-server# chef-solo -c /etc/chef/solo.rb -j ~/chef.json -r http://s3.amazonaws.com/chef-solo/bootstrap-latest.tar.gz  chef-server が完成したはずです。workstation で knife を使うために \u0026lsquo;client\u0026rsquo; を 作ります。chef 10.x では user ではなく client が knife を実行します。\nchef-server% mkdir ~/.chef chef-server% sudo cp /etc/chef/validation.pem ~/.chef/ chef-server% sudo cp /etc/chef/webui.pem ~/.chef/ chef-server% sudo chown \u0026lt;my-username\u0026gt;:\u0026lt;my-group\u0026gt; ~/.chef  knife を使うために下記の操作を行います。\nchef-server% cd ~ chef-server% knife configure -i Where should I put the config file? [~/.chef/knife.rb] Please enter the chef server URL: [http://10.0.0.1:4000] Please enter a clientname for the new client: [jedipunkz] Please enter the existing admin clientname: [chef-webui] Please enter the location of the existing admin client's private key:[/etc/chef/webui.pem] /home/jedipunkz/.chef/webui.pem Please enter the validation clientname: [chef-validator] Please enter the location of the validation key:[/etc/chef/validation.pem] /home/jedipunkz/.chef/validation.pem Please enter the path to a chef repository (or leave blank): Creating initial API user... Created client[jedipunkz] Configuration file written to /home/jedipunkz/.chef/knife.rb  workstaion ノードで knife を操作するための \u0026lsquo;worker\u0026rsquo; client を作成します。\nchef-server% export EDITOR=vim chef-server% knife client create worker -a -f worker.pem chef-server% knife client list chef-validator chef-webui chefserver.example.com jedipunkz worker  openstack-chef-repo を使用する準備 workstation で openstack-chef-repo を使うための準備をします。\nまずは knife を使えるように下記のように knife.rb や pem の手元への転送を行います。\nworkstation% gem install chef librarian spiceweasel workstation% cd ~ workstation% git clone git://github.com/opscode/openstack-chef-repo.git workstation% cd openstack-chef-repo workstation% vim .chef/knife.rb log_level :info log_location STDOUT node_name 'worker' client_key '/home/jedipunkz/openstack-chef-repo/.chef/worker.pem' validation_client_name 'chef-validator' validation_key '/home/jedipunkz/openstack-chef-repo/.chef/validation.pem' chef_server_url 'http://10.0.0.10:4000' cache_type 'BasicFile' cache_options( :path =\u0026gt; '/home/jedipunkz/openstack-chef-repo/.chef/checksums' ) cookbook_path '/home/jedipunkz/openstack-chef-repo/cookbooks' workstation% scp 10.0.0.10:~/worker.pem .chef/ workstation% scp 10.0.0.10:~/.chef/validation.pem .chef/  librarian を使って OpenStack 構築に必要な cookbooks をダンロードします。Cheffile というファイルに何をどこから取得するのか記されいてそれにしたがってダウンロードされます。\nworkstation% libratrian-chef update  環境に合わせて production.yml を修正します。今回必要最低限の箇所のみ修正します。 \u0026ldquo;osops_networks\u0026rdquo; という箇所に nova-network に渡すネットワーク情報があるので今回の 環境 10.0.0.0/24 に修正します。全体はこのような内容になります。\nworkstation% ${EDITOR} production.yml name \u0026quot;production\u0026quot; description \u0026quot;Defines the network and database settings you're going to use with OpenStack. The networks will be used in the libraries provided by the osops-utils cookbook. This example is for FlatDHCP with 2 physical networks.\u0026quot; override_attributes( \u0026quot;glance\u0026quot; =\u0026gt; { \u0026quot;image_upload\u0026quot; =\u0026gt; true, \u0026quot;images\u0026quot; =\u0026gt; [\u0026quot;precise\u0026quot;,\u0026quot;cirros\u0026quot;], }, \u0026quot;mysql\u0026quot; =\u0026gt; { \u0026quot;allow_remote_root\u0026quot; =\u0026gt; true, \u0026quot;root_network_acl\u0026quot; =\u0026gt; \u0026quot;%\u0026quot; }, \u0026quot;osops_networks\u0026quot; =\u0026gt; { \u0026quot;public\u0026quot; =\u0026gt; \u0026quot;10.0.0.0/24\u0026quot;, \u0026quot;management\u0026quot; =\u0026gt; \u0026quot;10.0.0.0/24\u0026quot;, \u0026quot;nova\u0026quot; =\u0026gt; \u0026quot;10.0.0.0/24\u0026quot; }, \u0026quot;nova\u0026quot; =\u0026gt; { \u0026quot;network\u0026quot; =\u0026gt; { \u0026quot;fixed_range\u0026quot; =\u0026gt; \u0026quot;192.168.100.0/24\u0026quot;, \u0026quot;public_interface\u0026quot; =\u0026gt; \u0026quot;eth0\u0026quot; }, \u0026quot;networks\u0026quot; =\u0026gt; [ { \u0026quot;label\u0026quot; =\u0026gt; \u0026quot;public\u0026quot;, \u0026quot;ipv4_cidr\u0026quot; =\u0026gt; \u0026quot;192.168.100.0/24\u0026quot;, \u0026quot;num_networks\u0026quot; =\u0026gt; \u0026quot;1\u0026quot;, \u0026quot;network_size\u0026quot; =\u0026gt; \u0026quot;255\u0026quot;, \u0026quot;bridge\u0026quot; =\u0026gt; \u0026quot;br100\u0026quot;, \u0026quot;bridge_dev\u0026quot; =\u0026gt; \u0026quot;eth0\u0026quot;, \u0026quot;dns1\u0026quot; =\u0026gt; \u0026quot;8.8.8.8\u0026quot;, \u0026quot;dns2\u0026quot; =\u0026gt; \u0026quot;8.8.4.4\u0026quot; } ] } )  infrastructure.yml という spiceweasel が参照するファイルの修正を行います。 cookbooks, roles, environments, data bags, node とパラメータがあり node のみ記されていないので今回用意したターゲットノードの情報を追記します。\nnode には予め SSH 公開鍵を設置する必要があります。また下記は root ユーザでログ インしていますが、sudo してもらっても構わないです。role で指定した \u0026lsquo;allinone\u0026rsquo; は OpenStack の全てのコンポーネントを一台のノードにインストールするためのもの です。これを他の role 例えば \u0026lsquo;keystone\u0026rsquo; 等を指定して複数のノードに OpenStack を展開することも可能なはずです。試していませんが。\nworkstation% vim infrastructure.yml ... 省略 nodes: - 10.0.0.12: run_list: role[allinone] options: -i ~/.ssh/id_rsa -x root -E production  openstack-chef-repo 実行 準備が整ったので spiceweasel を使って knife コマンドの出力チェックと実行をして みます。先ほど追記した infrastructure.yml を使います。\nworkstation% spiceweasel infrastructure.yml knife cookbook upload apache2 knife cookbook upload apt knife cookbook upload aws knife cookbook upload build-essential knife cookbook upload ntp knife cookbook upload openssh knife cookbook upload openssl knife cookbook upload postgresql knife cookbook upload selinux knife cookbook upload xfs knife cookbook upload yum knife cookbook upload erlang knife cookbook upload mysql knife cookbook upload rabbitmq knife cookbook upload database knife cookbook upload omnibus_updater knife cookbook upload lxc knife cookbook upload sysctl knife cookbook upload osops-utils knife cookbook upload mysql-openstack knife cookbook upload rabbitmq-openstack knife cookbook upload keystone knife cookbook upload glance knife cookbook upload nova knife cookbook upload horizon knife environment from file production.rb knife role from file base.rb knife role from file lxc.rb knife role from file mysql-master.rb knife role from file rabbitmq-server.rb knife role from file keystone.rb knife role from file glance-api.rb knife role from file glance-registry.rb knife role from file glance.rb knife role from file nova-setup.rb knife role from file nova-scheduler.rb knife role from file nova-api-ec2.rb knife role from file nova-api-os-compute.rb knife role from file nova-volume.rb knife role from file nova-vncproxy.rb knife role from file horizon-server.rb knife role from file single-controller.rb knife role from file single-compute.rb knife role from file allinone.rb knife bootstrap 10.0.0.12 -i ~/.ssh/id_rsa -x root -E production -r 'role[allinone]'  この操作で spiceweasel は role ファイルの中身と yml ファイルを読み、依存関係を チェックしてくれます。\nではいよいよ実行。\nworkstaion% spiceweasel -e infrastructure.yml  数分で OpenStack 環境が構築出来ると思います。\n所感 現時点ではまだ essex ベースの OpenStack しか構築出来ない。folsom 以降について は直ちに行われる。また先にも記したが Opscode と RackSpace のエンジニアが共同で 作業に入ったので今後に期待。複数の OpenStack cookboooks を見てきたが個人で開発 するのは厳しいと感じています。fedra, centos, ubuntu, debian \u0026hellip; いろんなプラッ トフォームを前提に開発するのは厳しい\u0026hellip;。pull request する余力があればやってみ たい。また合わせて各コンポーネントの cookbooks についても同様に参加していきた い。\nchef は繰り返し実行されるので継続的にデプロイが可能であり、chef Resources が我々 の操作を抽象化してくれる。尚且つ API で開発も容易になる。学習コストは若干高い し、cookbooks の開発も正直しんどい。だけどインフラを chef でコントロールする意 味はとても大きい。その他のメリットとして属人的な操作に依存したシステムを無くす という事もある。\nOpenStack のドキュメントにはデプロイ方法として dodai-deploy と puppet が掲載さ れている。chef は載っていない。これは cookbooks の開発が遅れている状況が理由に あると思う。今回 Opscode の中の Matt Ray さんの下記の資料\nhttp://www.slideshare.net/mattray/chef-11-previewchef-for-openstack\nを見て、これからに期待したいと感じた。\n","permalink":"https://jedipunkz.github.io/post/2013/02/10/di-11hui-openstack-study11-openstack-chef-repo/","summary":"2013年2月9日に行われた OpenStack 勉強会第11回で話してきました。\nopenstack-chef-repo と言う、Opscode Chef で OpenStack を構築する内容を話して きました。その時に説明出来なかった詳細についてブログに書いておきます。\n説明で使ったスライドです。\nまえがき Essex ベースで構築することしか今は出来ません。Folsom に関しては \u0026lsquo;直ちに開発が スタートする\u0026rsquo; と記されていました。今回は Opscode と RackSpace のエンジニアが共 同で開発を進めているので期待しています。今まで個人で OpenStack の各コンポーネ ントの cookbook を開発されていた方がいらっしゃるのだけど汎用性を持たせるという 意味で非常に難しく、またどの方の開発に追従していけばよいか判断困っていました。 よって今回こそ期待。\n前提の構成 +-------------+ | chef-server | +-------------+ 10.0.0.10 | +---------------+ 10.0.0.0/24 | | +-------------+ +-------------+ | workstation | | node | +-------------+ +-------------+ 10.0.0.11 10.0.0.12   chef-server : chef API を持つ chef-server 。cookbook, role..などのデータを持つ workstation : openstack-chef-repo を使うノード。knife が使える必要がある。 node : OpenStack を構築するターゲットノード  目次  chef-server の構築 (BootStrap 使う) openstack-chef-repo を使用する準備 openstack-chef-repo 実行  chef-server の構築 Opscode の wiki に記されている通りなのですが、簡単に書いておきます。今回は bootstrap 方式で用意します。","title":"第11回OpenStack勉強会で話してきた"},{"content":"Spiceweasel https://github.com/mattray/spiceweasel#cookbooks を使ってみた。\nSpiceweasel は Chef の cookbook のダウンロード, role/cookbook の chef server へのアップロード, ブートストラップ等をバッチ処理的に行なってくれる(もしくはコ マンドラインを出力してくれる)ツールで、自分的にイケてるなと感じたのでブログに 書いておきます。\nクラウドフェデレーション的サービスというかフロントエンドサービスというか、複数 のクラウドを扱えるサービスは増えてきているけど、chef を扱えるエンジニアであれ ば、この Spiceweasel で簡単・一括デプロイ出来るので良いのではないかと。\n早速だけど chef-repo にこんな yamp ファイルを用意します。\ncookbooks: - apt: - nginx: roles: - base: nodes: - 172.24.17.3: run_list: role[base] options: -i ~/.ssh/testkey01 -x root -N webset01 - 172.24.17.4: run_list: role[base] options: -i ~/.ssh/testkey01 -x root -N webset02  上から説明すると\u0026hellip;\n \u0026lsquo;apt\u0026rsquo;, \u0026lsquo;nginx\u0026rsquo; の cookbook を opscode レポジトリからダウンロード \u0026lsquo;apt\u0026rsquo;, \u0026lsquo;nginx\u0026rsquo; の cookbook を chef-server へアップロード roles/base.rb を chef-server へアップロード 2つのノードに対して bootstrap 仕掛ける  ってことをやるためのファイルです。予め chef-repo と roles は用意してあげる必要 があります。この辺りは knife の操作のための準備と全く同じ。また Spiceweasel は、 この yaml フィアル内の各パラメータや指定した role の内容の依存関係をチェックし てくれます。\nでは、このファイルに対して\n% spiceweasel \u0026lt;yaml ファイル名\u0026gt;  すると、結果として下記のようなバッチが取得出来る。\nknife cookbook site download apt --file cookbooks/apt.tgz tar -C cookbooks/ -xf cookbooks/apt.tgz rm -f cookbooks/apt.tgz knife cookbook upload apt knife cookbook site download test --file cookbooks/test.tgz tar -C cookbooks/ -xf cookbooks/test.tgz rm -f cookbooks/test.tgz knife cookbook upload test knife role from file base.rb knife bootstrap 172.24.17.3 -i ~/.ssh/testkey01 -x root -N webset01 -r 'role[base]' knife bootstrap 172.24.17.4 -i ~/.ssh/testkey01 -x root -N webset02 -r 'role[base]'  尚且つ -e オプションを指定すると、実際にこれらのバッチを実行出来る。cookbooks ディレクトリに予めレシピが存在すればダウンロードのバッチは省略されるぽいし、-d を指定すると逆にノード削除バッチ処理、-r でリビルドのためのバッチ処理が得られ る。\n削除系・リビルド系は現時点では不具合が見られました。実行すると他の環境にも影響 出るので注意が必要。\n個人的にはプライベートレポジトリの cookbooks も取ってこれるようになると嬉しい。 まぁ、Berkshelf 使えばいいのだけど。http://berkshelf.com/\n開発者の Matt Ray さんの資料によれば Chef for OpenStack もこの Spiceweasel を 使う方向で修正が掛かったらしい。個人的には一番興味あるところ。ちなみに Chef for OpenStack は folsom ベースが現在 \u0026lsquo;active development\u0026rsquo; 状態らしい。\nOpscode のサイトで紹介されているサンプルは古いバージョンでの指定方法らしく、注 意が必要です。\nhttp://wiki.opscode.com/display/chef/Spiceweasel\n所感 chef, knife 周りはいろんな関連技術があるので技術を選定する上で迷ってしまうこと が多いのだけど、この Spiceweasel には可能性を感じました。って言うのは、Chef が 実装出来ていないインテグレーションやオーケストレーションっていう所まで踏み込め る可能性があるから。ノード間の関連付けが出来るんです。Swift-Storage と Swift-Proxy の関連付け、または Load-Balancer と HTTP-Server の関連付け等。継続 的デリバリなインテグレーションって Chef を使ってどう実現するんだ？って思ってい た時があったのですが、こういったラッパーツールの登場で解決されそうな気がします。\n","permalink":"https://jedipunkz.github.io/post/2013/02/01/spiceweasel-knife-bootstrap/","summary":"Spiceweasel https://github.com/mattray/spiceweasel#cookbooks を使ってみた。\nSpiceweasel は Chef の cookbook のダウンロード, role/cookbook の chef server へのアップロード, ブートストラップ等をバッチ処理的に行なってくれる(もしくはコ マンドラインを出力してくれる)ツールで、自分的にイケてるなと感じたのでブログに 書いておきます。\nクラウドフェデレーション的サービスというかフロントエンドサービスというか、複数 のクラウドを扱えるサービスは増えてきているけど、chef を扱えるエンジニアであれ ば、この Spiceweasel で簡単・一括デプロイ出来るので良いのではないかと。\n早速だけど chef-repo にこんな yamp ファイルを用意します。\ncookbooks: - apt: - nginx: roles: - base: nodes: - 172.24.17.3: run_list: role[base] options: -i ~/.ssh/testkey01 -x root -N webset01 - 172.24.17.4: run_list: role[base] options: -i ~/.ssh/testkey01 -x root -N webset02  上から説明すると\u0026hellip;\n \u0026lsquo;apt\u0026rsquo;, \u0026lsquo;nginx\u0026rsquo; の cookbook を opscode レポジトリからダウンロード \u0026lsquo;apt\u0026rsquo;, \u0026lsquo;nginx\u0026rsquo; の cookbook を chef-server へアップロード roles/base.","title":"Spiceweasel で knife バッチ処理"},{"content":"(2013/08/31 修正しました)\n自宅のノート PC にいつも Debian Gnu/Linux unstable を入れて作業してたのだけど、 Arch Linux が試したくなって入れてみた。すごくイイ。ミニマル思考で常に最新。端 末に入れる OS としては最適かも!と思えてきた。Ubuntu はデスクトップ環境で扱う にはチト大きすぎるし。FreeBSD のコンパイル待ち時間が最近耐えられないし\u0026hellip;。\n前リリースの Arch Linux には /arch/setup という簡易インストーラがあったのだけ ど、それすら最近無くなった。環境作る方法を自分のためにもメモしておきます。\nOS イメージ iso 取得とインストール用 USB スティック作成 Linux, Windows, Mac で作り方が変わるようだけど、自分は Mac OSX を使ってインス トール USB スティックを作成した。\ndiskutil で USB スティック装着前後の disk デバイス番号を覚える\n% diskutil list  (ここでは /dev/rdisk4 として進める。)\nアンマウントする。\n% sudo diskutil unmountDisk /dev/rdisk4  ダウンロードした iso を USB スティックに書き込む。\n% sudo dd if=/path/to/downloaded/iso of=/dev/rdisk4 bs=8192 % sudo diskutil eject /dev/rdisk4  USB スティック装着しインストール開始 起動するとメニューが表示されるので x86_64 を選んで起動。プロンプトが表示される。\nまず disk のパーティション作成。\n# fdisk /dev/sda # 環境に合わせてデバイス名変更 # fdisk -l /dev/sda デバイス ブート 始点 終点 ブロック Id システム /dev/sda1 2048 4196351 2097152 82 Linux スワップ / Solaris /dev/sda2 * 4196352 156301487 76052568 83 Linux  上記のように切りました。fdisk の使い方は\u0026hellip;ぐぐってください。また /boot にあたるパーティション にブートフラグを必ず付けること。\nファイルシステムを作ってマウント。\n# mkfs.ext4 /dev/sda2 # mount -t ext4 /dev/sda2 /mnt  コアシステムのインストール\n# pacstrap /mnt base base-devel  fstab の配置。必要に応じて /dev/sda1 (上記で作った) をスワップとして加筆。\n# genfstab -p /mnt \u0026gt;\u0026gt; /mnt/etc/fstab  chroot して /dev/sda2 内コアシステムで作業 chroot する。\n# arch-chroot /mnt  キーマップを us 配列で CTRL と Caps をスワップしたいので下記の作業を実施。\n# cd /usr/share/kbd/keymaps/i386/qwerty # cp us.map.gz usx.map.gz # gunzip usx.map.gz # vi usx.map keycode 58 = Control # Caps_Lock を Control に置き換え # gzip usx.map # loadkeys usx # キーマップをロード # vi /etc/vconsole.conf KEYMAP=usx # 追記  ロケールの生成。\n# vi /etc/locale.gen # ja_JP.UTF-8 のコメントアウトを削除 # locale-gen  タイムゾーンの設定。\n# ln -s /usr/share/zoneinfo/Asia/Tokyo /etc/localtime  ホスト名修正。\n# echo \u0026quot;\u0026lt;ホスト名\u0026gt;\u0026quot; \u0026gt; /etc/hostname # vi /etc/hosts # 適宜修正  ネットワークの設定。まずは有線。\n# systemctl enable dhcpcd@eth0.service  ブートローダとして syslinux を使う。ディスクデバイス名だけ修正。syslinux のインストーラが うまくデバイス名を拾ってくれない。\n# pacman -S syslinux # vi /boot/syslinux/syslinux.cfg LABEL arch MENU LABEL Arch Linux LINUX ../vmlinuz-linux APPEND root=/dev/sda2 ro INITRD ../initramfs-linux.img  パスワード設定。\n# passwd  再起動し システムをインストールした /dev/sda2 で起動する。 再起動し USB スティックを抜く。起動したら root ユーザでログイン。\n無線デバイスの設定。\n# pacman -S dialog wpa_supplicant # wifi-menu  検知されたアクセスポイント名が表示されるので希望のモノを選択しパスフレーズを入力。\n一般ユーザの作成と sudo 設定。\n# useradd -u \u0026lt;UID) -d /home/\u0026lt;UESRNAME\u0026gt; -m \u0026lt;USERNAME\u0026gt; # passwd \u0026lt;USERNAME\u0026gt; # pacman -S sudo # vigr # wheel グループに自分を追記 # visudo # wheel のところのコメントアウトを削除 %wheel ALL=(ALL) ALL  X 周りの設定 X とウィンドウマネージャのインストール。ウィンドウマネージャは好きなものを。私 は awesome を選びました。\n# pacman -S xorg xorg-xinit xterm rxvt-unicode xf86-video-intel awesome  日本語フォントのインストールと日本語インプットメソッドの設定。ibus と mozc を選んだ。 結構賢い。とりあえずのフォントとして sazanami を。\n# vi /etc/pacman.conf # 下記のレポジトリを追記 [pnsft-pur] SigLevel = Optional TrustAll Server = http://downloads.sourceforge.net/project/pnsft-aur/pur/$arch # pacman -Syy # pacman -S ttf-sazanami ibus-mozc mozc  一般ユーザの ~/.xinitrc に下記を追記。\n% vi ~/.xinitrc xrdb -merge $HOME/.Xresources export LANG=ja_JP.UTF-8 export LANGUAGE=ja_JP.UTF-8 export LC_ALL=ja_JP.UTF-8 export LC_CTYPE=ja_JP.UTF-8 export GTK_IM_MODULE=ibus export QT_IM_MODULE=xim export XMODIFIERS=@im=ibus ibus-daemon --daemonize --xim \u0026amp; setxkbmap -layout us -option ctrl:nocaps exec awesome  X を起動。\n% startx  私は awesome を選びましたが、たまに enlightenment も選択します。この時点で enlightenment の メニューなど、日本語化されているはずだが、厳しければ\nhttps://wiki.archlinux.org/index.php/Input_Japanese_using_uim_(%E6%97%A5%E6%9C%AC%E8%AA%9E)\nにある IPA フォント、VL ゴシックなどを入れる。こちらのほうが数段綺麗。\nIPA フォントの入れ方だけメモっておく。\n% wget https://aur.archlinux.org/packages/ot/otf-ipafont/otf-ipafont.tar.gz % tar zxvf otf-ipafont.tar.gz % cd otf-ipafont % makepkg -s % sudo pacman -U \u0026lt;生成された pkg.tar.xz ファイル\u0026gt;  Thinkpad の真ん中ボタンでスクロールする。\n# vi /etc/X11/xorg.conf.d/10-thinkpad.conf # 新規生成 Section \u0026quot;InputClass\u0026quot; Identifier \u0026quot;Trackpoint Wheel Emulation\u0026quot; MatchProduct \u0026quot;TPPS/2 IBM TrackPoint|DualPoint Stick|Synaptics Inc. Composite TouchPad / TrackPoint|ThinkPad USB Keyboard with TrackPoint|USB Trackpoint pointing device|Composite TouchPad / TrackPoint\u0026quot; MatchDevicePath \u0026quot;/dev/input/event*\u0026quot; Option \u0026quot;EmulateWheel\u0026quot; \u0026quot;true\u0026quot; Option \u0026quot;EmulateWheelButton\u0026quot; \u0026quot;2\u0026quot; Option \u0026quot;Emulate3Buttons\u0026quot; \u0026quot;false\u0026quot; Option \u0026quot;XAxisMapping\u0026quot; \u0026quot;6 7\u0026quot; Option \u0026quot;YAxisMapping\u0026quot; \u0026quot;4 5\u0026quot; EndSection  X の再起動をして終了。\nまとめ 今回はブログというより雑多なまとめになったけど、メモするだけでも意味があるので 残しておいた。Debian / Ubuntu より手間は掛かるけど、エンジニアが何をやっている か掴めるのでいいカンジ。無駄なプロセスいないので起動もめちゃ速いし。何より最新 のソフトウェア (安定したもの) が簡単に使えるのは嬉しい。ここに書いた手順は時間 がすぎるに連れて意味がないものになっていくだろう。開発が活発でここ数年でも結構 劇的にシステムが変更になってるみたい。systemd .. とか。理解するのに苦労する所 はないので、エンジニアであれば誰でも扱えそうなところも魅力。\nOS なんて何でもいい時代 (抽象化されつつあるから) だけど、手元の端末に入れる OS だけは Arch Linux がいいなぁ。\n","permalink":"https://jedipunkz.github.io/post/2013/01/14/arch-linux-setup/","summary":"(2013/08/31 修正しました)\n自宅のノート PC にいつも Debian Gnu/Linux unstable を入れて作業してたのだけど、 Arch Linux が試したくなって入れてみた。すごくイイ。ミニマル思考で常に最新。端 末に入れる OS としては最適かも!と思えてきた。Ubuntu はデスクトップ環境で扱う にはチト大きすぎるし。FreeBSD のコンパイル待ち時間が最近耐えられないし\u0026hellip;。\n前リリースの Arch Linux には /arch/setup という簡易インストーラがあったのだけ ど、それすら最近無くなった。環境作る方法を自分のためにもメモしておきます。\nOS イメージ iso 取得とインストール用 USB スティック作成 Linux, Windows, Mac で作り方が変わるようだけど、自分は Mac OSX を使ってインス トール USB スティックを作成した。\ndiskutil で USB スティック装着前後の disk デバイス番号を覚える\n% diskutil list  (ここでは /dev/rdisk4 として進める。)\nアンマウントする。\n% sudo diskutil unmountDisk /dev/rdisk4  ダウンロードした iso を USB スティックに書き込む。\n% sudo dd if=/path/to/downloaded/iso of=/dev/rdisk4 bs=8192 % sudo diskutil eject /dev/rdisk4  USB スティック装着しインストール開始 起動するとメニューが表示されるので x86_64 を選んで起動。プロンプトが表示される。","title":"Arch Linux セットアップまとめ"},{"content":"以前紹介した OpenStack Folsom 構築 bash スクリプトなのだけど quantum の代わり に nova-network も使えるようにしておいた。\n構築 bash スクリプトは、\nhttps://github.com/jedipunkz/openstack_folsom_deploy/blob/master/README_jp.md\nに詳しい使い方を書いておきました。またパラメータを修正して実行するのだけどパラ メータについては、\nhttps://github.com/jedipunkz/openstack_folsom_deploy/blob/master/README_parameters_jp.md\nに書いておきました。\n手持ちの Thinkpad に OpenStack folsom 入れた写真。この写真の OpenStack Folsom を構築した時の手順を書いておくよ。\nOS をインストール OS をインストールします。12.10 を使いました。(12.04 LTS でも可)。/dev/sda6 等、 Cinder 用に一つパーティションを作ってマウントしないでおきます。また固定 IP ア ドレスを NIC に付与しておきます。\nスクリプト取得 スクリプトを取得する。\n% sudo apt-get update; sudo apt-get install git-core % git clone https://github.com/jedipunkz/openstack_folsom_deploy.git % cd openstack_folsom_deploy  パラメータ修正 deploy_with_nova-network.conf 内のパラメータを修正します。オールインワン構成な ので、ほぼほぼ修正せずに実行しますが\nHOST_IP='\u0026lt;Thinkpad の IP アドレス\u0026gt;'  だけ修正。\n実行\u0026hellip; 実行する。\n% sudo ./deploy.sh allinone nova-network  \u0026hellip; 最近 ./deploy.sh create_network nova-network を実行しなくて済むようにしました。\nHorizon にアクセスする ブラウザで http://localhost/horizon にアクセスすれば horizon にアクセス出来る。 ユーザ情報は\u0026hellip;\nuser : demo pass : demo  です。Horizon から後でユーザ情報変更してもらって構わないです。\nコマンドラインからアクセスする。 実行したユーザのホームディレクトリに ~/openstackrc がある。\n ~/openstackrc # admin アカウント用 ~openstackrc-demo # demo ユーザ用  読み込んで OpenStack のコマンドを実行する。下記は例です。\n% source ~/openstackrc-demo % nova list +--------------------------------------+----------+--------+---------------------------------+ | ID | Name | Status | Networks | +--------------------------------------+----------+--------+---------------------------------+ | 81730be5-f2b3-411c-bfef-9a879e3d7d56 | grievous | ACTIVE | private=10.0.0.5, 192.168.1.195 | | 0ba8416a-58bd-476d-96a8-2ecc37ae53e6 | luke | ACTIVE | private=10.0.0.2, 192.168.1.194 | | 5b1f23b3-c15e-4260-bf6c-f3e965bcd546 | r2d2 | ACTIVE | private=10.0.0.4, 192.168.1.193 | +--------------------------------------+----------+--------+---------------------------------+  次のリリース版は\u0026hellip; bash で書いても\u0026hellip;満足感得られないしコード汚くなるし、人に読んでもらえないし。 ruby で書いても perl で書いても同じ。コマンドをバシバシ打つインフラ系のコード は汚くなるしまともにテストも出来ない。これからはインフラ系もコードを書く時代だっ て言っておいてこれじゃアカンぉ。\nならば情報が整理されるって意味だけでも chef のようなフレームワークを使う意味は 大きい。資源を他の人に有効活用してもらえるチャンスも増えるし。chef のクックブッ クなんてコードじゃないってアプリエンジニアに言われようが、やっぱりフレームワー ク使うべきだし使いたい。次の OpenStack のリリースの時は Chef のクックブック作 るって決めたよぉー。why-run 出来たり、テストも出来るしね。\n","permalink":"https://jedipunkz.github.io/post/2013/01/12/openstack-on-thinkpad/","summary":"以前紹介した OpenStack Folsom 構築 bash スクリプトなのだけど quantum の代わり に nova-network も使えるようにしておいた。\n構築 bash スクリプトは、\nhttps://github.com/jedipunkz/openstack_folsom_deploy/blob/master/README_jp.md\nに詳しい使い方を書いておきました。またパラメータを修正して実行するのだけどパラ メータについては、\nhttps://github.com/jedipunkz/openstack_folsom_deploy/blob/master/README_parameters_jp.md\nに書いておきました。\n手持ちの Thinkpad に OpenStack folsom 入れた写真。この写真の OpenStack Folsom を構築した時の手順を書いておくよ。\nOS をインストール OS をインストールします。12.10 を使いました。(12.04 LTS でも可)。/dev/sda6 等、 Cinder 用に一つパーティションを作ってマウントしないでおきます。また固定 IP ア ドレスを NIC に付与しておきます。\nスクリプト取得 スクリプトを取得する。\n% sudo apt-get update; sudo apt-get install git-core % git clone https://github.com/jedipunkz/openstack_folsom_deploy.git % cd openstack_folsom_deploy  パラメータ修正 deploy_with_nova-network.conf 内のパラメータを修正します。オールインワン構成な ので、ほぼほぼ修正せずに実行しますが\nHOST_IP='\u0026lt;Thinkpad の IP アドレス\u0026gt;'  だけ修正。","title":"OpenStack Folsom on Thinkpad"},{"content":"FreeBSD を OpenStack で管理したいなぁと思って自宅に OpenStack 環境作ってました。\nお正月なのに\u0026hellip;\n使ったのは folsom ベースの OpenStack (nova-network) と FreeBSD 9.1 です。8 系 の FreeBSD でも大体同じ作業で実現出来るぽいです。あと nova-network でって書い たのは自宅に quantum だと少し厳しいからです。FlatDHCPManager が調度良かった。\n今回のポイントは FreeBSD の HDD, NIC のドライバに virtio を使うように修正する ところです。OpenStack (KVM) は virtio 前提なので、そうせざるを得なかったです。\n今回使ったソフトウェア  OpenStack Folsom (nova-network) Ubuntu Server 12.10 FreeBSD 9.1 amd64  作業方法 準備としてこれらが必要になります。事前に行なってください。\n FreeBSD-9.1-RELEASE-amd64-disc1.iso ダウンロード 作業ホスト (Ubuntu Server 12.10) に qemu-kvm をインストール  freebsd9.img として qcow2 イメージを作成します。\n% kvm-img create -f qcow2 freebsd9.img 8G  作成したイメージファイルに FreeBSD 9.1 をインストールします。\n% kvm -m 256 -cdrom ./FreeBSD-9.1-RELEASE-amd64-disc1.iso \\ -drive file=./freebsd9.img -boot d -net nic -net user -nographic -vnc :10  VNC のディスプレイ番号は空いているものを使ってください。空いていれば他でも構い ません。\nVNC viewer をインストールし localhost:10 に接続します。ここでは作業ホストに X Window System が入っていることを前提に書いていますが、Non-X な方は他のホストか らその他の VNC ソフトウェアを使ってアクセスしてもらっても構わないです。\n% sudo apt-get update % sudo apt-get gvncviewer %gvncviewer localhost:10  インストーラが起動しているのでインストール行なってください。気をつける点として は一つ。src をインストールしてください。後に virtio をインストールするのに必要 になってくるからです。kmod ファイルをコンパイルしてインストールすることになります。\nインストールが終わったら今度は仮想マシンを HDD から起動します。\n% kvm -m 256 -drive file=./freebsd9.img -boot c -net nic -net user \\ -nographic -vnc :10  VNC で再度接続し仮想マシン上で emulators/virtio-kmod をインストールします。\nfreebsd9% cd /usr/ports/emulators/virtio-kmod/ freebsd9% su freebsd9# make install clean  次に virtio ドライバを扱うように HDD, NIC ドライバ周りの設定を変更します。 virtio のインストールが終わった後に「こうしろ」とメッセージが出てきますので それを参考に行います。一部、僕の環境ではそのままではダメだったので修正して 使いました。\nインストールした virtio を起動時に読み込むために下記を追記します。\nfreebsd9# vi /boot/loader.conf virtio_load=\u0026quot;YES\u0026quot; virtio_pci_load=\u0026quot;YES\u0026quot; virtio_blk_load=\u0026quot;YES\u0026quot; if_vtnet_load=\u0026quot;YES\u0026quot; virtio_balloon_load=\u0026quot;YES\u0026quot;  HDD ドライバを virtio を使うように変更します。これによってデバイス名が変わって くるので /etc/fstab を編集します。\nfreebsd9# sed -i.bak -Ee 's|/dev/ada?|/dev/vtbd|' /etc/fstab  NIC も virtio を使います。僕の環境では ifconfig_re0 でした。これを vtnet0 に変 更します。\nfreebsd9# cat /etc/rc.conf ifconfig_vtnet0=\u0026quot;DHCP\u0026quot; ..\u0026lt;snip\u0026gt;..  qemu の起動方法、もしくはデフォルトのハードウェア定義によって re0 は変わってく るかもしれません。適宜変更します。\n仮想マシンをシャットダウンします。\nfreebsd9# shutdown -p now  完成したイメージファイル freebsd9.img を openstack 環境に転送し (openstack 環 境で作業している方は必要無いです) glance に登録します。\n環境変数諸々を揃えて\u0026hellip;\n% glance add name=\u0026quot;FreeBSD 9\u0026quot; is_public=true container_format=ovf disk_format=qcow2 \u0026lt; freebsd9.img % glance image-list +--------------------------------------+------------------------+-------------+------------------+-------------+--------+ | ID | Name | Disk Format | Container Format | Size | Status | +--------------------------------------+------------------------+-------------+------------------+-------------+--------+ | 1af6f41b-1048-4d78-9715-87935c0bc6ae | FreeBSD 9 | qcow2 | ovf | 4305584128 | active | +--------------------------------------+------------------------+-------------+------------------+-------------+--------+  以上です。\nまとめ FreeBSD は仕事場でもプライベートでもまだまだ現役だし OpenStack で扱えるように することは僕にとってとても重要でした。なので満足ｗ FreeBSD 8 系では同じ手順で 扱えるらしいのだけど、それ以前のバージョンになるとどうか\u0026hellip; virtio がポイント なのと、KVM とゲスト OS バージョンって相性がめちゃ有るのでここもポイントになり そう。ハイパーバイザに VMWare も使えるらしいので今度やってみるかな。VMWare で も相性はあるけど、KVM ほどシビアにならなくていい印象がある。FreeBSD 3系でも頑 張れば動いたし。\nあとは cloud-init 。まだ開発途中だそうです。なので metadata サーバにアクセスし て、色んな事しようと思ってもまだ難しい。\nこの年末に Windows も OpenStack に乗せてみたので、そちらの記事も時間があったら 載せようっと。\n2013/01/02 追記 FreeBSD 8.3 で試してみましたが、全く同じ手順で起動してくれました。ただ、 emulators/virtio-kmod が 8.2 or 9 に対応しているとあったので Makefile を修正し て virtio-kmod をインストールしています。今のところ何も問題出ていません。\n# diff -u /usr/ports/emulators/virtio-kmod/Makefile.org /usr/ports/emulators/virtio-kmod/Makefile --- /usr/ports/emulators/virtio-kmod/Makefile.org 2013-01-02 06:02:48.000000000 +0900 +++ /usr/ports/emulators/virtio-kmod/Makefile 2013-01-02 06:02:59.000000000 +0900 @@ -28,7 +28,7 @@ .include \u0026lt;bsd.port.pre.mk\u0026gt; -.if ${OSREL} != \u0026#34;8.2\u0026#34; \u0026amp;\u0026amp; ${OSREL} != \u0026#34;9.0\u0026#34; +.if ${OSREL} != \u0026#34;8.3\u0026#34; \u0026amp;\u0026amp; ${OSREL} != \u0026#34;9.0\u0026#34; IGNORE=not supported $${OSREL} (${OSREL}) .endif ","permalink":"https://jedipunkz.github.io/post/2013/01/01/freebsd-on-openstack/","summary":"FreeBSD を OpenStack で管理したいなぁと思って自宅に OpenStack 環境作ってました。\nお正月なのに\u0026hellip;\n使ったのは folsom ベースの OpenStack (nova-network) と FreeBSD 9.1 です。8 系 の FreeBSD でも大体同じ作業で実現出来るぽいです。あと nova-network でって書い たのは自宅に quantum だと少し厳しいからです。FlatDHCPManager が調度良かった。\n今回のポイントは FreeBSD の HDD, NIC のドライバに virtio を使うように修正する ところです。OpenStack (KVM) は virtio 前提なので、そうせざるを得なかったです。\n今回使ったソフトウェア  OpenStack Folsom (nova-network) Ubuntu Server 12.10 FreeBSD 9.1 amd64  作業方法 準備としてこれらが必要になります。事前に行なってください。\n FreeBSD-9.1-RELEASE-amd64-disc1.iso ダウンロード 作業ホスト (Ubuntu Server 12.10) に qemu-kvm をインストール  freebsd9.img として qcow2 イメージを作成します。\n% kvm-img create -f qcow2 freebsd9.","title":"FreeBSD on OpenStack"},{"content":"今日は \u0026ldquo;OpenStack Advent Calendar 2012 JP\u0026rdquo; というイベントのために記事を書きた いと思います。Advent Calendar とはキリスト生誕を祝うため 12/25 まで毎日誰かがブログ 等で特定の話題について述べるもの、らしいです。CloudStack さん, Eucalyptus さん も今年はやっているそうですね。\nイベントサイト : http://atnd.org/events/34389\nでは早速！(ただ..CloudStack の Advent Calendar とネタがかぶり気味です..。)\n御存知の通り OpenStack は API を提供していてユーザがコードを書くことで OpenStack のコマンド・Horizon で出来ることは全て可能です。API を叩くのに幾つか フレームワークが存在します。\n fog libcloud deltacloud  などです。\nここでは内部で fog を使っている knife-openstack を利用して API に触れてみよう かと思います。API を叩くことを想像してもらって、インフラエンジニアの仕事内容の 変化まで述べられたらいいなぁと思っています。\nOpenStack 環境の用意 予め OpenStack 環境は揃っているものとしますです。お持ちでなければ\nhttp://jedipunkz.github.com/blog/2012/11/10/openstack-folsom-install/\nこの記事を参考に環境を作ってみて下さい。あ、devstack でも大丈夫です。\nchef, knife-openstack の用意 chef, knife-openstack を入れるのは OpenStack 環境でも、別のノードでも構いません。\nchef が確か 1.9.2 ベースが推奨だったので今回は 1.9.2-p320 使います。 ruby は rbenv で入れるのがオススメです。knife-openstack, chef のインストールは\u0026hellip;\n% sudo apt-get install libreadline-dev libxslt1-dev libxml2-dev % gem install chef --no-rdoc --no-ri % gem install knife-openstack --no-rdoc --no-ri % rbenv rehash # rbenv を使っている際に実行..  次に knife.rb を用意します。情報として下記を\n OS_USERNAME : :openstack_username OS_PASSWORD : :openstack_password OS_AUTH_URL : :openstack_auth_url OS_TENANT_NAME : :openstack_tenant  追加します。例として下記を参考にしてください。\n% mkdir .chef % ${EDITOR} .chef/knife.rb knife[:openstack_username] = \u0026quot;demo\u0026quot; knife[:openstack_password] = \u0026quot;demo\u0026quot; knife[:openstack_auth_url] = \u0026quot;http://172.16.1.11:5000/v2.0/tokens\u0026quot; knife[:openstack_tenant] = \u0026quot;service\u0026quot;  ここで注意なのが OS_AUTH_URL がいつもコマンドラインで扱うものと違い /tokens が 付いています。fog を直に扱う時も同じですがこれが必要です。\nssh keypair の用意 ssh keypair が必要になってくるので用意します。\n% nova keypair-add testkey01 \u0026gt; testkey01  knife-openstack の操作方法 いよいよ knife-openstack を使って OpenStack を操作してみましょう。\nまずは flavor のリストを取得します。\n% knife openstack flavor list ID Name Virtual CPUs RAM Disk 1 m1.tiny 1 512 MB 0 GB 2 m1.small 1 2048 MB 20 GB 3 m1.medium 2 4096 MB 40 GB 4 m1.large 4 8192 MB 80 GB 5 m1.xlarge 8 16384 MB 160 GB  image リストを取得します。id が必要になります。\n% knife openstack image list ID Name 436deba5-8fab-4bb7-9205-41e33fe22744 Cirros 0.3.0 x86_64  VM を生成してみましょー。\n% knife openstack server create -f 1 -I 436deba5-8fab-4bb7-9205-41e33fe22744 -S testkey01 -N knifetest01 Instance Name: knifetest01 Instance ID: 1e20850d-9572-46c8-a41e-fdf56f4f65a7 SSH Keypair: testkey Waiting for server............ Flavor: 1 Image: 436deba5-8fab-4bb7-9205-41e33fe22744  出来たかどうか、チェック。\n% knife openstack server list Instance ID Name Public IP Private IP Flavor Image Keypair State 1e20850d-9572-46c8-a41e-fdf56f4f65a7 knifetest01 1 436deba5-8fab-4bb7-9205-41e33fe22744 hogehoge active  できました。逆に VM を削除するには\n% knife openstack server delete 1e20850d-9572-46c8-a41e-fdf56f4f65a7 Instance ID: 1e20850d-9572-46c8-a41e-fdf56f4f65a7 Instance Name: knifetest01 Flavor: 1 Image: 436deba5-8fab-4bb7-9205-41e33fe22744 Do you really want to delete this server? (Y/N) y WARNING: Deleted server 1e20850d-9572-46c8-a41e-fdf56f4f65a7 WARNING: Corresponding node and client for the 1e20850d-9572-46c8-a41e-fdf56f4f65a7 server were not deleted and remain registered with the Chef Server  です。\n残念なところとしては knife-openstack 自体はまだまだ機能が充実していません。VM の基本的な操作くらいしか出来ないので、追加実装したいという方がいらっしゃいまし たら Pull リクエスト送ると良いのではないでしょうか。\nknife-openstack 公式サイト : https://github.com/opscode/knife-openstack\nまぁ、ここまで書いてアレですが.. 気がついた方もいらっしゃると思います。 knife-openstack で出来ることは openstack コマンド群で全て出来るのであまり意味 はないですよね。ただ fog を使って API を叩くことを想像して欲しくて..( -_- )\nFog 単体で操作してみる 今日はまだまだ書ける！\nknife-openstack では基本的な操作しか出来ませんでしたが fog はより多くの機能を実装しています。(2012/12/08 現在 quantum 周りの開発は未 完成らしいです。floating-ip 周りがぁ。誰かコミットして。)\nfog 単体で OpenStack API を操作するには、下記の通り実行します。fog をインストー ルし\u0026hellip;\n% gem install fog --no-rdoc --no-ri % rbenv rehash  環境変数を入力し\u0026hellip; (情報は例です)\n% cat env export OS_TENANT_NAME=service export OS_USERNAME=demo export OS_PASSWORD=demo export OS_AUTH_URL=\u0026quot;http://172.16.1.11:5000/v2.0/\u0026quot; export OS_AUTH_URL_FOG=\u0026quot;http://172.16.1.11:5000/v2.0/tokens\u0026quot; % source env  コードを下記のように記述すると VM の生成が行えます。\n#!/usr/bin/env ruby require \u0026#39;fog\u0026#39; require \u0026#39;pp\u0026#39; conn = Fog::Compute.new({ :provider =\u0026gt; \u0026#39;OpenStack\u0026#39;, :openstack_api_key =\u0026gt; ENV[\u0026#39;OS_PASSWORD\u0026#39;], :openstack_username =\u0026gt; ENV[\u0026#34;OS_USERNAME\u0026#34;], :openstack_auth_url =\u0026gt; ENV[\u0026#34;OS_AUTH_URL_FOG\u0026#34;], :openstack_tenant =\u0026gt; ENV[\u0026#34;OS_TENANT_NAME\u0026#34;] }) flavor = conn.flavors.find { |f| f.name == \u0026#39;m1.tiny\u0026#39; } image_name = \u0026#39;Cirros 0.3.0 x86_64\u0026#39; image = conn.images.find { |i| i.name == image_name } puts \u0026#34;#{\u0026#39;Creating server\u0026#39;}from image #{image.name}...\u0026#34; server = conn.servers.create :name =\u0026gt; \u0026#34;fogvm-#{Time.now.strftime \u0026#39;%Y%m%d-%H%M%S\u0026#39;}\u0026#34;, :image_ref =\u0026gt; image.id, :flavor_ref =\u0026gt; flavor.id, :key_name =\u0026gt; \u0026#39;testkey01\u0026#39; server.wait_for { ready? } 実行 !\n% ruby \u0026lt;CODENAME\u0026gt;.rb Creating server from image Cirros 0.3.0 x86_64... %  出来ました。VM が生成されたか先ほどの knife-openstack で確認してみましょう。\n% knife openstack server list Instance ID Name Public IP Private IP Flavor Image Keypair State 1e20850d-9572-46c8-a41e-fdf56f4f65a7 knifetest01 1 436deba5-8fab-4bb7-9205-41e33fe22744 hogehoge active f5c314a3-e32f-498f-984f-b79078d76a5a fogvm-20121207-104743 1 aedef2a1-f820-43a6-96cd-f5361d27df3f testkey01 active  まとめと 考察 今回は API をみんな叩いてるねん！ってことに気がついて欲しくて、こんな記事を書 いてみました。実は OpenStack のコマンドも \u0026ndash;debug を付けて (Quantum だけは -v) 実行すると OpenStack の API を叩いているメッセージが出力されると OpenStack ユー ザ会の方から聞きました。是非やっていてください。\n% nova --debug list % quantum -v net-list  API を実装するのか、ツールを実装するのか？といった話題が以前の OpenStack Summit で常に話題になっていたらしいですが、最近は API で決まり、といったと ころでしょうか。コードを書いてインラフを定義する時代に突入です。ちなみに fog は AWS も扱えます。また Opscode Chef や Puppet, JuJu 等のデプロイフレームワー クを使えば VM 上のサービス構築もコードを書くことで出来ます。また、以前ブログに 書いたのですが OpenFlow といった技術を使うとネットワークをコードを書くことで設 計出来る、かもしれない。\nhttp://jedipunkz.github.com/blog/2012/11/21/openflow-trema-handson-report/\nつまり、コードを書くことで\n サーバ構築 ネットワーク構築 サービス構築  を一貫して行えることになります。\nインフラエンジニアの僕としては時代の変化に着いて行かねば！という焦りで一杯です。 もちろんレガシなインフラエンジニアも生き残るのでしょうが、働く場所が限られてき そう。より高度な(低レイヤからの深い？)技術を有している人が限定された場所で成果 を上げていく印象。僕らの様な一般的なインフラエンジニアは OpenStack, AWS, HP Cloud, CloudStack の様なクラウドインフラを相手にコードを書く、もしくは抽象化さ れた技術をより扱いやすいソフトウェアを介して構築・管理していくことになるのでしょ うか。これから覚えることは山積みですが、楽しい時代です。\n","permalink":"https://jedipunkz.github.io/post/2012/12/08/knife-fog-openstack-api/","summary":"今日は \u0026ldquo;OpenStack Advent Calendar 2012 JP\u0026rdquo; というイベントのために記事を書きた いと思います。Advent Calendar とはキリスト生誕を祝うため 12/25 まで毎日誰かがブログ 等で特定の話題について述べるもの、らしいです。CloudStack さん, Eucalyptus さん も今年はやっているそうですね。\nイベントサイト : http://atnd.org/events/34389\nでは早速！(ただ..CloudStack の Advent Calendar とネタがかぶり気味です..。)\n御存知の通り OpenStack は API を提供していてユーザがコードを書くことで OpenStack のコマンド・Horizon で出来ることは全て可能です。API を叩くのに幾つか フレームワークが存在します。\n fog libcloud deltacloud  などです。\nここでは内部で fog を使っている knife-openstack を利用して API に触れてみよう かと思います。API を叩くことを想像してもらって、インフラエンジニアの仕事内容の 変化まで述べられたらいいなぁと思っています。\nOpenStack 環境の用意 予め OpenStack 環境は揃っているものとしますです。お持ちでなければ\nhttp://jedipunkz.github.com/blog/2012/11/10/openstack-folsom-install/\nこの記事を参考に環境を作ってみて下さい。あ、devstack でも大丈夫です。\nchef, knife-openstack の用意 chef, knife-openstack を入れるのは OpenStack 環境でも、別のノードでも構いません。\nchef が確か 1.9.2 ベースが推奨だったので今回は 1.","title":"OpenStack API を理解しインフラエンジニアの仕事の変化を感じる"},{"content":"InternetWeek2012 で開かれた \u0026ldquo;OpenFlow Trema ハンズオン\u0026rdquo; に参加してきました。\n講師 : Trema 開発チーム 鈴木一哉さま, 高宮安仁さま 開催日 : 2012年11月21日  OpenStack の Quantum Plugin として Trema が扱えるという話だったので興味を持っ たのがきっかけです。また Ruby で簡潔にネットワークをコード化出来る、という点も 個人的に非常に興味を持ちました。OpenStack, CloudStack 等のクラウド管理ソフトウェ アが提供する API といい、Opscode Chef, Puppet 等のインフラソフトウェア構築フレー ムワークといい、この OpenFlow もインフラを形成する技術を抽象化し、技術者がコー ドを書くことでインフラ構築を行える、という点ではイマドキだなと思います。\nGoogle は既にデータセンター間の通信を 100% 、OpenFlow の仕様に沿った機器・ソフ トウェアをを独自に実装しさばいているそうですし、我々が利用する日も近いと想像し ます。\nOpenFlow のモチベーション OpenFlow の登場には理由が幾つかあって、既存のネットワークの抱えている下記の幾 つかの問題を解決するためです。\n 装置仕様の肥大化 多様なプロトコルが標準化 装置のコスト増大 ある意味、自律したシステムが招く複雑さ  一方、OpenFlow を利用すると..\n コモディティ化された HW の利用が可能 OpenFlow はコントローラ (神) が集中管理するので楽な場合もある ネットワーク運用の自動化が図れる アプリケーションに合わせた最適化 柔軟な自己修復  等のメリットが。\nOpenFlow と Trema とは? OpenFlow は \u0026lsquo;OpenFlow コントローラ\u0026rsquo;, \u0026lsquo;OpenFlow スイッチ\u0026rsquo; から成る。OpenFlow コ ントローラと OpenFlow スイッチの間の通信は OpenFlow プロトコルでされる。今日の 話題 Trema はこの..\n OpenFlow コントローラのフレームワーク エミュレータ trema コマンド  のセットである。自宅の PC 一台で OpenFlow プログラムが行え、エミュレーションも 行える手軽さ、また Ruby による簡潔な記述が可能でプログラミング初心者でも扱いや すい、という趣味ユーザにはもってのほかだ。\nHellow, Trema ! 早速 プログラミングの初歩、Hello World から。\nコード hello-trema.rb は\nclass HelloTrema \u0026lt; Controller def start puts \u0026quot;Hello, Trema!\u0026quot; end end  実行\u0026hellip;\n% trema run hello-trema.rb Hello, Trema!  Trema が提供する Controller クラスを \u0026lsquo;継承\u0026rsquo; し HelloTrema クラスを定義した。 Controller クラスには幾つものハンドラが用意されていて start ハンドラもその一つ。 他にも色んなメソッドが用意されている。詳しくは後ほど。\nTrema によるスイッチの起動 次にスイッチを起動してみる。Trema は ruby の DSL でコンフィギュレーションを定 義出来る。hello-switch.conf として下記の内容\u0026hellip;\nvswitch { dpid \u0026quot;0xabc\u0026quot; } vswitch { dpid \u0026quot;0x1\u0026quot; } vswitch { dpid \u0026quot;0x2\u0026quot; }  hello-switch.rb として\nclass HelloSwitch \u0026lt; Controller def switch_ready dpid puts \u0026quot;Hello #{ dpid.to_hex }!\u0026quot; end def switch_disconnected dpid puts \u0026quot;Killed ! :D #{ dpid.to_hex }!\u0026quot; end end  実行すると\n% trema run hello-switch.rb -c hello-switch.conf Hello 0xabc! Hello 0x1! Hello 0x2!  となる。スイッチを3つ起動したわけだ。switch_ready とは Trema が提供するコント ローラで定義された \u0026ldquo;スイッチが稼働した時に実行されるハンドラ\u0026rdquo; だ。スイッチが起 動したため \u0026ldquo;Hello ..\u0026rdquo; なるメッセージが出力された、と理解すればいい。\nまたこの起動中に\n% trema kill 0x1 Killed ! :D 0x1!  と実行することでスイッチを停止出来る。この際 switch_disconnected ハンドラが実 行され上記のメッセージを出力したというわけだ。また逆に再稼働させるには trema up コマンドを用いる。\nその他のハンドラ一覧 紹介した start, switch_ready 等のハンドラ以外にも下記のモノがある。\nstart switch_ready switch_disconnected packet_in flow_removed port_status openflow_error features_reply stats_reply barrier_reply get_config_reply queue_get_config_reply vendor  ドキュメントは\nhttp://rubydoc.info/github/trema/trema/master/frames  にあるので参照すると良い。\nL2 スイッチの実装 OpenFlow はネットワーク機器を実装出来るモノなので、ここで L2 スイッチを実装し てみたい。L2 スイッチの動作は\n 既に ARP テーブルを持っていればパケットを受け流す 自分の ARP テーブルで管理されていないパケットは学習してからパケットを受け流 す  が基本だ。これを実装する。\nl2-switch.conf として\nvswitch { dpid \u0026quot;0xabc\u0026quot; } vhost (\u0026quot;host1\u0026quot;) { ip \u0026quot;192.168.0.1\u0026quot; netmask \u0026quot;255.255.0.0\u0026quot; mac \u0026quot;00:00:00:01:00:01\u0026quot; } vhost (\u0026quot;host2\u0026quot;) { ip \u0026quot;192.168.0.2\u0026quot; netmask \u0026quot;255.255.0.0\u0026quot; mac \u0026quot;00:00:00:01:00:02\u0026quot; } link \u0026quot;0xabc\u0026quot;, \u0026quot;host1\u0026quot; link \u0026quot;0xabc\u0026quot;, \u0026quot;host2\u0026quot;  ここでは detapath_id \u0026ldquo;0xabc\u0026rdquo; なる仮想スイッチを一つ定義し、サーバホスト host1, host2 を定義した。それぞれで IP アドレス・MAC アドレスを定義している。また link により仮想スイッチとサーバホストの I/F を接続している。\nいよいよ L2 スイッチのコード。l2-switch.rb として、下記を記述する。\nrequire \u0026quot;fdb\u0026quot; class LearningSwitch \u0026lt; Controller def start @fdb = FDB.new end def packet_in dpid, message @fdb.learn message.macsa, message.in_port port_no = @fdb.lookup( message.macda ) if port_no flow_mod dpid, message, port_no packet_out dpid, message, port_no else flood dpid, message end end def flow_mod dpid, message, port_no send_flow_mod_add( dpid, :match =\u0026gt; ExactMatch.from( message ), :actions =\u0026gt; ActionOutput.new( port_no ) ) end def flow_mod dpid, message, port_no send_flow_mod_add( dpid, :match =\u0026gt; ExactMatch.from( message ), :actions =\u0026gt; ActionOutput.new( port_no ) ) end def packet_out dpid, message, port_no send_packet_out( dpid, :packet_in =\u0026gt; message, :actions =\u0026gt; ActionOutput.new( port_no ) ) end def flood dpid, message packet_out dpid, message, OFPP_FLOOD end end  まず実行してみる。\n% trema run l2-switch.rb -c l2-switch.conf  異なる shell でパケットの送信と状態表示を行う。\n% trema send_packet --source host1 --dest host2 % trema show_stats host1 ip_dst,tp_dst,ip_src,tp_src,n_pkts,n_octets 192.168.0.2,1,192.168.0.1,1,1,50 % trema send_packet --source host1 --dest host2 % trema send_packet --source host2 --dest host1 % trema dump_flows 0xabc NXST_FLOW reply (xid=0x4): cookie=0x2, duration=67.343s, table=0, n_packets=0, n_bytes=0, priority=65535,udp,in_port=1,vlan_tci=0x0000,dl_src=00:00:00:01:00:02,dl_dst=00:00:00:01:00:01,nw_src=192.168.0.2,nw_dst=192.168.0.1,nw_tos=0,tp_src=1,tp_dst=1 actions=output:2 cookie=0x1, duration=70.339s, table=0, n_packets=0, n_bytes=0,\tpriority=65535,udp,in_port=2,vlan_tci=0x0000,dl_src=00:00:00:01:00:01,dl_dst=00:00:00:01:00:01,nw_src=192.168.0.1,nw_dst=192.168.0.1,nw_tos=0,tp_src=1,tp_dst=1 actions=output:2  host1 から host2 に対して trema send_packet で通信を行い、host1 の状態を表示し た。また交互に通信をさせフローをダンプしたのが上記だ。\n一番基本なところらしいので、コードを詳しく解説。l2-switch.rb の下記の部分。\n def packet_in dpid, message # ---(1) @fdb.learn message.macsa, message.in_port # ---(2) port_no = @fdb.lookup( message.macda ) # ---(3) if port_no # ---(4) flow_mod dpid, message, port_no packet_out dpid, message, port_no else # ---(5) flood dpid, message end end  (1) では packet_in ハンドラを利用した。(2) で fdb (floating DB) の learn メソッ ドで port と mac アドレスの学習を行った。(3) で @fdb にすでに mac アドレスの記 述があれば port_no に値が入る。値が入っていれば (4) を。スイッチのフローテーブ\nルを更新しパケットを packet_out する。入っていなければ (5)を実行しパケットを flood する。\nflow_mod, packet_out, flood はこのプログラム内で定義しているプライベートなメソッ ドだ。\nこの時のソフトウェア構成 このコードを動作させた際に実行したホスト上のプロセスを見てみた。\n ovs-openflowd phost switch_manager  が居た。siwtch_manager が 6633 番ポート待ち受けているコントローラ自身だろう。 phost は仮想サーバで ovs-openflowd (OpenvSwitch) が仮想スイッチとして動作して いると想像出来る。またこの時にネットワークインターフェースが\n trema0-0 trema0-1 trema1-0 trema1-1  と起動していた。これは 0xabc スイッチと host1, host2 との接続で使われている I/F だろう。\nまとめ Ruby で記述出来るので技術者にとって Trema は優しい。また簡潔な記述が行えるとい うのも Ruby ならではだろう。github.com には高度な利用サンプルが幾つか掲載され ている。\nhttps://github.com/trema/apps  また簡単なサンプル集ということであれば\nhttps://github.com/trema/trema/tree/develop/src/examples  がうってつけのリファレンスになる。\n冒頭でも書いたがスイッチのエミュレータが同封されているので、技術者はすぐに開発 に入ることが出来る。\nOpenStack は仮想マシンを管理・構成するため API を提供し、そしてネットワークを管理・ 構成するため OpenFlow がある。OpenStack の API を叩くコードを書くのと同様に OpenFlow を Trema という OpenFlow コントローラフレームワークを用いてコードを書 くことが出来る。インフラエンジニアの仕事の範囲は確実にここ数年で変化し、その変 化に追いつくには \u0026ldquo;コードを書く\u0026rdquo; ことを念頭に置かなくてはならないだろう。\n最後に、この機会を与えてくださった 鈴木様・高宮様にお礼を申し上げます。\n","permalink":"https://jedipunkz.github.io/post/2012/11/21/openflow-trema-handson-report/","summary":"InternetWeek2012 で開かれた \u0026ldquo;OpenFlow Trema ハンズオン\u0026rdquo; に参加してきました。\n講師 : Trema 開発チーム 鈴木一哉さま, 高宮安仁さま 開催日 : 2012年11月21日  OpenStack の Quantum Plugin として Trema が扱えるという話だったので興味を持っ たのがきっかけです。また Ruby で簡潔にネットワークをコード化出来る、という点も 個人的に非常に興味を持ちました。OpenStack, CloudStack 等のクラウド管理ソフトウェ アが提供する API といい、Opscode Chef, Puppet 等のインフラソフトウェア構築フレー ムワークといい、この OpenFlow もインフラを形成する技術を抽象化し、技術者がコー ドを書くことでインフラ構築を行える、という点ではイマドキだなと思います。\nGoogle は既にデータセンター間の通信を 100% 、OpenFlow の仕様に沿った機器・ソフ トウェアをを独自に実装しさばいているそうですし、我々が利用する日も近いと想像し ます。\nOpenFlow のモチベーション OpenFlow の登場には理由が幾つかあって、既存のネットワークの抱えている下記の幾 つかの問題を解決するためです。\n 装置仕様の肥大化 多様なプロトコルが標準化 装置のコスト増大 ある意味、自律したシステムが招く複雑さ  一方、OpenFlow を利用すると..\n コモディティ化された HW の利用が可能 OpenFlow はコントローラ (神) が集中管理するので楽な場合もある ネットワーク運用の自動化が図れる アプリケーションに合わせた最適化 柔軟な自己修復  等のメリットが。","title":"OpenFlow Trema ハンズオン参加レポート"},{"content":"※2012/12/04 に内容を修正しました。Network Node を切り出すよう修正。 ※213/01/09 に内容を修正しました。パラメータ修正です。\nOpenStack の Folsom リリースからメインコンポーネントの仲間入りした Quantum を 理解するのに時間を要してしまったのだけど、もう数十回とインストールを繰り返して だいぶ理解出来てきました。手作業でインストールしてると日が暮れてしまうのでと思っ て自分用に bash で構築スクリプトを作ったのだけど、それを公開しようと思います。\nOpenStack Folsom の構築に四苦八苦している方に使ってもらえたらと思ってます。\nhttp://jedipunkz.github.com/openstack_folsom_deploy/\nchef や puppet, juju などデプロイのフレームワークは今流行です。ただどれも環境 を予め構築しなくてはいけないので、誰でもすぐに使える環境ってことで bash スクリ プトで書いています。時間があれば是非 chef の cookbook を書いていきたいです。と いうか予定です。でも、もうすでに opscode 等は書き始めています。(汗\nではでは、紹介を始めます。\n前提の構成 management segment 172.16.1.0/24 +--------------------------------------------+------------------+----------------- | | | | | | | eth2 172.16.1.13 | eth2 172.16.1.12 | eth2 172.24.1.11 +------------+ +-----------+ +------------+ | | eth1 ------------------- eth1 | | | | | network | vlan/gre seg = 172.24.17.0/24 | compute | | controller | | node | data segment = 172.16.2.0/24 | node | | node | +------------+ 172.16.2.13 172.16.2.12 +-----------+ +------------+ | eth0 10.200.8.13 | eth0 10.200.8.11 | | | | +--------------------------------------------+------------------+----------------- | public segment 10.200.8.0/24 | | 10.200.8.1 +-----------+ | GW Router |-\u0026gt; The Internet +-----------+  Quantum は、\n API network Public network Data network Management Network  の4つのネットワークセグメントを前提に設計されています。4つ用意するのが大変なの で今回は\n API / Management network Public Network Data Network  と API と Management を兼務させた3つのネットワークセグメントを前提に話続けます。 もちろん Management を追加で切り出しても構わないです。NIC を追加するだけで OK 。\n詳しくは、\nhttp://docs.openstack.org/trunk/openstack-network/admin/content/connectivity.html\nに掲載されています。\nまた今回は public network に 10.200.8/24 を使ってます。サービス環境ではここが グローバルセグメントになります。\nインストール対象 OS は Ubuntu Server 12.04 LTS もしくは 12.10 で動作します。\n3つの NIC があるマシンを1台、NIC 2つのマシンを2台を用意します。 controller node x 1台 network node x 1台 + comupte node x n台 の構成で す。\n controller : glance, keystone, mysql, horizon, quantum server が稼働する Node network : quantum dhcp agent, quantum l3 agent, quantum openvswitch agent が稼働する Node compute : nova, quntum openvswitch agent が稼働する Node  上記の構成では下記の通り /etc/network/interface を設定します。controller 側の 設定です。\nauto lo iface lo inet loopback auto eth0 iface eth0 inet static address 10.200.8.11 netmask 255.255.255.0 dns-nameservers 8.8.8.8 8.8.4.4 dns-search cpi.ad.jp auto eth2 iface eth2 inet static address 172.16.1.11 netmask 255.255.255.0 gateway 172.16.1.1  Network Node は\u0026hellip;\nauto lo iface lo inet loopback auto eth0 iface eth0 inet static up ifconfig $IFACE 0.0.0.0 up up ip link set $IFACE promisc on down ip link set $IFACE promisc off down ifconfig $IFACE down address 10.200.8.21 netmask 255.255.255.0 #gateway 10.200.8.1 # dns-* options are implemented by the resolvconf package, if installed dns-nameservers 8.8.8.8 8.8.4.4 dns-search cpi.ad.jp auto eth1 iface eth1 inet static address 172.16.2.13 netmask 255.255.255.0 auto eth2 iface eth2 inet static address 172.16.1.13 netmask 255.255.255.0 gateway 172.16.1.1 dns-nameservers 8.8.8.8 8.8.4.4  compute Node は\u0026hellip;\nauto eth1 iface eth1 inet static address 172.16.2.12 netmask 255.255.255.0 auto eth2 iface eth2 inet static address 172.16.1.12 netmask 255.255.255.0 gateway 172.16.1.1 dns-nameservers 8.8.8.8 8.8.4.4  です。下記のコマンドでネットワークインターフェースを再起動してください。 ここまで用意出来たらいよいよ実行するのみです。\ncontroller% sudo /etc/init.d/networking restart network % sudo /etc/init.d/networking restart compute % sudo /etc/init.d/networking restart  スクリプト実行 下記の通りスクリプトを取得して\u0026hellip;\ncontroller% git clone https://github.com/jedipunkz/openstack_folsom_deploy.git controller% cd openstack_folsom_deploy  それぞれの構成に合わせて deploy.conf を修正します。上記の構成の場合下記のようになります。\nBASE_DIR=`pwd` CONTROLLER_NODE_IP='172.16.1.11' CONTROLLER_NODE_PUB_IP='10.200.8.11' NETWORK_NODE_IP='172.16.1.12' COMPUTE_NODE_IP='172.16.1.13' DATA_NIC_COMPUTE='eth1' MYSQL_PASS='secret' CINDER_VOLUME='/dev/sda6' DATA_NIC='eth1' PUBLIC_NIC='eth0' NETWORK_TYPE='gre' INT_NET_GATEWAY='172.24.17.254' INT_NET_RANGE='172.24.17.0/24' EXT_NET_GATEWAY='10.200.8.1' EXT_NET_START='10.200.8.36' EXT_NET_END='10.200.8.40' EXT_NET_RANGE='10.200.8.0/24' OS_IMAGE_URL=\u0026quot;https://launchpad.net/cirros/trunk/0.3.0/+download/cirros-0.3.0-x86_64-disk.img\u0026quot; OS_IMAGE_NAME=\u0026quot;Cirros 0.3.0 x86_64\u0026quot;  環境に合わせて設定すれば OK です。上記の前提の構成の場合このような値を入れてい きます。重要なところだけ説明すると..\n CONTROLLER_NODE_IP : Controller Node の IP アドレス NETWROK_NODE_IP : Network Node の IP アドレス COMPUTE_NODE_IP : Compute Node の IP アドレス INT_NET_.. : Quantum で管理する内部ネットワーク EXT_NET-.. : Quantum で管理する外部ネットワーク  です。\ndeploy.conf を修正したら、各 Node にディレクトリをコピーします。\nnetwork% scp -r \u0026lt;CONTROLLER_NODE_IP\u0026gt;:~/openstack_folsom_deploy . compute% scp -r \u0026lt;CONTROLLER_NODE_IP\u0026gt;:~/openstack_folsom_deploy .  いよいよ実行。それぞれの Node で順にスクリプトを実行します。\ncontroller% sudo ./deploy.sh controller quantum network % sudo ./deploy.sh network quantum compute % sudo ./deploy.sh compute quantum  また構築が終了したら quantum 上にネットワークを作成します。\ncontroller% sudo ./deploy.sh create_network qunatum  完成です。http://${CONTROLLER_NODE_IP}/horizon/ にアクセスすれば管 理画面が表示されるはずです。\n更に compute node を追加したければ\u0026hellip;\ncompute02% scp -r \u0026lt;CONTROLLER_NODE_IP\u0026gt;:~/openstack_folsom_deploy . compute02% cd openstack_folsom_deploy compute02% vim deploy.conf # $COMPUTE_NODE_IP を修正する。その他はそのまま。 compute02% sudo ./deploy.sh compute quantum  と deploy.conf 内 $COMPUTE_NODE_IP を更新して実行すれば OK です。\nFloating IP の利用 現在、folsom リリース版には floating ip が horizon 経由で利用できない問題があ ります。コマンドラインでは利用できるのでその方法を。\n% source $HOME/openstackrc % quantum net-list % quantun floatingip-create \u0026lt;ext_net_id\u0026gt; % quantun floatingip-list % quantum port-list % quantum floatingip-associate \u0026lt;floatingip_id\u0026gt; \u0026lt;vm_port_id\u0026gt;  Quantum と I/O と..所感 http://www.readability.com/read?url=http://docs.openstack.org/trunk/openstack-network/admin/content/services.html\n最近、メーリングリストで挙がっている話題。今回の構成だと Quantum は controller 上で稼働し全ての VM がこの Quantum を利用することになります。つまり単一障害点っ ていうだけではなく I/O が集中するので負荷も上昇する。前リリース版 ESSEX の nova-network の時は追加する compute node 上全てで nova-network を稼働させ、 node が増えるにつれ I/O も拡張出来るシンプルな構成が組めるモノだったのですが、 Quantum の構成になって、そう簡単にいかなくなった。\nオールインワン構成等、ほかの構成について 2013/01/09 に nova-network にも対応しました。\nオールインワン構成や quantum に代わって nova-network を使う構成等、その他の構成構築方法 については下記のドキュメントを参考にしてください。\nhttps://github.com/jedipunkz/openstack_folsom_deploy/blob/master/README_jp.md\n","permalink":"https://jedipunkz.github.io/post/2012/11/10/openstack-folsom-install/","summary":"※2012/12/04 に内容を修正しました。Network Node を切り出すよう修正。 ※213/01/09 に内容を修正しました。パラメータ修正です。\nOpenStack の Folsom リリースからメインコンポーネントの仲間入りした Quantum を 理解するのに時間を要してしまったのだけど、もう数十回とインストールを繰り返して だいぶ理解出来てきました。手作業でインストールしてると日が暮れてしまうのでと思っ て自分用に bash で構築スクリプトを作ったのだけど、それを公開しようと思います。\nOpenStack Folsom の構築に四苦八苦している方に使ってもらえたらと思ってます。\nhttp://jedipunkz.github.com/openstack_folsom_deploy/\nchef や puppet, juju などデプロイのフレームワークは今流行です。ただどれも環境 を予め構築しなくてはいけないので、誰でもすぐに使える環境ってことで bash スクリ プトで書いています。時間があれば是非 chef の cookbook を書いていきたいです。と いうか予定です。でも、もうすでに opscode 等は書き始めています。(汗\nではでは、紹介を始めます。\n前提の構成 management segment 172.16.1.0/24 +--------------------------------------------+------------------+----------------- | | | | | | | eth2 172.16.1.13 | eth2 172.16.1.12 | eth2 172.24.1.11 +------------+ +-----------+ +------------+ | | eth1 ------------------- eth1 | | | | | network | vlan/gre seg = 172.","title":"OpenStack Folsom 構築スクリプト"},{"content":"最近、OpenStack にどっぷり浸かってる @jedipunkzです。\nFolsom がリリースされて Quantum を理解するのにめちゃ苦労して楽しい真っ最中なのだけど、 今日は OpenStack の中でも最も枯れているコンポーネント Swift を使ったオブジェクトストレー ジ構築について少し書こうかなぁと思ってます。\n最近は OpenStack を構築・デプロイするのに皆、Swift 入れてないのね。仲間はずれ 感たっぷりだけど、一番安定して動くと思ってる。\nこれを読んで、自宅にオブジェクトストレージを置いちゃおぅ。\n構成は ?\u0026hellip;  +--------+ | client | +--------+ | +-------------+ | swift-proxy | +-------------+ 172.16.0.10 | +-------------------+-------------------+ 172.16.0.0/24 | | | +-----------------+ +-----------------+ +-----------------+ | swift-storage01 | | swift-storage02 | | swift-storage03 | +-----------------+ +-----------------+ +-----------------+ 172.16.0.11 172.16.0.12 172.16.0.13  となる。IP アドレスは\u0026hellip;\n client : 172.16.0.0/24 のどこか swift-proxy : 172.16.0.10 swift-storage01 : 172.16.0.11 swift-storage02 : 172.16.0.12 swift-storage03 : 172.16.0.13  これはサンプル。自宅の環境に合わせて読み替えてください。\n全て同じネットワークセグメントに。なので上の図は概念的な図です。通信の流れだけ 把握出来ればいいなぁと。向き書いてないけど..。四角で囲まれているのがノード (サー バ) です。物理サーバでも仮想マシンでも大丈夫！\nマシンの準備 Ubuntu Server 12.04 もしくは 12.10 を用意。swift-storage のマシン3台だけは /dev/sda6 など Disk デバイスを swift 用に用意してあげてください。普通にハード ディスクのパーティションを切ってあげるだけでいいです。高価な Disk を使うまでも ないので。それが分散ストレージの良いところ！ Disk ・ノードが壊れてもデータが失 われないんです！\nswift-proxy の構築 今回は簡易認証機能の tempauth を使います。Keystone を使った構成を説明しようか 迷ったのだけど、Keystone の構築だけで一つ記事が書けるくらいになるので..、諦め ました。tempauth は Swift 本体に実装されていますよ。\n構築は簡単、まず下記をコピペしてください。\napt-get update apt-get install swift python-swift apt-get install python-keystoneclient python-keystone mkdir /etc/swift chown -R swift:swift /etc/swift export PROXY_LOCAL_NET_IP=10.200.4.133 export POUND_NET=10.200.4.138 apt-get install swift-proxy memcached perl -pi -e \u0026quot;s/-l 127.0.0.1/-l $PROXY_LOCAL_NET_IP/\u0026quot; /etc/memcached.conf service memcached restart cat \u0026gt;/etc/swift/proxy-server.conf \u0026lt;\u0026lt;EOF [DEFAULT] #cert_file = /etc/swift/cert.crt #key_file = /etc/swift/cert.key bind_port = 8080 workers = 8 user = swift [pipeline:main] pipeline = healthcheck cache tempauth proxy-server [app:proxy-server] use = egg:swift#proxy allow_account_management = true account_autocreate = true [filter:tempauth] use = egg:swift#tempauth user_system_root = testpass .admin https://$PROXY_LOCAL_NET_IP:8080/v1/AUTH_system [filter:healthcheck] use = egg:swift#healthcheck [filter:cache] use = egg:swift#memcache memcache_servers = $PROXY_LOCAL_NET_IP01:11211 EOF  次に swift.conf とリング情報 (バランシングのための情報) を swift-proxy 上に用意します。これらは、 あとで各すべてのノードに配置するので重要です。これもコピペしてください。\ncat \u0026gt; /etc/swift/swift.conf \u0026lt;\u0026lt; EOF [swift-hash] # random unique string that can never change (DO NOT LOSE) swift_hash_path_suffix = `od -t x8 -N 8 -A n \u0026lt;/dev/random` EOF cd /etc/swift swift-ring-builder account.builder create 18 3 1 swift-ring-builder container.builder create 18 3 1 swift-ring-builder object.builder create 18 3 1 export STORAGE_LOCAL_NET_IP01=172.16.0.11 export STORAGE_LOCAL_NET_IP02=172.16.0.12 export STORAGE_LOCAL_NET_IP03=172.16.0.13 export ZONE01=1 export ZONE02=2 export ZONE03=3 export WEIGHT=100 export DEVICE=sda6 swift-ring-builder account.builder add z$ZONE01-$STORAGE_LOCAL_NET_IP01:6002/$DEVICE $WEIGHT swift-ring-builder container.builder add z$ZONE01-$STORAGE_LOCAL_NET_IP01:6001/$DEVICE $WEIGHT swift-ring-builder object.builder add z$ZONE01-$STORAGE_LOCAL_NET_IP01:6000/$DEVICE $WEIGHT swift-ring-builder account.builder add z$ZONE02-$STORAGE_LOCAL_NET_IP02:6002/$DEVICE $WEIGHT swift-ring-builder container.builder add z$ZONE02-$STORAGE_LOCAL_NET_IP02:6001/$DEVICE $WEIGHT swift-ring-builder object.builder add z$ZONE02-$STORAGE_LOCAL_NET_IP02:6000/$DEVICE $WEIGHT swift-ring-builder account.builder add z$ZONE03-$STORAGE_LOCAL_NET_IP03:6002/$DEVICE $WEIGHT swift-ring-builder container.builder add z$ZONE03-$STORAGE_LOCAL_NET_IP03:6001/$DEVICE $WEIGHT swift-ring-builder object.builder add z$ZONE03-$STORAGE_LOCAL_NET_IP03:6000/$DEVICE $WEIGHT swift-ring-builder account.builder swift-ring-builder container.builder swift-ring-builder object.builder swift-ring-builder account.builder rebalance swift-ring-builder container.builder rebalance swift-ring-builder object.builder rebalance  で、起動。\nswift-proxy# chown -R swift:swift /etc/swift swift-proxy# swift-init proxy start  おしまい。\nswift-storage 構築 swift-storage の構築は各ノードで下記の内容をコピペしてください。今回は3台分だ けど、マシンが余ってたら4台でも5台でも OK ！\napt-get update apt-get install swift python-swift mkdir /etc/swift chown -R swift:swift /etc/swift export STORAGE_LOCAL_NET_IP=172.16.0.11 apt-get install swift-account swift-container swift-object xfsprogs mkfs.xfs -i size=1024 /dev/sda6 echo \u0026quot;/dev/sda6 /srv/node/sda6 xfs noatime,nodiratime,nobarrier,logbufs=8 0 0\u0026quot; \u0026gt;\u0026gt; /etc/fstab mkdir -p /srv/node/sda6 mount /srv/node/sda6 chown -R swift:swift /srv/node cat \u0026gt;/etc/rsyncd.conf \u0026lt;\u0026lt;EOF uid = swift gid = swift log file = /var/log/rsyncd.log pid file = /var/run/rsyncd.pid address = $STORAGE_LOCAL_NET_IP [account] max connections = 2 path = /srv/node/ read only = false lock file = /var/lock/account.lock [container] max connections = 2 path = /srv/node/ read only = false lock file = /var/lock/container.lock [object] max connections = 2 path = /srv/node/ read only = false lock file = /var/lock/object.lock EOF perl -pi -e 's/RSYNC_ENABLE=false/RSYNC_ENABLE=true/' /etc/default/rsync service rsync start cat \u0026gt;/etc/swift/account-server.conf \u0026lt;\u0026lt;EOF [DEFAULT] bind_ip = $STORAGE_LOCAL_NET_IP workers = 2 [pipeline:main] pipeline = account-server [app:account-server] use = egg:swift#account [account-replicator] [account-auditor] [account-reaper] EOF cat \u0026gt;/etc/swift/container-server.conf \u0026lt;\u0026lt;EOF [DEFAULT] bind_ip = $STORAGE_LOCAL_NET_IP workers = 2 [pipeline:main] pipeline = container-server [app:container-server] use = egg:swift#container [container-replicator] [container-updater] [container-auditor] [container-sync] EOF cat \u0026gt;/etc/swift/object-server.conf \u0026lt;\u0026lt;EOF [DEFAULT] bind_ip = $STORAGE_LOCAL_NET_IP workers = 2 [pipeline:main] pipeline = object-server [app:object-server] use = egg:swift#object [object-replicator] [object-updater] [object-auditor] EOF  環境変数 ${STORAGE_LOCAL_NET_IP} を3台毎に変えて、各台で流し込んであげたら、 swift-proxy で生成した /etc/swift.conf とリング情報達を各 swift-storage に 配置します。\nswift-proxy # scp /etc/swift/swift.conf /etc/swift/*ring.gz 172.16.0.11:/tmp/ swift-proxy # scp /etc/swift/swift.conf /etc/swift/*ring.gz 172.16.0.12:/tmp/ swift-proxy # scp /etc/swift/swift.conf /etc/swift/*ring.gz 172.16.0.13:/tmp/ swift-storage01 # mv /tmp/swift.conf /tmp/*ring.gz /etc/swift/ swift-storage01 # chown -R swift:swift /etc/swift swift-storage02 # mv /tmp/swift.conf /tmp/*ring.gz /etc/swift/ swift-storage02 # chown -R swift:swift /etc/swift swift-storage03 # mv /tmp/swift.conf /tmp/*ring.gz /etc/swift/ swift-storage03 # chown -R swift:swift /etc/swift  swift-storage を各台で起動します。\nswift-storage01 # swift-init all start swift-storage02 # swift-init all start swift-storage03 # swift-init all start  アクセスしてみる 完成したので、swift クライアントでアクセスしてみる。\n% swift -A http://172.16.0.10:8080/auth/v1.0 -U system:root -K testpass stat Account: AUTH_system Containers: 4 Objects: 15 Bytes: 23866252 Connection: keep-alive Accept-Ranges: bytes  ファイルをアップロード・ダウンロードしてみる。\n% swift -A http://172.16.0.10:8080/auth/v1.0 -U system:root -K testpass upload test /etc/hosts % swift -A http://172.16.0.10:8080/auth/v1.0 -U system:root -K testpass list % test % cd /tmp/ % swift -A http://172.16.0.10:8080/auth/v1.0 -U system:root -K testpass download test % ls /tmp/etc/hosts  もちろん、HTTP な API なので curl 等の HTTP ブラウザを使ってもアクセスできる！\n% curl -k -v -H 'X-Storage-User: system:root' -H 'X-Storage-Pass: testpass' http://172.16.0.10:8080/auth/v1.0 \u0026lt; HTTP/1.1 200 OK \u0026lt; Server: nginx/1.1.19 \u0026lt; Date: Mon, 03 Sep 2012 04:58:30 GMT \u0026lt; Content-Length: 0 \u0026lt; Connection: keep-alive \u0026lt; X-Storage-Url: https://172.16.0.10:8080/v1/AUTH_system \u0026lt; X-Storage-Token: AUTH_tk8a19f76c9bce4077aee02aef76257020 \u0026lt; X-Auth-Token: AUTH_tk8a19f96c9bce4077aee02aef76257020 \u0026lt; * Connection #0 to host 172.16.0.10 left intact * Closing connection #0 * SSLv3, TLS alert, Client hello (1):  得られた X-Auth-Token, X-Storage-Url を使ってアクセスする。\n% curl -k -v -H 'X-Auth-Token: \u0026lt;token-from-x-auth-token-above\u0026gt;' \u0026lt;url-from-x-storage-url-above\u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; Server: nginx/1.1.19 \u0026lt; Date: Mon, 03 Sep 2012 04:59:47 GMT \u0026lt; Content-Type: text/plain; charset=utf-8 \u0026lt; Content-Length: 34 \u0026lt; Connection: keep-alive \u0026lt; X-Account-Object-Count: 15 \u0026lt; X-Account-Bytes-Used: 23866252 \u0026lt; X-Account-Container-Count: 4 \u0026lt; Accept-Ranges: bytes \u0026lt; test * Connection #0 to host 172.16.0.10 left intact * Closing connection #0 * SSLv3, TLS alert, Client hello (1):  さっき放り込んだ \u0026lsquo;test\u0026rsquo; が swift 上にあることが確認できる。\nhttp://cyberduck.ch/ ここにある CyberDuck という GUI なツールを使ってもアクセス出来るよ！ HTTPS が必須になるので一工夫する必要があるのだけど、そのあたりは頑張ってみてください。\nオンラインでのノードの追加・削除なんてことも出来ます。次回時間があったら解説しますね。\n今回は swift-ring-builder コマンドでレプリカ数 \u0026lsquo;3\u0026rsquo; を指定したので、アップロー ドしたファイル(オブジェクトと言う) は必ず 3 個配置される。なので 1 台の swift-storage が故障しても大丈夫。ノードを増やせばストレージ全体の容量も増やせ る。また、swift-proxy は単純な HTTP なので負荷分散機・もしくはソフトウェアのロー ドバランサを入れれば swift-proxy 自体の冗長も組めるほか、ストレージ I/O の拡張 にもつながる。pound, nginx などを使って冗長組んでみてください。その時に memcached の内容は共有させてあげる必要があるので、これまた一工夫が必要なのだけ ど。時間があったら今度解説します。\n自宅でも簡単に分散オブジェクトストレージが組める swift。使わない手は無いですよぉ。\n","permalink":"https://jedipunkz.github.io/post/2012/11/04/swift-tempauth/","summary":"最近、OpenStack にどっぷり浸かってる @jedipunkzです。\nFolsom がリリースされて Quantum を理解するのにめちゃ苦労して楽しい真っ最中なのだけど、 今日は OpenStack の中でも最も枯れているコンポーネント Swift を使ったオブジェクトストレー ジ構築について少し書こうかなぁと思ってます。\n最近は OpenStack を構築・デプロイするのに皆、Swift 入れてないのね。仲間はずれ 感たっぷりだけど、一番安定して動くと思ってる。\nこれを読んで、自宅にオブジェクトストレージを置いちゃおぅ。\n構成は ?\u0026hellip;  +--------+ | client | +--------+ | +-------------+ | swift-proxy | +-------------+ 172.16.0.10 | +-------------------+-------------------+ 172.16.0.0/24 | | | +-----------------+ +-----------------+ +-----------------+ | swift-storage01 | | swift-storage02 | | swift-storage03 | +-----------------+ +-----------------+ +-----------------+ 172.16.0.11 172.16.0.12 172.16.0.13  となる。IP アドレスは\u0026hellip;\n client : 172.16.0.0/24 のどこか swift-proxy : 172.16.0.10 swift-storage01 : 172.","title":"Swift で簡単に分散オブジェクトストレージ"},{"content":"昨日、開かれた \u0026ldquo;Opscode Chef のシークレットトレーニング\u0026rdquo; に参加してきました。\n場所はうちの会社で KDDI ウェブコミュニケーションズ。主催はクリエーションオンラ インさんでした。講師は Sean OMeara (@someara) さん。今後 Chef のトレーニングを 日本で開くため、事前に内容についてフィードバックが欲しかったそうで、オープンな レッスンではありませんでしたが、次回以降、日本でも期待できそうです。\n内容は chef の基本・メリット・考え方などを網羅した資料で1時間程進められ、その 後はハンズオンがメインでした。今日は実際にハンズオンの内容を書いていこうかと思 います。\nchef workstation 環境は揃っている前提にします。また chef server として opscode の hosted chef (opscode が提供している chef のホスティングサービス, chef-server として動作します) を使います。またターゲットホストは当日は ec2 イ ンスタンスを使いましたが、chef ワークステーションから到達できるホストであれば 何でも良いでしょう。\nまずは chef-repo のクローン。講習会で使われたものです。\ngit clone https://github.com/opscode/chef-repo-workshop-sysadmin.git chef-repo  予め cookbook が入っています。\n次に、manage.opscode.com へアクセスしアカウントを作ります。Free アカウントが誰 でも作れるようになっています。\nhttps://manage.opscode.com へアクセス -\u0026gt; Sign Up をクリック -\u0026gt; アカウント情報 を入力 -\u0026gt; submit -\u0026gt; メールにて verify -\u0026gt; 自分のアカウント名をクリック -\u0026gt; Get a new key をクリックし \u0026lt;アカウント名\u0026gt;.pem をダウンロード -\u0026gt; create a organization をクリックし Free を選択し、適当な名前で organization を作成。 validation key と knife.rb をダウンロード\nこれらで得た3つのファイル (2つの pem とknife.rb) を chef-repo/.chef/ 配下に置 きます。これで準備 OK。knife.rb を見ると、opscode の chef ホスティングにアクセ スするよう記述があります。\n% cp \u0026lt;somewhere\u0026gt;/knife.rb \u0026lt;somewhere\u0026gt;/\u0026lt;account\u0026gt;.pem \u0026lt;somewhere\u0026gt;/\u0026lt;account\u0026gt;-validation.pem chef-repo/.chef/  トレーニング用 chef-repo には予め幾つかの cookbook と role, data_bag が入って います。これらを knife を使ってアップロードします。\n% cd chef-repo % knife cookbook upload -a % knife role from file role/*.rb % knife data bag create users % knife data bag from file users nagiosadmin.json  そして bootstrap を実行。.chef/bootstrap ディレクトリに chef-full.erb ファイル が入っていて bash スクリプトになっている。knife bootstrap でこの bash スクリプ トをターゲットホスト上で実行することになる。中身はと言うと chef 環境をインストー ルしているようだ。\nchef 環境をどうやってノードに入れているかなぁ。preseed 使うかな。手作業じゃ意 味ないしなぁと考えていたのですが、こうやればいいのですね。参考になります。これ は簡単。\nではいよいよ bootstrap を実行。\n% knife bootstrap \u0026lt;IPADDRESS\u0026gt; -r 'role[base],role[monitoring]' --sudo -x \u0026lt;USER\u0026gt; -P \u0026lt;PASSWD\u0026gt;  IP アドレス、ssh ユーザ名・パスワードは適宜入れてください。これで cookbooks ディ レクトリ配下の各種 cookbook が実行されました。中身は nagios とそれに依存する apache2, openssl, mysql, php 等。\n次に cookbook を新たにダウンロードし knife upload してみます。\n% knife cookbook site download chef-client % tar zxvf chef-client-1.2.0.tar.gz -C cookbooks/ % knife cookbook upload chef-client  「アカウントの pem がない場合 validation が用いられるが、一旦サーバと通信出来た ら validation は必要なくなる。それどころか wi-fi パスワードのようなものなので validation は削除することを強くすすめる」と Sean が言っていました。chef 環境は すでに bootstrap で入っているものの、validation を削除するための recipe を base という role に追加し、実行してみます。\nSean の言っていたこと、間違っていたら指摘してください( \u0026gt;\u0026lt; ) 英語だったので自信 ないです。\n% vim role/base.rb name \u0026quot;base\u0026quot; description \u0026quot;Base role applied to all nodes.\u0026quot; run_list( \u0026quot;recipe[apt]\u0026quot;, \u0026quot;recipe[nagios::client]\u0026quot; \u0026quot;recipe[chef-client::delete_validation]\u0026quot; ) default_attributes( \u0026quot;nagios\u0026quot; =\u0026gt; { \u0026quot;server_role\u0026quot; =\u0026gt; \u0026quot;monitoring\u0026quot; } )  role をアップロード。\n% knife role from file roles/base.rb  ターゲットホストで chef-client を実行。ここで バージョン 10.14 から登場した why-run を試してから実行。\ntarget# chef-client -Fdoc -lfatal --color --why-run target# chef-client -Fdoc -lfatal --color  もしくはワークステーションから knife ssh を使っても良い。\n% knife search node \u0026quot;role:base\u0026quot; -a cloud.public_ipv4 % knife ssh \u0026quot;role:base\u0026quot; \u0026quot;sudo chef-client -Fmin\u0026quot; -x ubuntu -P opscodechef -a cloud.public_ipv4  -a cloud.public_ipv4 とは、AWS EC2 や OpenStack, CloudStack 環境ではインスタン スの eth0 インターフェースにプライベート IP アドレスが付与されているケースが 殆どなため、グローバル IP アドレスを検索し実行している。\n大きな流れはこれでおしまい。\nトレーニングでは bento, minitest, cucumber-chef 等の紹介があったが、詳細な内容 については見送られた。昼休み1時間をはさんで計7時間の長丁場だったが、chef 初級 を脱するのには最適なトレーニングだった。これからトレーニング内容や種別を組んで いくそうなので、内容は変わってくるかもしれない。あとでフィードバックをしなく ちゃ。\n個人的には OpenStack に興味を持っているので chef を使って OpenStack をデプロイ したい。今年6月には \u0026lsquo;Chef for OpenStack\u0026rsquo; というアナウンスがあり、DELL, RackSpace, HP 等の大企業がパートナーとして参加することになったと発表があった。 今まで openstack cookbook は長くメンテナンスされてこなかったので、これには期待 している。\n最後にこの機会を作って下さった クリエーションオンラインさん、opscode の Sean さん、mr.devops さん、ありがとうございましたー。貴重な体験でした。\n","permalink":"https://jedipunkz.github.io/post/2012/10/06/secret-training-of-opscode-chef/","summary":"昨日、開かれた \u0026ldquo;Opscode Chef のシークレットトレーニング\u0026rdquo; に参加してきました。\n場所はうちの会社で KDDI ウェブコミュニケーションズ。主催はクリエーションオンラ インさんでした。講師は Sean OMeara (@someara) さん。今後 Chef のトレーニングを 日本で開くため、事前に内容についてフィードバックが欲しかったそうで、オープンな レッスンではありませんでしたが、次回以降、日本でも期待できそうです。\n内容は chef の基本・メリット・考え方などを網羅した資料で1時間程進められ、その 後はハンズオンがメインでした。今日は実際にハンズオンの内容を書いていこうかと思 います。\nchef workstation 環境は揃っている前提にします。また chef server として opscode の hosted chef (opscode が提供している chef のホスティングサービス, chef-server として動作します) を使います。またターゲットホストは当日は ec2 イ ンスタンスを使いましたが、chef ワークステーションから到達できるホストであれば 何でも良いでしょう。\nまずは chef-repo のクローン。講習会で使われたものです。\ngit clone https://github.com/opscode/chef-repo-workshop-sysadmin.git chef-repo  予め cookbook が入っています。\n次に、manage.opscode.com へアクセスしアカウントを作ります。Free アカウントが誰 でも作れるようになっています。\nhttps://manage.opscode.com へアクセス -\u0026gt; Sign Up をクリック -\u0026gt; アカウント情報 を入力 -\u0026gt; submit -\u0026gt; メールにて verify -\u0026gt; 自分のアカウント名をクリック -\u0026gt; Get a new key をクリックし \u0026lt;アカウント名\u0026gt;.","title":"Secret Training of Opscode Chef"},{"content":"第7回 OpenStack 勉強会に参加してきました。\n開催日 : 2012年08月28日 開催場所 : 天王洲アイル ビットアイル  1年以上前から OpenStack, CloudStack 界隈はウォッチしていたのだけど、実際に構築 してってなると、今月始めばかりで、OpenStack も先週4日間掛けてやっとこさ構築出来たっ てところ\u0026hellip;orz。前回のブログ記事でへなちょこスクリプト公開しちゃったのを後悔しつ つ現地に向かいましたw あと、その他に Opscode Chef 等の技術にも興味持って調査し ていたので、今回の勉強会はまさに直ぐに活かせる内容だった。\nでは早速、報告があった内容と自分の感想を交えつつ書いていきます。\nHP さんのクラウドサービス HP Cloud Services 日本 HP 真壁さま  HP さんは既に Public クラウドサービスを提供し始めていて Ojbect Storage, CDN 部 分は既にリリース済みだそうだ。compute, block storage 等はベータ版状態でこれか らリリース。OpenStack ベースな構成で Horizon 部分は自前で開発したもの。既 にサーバ数は万の桁まで到達！ MySQL な DaaS も登場予定だとか。\nあと HP だけにクラウドサービスに特化したサーバ機器も出していて、それが HP Project Moonshot 。ARM/Atom 搭載のサーバで 2,880 nodes/rack が可能だとか！す げぇ。もちろん電源等のボトルネックとなるリソースは他にも出てきそうだけど。\nノード数って増えると嬉しいのかな？コア数が増えるのは嬉しいけど。\nCanonical JuJu Canonical 松本さま  JuJu は Canonical が提供しているデプロイツールで charms と呼ばれるレシピ集 (っ て言うと語弊があるのか) に従ってソフトウェアの配布を行うツール。MAAS という物 理サーバのプロビジョニングツールと組み合わせればハードウェアを設置した後のプロ ビジョニング操作は一気通貫出来る、といったもの。具体的な操作例を挙げてくれたの で添付してきます。\n% juju deploy --repository=/tmp swift-proxy % juju deploy --repository=/tmp swift-storage % juju add-relation swift-storage:swift-proxy % swift-proxy:swift-proxy % juju add-unit swift-storage # node 追加  swift-proxy, swift-storage をデプロイし、その後それぞれを関係付けているのが add-relation。また swift-proxy に対して swift-storage node を追加してくといっ た操作が add-unit らしい。\nCharms と呼ばれるモノの中をのぞかせてもらったが Shell Script と json ファイル 集になっていた。インフラ系のエンジニアに操作してもらうにはこれがベスト、といっ たところなのだろう。Opscode Chef の様な自由度があるかどうかは、触ってみないと 分からない。時間を見つけて調べてみるかぁ。\nちなにみ今日 MAAS について調べたのですが、これは PXE Boot と DHCP のコンフィギュ レーションを GUI でするってものなのですね。後に出てくる Crowbar とはだいぶ違う。 間違っていたら指摘してください..。\n参考 URL : https://wiki.ubuntu.com/ServerTeam/MAAS\nRedHat の OpenStack への取り組み RedHat 中井さま  OpenStack ディストリビューションを提供し始めたことで最近話題になっていたが、い よいよ今年2012年10月リリース予定の folsom をベースとしたリリースを2013年に控え ているそうだ。ここで初めて有償サポートが開始されるそう。より簡単に構築が出来て、 技術的な不安定を持っているユーザを取り込んでいくのだろう。面白いネタも貰えた。 将来、Swift 代替で Gluster-FS が扱えるようになる可能性があるそうだ。また KVM にコミットしているエンジニアを抱えている彼らだが、KVM から直接 Gluster-FS 上の VM イメージを操作出来るように修正加える案も出ているそうだ。これが実現すれば、 nova ノードのサーバリソースに依存しない大きな Disk イメージを扱うことも可能に なるだろう。\nまた、Canonical JuJu, Opscode Chef に並ぶツールの紹介もあった。CloudForms がそ れなのだが、各社と共通するコンセプトを持っているようだ。開発環境・本番環境への シームレスなデプロイ、と。\nDELL Crowbar DELL 増月さま  ある意味、僕にとって一番の収穫だったのが DELL の Crowbar。DELL サーバに依存せ ず使えるデプロイツールで、IPMI, RAID, BIOS 等のハードウェア構成も自動構築が出 来るそう！また Opscode Chef がベースになっていて barclamp と呼ばれる Chef で 言う Cookbooks を元にソフトウェアをデプロイしていくそうだ。Chef Server 環境が 必須で chef-solo のような操作には対応していなそうなのが残念だった。Web ベース の GUI インターフェースで操作するらしい。デモも当日見れました。\nハードウェア構成も自動構築出来るツールは Canonical の MAAS があるが、一歩踏み 込んだ構成が組めそう。尚、Opscode の Cookbooks を再利用するのは少し難しい状況 のようだ。この辺りは 2.0 バージョンで改善されるそうだ。Chef との依存関係をより シンプルなものにするそう。\n他ベンダのと差別化を図るのかと思いきや、サーバ機器に依存しないツールを出してく る DELL さんの思いは、どこにあるのだろう。あと BIOS, RAID 周りをソフトウェアで プロビジョニングしていく受け口の I/F は IPMI なのかな？質問すればよかった。\nGMO お名前 KVM 先月の OSC で発表になった資料をベースに説明して頂いた。diablo ベースで CentOS 上に構築されているらしい。また griddynamic.net のパッケージ (知らなかった) を 利用して (なぜ素直に Ubuntu 使わないのかな？) 構築したそうだ。griddynamic.net のパッケージは既にエキスパイアしているらしい\u0026hellip;。nova ノードは既に200台規模。 libvirt, kvm 周りにパッチを独自で当てているそうで、その具体的内容が聞けた。た だこの辺は OpenStack のアップデートに追いつく作業がめちゃ大変になるだろうなぁ と想像する。..\n全体を通して Opscode Chef に並ぶ技術が出始めてきた。OpenStack で nodes を追加するところまで 自動化したとしても vm 上の操作を手作業するわけにはいかないし、必然なのだろう。 よりディストリビューションに依存しない、インフラ系・アプリ系共に理解出来る、自 由度のある、汎用性のある技術を我々が選びながら使っていく必要がありそう。僕らイ ンフラ系エンジニアの仕事内容も、この辺りにシフトしていく時代はもう目の前まで来 ているだろう。OpenStack を構築する手順が JuJu 等でコマンド一発なのを見て唖然と したのも確か。誰でもできる操作になるのも必然で、ただどういう操作がされているか を理解し、必要に応じて改変して開発していく力は身に着けておかないと、インフラ系 は特に、仕事内容が単純化していく一方になる気がする。危機を感じつつチャンスに結 びつけるいい機会なのかなぁ。あとは監視周りも自動コンフィギュレーションされない と、真の自動化には至らないなぁ。\n当日は BitIsle スタッフのみなさん、コミュニティのみなさん、ありがとうございま したぁー。\n","permalink":"https://jedipunkz.github.io/post/2012/08/29/7th-openstack-meetup/","summary":"第7回 OpenStack 勉強会に参加してきました。\n開催日 : 2012年08月28日 開催場所 : 天王洲アイル ビットアイル  1年以上前から OpenStack, CloudStack 界隈はウォッチしていたのだけど、実際に構築 してってなると、今月始めばかりで、OpenStack も先週4日間掛けてやっとこさ構築出来たっ てところ\u0026hellip;orz。前回のブログ記事でへなちょこスクリプト公開しちゃったのを後悔しつ つ現地に向かいましたw あと、その他に Opscode Chef 等の技術にも興味持って調査し ていたので、今回の勉強会はまさに直ぐに活かせる内容だった。\nでは早速、報告があった内容と自分の感想を交えつつ書いていきます。\nHP さんのクラウドサービス HP Cloud Services 日本 HP 真壁さま  HP さんは既に Public クラウドサービスを提供し始めていて Ojbect Storage, CDN 部 分は既にリリース済みだそうだ。compute, block storage 等はベータ版状態でこれか らリリース。OpenStack ベースな構成で Horizon 部分は自前で開発したもの。既 にサーバ数は万の桁まで到達！ MySQL な DaaS も登場予定だとか。\nあと HP だけにクラウドサービスに特化したサーバ機器も出していて、それが HP Project Moonshot 。ARM/Atom 搭載のサーバで 2,880 nodes/rack が可能だとか！す げぇ。もちろん電源等のボトルネックとなるリソースは他にも出てきそうだけど。\nノード数って増えると嬉しいのかな？コア数が増えるのは嬉しいけど。\nCanonical JuJu Canonical 松本さま  JuJu は Canonical が提供しているデプロイツールで charms と呼ばれるレシピ集 (っ て言うと語弊があるのか) に従ってソフトウェアの配布を行うツール。MAAS という物 理サーバのプロビジョニングツールと組み合わせればハードウェアを設置した後のプロ ビジョニング操作は一気通貫出来る、といったもの。具体的な操作例を挙げてくれたの で添付してきます。","title":"第7回 OpenStack 勉強会参加レポート"},{"content":"OpenStack のインストールってしんどいなぁ、って感じて devstack http://devstack.org/ とかで構築して中を覗いていたのですが、そもそも devstack って再起動してしまえば何も起動してこないし、swift がインストールされないしで。 やっぱり公式のマニュアル見ながらインストールするしかないかぁって\u0026hellip;。感じてい たのですが\u0026hellip;。\nhttp://docs.openstack.org/essex/openstack-compute/starter/os-compute-starterguide-trunk.pdf\nこのマニュアルの前提は、ネットワーク2セグメント・server1, server2 の計2台が前 提なのですが、環境作るのがしんどいので、オールインワンな構築がしたい！サーバ1台で OpenStack ESSEX を インストールしたい！で、シェルスクリプトを作ったのでそれを使ったインストール方法を紹介します。\nぼくの Thinkpad に OpenStack ESSEX をインストールしてブラウザで localhost に接続して いる画面です。ちゃんと KVM VM が起動して noVNC で接続できています。自己満足やぁ。\n前提条件  Ubuntu Server 12.04 LTS amd64 がインストールされていること Intel-VT もしくは AMD−Vなマシン NIC が一つ以上ついているマシン /dev/sda6, /dev/sda7 (デバイス名は何でもいい) の2つが未使用で空いていること  です。\n構成 1 NIC を前提に eth0 と eth0:0 の2つを想定すると、こんな構成になります。eth0:0 は完全にダミーで IP アドレスは何でもいいです。br100 ブリッジデバイス上で VM が NW I/F を持ちます。floating range ってのは OpenStack で言うグローバル IP レン ジ。グローバルである必要は無いですが eth0 と同じレンジの IP アドレスを VM に付 与出来ます。/dev/sda6 が nova-volumes で /dev/sda7 が swift 。なので OS インス トール時に2つのデバイスを未使用で空けておいてください。\n+--+--+--+ |VM|VM|VM| 192.168.4.32/27 +--+--+--+.. +----------+ +--------+ | | | br100 | 192.168.4.33/27 -\u0026gt; floating range : 10.200.8.32/27 | | +--------+ | | | eth0:0 | 192.168.3.1 disk devices | Host | +--------+ (dummy) +------------------------+ | | | /dev/sda6 nova-volumes | | | +--------+ +------------------------+ | | | eth0 | ${HOST_IP} | /dev/sda7 swift | +----------+ +--------+ +------------------------+ | nw I/Fs +----------+ | CPE | +----------+  インストール手順 インストール手順は簡単です。\n% sudo -i # git clone git://github.com/jedipunkz/openstack_install.git  して取得したスクリプトを環境に合わせて環境変数設定します。スクリプト上部のこれ らの内容を、環境に合わせて設定。${HOST_IP} と ${NOVA_VOLUMES_DEV}, ${SWIFT_DEV} だけ気をつければ OK です。その他は内部ネットワークの設定なので何 でもつながります。\n# ----------------------------------------------------------------- # Environment Parameter # ----------------------------------------------------------------- HOST_IP='10.200.8.15' HOST_MASK='255.255.255.0' HOST_NETWORK='10.200.8.0' HOST_BROADCAST='10.200.8.255' GATEWAY='10.200.8.1' MYSQL_PASS='secret' FIXED_RANGE='192.168.4.1/27' FLOATING_RANGE='10.200.8.32/27' FLAT_NETWORK_DHCP_START='192.168.4.33' ISCSI_IP_PREFIX='192.168.4' NOVA_VOLUMES_DEV='/dev/sda6' SWIFT_DEV='/dev/sda7'  で実行。\n# chmod +x openstack_install/openstack_install.sh # ./openstak_install/openstack_install.sh allinone ( wait some minutes...)  マシンによりますが、10分弱すると OpenStack が構築されているはずです。\nOS イメージと SSH キーペアのインストール 上記の手順で OpenStack は構築されるのですが、VM を起動するには OS イメージが必 要ですよね。これは自分で用意するしかないです。ただ、これは簡単で下記の手順で出 来ます。\nOS イメージ作成 サンプルで Ubuntu Server 12.04 LTS amd64 なイメージをここで作ってみます。\n# kvm-image create -f qcow2 server.img 5G # wget http://gb.releases.ubuntu.com//precise/ubuntu-12.04-server-amd64.iso # kvm -m 256 -cdrom ubuntu-12.04-server-amd64.iso -drive file=server.img,if=virtio,index=0 -boot d -net nic -net user -nographic -vnc :0  手元の端末の VNC ツールで ${HOST_IP}:0 に接続し OS のインストールを済ませます。 その後、下記のコマンドで HDD から起動してあげて\u0026hellip;\n# kvm -m 256 -drive file=server.img,if=virtio,index=0 -boot c -net nic -net user -nographic -vnc :0  再度、VNC で VM に接続して..\n# sudo rm -rf /etc/udev/rules.d/70-persistent-net.rules # shutdown -h now  上記の操作をしたら OS イメージ作成は終わり。その他のディストリビューションでの イメージ作成については公式マニュアルに書いてあります。\nGlance に OS イメージをインストール 作成した OS イメージを Glance に追加します。先ほどのスクリプトで生成された /root/.openstack が Glance に接続するために必要なので zsh の場合 source してから\u0026hellip;\n# source /root/.openstack # glance add name=\u0026quot;Ubuntu Server 12.04LTS\u0026quot; is_public=true container_format=ovf disk_format=qcow2 \u0026lt; server.img  で追加出来ます。.openstack は bash でも取得できるのでその際は\n# . /root/.openstack  してください。root ユーザ以外でも操作出来ます。\nSSH キーペアの生成とインストール VM に割り当てる SSH キーペアを作ってインストールする手順です。\n# ssh-keygen # nova keypair-add --pub_key .ssh/id_rsa.pub mykey # nova keypair-list  Horizon へ接続 いよいよ Horizon へ接続です。Horizon は OpenStack ESSEX から取り込まれた Web UI です。VM の作成・削除、ネットワークの設定等がブラウザで操作出来ます。\nhttp://${HOST_IP  に接続してユーザ : \u0026lsquo;admin\u0026rsquo;, パスワード \u0026lsquo;admin\u0026rsquo; でログインしてください。\nまとめ OpenStack はしんどいｗ ですが来月 2012/09 リリース予定の Folsom は \u0026lsquo;Easy Setup\u0026rsquo; がフューチャされてるそうです。期待。手動で構築していると glance のとこ ろで ID 地獄にハマりますｗ 今回の手順で all in one な環境ができたら、色々覗い てみてコンポーネント毎に Node を切り出すってことも考えないといけないと思います。 それぞれは HTTP ベースの API で接続できれば OK なので切り出すこと自体は簡単。 冗長を組む方法は.. これから調べます。keystone, glance を拡張・冗長させるって出 来るのか？難しそう。CloudStack と違って rabbitmq-server でキューイングしてくれ るので、Node が増えた時の対処は考えれれているよう。\nあと、OS イメージではなくて AMI で VM を作る方法もあるのですが AMI の作成方法は Web を見ていると沢山載っていますので参考にして作ってみてください。\n","permalink":"https://jedipunkz.github.io/post/2012/08/26/all-in-one-openstack-installation/","summary":"OpenStack のインストールってしんどいなぁ、って感じて devstack http://devstack.org/ とかで構築して中を覗いていたのですが、そもそも devstack って再起動してしまえば何も起動してこないし、swift がインストールされないしで。 やっぱり公式のマニュアル見ながらインストールするしかないかぁって\u0026hellip;。感じてい たのですが\u0026hellip;。\nhttp://docs.openstack.org/essex/openstack-compute/starter/os-compute-starterguide-trunk.pdf\nこのマニュアルの前提は、ネットワーク2セグメント・server1, server2 の計2台が前 提なのですが、環境作るのがしんどいので、オールインワンな構築がしたい！サーバ1台で OpenStack ESSEX を インストールしたい！で、シェルスクリプトを作ったのでそれを使ったインストール方法を紹介します。\nぼくの Thinkpad に OpenStack ESSEX をインストールしてブラウザで localhost に接続して いる画面です。ちゃんと KVM VM が起動して noVNC で接続できています。自己満足やぁ。\n前提条件  Ubuntu Server 12.04 LTS amd64 がインストールされていること Intel-VT もしくは AMD−Vなマシン NIC が一つ以上ついているマシン /dev/sda6, /dev/sda7 (デバイス名は何でもいい) の2つが未使用で空いていること  です。\n構成 1 NIC を前提に eth0 と eth0:0 の2つを想定すると、こんな構成になります。eth0:0 は完全にダミーで IP アドレスは何でもいいです。br100 ブリッジデバイス上で VM が NW I/F を持ちます。floating range ってのは OpenStack で言うグローバル IP レン ジ。グローバルである必要は無いですが eth0 と同じレンジの IP アドレスを VM に付 与出来ます。/dev/sda6 が nova-volumes で /dev/sda7 が swift 。なので OS インス トール時に2つのデバイスを未使用で空けておいてください。","title":"OpenStack ESSEX オールインワン インストール"},{"content":"仕事で Opesocd Chef の情報収集をしてたのですが、僕が感じるにこれはインフラエン ジニアの未来だと。逆に言うとインフラエンジニアの危機。AWS のようなクラウドサー ビスがあればアプリケーションエンジニアが今までインフラエンジニアが行っていた作 業を自ら出来てしまうからです。\nインフラエンジニアなら身に付けるしかない！って僕が感じる Chef について chef-solo を通して理解するために情報まとめました。\nchef には chef-server 構成で動作するものと chef-solo というサーバ無しで動作す るものがある。chef-server は構築するのが少し大変 (後に方法をブログに書きたい) なので今回は chef-solo を使ってみる。ちなみに Opscode が chef-server のホスティ ングサービスを展開している。彼らとしてはこちらがメイン。\nchef-solo の入れ方 opscode が推奨している ruby-1.9.2 をインストールする。rvm は色々問題を招き寄せ るので rbenv を使って環境整えます。root ユーザ環境内に入れてください。\n必要なパッケージをインストール\n% sudo apt-get update % sudo apt-get install build-essential zlib1g-dev libssl-dev  root ユーザにてrbenv をインストール\n% sudo -i # cd ~ # git clone git://github.com/sstephenson/rbenv.git .rbenv # echo 'export PATH=\u0026quot;$HOME/.rbenv/bin:$PATH\u0026quot;' \u0026gt;\u0026gt; ~/.zshrc # echo 'eval \u0026quot;$(rbenv init -)\u0026quot;' \u0026gt;\u0026gt; ~/.zshrc  ruby-build をインストール\n# mkdir -p ~/.rbenv/plugins # cd ~/.rbenv/plugins # git clone git://github.com/sstephenson/ruby-build.git  opscode が推奨している ruby バージョン 1.9.2 をインストール\n# rbenv install 1.9.2-p290 # rbenv global 1.9.2-p290 # rbenv rehash  capistrano, chef のインストールを gem を使い行う\n# gem install chef # rbenv rehash  これらは \u0026lsquo;root\u0026rsquo; ユーザ環境内に構築する必要がある。chef がそれを前提としている からだ。また、perl と違い ruby は後方互換性がないので将来のことを考え rbenv で バージョンを管理し続ける必要がある、と思う。\nchef-solo の設定 /etc/chef/chef.json ファイルを修正することで、chef-solo で実行する recipe の追 加を行う。これは複数指定することが可能。\n{ \u0026quot;run_list\u0026quot;: [ \u0026quot;recipe[ntp]\u0026quot;, ] }  上記は ntp レシピ を追加した例。次に /etc/chef/solo.rb を生成する。これは chef-solo 動作に必要な PATH 指定を主に行う。\nfile_cache_path \u0026quot;/tmp/chef-solo\u0026quot; cookbook_path [\u0026quot;/home/jedipunkz/cookbooks\u0026quot;] role_path \u0026quot;/home/jedipunkz/role\u0026quot; log_level :debug  上記パラメータの説明は下記の通り。\n file_cache_path : cache 用のディレクトリ指定 cookbook_path : cookbook を配置するディレクトリ指定 role_path : role ディレクトリ指定 log_level : Log Level の指定  サンプルクックブックのダウンロードと理解 サンプルとして opscode が提供している \u0026lsquo;ntp\u0026rsquo; を持ってくる。中身が簡単に理解出来 るものなので最初理解するために持ってくるものとしては最適。\n% cd /home/jedipunkz/cookbooks % git clone https://github.com/opscode-cookbooks/ntp.git  持ってきたクックブックの構造は\nntp . |- attributes |- templates |- recipes |- meradata.rb |-.. |-..  となっている。attribute/default.rb の一部分を抜粋を記してみた。\ndefault['ntp']['servers'] = %w{ 0.pool.ntp.org 1.pool.ntp.org 2.pool.ntp.org 3.pool.ntp.org } default['ntp']['packages'] = %w{ ntp ntpdate } default['ntp']['service'] = \u0026quot;ntp\u0026quot; default['ntp']['varlibdir'] = \u0026quot;/var/lib/ntp\u0026quot; default['ntp']['conf_owner'] = \u0026quot;root\u0026quot; default['ntp']['conf_group'] = \u0026quot;root\u0026quot; default['ntp']['var_owner'] = \u0026quot;ntp\u0026quot; default['ntp']['var_group'] = \u0026quot;ntp\u0026quot;  chef は ruby の DSL で記述するが template や recipe 内で指定するパラメータ集と なるのが attribute となる。上を見てみると ntp のパッケージ名やディレクトリのオー ナー情報等が記されている。\ntemplates/default/ntp.conf.erb を見てみると\u0026hellip;\ndriftfile \u0026lt;%= node['ntp']['driftfile'] %\u0026gt; statsdir \u0026lt;%= node['ntp']['statsdir'] %\u0026gt; statistics loopstats peerstats clockstats filegen loopstats file loopstats type day enable filegen peerstats file peerstats type day enable filegen clockstats file clockstats type day enable \u0026lt;%# If ntp.peers is not empty %\u0026gt; \u0026lt;% unless node['ntp']['peers'].empty? -%\u0026gt; \u0026lt;%# Loop through defined peers, but don't peer with ourself %\u0026gt; \u0026lt;% node['ntp']['peers'].each do |ntppeer| -%\u0026gt; \u0026lt;% if node['ipaddress'] != ntppeer and node['fqdn'] != ntppeer %\u0026gt; peer \u0026lt;%= ntppeer %\u0026gt; iburst restrict \u0026lt;%= ntppeer %\u0026gt; nomodify \u0026lt;% end -%\u0026gt; \u0026lt;% end -%\u0026gt; \u0026lt;% end -%\u0026gt;  これはインストールする /etc/ntp.conf (recipde で後に指定する) の内容そのままだ。 先程も書いたが chef は ruby の DSL 記述が基本なので attribute で指定したパラメー タを持ってきて、こういったコンフィグファイルを生成出来る。では最後に recipe を 見てみる。この recipe が chef の本体と言っていいところですね。上部の抜粋です。\nnode['ntp']['packages'].each do |ntppkg| package ntppkg end  この [\u0026lsquo;ntp\u0026rsquo;][\u0026lsquo;packages\u0026rsquo;] は attributes/default.rb に %w{ ntp ntpdate } と書い てある。つまり ntp, ntpdate の配列を ntppkg として回して chef resources の \u0026lsquo;package\u0026rsquo; を使ってインストールしている。resources については chef の公式ドキュ メントを読むと良い。recipe で使える記述全てが1ページにまとまっている。\nhttp://wiki.opscode.com/display/chef/Resources\n次に recipe の下部を抜粋してみた。chef resources の template によって ntp.conf をインストールしている。\ntemplate \u0026quot;/etc/ntp.conf\u0026quot; do source \u0026quot;ntp.conf.erb\u0026quot; owner node['ntp']['conf_owner'] group node['ntp']['conf_group'] mode \u0026quot;0644\u0026quot; notifies :restart, resources(:service =\u0026gt; node['ntp']['service']) end  source によって template/default/ntp.conf.erb を呼び出し owner, group でファイ ルのオーナー情報を、mode でパーミッションを指定している。また修正が入った際に ntp サービスの再起動を行っているのが最終行だ。\n抜粋で例を挙げながらだったが、Resources の記述方法さえ理解してしまえば全てが理 解出来るだろうし、自分でクックブックを作ることも簡単だろう。\nchef-solo の実行 ではいよいよ chef-solo の実行。\n上記で生成した chef.json と solo.rb を指定し chef-solo を実行することで上記 run_list で指定した recipe \u0026lsquo;ntp\u0026rsquo; が実行される。\n% sudo -i # chef-solo -c /etc/chef/solo.rb -j /etc/chef/chef.json  まとめ chef-server 構成の組み方は後日ブログで書いてみたい。先日 #DevLOVE に参加した際 にも話題になったが、chef-solo を使うか chef-server 構成を組むか、まだ議論が必 要そう。chef-server 構成を組むことは簡単では無いが普通にエンジニアなら組めるだ ろう。が、組んだところで拡張性・冗長性・を考えた構成を組むにはまだまだノウハウ が足りない。また couchDB, rabbitmq など比較的新しいミドルウェアが使われている ので、これから経験積まないと難しいだろう。それに比べて chef-solo は上記の通り とてもシンプル。しかも chef-solo を実行する node 自身は必然的に数が増え拡張す るし、それを受ける apt レポジトリは単純な HTTP なので拡張・冗長は簡単だろう。\nまた、capistrano と chef-solo を組み合わせることで、role といった概念をもたせ たり、workstation で一括操作といった利便性も持たせることが出来る。ある意味 chef-solo を使うなら必然的な点になりそう。capistrano との組み合わせについても 後日ブログで書いてみたい。\nchef を理解すること自体はそんなには難しくないし、これからの時代に必要になるこ とは眼に見えているので、学んでおいて損はしないだろう。\n","permalink":"https://jedipunkz.github.io/post/2012/08/18/chef-solo/","summary":"仕事で Opesocd Chef の情報収集をしてたのですが、僕が感じるにこれはインフラエン ジニアの未来だと。逆に言うとインフラエンジニアの危機。AWS のようなクラウドサー ビスがあればアプリケーションエンジニアが今までインフラエンジニアが行っていた作 業を自ら出来てしまうからです。\nインフラエンジニアなら身に付けるしかない！って僕が感じる Chef について chef-solo を通して理解するために情報まとめました。\nchef には chef-server 構成で動作するものと chef-solo というサーバ無しで動作す るものがある。chef-server は構築するのが少し大変 (後に方法をブログに書きたい) なので今回は chef-solo を使ってみる。ちなみに Opscode が chef-server のホスティ ングサービスを展開している。彼らとしてはこちらがメイン。\nchef-solo の入れ方 opscode が推奨している ruby-1.9.2 をインストールする。rvm は色々問題を招き寄せ るので rbenv を使って環境整えます。root ユーザ環境内に入れてください。\n必要なパッケージをインストール\n% sudo apt-get update % sudo apt-get install build-essential zlib1g-dev libssl-dev  root ユーザにてrbenv をインストール\n% sudo -i # cd ~ # git clone git://github.com/sstephenson/rbenv.git .rbenv # echo 'export PATH=\u0026quot;$HOME/.","title":"chef-solo で学ぶ chef の基本動作"},{"content":"chef-server の構築は少し面倒だと前回の記事 http://jedipunkz.github.com/blog/2012/08/18/chef-solo/ に書いたのですが、 opscode が提供している bootstrap を用いると、構築作業がほぼ自動化出来ます。 今回はこの手順を書いていきます。\nchef のインストール 前回同様に rbenv を使って ruby をインストールし chef を gem でインストールして いきます。\n% sudo apt-get update % sudo apt-get install zlib1g-dev build-essential libssl-dev % sudo -i # cd ~ # git clone git://github.com/sstephenson/rbenv.git .rbenv # echo 'export PATH=\u0026quot;$HOME/.rbenv/bin:$PATH\u0026quot;' \u0026gt;\u0026gt; ~/.zshrc # echo 'eval \u0026quot;$(rbenv init -)\u0026quot;' \u0026gt;\u0026gt; ~/.zshrc  ruby-build をインストールします。\n# mkdir -p ~/.rbenv/plugins # cd ~/.rbenv/plugins # git clone git://github.com/sstephenson/ruby-build.git  ruby-1.9.2 インストール\n# rbenv install 1.9.2-p290 # rbenv global 1.9.2-p290 # rbenv rehash  gem を使って、chef, chef-server-api をインストールします\n# gem install chef # gem install chef-server-api  opscode bootstrap を使った chef-server の構築 まずは chef-solo の環境整備。この bootstrap は chef-solo で実行出来るクックブッ ク集になっています。中を覗くと \u0026lsquo;apache2, chef, chef-server, couchdb, erlang, rabgitmq\u0026rsquo; などなど必要なミドルウェア群のクックブックが入っていることが判ります。\nまずは chef-solo の環境を整備します。\n# mkdir /etc/chef # vi /etc/chef/solo.rb # cat /etc/chef/solo.rb file_cache_path \u0026quot;/tmp/chef-solo\u0026quot; cookbook_path \u0026quot;/tmp/chef-solo/cookbooks\u0026quot; # vi /etc/chef/chef.json # cat /etc/chef/chef.json { \u0026quot;chef_server\u0026quot;: { \u0026quot;server_url\u0026quot;: \u0026quot;http://localhost:4000\u0026quot; }, \u0026quot;run_list\u0026quot;: [ \u0026quot;recipe[chef-server::rubygems-install]\u0026quot; ] }  solo.rb で指定したパスは任意のもので構いません。\nAmazon S3 上に opscode bootstrap があります。いよいよ chef-solo で bootstrap を実行します。\n# chef-solo -c /etc/chef/solo.rb -j /etc/chef/chef.json -r http://s3.amazonaws.com/chef-solo/bootstrap-latest.tar.gz  これで chef-server 構築は終わりです。実行したホストに couchDB, Erlang などが構 成されていることが分かると思います。\n次に knife の環境を整備します。knife は chef の操作を行うためのコマンドライン ツールです。\n# mkdir -p ~/.chef # cp /etc/chef/validation.pem /etc/chef/webui.pem ~/.chef # sudo chown -R root ~/.chef # knife configure -i Where should I put the config file? [~/.chef/knife.rb] Please enter the chef server URL: [http://localhost:4000] Please enter a clientname for the new client: [root] Please enter the existing admin clientname: [chef-webui] Please enter the location of the existing admin client's private key: # [/etc/chef/webui.pem] /root/.chef/webui.pem Please enter the validation clientname: [chef-validator] Please enter the location of the validation key: # [/etc/chef/validation.pem] /root/.chef/validation.pem Please enter the path to a chef repository (or leave blank): WARN: Creating initial API user... INFO: Created (or updated) client[root] WARN: Configuration file written to /home/root/.chef/knife.rb  パラメータはほぼそのまま入力してください。pem ファイルのパス指定だけ先ほど root ユーザの HOME ディレクトリにインストールしたモノを指定してください。\nこれで下記のように knife が使えるようになっています。\n# knife client list chef-validator chef-webui root  今回は以上です。この chef-server の構築を knife::server というツールで実現する 方法もあります。\nhttp://fnichol.github.com/knife-server/\n同じく bootstrap を指定して構築するツールなのですが、これを使う利点として\n chef-server 構成のバックアップが取れる バックアップを元に復元できる  があります。これは便利。ただし knife が最初から使える環境に限ります。\nこれらツールを使うことを前提にしないと、作業が複雑化するため、必須だと思います。 手作業はミスの元ですし。ただ、一度は手作業でやってみて構成を理解することはしな くてはならないでしょう。couchDB, Rabbitmq などの新しいミドルウェアについての理 解も深めなくてはならないでしょうし、これらをスケールさせることを前提にしておか ないとノードの数が増える度に負荷が上昇していくので将来困るでしょう。前回の記事 でも触れましたが chef-solo を用いた場合はそれらの心配が無くなります。\n","permalink":"https://jedipunkz.github.io/post/2012/08/18/opscode-bootstrap-chef-server/","summary":"chef-server の構築は少し面倒だと前回の記事 http://jedipunkz.github.com/blog/2012/08/18/chef-solo/ に書いたのですが、 opscode が提供している bootstrap を用いると、構築作業がほぼ自動化出来ます。 今回はこの手順を書いていきます。\nchef のインストール 前回同様に rbenv を使って ruby をインストールし chef を gem でインストールして いきます。\n% sudo apt-get update % sudo apt-get install zlib1g-dev build-essential libssl-dev % sudo -i # cd ~ # git clone git://github.com/sstephenson/rbenv.git .rbenv # echo 'export PATH=\u0026quot;$HOME/.rbenv/bin:$PATH\u0026quot;' \u0026gt;\u0026gt; ~/.zshrc # echo 'eval \u0026quot;$(rbenv init -)\u0026quot;' \u0026gt;\u0026gt; ~/.zshrc  ruby-build をインストールします。\n# mkdir -p ~/.rbenv/plugins # cd ~/.rbenv/plugins # git clone git://github.","title":"Opscode Bootstrap を使った Chef-Server 構築"},{"content":"2012年07月21日に大崎のフィーチャーアーキテクトさんで行われた #DevLOVE (Chef De DevOps) に参加してきました。\n開催日 : 2012年07月21日(土曜日) 15:00 - 20:40 場所 : 大崎 フューチャーアーキテクトさま URL : http://www.zusaar.com/event/314003  仕事場でも Chef の利用を考え始めていて今回いい機会でした。半年前と比べるとだい ぶ揃って来ましたが、まだまだ資料の少ない Chef。貴重な機会でした。\nプログラムは下の通り。\n*『Chefの下準備』by Ryutaro YOSHIBA [ @ryuzee ] *『Chef自慢のレシピ披露』 by 中島弘貴さん [ @nakashii_ ] * ワークショップ『みんなでCooking』 * dialog『試食会』 * Niftyさんのスーパー宣伝タイム!!! *『渾身会』  @ryuzee さんの \u0026ldquo;Chef の下準備\u0026rdquo; は SlideShare に使った資料が公開されています。\n印象的だったのが、VirtualBox + Vagrant という環境。実際に使っていらっしゃいま した。MacBook の中に仮想環境とそのインターフェースである Vegrant を使って、デ プロイのテスト等が実施できるそうです。また、Vegrant 設定ファイルは Chef のレシ ピを自動読込して、常に本番環境と同じ状態にしているそうです。CloudFormation と いうキーワードや Capistrano というキーワードが出てきました。最近よく耳にするワー ドです。また CI は Jenkins だよね、だったり。\n3番目のプログラム \u0026ldquo;ワークショップ、みんなで Cooking\u0026rdquo; は 4-6 人の組みになり、実 際に Chef のレシピを書いてみようというもの。今回は wordpress の構築のためのレ シピを書いていきました。僕らの組みも時間ギリギリでレシピの投入を終えました。 wordpress の設定ファイル wp-config.php と mysql への wordpress 用データベース の作成を実際に attribute, template, script ら Resources を使ってレシピを書きま した。簡単な例でしたが、みなとコミュニケーションがとれたのでいい機会でした。\ndialog \u0026ldquo;試食会\u0026rdquo; では、グループ毎、またグループを入れ糧のディスカッション。使っ てみてどうだったか？を話し合ってそこから議論を伸ばしていきました。\nやっぱり僕も参加する前からそうだったのですが、一番皆が気になっているのが、\n\u0026quot;chef-solo + capistrano ? それとも chef-server がいいの？\u0026quot;  でした。実際に使っていらっしゃる CA さんの話によると、20台までは chef-solo + capistrano の構成が良いと。そして100台規模になると chef-server が良いそうです。 具体的に何が chef-solo で問題になるのかが聞けなかったのですが、ちょっと僕らの 課題にしてみようかと思います。また、CA さんでは puppet を使っていらっしゃる部 署もあるそうで、chef-server の動作が不安定？とかで400台規模になると puppet が 良くなると。そして chef-server はいまは rabbitmq, couchDB, erlang などの構成で 出来ているのですが、そのうちどれかが (erlang の solr ?) が mysql に置き換わる と噂を聞きました。\nまた、nifty さんが現在 opscode chef wiki を翻訳されているのですが、僕も非常に 興味があります。参加させてもらおうかな。また nifty さんが nifty cloud 用 knife-ec2 を公開しているそうで説明がありました。自社にクラウドシステムがあると、 色々楽しめるよなぁ\u0026hellip;。いいな。\n[2012/07/23 15:00 追記] nifty の @tily さんが当日使った資料を公開なさったので貼り付けておきます。\n以上が \u0026ldquo;Chef De DevOps\u0026rdquo; の参加レポートで、ここからは僕らの課題でありここ1週間 くらいで取り組もうとしている点。\nchef-server の構築ですが、ubuntu + opscode の deb package を使う方法もあるので すが、これだと opscode が推奨している ruby のバージョン 1.9.2 ではなく 1.8.7 になってしまいます。これは ubuntu, debian の stable release もしくは development release が frozen しているからです。なので、今僕が考えているのは、 chef-solo と opscode bootstrap の組み合わせで構築する方法です。\nhttp://wiki.opscode.com/display/chef/Installing+Chef+Server+using+Chef+Solo\nこの bootstrap は opscode が chef-server 構築用レシピとして公開しているもので す。これを使えば chef client さえ入っていれば構築できてしまうという。この手順 は一度手元でも確認してみました。(Ubuntu Server 12.04 LTS amd64).\nまた Chef のメーリングリストで聞いてみたところ、やはり推奨の ruby バージョンは 1.9.2 であって、この bootstrap の方法と、あとは knife::server というものです。\nまだ僕は手元で確認できていないのですが、こちらにあるようです。\nhttp://fnichol.github.com/knife-server/\nchef-server 構成だと rabbitmq に queue が溜まったり、couchDB のディスクを溢れ させたりとトラブル起きた時の切り分けだったり対処方法だったり、まだまだ情報少な いので苦労するだろうなぁと個人的には感じました。chef-solo + capistrano の構成 も考慮しつつ、このあたり調べて行こうと思います。\n","permalink":"https://jedipunkz.github.io/post/2012/07/22/number-devlove-nican-jia-sitekimasita.-chef-de-devops/","summary":"2012年07月21日に大崎のフィーチャーアーキテクトさんで行われた #DevLOVE (Chef De DevOps) に参加してきました。\n開催日 : 2012年07月21日(土曜日) 15:00 - 20:40 場所 : 大崎 フューチャーアーキテクトさま URL : http://www.zusaar.com/event/314003  仕事場でも Chef の利用を考え始めていて今回いい機会でした。半年前と比べるとだい ぶ揃って来ましたが、まだまだ資料の少ない Chef。貴重な機会でした。\nプログラムは下の通り。\n*『Chefの下準備』by Ryutaro YOSHIBA [ @ryuzee ] *『Chef自慢のレシピ披露』 by 中島弘貴さん [ @nakashii_ ] * ワークショップ『みんなでCooking』 * dialog『試食会』 * Niftyさんのスーパー宣伝タイム!!! *『渾身会』  @ryuzee さんの \u0026ldquo;Chef の下準備\u0026rdquo; は SlideShare に使った資料が公開されています。\n印象的だったのが、VirtualBox + Vagrant という環境。実際に使っていらっしゃいま した。MacBook の中に仮想環境とそのインターフェースである Vegrant を使って、デ プロイのテスト等が実施できるそうです。また、Vegrant 設定ファイルは Chef のレシ ピを自動読込して、常に本番環境と同じ状態にしているそうです。CloudFormation と いうキーワードや Capistrano というキーワードが出てきました。最近よく耳にするワー ドです。また CI は Jenkins だよね、だったり。","title":"#DevLOVE に参加してきました。Chef De DevOps"},{"content":"interop 2012 で \u0026lsquo;DNS の猛威とその対策\u0026rsquo; を傍聴してきました。簡単にレポート書い ておきます。ちょっと油断しただけで大きな問題のアップデートが出てくる DNS。怖い です..。\n本講義の概要 ブラジルで大規模な ISP への DNS ポイゾニング攻撃が発生。それら猛威について理解 すると同時に技術的対応策や具体的な対応プロセスについて説明。\nイントロダクション : インターネットエクスチェンジ 石田さん DNS を取り巻く状況\n DNS を乗っ取って悪事を働く試み : DNS Changer DNS そのものをセキュアにする方向性 : DNSSEC DNS に様々な制御を任せる方向性 : SPF, AAAA, DANE, 児童ポルノブロッキング  事例 : IIJ 松崎さん 2011/11 ブラジルの事例 著名サイトへのアクセスを行うと malware が仕込まれたサイトへ誘導。ホームルータ のキャッシュがポイゾニングされた。\nとあるホームルータの問題  admin パスワードが管理 web で見える wan からのアクセスが有効 同じチップセットを使っている製品で同様の問題.. wan からルータへアクセスすると html ソースにアカウント情報が平文で書かれてい た  これは怖い..。\n攻撃活動の実施 攻撃者が行う手順。\n 脆弱性な CPE 発見 パスワード書き換え CPE が参照する DNS の書き換え 著名サイト向け DNS への応答を書き換え malware サイトへ誘導 銀行の安全客員ツールを無効にする malware をインストールさせる 幾つかの銀行向け DNS 応答を書き換え、目的のフィッシングサイトに誘導。DNS 書 き換えは短い期間のみであった。  規模 2011年時点、450万の CPE の DNS が書き換えられていた。今年も 30万以上の CPE が 影響を受けたまま。\nその後 攻撃者が脆弱な CPE を探す試みはまだ続いている。ブラジル以外でも被害報告が。\nその他の事例 DNS Changer LAN 内の DHCP サーバに攻撃して配布する DNS を書き換えるとも言われている。 その後、FBI が参照用 DNS(攻撃者管理) を差し押さえ。いきなり止めると、ユーザが インターネットへの接続ができなくなるので、同じ IP アドレスでキャッシュサーバを 運用。まだ35万程度の感染ホストがあるので、キャッシュ DNS の運用を 2012/7/9 ま で延長決定。\n JPCERT が用意している、確認サイト  このサイトにアクセスすることで自環境が汚染されているかどうかが判ります。\nhttp://www.dns-ok.jpcert.or.jp/\n幽霊ドメイン名について : JPRS 坂口さん 近年注目されている猛威  プロトコルによる定義が明確でない部分を突いた攻撃 (幽霊ドメイン名) プロトコルの脆弱性をついた攻撃 (キャッシュポイゾニング)  傾向と対策  1980年代から利用されていた 性善説から生まれた 利用者用途が大きく変動 DNS の重要性は未だ変わっていない  DNS プロトコルを正しく理解が対策の第一歩!\n幽霊ドメイン名について  2012年2月8日 中国 Haixin Duan さんによる論文として発表 その前日にISC が緊急セキュリティアドバイザリを発表  幽霊ドメインが引き起こす問題  ドメインを管理していたものからそのドメインを引き剥がすことができる 強制的にドメイン名を使用不可にしても使われ続ける 強制的に移転させても使われ続ける  動作原理 キャッシュ -\u0026gt; Root -\u0026gt; JP DNS -\u0026gt; 権威 DNS  の場合、キャッシュ上の TTL が切れたレコードに対して権威 DNS に問い合わせするが、 TTL が切れていないレコードについても問い合わせる。その際に新しい情報が入ってき た場合上書きするか？破棄するか？は、実装次第。NS ホストを定期的に問い合わせす ることで TTL を巻き戻し、永遠にキャッシュを利用させる攻撃手法。悪意のあるサイ トのドメインを差し押さえても、使用され続ける可能性が出てくる。\n対策  幽霊ドメインが発生しない実装へ書き換え (bind9, unbound は実装済み) 幽霊ドメイン名のキャッシュ情報をクリアする (ISC が推奨)  Unbound, Bind9 で取られた対策 権威 DNS に問い合わせはするが、TTL については上書きしない\n問題の対策 : IIJ 山本さん 前提\n ブラジルの事例についてはキャッシュサーバで対策しても無益。  従来  従来のキャッシュ DNS への攻撃はキャッシュポイゾニング ISP のキャッシュ DNS サーバが標的 歴史的経緯でアクセスコントロールが緩い プロトコル/実装の脆弱性を突き、偽のレコードをキャッシュさせる 理論的には20年前から知られていたが、実際の攻撃が困難だった  近年  2008, カミンスキー攻撃が発表される キャッシュしたレコードは TTL が来るまで再度検索されないが、これを可能にした 理論的な猛威から現実的なそれへ  対策  問い合わせる際の Port 番号をランダム化 (queryID(16bit) x port (16bit)) 一部商用サーバでは攻撃を検知する機能がアリ (攻撃を検知したら問い合わせを tcp に切り替える) ISP では ingress-filtering を可能な限り実施する Open Recursive はやめる -\u0026gt; 攻撃の機会を大幅に低減できる DNSSEC の導入をすすめる -\u0026gt; 中長期的な対策  DNSSEC が短期的対策にならない理由  検証するキャッシュサーバがまだ少ない DNSSEC で署名しているドメイン名がまだほとんど無い 鶏と卵問題 DNSSEC そのものの複雑さ -\u0026gt; 理解しているエンジニアの数が少ない  DNS Changer への対策  CPE ベンダで対策するしか無い DNS サーバでできることは無い  ISP がやるとしたら\u0026hellip;\n OP25B ならぬ OP53B 対策を行うか？ DNS トラフィックを見張る (Google Public DNS など例外はあるが、自社キャッシュ 以外への問い合わせは誤差の範囲のはず)  DNS の課題 : IIJ 松崎さま  児童ポルノフィルタ : 問題のあるサイトのドメイン名を書き換える DNS64 : A へ書き換える -\u0026gt; DNSSEC との併用が難しいことが取り上げられている \u0026lt; RFC  \u0026lsquo;アクセス制御\u0026rsquo;・\u0026lsquo;書き換え\u0026rsquo;, これらは攻撃者が行うことと同じ!\n所感, まとめ 以上簡単でしたが、レポートでした。こういったイベントに1年出席しないだけで、知 らないことが出てくる DNS 界隈。毎年大きな脆弱性の出る Bind。使えて当たり前、障 害が起こると大問題になる DNS。管理者にとってつらい状況だけど、ユーザにとって無 くてはならないシステムなので、運用・開発に携わっている人間は、近年の状況をウォッ チし続けていく必要がある。この講義を傍聴しているなかで、自社の環境の組み換えを ぼんやり考えていました。\n","permalink":"https://jedipunkz.github.io/post/2012/06/16/dns-report-interop2012/","summary":"interop 2012 で \u0026lsquo;DNS の猛威とその対策\u0026rsquo; を傍聴してきました。簡単にレポート書い ておきます。ちょっと油断しただけで大きな問題のアップデートが出てくる DNS。怖い です..。\n本講義の概要 ブラジルで大規模な ISP への DNS ポイゾニング攻撃が発生。それら猛威について理解 すると同時に技術的対応策や具体的な対応プロセスについて説明。\nイントロダクション : インターネットエクスチェンジ 石田さん DNS を取り巻く状況\n DNS を乗っ取って悪事を働く試み : DNS Changer DNS そのものをセキュアにする方向性 : DNSSEC DNS に様々な制御を任せる方向性 : SPF, AAAA, DANE, 児童ポルノブロッキング  事例 : IIJ 松崎さん 2011/11 ブラジルの事例 著名サイトへのアクセスを行うと malware が仕込まれたサイトへ誘導。ホームルータ のキャッシュがポイゾニングされた。\nとあるホームルータの問題  admin パスワードが管理 web で見える wan からのアクセスが有効 同じチップセットを使っている製品で同様の問題.. wan からルータへアクセスすると html ソースにアカウント情報が平文で書かれてい た  これは怖い..。\n攻撃活動の実施 攻撃者が行う手順。\n 脆弱性な CPE 発見 パスワード書き換え CPE が参照する DNS の書き換え 著名サイト向け DNS への応答を書き換え malware サイトへ誘導 銀行の安全客員ツールを無効にする malware をインストールさせる 幾つかの銀行向け DNS 応答を書き換え、目的のフィッシングサイトに誘導。DNS 書 き換えは短い期間のみであった。  規模 2011年時点、450万の CPE の DNS が書き換えられていた。今年も 30万以上の CPE が 影響を受けたまま。","title":"DNS の猛威とその対策, 参加レポ #interop2012"},{"content":"interop 2012 で開催された \u0026ldquo;仮想ルータ Vyatta を使ったネットワーク構築法\u0026rdquo; に参 加してきました。簡単ですがレポートを書いておきます。\n開催日 : 2012/06/14(木) 場所 : 幕張メッセ Interop 2012  最初に所感。反省です。サーバエンジニアの視点でしか感じられていなかった。次期バー ジョンの vPlane 実装などエンタープライズ向けとしても利用出来る可能性を感じる し、比較的小さなリソースでハンズオン参加者30人程度の vyatta ルータを動かしてサ クサク動いているのを見て、簡単にパフォーマンスがどうとか 前回の記事で言うべ きじゃなかったぁ。ホント反省。\nではハンズオンの内容。\n最初に、基本情報の話 有償版と無償版の違い\n メジャーリリースのみの無償版に対して有償版はマイナーリリースもあり 有償版は保守あり 仮想 image template 機能が有償版であり API, Web GUI, Config Sync, Systen Image Cloning が有償版であり  image template, config sync, cloning など、有償版ではあると嬉しい機能がモリモ リ。\n最近の利用ケース  Vyatta + VM で VPN 接続環境構築 キャンパスネットワーク  キャンパスネットワークのユースケースが一番多いそうだ。また、会場内にアンケート をとった結果、仮想環境での構築を想定されている方が多数だった。\n構成と特徴  Debian Gnu/Linux ベース Quagga, StrongSwam が内部で動作  apt-get など馴染み深いコマンドが使えます。 http://www.vyatta4people.org/tag/vybuddy/ ここに色々ノウハウがあるらしい。僕はまだ見ていません。まほろば工房の近藤さんに 教えて頂きました。あとでチェックします。\nバージョン stable リリースは 6.4 。次期バージョン 6.5 では vPlane の実装が予定されている。これ は IA サーバのコア毎に役割を変えるといったモノ。ノードが忙しくなった時に例えば 転送を司るコアには影響を与えない等。これは重要だ。サーバと違ってネットワーク機 器は何があっても死守しなくちゃいけない機能があるもんなぁ。6.5 に期待。\nここで、ハンズオンで課題になった IPSec の話を少しだけ説明します。\n192.168.18.0/24 +------+ +----------+ | vm01 |-------| vyatta01 |pppoe XXX.XXX.XXX.XXX +------+ +----------+ | |192.168.20.0/24 | IPSec +------+ +----------+ | | vm02 |-------| vyatta02 |pppoe YYY.YYY.YYY.YYY +------+ +----------+ 192.168.19.0/24  上図の環境で vm01 が vm02 に IPSec を用いて接続するための方法を記しています。 vyatta 同士は 192.168.20.0/24 のネットワークで接続されていますが、互いにルーティ ングテーブルは書いていないものとします。まずは vyatta01 に対しての設定。\nあ、pppoe による NAT の設定は予めしてあるものとします。\n# set vpn ipsec ipsec-interfaces interface pppoe1 # set vpn ipsec ike-group IKE-G proposal 1 encryption aes256 # set vpn ipsec ike-group IKE-G proposal 1 hash sha1 # set vpn ipsec ike-group IKE-G lifetime 3600  鍵交換の方式 IKE の設定をします。\n# set vpn ipsec esp-group ESP-G proposal 1 encryption aes256 # set vpn ipsec esp-group ESP-G proposal 1 hash sha1 # set vpn ipsec esp-group ESP-G lifetime 1800  ESP の設定をします。鍵の破棄・生成時間 1800 を設定しています。\n# edit vpn ipsec site-to-site peer YYY.YYY.YYY.YYY # set authentication mode pre-shared-secret # set authentication pre-shared-secret hogehoge  site-to-site peer を指定して接続先 vyatta のグローバル IP アドレスを指定します。 また pre-shared-secret でパスフレーズを指定します。\n# set ike-group IKE-G # set local-ip YYY.YYY.YYY.YYY # set tunnel 1 local subnet 192.168.18.0/24 # set tunnel 1 remote subnet 192.168.19.0/24 # set tunnel 1 esp-group ESP-G  自グローバル IP, 自プライベートネットワーク、リモートネットワークの情報諸々、 設定します。\n# set nat source rule 10 destination address !192.168.19.0/24 # commit  また最後に IPsec で接続する先のネットワークへのパケットの場合、NAT しないよう にします。\n次は逆に vyatta01 に対しても設定を投入します。諸々の値は逆にする必要があります。 これを実施すると、vm01 から vm02 に対して pppoe interface 同士の IPsec を介し て接続できるのが確認出来ました。\n私の職場で開発した製品にも投入してみようかなと目論んでいます。楽しかった。\n最後に、この場を提供して下さった方々に感謝します。\n講演者  株式会社まほろば工房 近藤邦昭さま 有限会社銀座堂 浅間正和さま さくらインターネット 大久保修一さま 伊藤忠テクノソリューションズ 伊藤哲史さま  ","permalink":"https://jedipunkz.github.io/post/2012/06/14/vyatta-handson-interop2012/","summary":"interop 2012 で開催された \u0026ldquo;仮想ルータ Vyatta を使ったネットワーク構築法\u0026rdquo; に参 加してきました。簡単ですがレポートを書いておきます。\n開催日 : 2012/06/14(木) 場所 : 幕張メッセ Interop 2012  最初に所感。反省です。サーバエンジニアの視点でしか感じられていなかった。次期バー ジョンの vPlane 実装などエンタープライズ向けとしても利用出来る可能性を感じる し、比較的小さなリソースでハンズオン参加者30人程度の vyatta ルータを動かしてサ クサク動いているのを見て、簡単にパフォーマンスがどうとか 前回の記事で言うべ きじゃなかったぁ。ホント反省。\nではハンズオンの内容。\n最初に、基本情報の話 有償版と無償版の違い\n メジャーリリースのみの無償版に対して有償版はマイナーリリースもあり 有償版は保守あり 仮想 image template 機能が有償版であり API, Web GUI, Config Sync, Systen Image Cloning が有償版であり  image template, config sync, cloning など、有償版ではあると嬉しい機能がモリモ リ。\n最近の利用ケース  Vyatta + VM で VPN 接続環境構築 キャンパスネットワーク  キャンパスネットワークのユースケースが一番多いそうだ。また、会場内にアンケート をとった結果、仮想環境での構築を想定されている方が多数だった。\n構成と特徴  Debian Gnu/Linux ベース Quagga, StrongSwam が内部で動作  apt-get など馴染み深いコマンドが使えます。 http://www.","title":"Vyatta ハンズオン参加レポ #interop2012"},{"content":"Vyatta で VPN しようと思ったら信じられないくらい簡単に構築できたので共有します。\n今回は PPTP (Point-to-Point Tunneling Protocol) を用いました。\nさっそく手順に。\n$ configure # set vpn pptp remote-access authentication local-users username ${USER} password ${PASSWORD} # set vpn pptp remote-access authentication mode local # set vpn pptp remote-access client-ip-pool start ${IP_START} # set vpn pptp remote-access client-ip-pool stop ${IP_END} # set vpn pptp remote-access outside-address ${GLOBAL_IP} # set vpn pptp remote-access dns-servers server-1 8.8.8.8 # set vpn pptp remote-access dns-servers server-1 8.8.4.4 # commit # save  これだけです。\n認証のためのユーザを1行目で作っています。client-ip-pool で VPN 接続端末に付与 する IP アドレスのレンジを指定して outside-address で Listen Port を指定します。 DNS リゾルバに指定させたいアドレスを dns-servers で指定したら終わりです。\n私は手元の iPhone で自宅に VPN 接続して使っています。これで iPhone か 自宅内の 機器に直接アクセスできるので便利です。\nあと、Vyatta をしばらく自宅で使ってみての所感です。\nVyatta は汎用機にインストールできるし VM としても動作させられるし便利です。が、 実際に起動しているプロセス等を見ていくと、Linux 界では古くからあるレガシなソフ トウェアが起動しているだけに見えます。Vyatta の UI はこれらソフトウェアのコン フィグレーションを簡易化するラッパー的なモノになっているのが分かります。\n汎用性がある一方、パフォーマンスはあまり期待出来ないかもしれません。実際にリッ チなコンテンツを閲覧した時のパフォーマンスは Buffalo 製コンシューマ向け機器に 劣ります。Linux なのでチューニングは出来ますが、予め幾つかのチューニングは入っ ていますし、あくまでもチューニングなので劇的なパフォーマンス改善には繋がりませ ん。\nFreeBSD 系 ? で pfsense という \u0026lsquo;open source firewall distribution\u0026rsquo; もあるよう なので、いずれ試してみたいです。\n","permalink":"https://jedipunkz.github.io/post/2012/06/13/vyatta-vpn/","summary":"Vyatta で VPN しようと思ったら信じられないくらい簡単に構築できたので共有します。\n今回は PPTP (Point-to-Point Tunneling Protocol) を用いました。\nさっそく手順に。\n$ configure # set vpn pptp remote-access authentication local-users username ${USER} password ${PASSWORD} # set vpn pptp remote-access authentication mode local # set vpn pptp remote-access client-ip-pool start ${IP_START} # set vpn pptp remote-access client-ip-pool stop ${IP_END} # set vpn pptp remote-access outside-address ${GLOBAL_IP} # set vpn pptp remote-access dns-servers server-1 8.8.8.8 # set vpn pptp remote-access dns-servers server-1 8.","title":"Vyatta で構築する簡単 VPN サーバ"},{"content":"普段は Mac, Linux がメインなのですが、NetBSD もたまにデスクトップ機として利用 するので、初期設定手順をまとめ。\nConsole 設定, キーリマップ Caps と Ctrl キーを入れ替えます。お約束。\n# wsconsctl -w encoding=us.swapctrlcaps # vi /etc/wscons.conf # 同様の設定を入れる  キーリピートを速くします。標準では遅すぎる。値は好みで。\n# wsconsctl -w repeat.del1=300 # リピートが始まるまでの時間 # wsconsctl -w repeat.deln=40 # 反復間隔 # vi /etc/wscons.conf # 同様の値を入れる  pkgsrc をインストール xx, y は次期に応じて変える。例 :2012Q1\n# cd /usr # cvs -q -z3 -d anoncvs@anoncvs.NetBSD.org:/cvsroot checkout -r pkgsrc-20xxQy -P pkgsrc  最新の状態に更新する場合。リリースタグを利用したいなら不要。\n# cd /usr/pkgsrc # cvs update -dP  grub のインストール grub をインストール。デフォルトでも良いのだけど、安心したいから。他の OS とデュ アル・トリプルブートに設定することが多いので必要を感じて入れている場合もある。 もちろん Linux 側からインストールしても OK。\n# cd /usr/pkgsrc/sysutils/grub # make install # /usr/pkg/sbin/grub-install '(hd0)' # vi /grub/menu.lst # 適当に設定  X の設定 # X -configure # cp /root/Xorg.conf.new /etc/X11/xorg.conf  ウィンドウマネージャは普段は twm を利用しているので、何も入れない。\n日本語フォントとして efont を使う。スグレモノフォント。\n# cd /usr/pkgsrc/fonts/efont-unicode # make install  xorg.conf に FontPath を通す。\nFontPath \u0026quot;/usr/pkg/lib/XZ11/fonts/efont\u0026quot;  urxvt と zsh ターミナルエミュレータとして rxvt を利用。Unicode 対応な urxvt をインストール する。zsh はマルチバイト対応な zsh-current をインストール。\n# cd /usr/pkgsrc/shells/zsh-curren # make install # cd /usr/pkgsrc/x11/rxvt-unicode # make install  日本語入力設定 日本語のインプットメソッド uim, anthy をインストール。\n# cd /usr/pkgsrc/inputmethod/uim # make install  環境変数を設定。.zshrc でも .xinitrc でも可。\nexport LANG=ja_JP.UTF-8 export LC_ALL=ja_JP.UTF-8 export LC_CTYPE=ja_JP.UTF-8 export XMODIFIERS=@im=uim export GTK_IM_MODULE=uim exec uim-xim \u0026amp; exec uim-toolbar-gtk * exec /usr/X11R7/bin/twm  所感 昔は FreeBSD を使っていたのだけど、Mac, Linux, NetBSD だけになってしまった。 NetBSD は実装が綺麗なのでスーッと入っていけるので好き。ただ、source からのビル ドは時間が掛かるので、git や chef, puppet 等の仕組みを利用してビルドする方法を 確立したいなぁと最近思ってる。\n","permalink":"https://jedipunkz.github.io/post/2012/05/12/netbsd-first-setup/","summary":"普段は Mac, Linux がメインなのですが、NetBSD もたまにデスクトップ機として利用 するので、初期設定手順をまとめ。\nConsole 設定, キーリマップ Caps と Ctrl キーを入れ替えます。お約束。\n# wsconsctl -w encoding=us.swapctrlcaps # vi /etc/wscons.conf # 同様の設定を入れる  キーリピートを速くします。標準では遅すぎる。値は好みで。\n# wsconsctl -w repeat.del1=300 # リピートが始まるまでの時間 # wsconsctl -w repeat.deln=40 # 反復間隔 # vi /etc/wscons.conf # 同様の値を入れる  pkgsrc をインストール xx, y は次期に応じて変える。例 :2012Q1\n# cd /usr # cvs -q -z3 -d anoncvs@anoncvs.NetBSD.org:/cvsroot checkout -r pkgsrc-20xxQy -P pkgsrc  最新の状態に更新する場合。リリースタグを利用したいなら不要。\n# cd /usr/pkgsrc # cvs update -dP  grub のインストール grub をインストール。デフォルトでも良いのだけど、安心したいから。他の OS とデュ アル・トリプルブートに設定することが多いので必要を感じて入れている場合もある。 もちろん Linux 側からインストールしても OK。","title":"NetBSD インストール直後の初期設定まとめ"},{"content":"Hacker News で取り上げられていた emacs-nav を使ってみた。\nhttp://code.google.com/p/emacs-nav/\nインストール方法は簡単で\n% wget http://emacs-nav.googlecode.com/files/emacs-nav-20110220a.tar.gz % tar zxvf emacs-nav-20110220a.tar.gz % mv emacs-nav-20110220a ~/.emacs.d/emacs-nav  して\n;; emacs-nav (add-to-list 'load-path \u0026quot;~/.emacs.d/emacs-nav/\u0026quot;) (require 'nav)  するだけ。\n見た目はこんな感じ。\n起動は M-x nav と入力。ウィンドウの左にファイルブラウザが開いてファイルを選択 出来る。これだけだと、使うメリットを感じないが、面白いのがマウスで選択出来ると ころ。TextMate のブラウザのような感じだ。\n今だと anything.el が便利すぎて、こちらを利用する価値を見出せるか分からないけ ど、暫く使ってみようと思う。\n","permalink":"https://jedipunkz.github.io/post/2012/05/04/emacs-nav/","summary":"Hacker News で取り上げられていた emacs-nav を使ってみた。\nhttp://code.google.com/p/emacs-nav/\nインストール方法は簡単で\n% wget http://emacs-nav.googlecode.com/files/emacs-nav-20110220a.tar.gz % tar zxvf emacs-nav-20110220a.tar.gz % mv emacs-nav-20110220a ~/.emacs.d/emacs-nav  して\n;; emacs-nav (add-to-list 'load-path \u0026quot;~/.emacs.d/emacs-nav/\u0026quot;) (require 'nav)  するだけ。\n見た目はこんな感じ。\n起動は M-x nav と入力。ウィンドウの左にファイルブラウザが開いてファイルを選択 出来る。これだけだと、使うメリットを感じないが、面白いのがマウスで選択出来ると ころ。TextMate のブラウザのような感じだ。\n今だと anything.el が便利すぎて、こちらを利用する価値を見出せるか分からないけ ど、暫く使ってみようと思う。","title":"Emacs でファイルブラウザ emacs-nav を利用"},{"content":"http://www.emacswiki.org/emacs-en/PowerLine\nvim の powerline に似たモードを emacs で実現できる powerline.el を試してみた。 見た目が派手でかわいくなるので、気に入った。w すでに下記のようなサイトで紹介さ れつつある。\nhttp://d.hatena.ne.jp/kenjiskywalker/20120502/1335922233 http://n8.hatenablog.com/entry/2012/03/21/172928\nインストールすると見た目はこんな感じになる。\nうん、かわいい。\nインストール方法は簡単。auto-install 環境が構築されているとして\nauto-install-from-emacs-wiki powerline.el  する。auto-install が入っていなければ http://www.emacswiki.org/emacs/powerline.el ここにある powerline.el をダウンロードして ~/.emacs.d/lisp/ 配下など path の通っ ている所に入れれば OK。\nあとは .emacs 内には下記を追記するだけだ。\n;; powerline.el (defun arrow-right-xpm (color1 color2) \u0026quot;Return an XPM right arrow string representing.\u0026quot; (format \u0026quot;/* XPM */ static char * arrow_right[] = { \\\u0026quot;12 18 2 1\\\u0026quot;, \\\u0026quot;. c %s\\\u0026quot;, \\\u0026quot; c %s\\\u0026quot;, \\\u0026quot;. \\\u0026quot;, \\\u0026quot;.. \\\u0026quot;, \\\u0026quot;... \\\u0026quot;, \\\u0026quot;.... \\\u0026quot;, \\\u0026quot;..... \\\u0026quot;, \\\u0026quot;...... \\\u0026quot;, \\\u0026quot;....... \\\u0026quot;, \\\u0026quot;........ \\\u0026quot;, \\\u0026quot;......... \\\u0026quot;, \\\u0026quot;......... \\\u0026quot;, \\\u0026quot;........ \\\u0026quot;, \\\u0026quot;....... \\\u0026quot;, \\\u0026quot;...... \\\u0026quot;, \\\u0026quot;..... \\\u0026quot;, \\\u0026quot;.... \\\u0026quot;, \\\u0026quot;... \\\u0026quot;, \\\u0026quot;.. \\\u0026quot;, \\\u0026quot;. \\\u0026quot;};\u0026quot; color1 color2)) (defun arrow-left-xpm (color1 color2) \u0026quot;Return an XPM right arrow string representing.\u0026quot; (format \u0026quot;/* XPM */ static char * arrow_right[] = { \\\u0026quot;12 18 2 1\\\u0026quot;, \\\u0026quot;. c %s\\\u0026quot;, \\\u0026quot; c %s\\\u0026quot;, \\\u0026quot; .\\\u0026quot;, \\\u0026quot; ..\\\u0026quot;, \\\u0026quot; ...\\\u0026quot;, \\\u0026quot; ....\\\u0026quot;, \\\u0026quot; .....\\\u0026quot;, \\\u0026quot; ......\\\u0026quot;, \\\u0026quot; .......\\\u0026quot;, \\\u0026quot; ........\\\u0026quot;, \\\u0026quot; .........\\\u0026quot;, \\\u0026quot; .........\\\u0026quot;, \\\u0026quot; ........\\\u0026quot;, \\\u0026quot; .......\\\u0026quot;, \\\u0026quot; ......\\\u0026quot;, \\\u0026quot; .....\\\u0026quot;, \\\u0026quot; ....\\\u0026quot;, \\\u0026quot; ...\\\u0026quot;, \\\u0026quot; ..\\\u0026quot;, \\\u0026quot; .\\\u0026quot;};\u0026quot; color2 color1)) (defconst color1 \u0026quot;#FF6699\u0026quot;) (defconst color3 \u0026quot;#CDC0B0\u0026quot;) (defconst color2 \u0026quot;#FF0066\u0026quot;) (defconst color4 \u0026quot;#CDC0B0\u0026quot;) (defvar arrow-right-1 (create-image (arrow-right-xpm color1 color2) 'xpm t :ascent 'center)) (defvar arrow-right-2 (create-image (arrow-right-xpm color2 \u0026quot;None\u0026quot;) 'xpm t :ascent 'center)) (defvar arrow-left-1 (create-image (arrow-left-xpm color2 color1) 'xpm t :ascent 'center)) (defvar arrow-left-2 (create-image (arrow-left-xpm \u0026quot;None\u0026quot; color2) 'xpm t :ascent 'center)) (setq-default mode-line-format (list '(:eval (concat (propertize \u0026quot; %b \u0026quot; 'face 'mode-line-color-1) (propertize \u0026quot; \u0026quot; 'display arrow-right-1))) '(:eval (concat (propertize \u0026quot; %m \u0026quot; 'face 'mode-line-color-2) (propertize \u0026quot; \u0026quot; 'display arrow-right-2))) ;; Justify right by filling with spaces to right fringe - 16 ;; (16 should be computed rahter than hardcoded) '(:eval (propertize \u0026quot; \u0026quot; 'display '((space :align-to (- right-fringe 17))))) '(:eval (concat (propertize \u0026quot; \u0026quot; 'display arrow-left-2) (propertize \u0026quot; %p \u0026quot; 'face 'mode-line-color-2))) '(:eval (concat (propertize \u0026quot; \u0026quot; 'display arrow-left-1) (propertize \u0026quot;%4l:%2c \u0026quot; 'face 'mode-line-color-1))) )) (make-face 'mode-line-color-1) (set-face-attribute 'mode-line-color-1 nil :foreground \u0026quot;#fff\u0026quot; :background color1) (make-face 'mode-line-color-2) (set-face-attribute 'mode-line-color-2 nil :foreground \u0026quot;#fff\u0026quot; :background color2) (set-face-attribute 'mode-line nil :foreground \u0026quot;#fff\u0026quot; :background color3 :box nil) (set-face-attribute 'mode-line-inactive nil :foreground \u0026quot;#fff\u0026quot; :background color4)  色を少し変えただけで、ほぼ emacswiki に載っているコードそのままだ。no window だと寂しいことになるが、まぁ仕方ない。vim でも powerline を使って気に入ってい たので emacs でも使えるとあって嬉しい限りだ。これから少しずつカスタマイズして いこうと思う。\n","permalink":"https://jedipunkz.github.io/post/2012/05/04/powerline.el-emacs/","summary":"http://www.emacswiki.org/emacs-en/PowerLine\nvim の powerline に似たモードを emacs で実現できる powerline.el を試してみた。 見た目が派手でかわいくなるので、気に入った。w すでに下記のようなサイトで紹介さ れつつある。\nhttp://d.hatena.ne.jp/kenjiskywalker/20120502/1335922233 http://n8.hatenablog.com/entry/2012/03/21/172928\nインストールすると見た目はこんな感じになる。\nうん、かわいい。\nインストール方法は簡単。auto-install 環境が構築されているとして\nauto-install-from-emacs-wiki powerline.el  する。auto-install が入っていなければ http://www.emacswiki.org/emacs/powerline.el ここにある powerline.el をダウンロードして ~/.emacs.d/lisp/ 配下など path の通っ ている所に入れれば OK。\nあとは .emacs 内には下記を追記するだけだ。\n;; powerline.el (defun arrow-right-xpm (color1 color2) \u0026quot;Return an XPM right arrow string representing.\u0026quot; (format \u0026quot;/* XPM */ static char * arrow_right[] = { \\\u0026quot;12 18 2 1\\\u0026quot;, \\\u0026quot;. c %s\\\u0026quot;, \\\u0026quot; c %s\\\u0026quot;, \\\u0026quot;. \\\u0026quot;, \\\u0026quot;.","title":"powerline.el で emacs モードラインを派手に"},{"content":"自宅ルータを Vyatta で運用開始したのだけど、無線ルータ化ができたのでメモしておき ます。\nまずはアクセスポイントとして稼働する無線カードの選定。下記の URL に Linux 系で 動作する無線カードチップ名の一覧が載っている。Vyatta は Linux 系ドライバを利用 しているので、この一覧が有効なはずだ。\nhttp://linuxwireless.org/en/users/Drivers\nIntel 系も結構アクセスポイントとしては動作しないことが判ったので Atheros の AR9k のカードを購入した。私が買ったのはAR9280 チップの mini PCI-E 無線カード。\n早速装着してみると、認識した！\n% dmesg | grep ath [ 11.528390] ath9k 0000:01:00.0: PCI INT A -\u0026gt; GSI 16 (level, low) -\u0026gt; IRQ 16 [ 11.528398] ath9k 0000:01:00.0: setting latency timer to 64 [ 11.961619] ath: EEPROM regdomain: 0x37 [ 11.961620] ath: EEPROM indicates we should expect a direct regpair map [ 11.961622] ath: Country alpha2 being used: AW [ 11.961623] ath: Regpair used: 0x37 [ 12.031676] ieee80211 phy0: Selected rate control algorithm 'ath9k_rate_control'  早速設定に入る。有線側有線 NIC と無線 NIC をブリッジ接続する。br0 デバイスにの み IP アドレスを振り、eth1, wlan0 は IP アドレスを振らない構成にする。\nまずはブリッジインターフェースを作る。\nset interfaces bridge br0 address ${IP_ADDRESS} set interfaces bridge br0 description LOCAL_NET  eth1 (ローカル側有線 NIC) をブリッジ br0 に含める。\nset interfaces ethernet eth1 bridge-group bridge br0 set interfaces ethernet eth1 description LOCAL_NET  wlan0 の設定。同じく br0 に含めて、諸々の設定を投入。\nset interfaces wireless wlan0 bridge-group bridge br0 set interfaces wireless wlan0 channel 2 set interfaces wireless wlan0 description LOCAL_NET set interfaces wireless wlan0 mode n set interfaces wireless wlan0 security wpa mode wpa2 set interfaces wireless wlan0 security wpa passphrase ${PASS} set interfaces wireless wlan0 ssid ${SSID} set interfaces wireless wlan0 type access-point commit save  ${IP_ADDRESS}, ${SSID}, ${PASS} は任意です。読み替えてください。\nここでコツ。これらの設定をして commit してもうまく無線接続出来なかった。vyatta を再起動させる (予め save すること) と、無線接続ができた。\n購入した無線モジュールは mini PCE-I Atheros　AR5BXB92 (AR9280 チップ)、 PCI-E に変換するカード 、あとバッファローのアンテナだ。\n今のところ利用できているが、無線の強度が少し弱い気がする。これから少し調べてみ ます。\n","permalink":"https://jedipunkz.github.io/post/2012/05/04/vyatta-wireless-ap/","summary":"自宅ルータを Vyatta で運用開始したのだけど、無線ルータ化ができたのでメモしておき ます。\nまずはアクセスポイントとして稼働する無線カードの選定。下記の URL に Linux 系で 動作する無線カードチップ名の一覧が載っている。Vyatta は Linux 系ドライバを利用 しているので、この一覧が有効なはずだ。\nhttp://linuxwireless.org/en/users/Drivers\nIntel 系も結構アクセスポイントとしては動作しないことが判ったので Atheros の AR9k のカードを購入した。私が買ったのはAR9280 チップの mini PCI-E 無線カード。\n早速装着してみると、認識した！\n% dmesg | grep ath [ 11.528390] ath9k 0000:01:00.0: PCI INT A -\u0026gt; GSI 16 (level, low) -\u0026gt; IRQ 16 [ 11.528398] ath9k 0000:01:00.0: setting latency timer to 64 [ 11.961619] ath: EEPROM regdomain: 0x37 [ 11.961620] ath: EEPROM indicates we should expect a direct regpair map [ 11.","title":"Vyatta で無線アクセスポイント"},{"content":"Vyatta を自宅ルータで使い始めて感じたのは、PS3 などのゲーム機や IP 電話など UPnP 接続が必要なことがあるってこと。ただ Vyatta は UPnP に対応していないので、 どうしようかと思っていたら、有志の方が作ってくれたソフトウェアがあり、うちでも これを使うことにした。今回はその方法を記していきます。\nhttps://github.com/kiall/vyatta-upnp\n上記のソースを取得して生成するのだが、vyatta 上で構築する環境を作りたくないの で、私は Debian Gnu/Linux マシン上で行いました。Ubuntu でも大丈夫だと思います。\ndebian% sudo apt-get \u0026amp;\u0026amp; sudo apt-get install build-essential debian% git clone https://github.com/kiall/vyatta-upnp.git debian% cd vyatta-upnp debian% dpkg-buildpackage -us -uc -d  一つ上のディレクトリに vyatta-upnp_0.2_all.deb という .deb ファイルができあがっ ているはずで、これが UPnP パッケージファイル vyatta-upnp_0.2_all.deb です。\n次に vyatta 上での作業。packages.vyatta.com から libupnp4 と linux-igd を取得、 その後先ほど生成した vyatta-upnp_0.2_all.deb を vyatta 上に持ってきてからイン ストールします。\nvyatta# cd /tmp/ vyatta# wget http://packages.vyatta.com/debian/pool/main/libu/libupnp4/libupnp4_1.8.0~svn20100507-1_amd64.deb vyatta# wget http://packages.vyatta.com/debian/pool/main/l/linux-igd/linux-igd_1.0+cvs20070630-3_amd64.deb vyatta# scp ${DEBIAN}:/${SOMEWHERE}/vyatta-upnp_0.2_all.deb . # 先ほど生成したファイル vyatta# dpkg -i libupnp4_1.8.0~svn20100507-1_amd64.deb linux-igd_1.0+cvs20070630-3_amd64.deb vyatta# dpkg -i vyatta-upnp_0.2_all.deb  これで設定が可能になりました。設定してみます。\nvyatta# configure vyatta# set service upnp listen-on eth1 vyatta# commit vyatta# save  これで完了です。eth1 は 自宅環境に合わせて下さい。ルータのプライベート側インター フェース名です。私の家は wlan0 と eth1 をブリッジしているので listen-on br0 に しました。\n私の環境では PS3 の \u0026ldquo;アンチャーテッド3\u0026rdquo; で動作確認しています。\n","permalink":"https://jedipunkz.github.io/post/2012/04/29/vyatta-upnp/","summary":"Vyatta を自宅ルータで使い始めて感じたのは、PS3 などのゲーム機や IP 電話など UPnP 接続が必要なことがあるってこと。ただ Vyatta は UPnP に対応していないので、 どうしようかと思っていたら、有志の方が作ってくれたソフトウェアがあり、うちでも これを使うことにした。今回はその方法を記していきます。\nhttps://github.com/kiall/vyatta-upnp\n上記のソースを取得して生成するのだが、vyatta 上で構築する環境を作りたくないの で、私は Debian Gnu/Linux マシン上で行いました。Ubuntu でも大丈夫だと思います。\ndebian% sudo apt-get \u0026amp;\u0026amp; sudo apt-get install build-essential debian% git clone https://github.com/kiall/vyatta-upnp.git debian% cd vyatta-upnp debian% dpkg-buildpackage -us -uc -d  一つ上のディレクトリに vyatta-upnp_0.2_all.deb という .deb ファイルができあがっ ているはずで、これが UPnP パッケージファイル vyatta-upnp_0.2_all.deb です。\n次に vyatta 上での作業。packages.vyatta.com から libupnp4 と linux-igd を取得、 その後先ほど生成した vyatta-upnp_0.2_all.deb を vyatta 上に持ってきてからイン ストールします。\nvyatta# cd /tmp/ vyatta# wget http://packages.","title":"vyatta で UPnP 接続"},{"content":"自宅ルータを Vyatta で構築してみたくなり、秋葉原の ark でマシンを調達しました。 Broadcom の BCM57780 チップが搭載された NIC がマザーボード J\u0026amp;W MINIX™ H61M-USB3 だったのですが、Vyatta.org によると Broadcom の NIC が Certificated Hardware に載っていなくて心配でした。まぁ定評のある NIC メーカだから動くだろう と楽観視していたのですけど、案の定動きました。vyatta.org の Certificated Hardware にコミットしたら \u0026ldquo;user tested\u0026rdquo; として掲載してもらえました。\nhttp://www.vyatta.org/hardware/interfaces\nこんな感じに見えています。\n# dmesg | grep Broadcom [ 3.284646] tg3 0000:03:00.0: eth0: attached PHY driver [Broadcom BCM57780](mii_bus:phy_addr=300:01) [ 3.524122] tg3 0000:05:00.0: eth1: attached PHY driver [Broadcom BCM57780](mii_bus:phy_addr=500:01)  今回は、基本的な設定 (PPPoE, NAT, DHCP) 周りを記していきます。\n環境は\u0026hellip;\n+--------+ | Modem | +--------+ | | pppoe0 +--eth0--+ | vyatta | +--eth1--+ | 192.168.1.0/24 +----------+ | |192.168.1.10 +--------+ +--------+ | CPE | | DNS | +--------+ +--------+  として記します。\nまずインストール。http://www.vyatta.org/downloads から 64bit VC6.3 Live CD iso をダウンロードしてきます。(2012/04/28現在最新). インストール対象のマシンに挿入 して CDROM ブートすると Vyatta が立ち上がるので、ユーザ vyatta, パスワード vyatta でログインし\n# install-system  します。インタラクティブに問い合わせられるので答えていってください。インストールが 終わったらマシンを再起動します。\neth1 にプライベート IP アドレスを振ります。IP アドレスは適当に読み替えてくださ い。また基本的な設定も行います。\n# set service ssh port 10022 # firewall 設定するまではこうしたほうが安心です # set system host-name ${HOSTNAME} # set system time-zone Asia/Tokyo # set system name-server 8.8.8.8 # set interface ethernet eth1 address 192.168.1.254  eth0 を PPPoE デバイスとして利用して PPPoE 接続を実際にします。\n# set interface ethernet eth0 pppoe 0 # set interface ethernet eth0 pppoe 0 user-id ${PPPoE_username} # set interface ethernet eth0 pppoe 0 password ${PPPoE_password} # set interface ethernet eth0 pppoe 0 name-server auto # set interface ethernet eth0 pppoe 0 defaultroute auto # set interface ethernet eth0 pppoe 0 local-address XXX.XXX.XXX.XXX # 固定IPがある場合 # commit  ここまでで vyatta ノードからインターネットに接続出来るようになります。\n次に、ローカルネットワーク上の CPE だったりサーバノードからのインターネット接 続のために NAT 設定をします。\n# set service nat rule 1 # set service nat rule 1 type masquerade # set service nat rule 1 source address 192.168.1.0/24 # set service nat rule 1 outbound-interface pppoe0 # commit  CPE からのインターネットへの接続が出来るようになりました。\nあとは自宅ルータなので DHCP サービスがあったほうが便利だよね、とうことで。\n# set service dhcp-server shared-network-name HOME subnet 198.168.1.0/24 start 192.168.1.100 stop 192.168.1.150 # set service dhcp-server shared-network-name HOME subnet 192.168.1.0/24 default-router 192.168.1.254 # set service dhcp-server shared-network-name HOME subnet 198.168.1.0/24 dns-server 8.8.8.8 # set service dhcp-server shared-network-name HOME subnet 198.168.1.0/24 dns-server 8.8.4.4 # commit  ローカルネットワーク上の CPE から DHCP リクエストを出してみてください。IP が取得できると思います。\nここまでで基本的な有線自宅ルータとしての構築はほぼ完了ですが、最後にローカルネットワーク上のサーバ を DNS サーバとして稼働させるための NAT 設定方法を記しておきます。\n# set service nat rule 2 # set service nat rule 2 destination # set service nat rule 2 type destination # set service nat rule 2 type destination # set service nat rule 2 inbound-interface pppoe0 # set service nat rule 2 protocol udp # set service nat rule 2 destination port 53 # set service nat rule 2 inside-address address 192.168.1.10 # commit  DNS は稀に TCP にフォールバックするので同様に TCP ようの NAT ルールも追記します。\n# set service nat rule 3 # set service nat rule 3 destination # set service nat rule 3 type destination # set service nat rule 3 type destination # set service nat rule 3 inbound-interface pppoe0 # set service nat rule 3 protocol tcp # set service nat rule 3 destination port 53 # set service nat rule 3 inside-address address 192.168.1.10  set service nat rule 3 destinationset service nat rule 3 destination # commit\n最後に設定を保存するために\u0026hellip;\n# save  して終わりです。次回は UPnP な設定方法を書いて行きたいと思います。\nちなみに、私の自宅ルータはこんなモノを使いました。\n","permalink":"https://jedipunkz.github.io/post/2012/04/28/vyattarouter/","summary":"自宅ルータを Vyatta で構築してみたくなり、秋葉原の ark でマシンを調達しました。 Broadcom の BCM57780 チップが搭載された NIC がマザーボード J\u0026amp;W MINIX™ H61M-USB3 だったのですが、Vyatta.org によると Broadcom の NIC が Certificated Hardware に載っていなくて心配でした。まぁ定評のある NIC メーカだから動くだろう と楽観視していたのですけど、案の定動きました。vyatta.org の Certificated Hardware にコミットしたら \u0026ldquo;user tested\u0026rdquo; として掲載してもらえました。\nhttp://www.vyatta.org/hardware/interfaces\nこんな感じに見えています。\n# dmesg | grep Broadcom [ 3.284646] tg3 0000:03:00.0: eth0: attached PHY driver [Broadcom BCM57780](mii_bus:phy_addr=300:01) [ 3.524122] tg3 0000:05:00.0: eth1: attached PHY driver [Broadcom BCM57780](mii_bus:phy_addr=500:01)  今回は、基本的な設定 (PPPoE, NAT, DHCP) 周りを記していきます。\n環境は\u0026hellip;\n+--------+ | Modem | +--------+ | | pppoe0 +--eth0--+ | vyatta | +--eth1--+ | 192.","title":"vyatta で自宅ルータ構築"},{"content":"今週/先週？、Hacker News で取り上げられた Mosh を自宅 と会社で使い始めた。SSH 代替なソフトウェアで、SSP (State Synchronization Protocol)over UDP で動作している。MIT が開発したそうだ。\n動作は、クライアントがシーケンス番号と共にデータグラムをサーバに送信し、同期し 続ける。クライアントがローミングし IP アドレスが代わる等した時、以前より大きい シーケンス番号と共に正当なパケットが送信されたとサーバが認識した場合のみ、サー バは新しいソース IP アドレスを新たなクライアントだと認識する。もちろん、この場 合のローミングは NAT 越しの IP 再アサイン時やクライアントのネットワークインター フェース切り替えやノート PC を新たな無線アクセスポイント配下へ移動した場合も同 様に動作する。めちゃ便利やん。Mosh は SSP を2経路持ち、1つはクライアントからサー バへユーザの打ったキーの同期を取る。もう一方はサーバからクライアントへで、スク リーンの状態をクライアントへ同期を取るためだ。\nつまり、ノート PC やその他モバイル機器の IP アドレスが変わったとしても接続性は 担保され、また ノート PC のスリープ解除後にも接続性は確保され続ける。また、UDP で動作しているので、フルスクリーンの vim や emacs 等での再描画の遅延等も起こり にくそうだ。あと Ctrl-C 。TCP だと、キータイプがサーバプログラムに伝わらない状 況はプログラムプロセスが混雑しているとよくあるのだが、SSP over UDP での Ctrl-C はそういうことが無いそうだ。\nまた、認証機構は SSH に任せているので sshd は引き続き稼働させておく必要がある。 mosh は接続する先のユーザが一般ユーザ権限で動作させるプログラムでしかない。つ まり mosh daemon は必要ないようだ。\n実際にインストールしてみた。Mac の場合、homebrew で\n% brew update % brew install mobile-shell  で完了。私はサーバに Debian と Ubuntu を使っているのだが、Debian の場合は testing, unstable でパッケージが容易されている。が、testing のパッケージを使っ た所、動作が不安定だった。文字を削除しても一文字消えない等。よって、github か ら最新のソースを取得。(unstable のパッケージでもイイかもしれない、私は試してな いです。)\n% sudo apt-get install protobuf-compiler libprotobuf-dev pkg-config \\ libboost-dev libncurses5-dev % cd gitwork % git clone https://github.com/keithw/mosh.git % cd mosh % ./autogen.sh \u0026amp;\u0026amp; ./configure \u0026amp;\u0026amp; make % sudo make install  Ubuntu の場合は、\n% sudo apt-get install python-software-properties % sudo add-apt-repository ppa:keithw/mosh % sudo apt-get update % sudo apt-get install mosh  だ。その他の destribution, OS の場合は下記のリンクを参照してみてください。\nhttp://mosh.mit.edu/#getting\n次にクライアント \u0026lt;-\u0026gt; サーバ間の経路が変わっても接続したままになるのか、テスト してみた。\nMacBook wireless nic \u0026lt;-\u0026gt; server を MacBook ethernet nic \u0026lt;-\u0026gt; server に接続しな おしてみた。また、その接続しなおしのタイミングの間、\n% while(true) do echo \u0026quot;is mosh alive ?\u0026quot; ;sleep 3; done  とシェル上で実行し続けた。結果、サーバに再接続され、また上のシェルプロセスも継 続され続けた！これには驚いた。\nということで、使い始めて1週間だが今のところ快適。\n会社の方に protobuf は Google が自社ネットワーク内のトラフィック軽減のために開 発したものだ、と言っていた。こんなところでも Google の技術力がすんごいことに改 めて気がついたよ。\n","permalink":"https://jedipunkz.github.io/post/2012/04/14/ssh-mosh/","summary":"今週/先週？、Hacker News で取り上げられた Mosh を自宅 と会社で使い始めた。SSH 代替なソフトウェアで、SSP (State Synchronization Protocol)over UDP で動作している。MIT が開発したそうだ。\n動作は、クライアントがシーケンス番号と共にデータグラムをサーバに送信し、同期し 続ける。クライアントがローミングし IP アドレスが代わる等した時、以前より大きい シーケンス番号と共に正当なパケットが送信されたとサーバが認識した場合のみ、サー バは新しいソース IP アドレスを新たなクライアントだと認識する。もちろん、この場 合のローミングは NAT 越しの IP 再アサイン時やクライアントのネットワークインター フェース切り替えやノート PC を新たな無線アクセスポイント配下へ移動した場合も同 様に動作する。めちゃ便利やん。Mosh は SSP を2経路持ち、1つはクライアントからサー バへユーザの打ったキーの同期を取る。もう一方はサーバからクライアントへで、スク リーンの状態をクライアントへ同期を取るためだ。\nつまり、ノート PC やその他モバイル機器の IP アドレスが変わったとしても接続性は 担保され、また ノート PC のスリープ解除後にも接続性は確保され続ける。また、UDP で動作しているので、フルスクリーンの vim や emacs 等での再描画の遅延等も起こり にくそうだ。あと Ctrl-C 。TCP だと、キータイプがサーバプログラムに伝わらない状 況はプログラムプロセスが混雑しているとよくあるのだが、SSP over UDP での Ctrl-C はそういうことが無いそうだ。\nまた、認証機構は SSH に任せているので sshd は引き続き稼働させておく必要がある。 mosh は接続する先のユーザが一般ユーザ権限で動作させるプログラムでしかない。つ まり mosh daemon は必要ないようだ。","title":"Mosh を使う"},{"content":"ノート PC を購入するといつも Debian Gnu/Linux sid をインストールするのだけれど も、X Window System や InputMethod をインストールして利用し始められるところま での手順っていつも忘れる。メモとしてブログに載せておきます。\nconsole 上での ctrl:caps swap 設定 取りあえずこの設定をしないと、何も操作出来ない。Caps Lock と Control キーを入 れ替える設定です。\n/etc/default/keyboard を下記のように修正\nXKBOPTIONS=\u0026quot;ctrl:swapcaps\u0026quot;  下記のコマンドで設定を反映。\n% sudo /etc/init.d/console-setup restart  sid の sources.list 設定 Debian Gnu/Linux をノート PC にインストールする時は必ず sid を入れます。新し目 のソフトウェアを使いたいから。\ndeb http://ftp.riken.jp/Linux/debian/debian/ unstable main contrib non-free deb-src http://ftp.riken.jp/Linux/debian/debian/ unstable main contrib non-free  下記のコマンドで dist-upgrade\n% sudo apt-get update % sudo apt-get dist-upgrade  iwlwifi のインストールとネットワーク設定 買うノート PC はいつも ThinkPad。大体 intel チップな Wi-Fi モジュールが搭載さ れているので、iwlwifi を使う。\n% sudo apt-get install firmware-iwlwifi % sudo modprobe iwl4965 # 各々の端末に合わせる。lspci コマンドで確認 % sudo modprobe iwlagn % sudo iwconfig % sudo ifconfig wlan0 up  次に /etc/network/interfaces の設定\nauto wlan0 iface wlan0 inet dhcp pre-up /sbin/wpa_supplicant -B -Dwext -c/etc/wpa_supplicant/wpa_supplicant.conf -iwlan0 pre-up iwconfig wlan0 essid ${SSID} pre-up ip link set wlan0 up pre-up iwconfig wlan0 ap any  /etc/wpa_supplicant/wpa_supplicant.conf の設定\n% sudo wpa_passphrase ${SSID} ${PASSPHRASE} \u0026gt; /etc/wpa_supplicant/wpa_supplicant.conf  生成した上記のファイルを修正・加筆\nnetwork={ proto=WPA WPA2 key_mgmt=WPA-PSK pairwise=CCMP TKIP ssid=\u0026quot;${SSD}\u0026quot; psk=..... }  ネットワークインターフェースの再起動で接続\n% sudo service networking restart  X Window 周りの設定 必要なパッケージをインストールする。ビデオカードドライバは端末に合わせてインス トールする。thinkpad は Intel なビデオカードの場合が多いので下記のように指定。\nまた、昔はよく enlightenment を使っていたのだけど、e17 が何時まで経っても完成 度上がらないので諦めて openbox を使うことに。\n% sudo apt-get insatll openbox obmenu xorg xserver-xorg-video-intel  console 同様に ctrl:caps を swap する。xorg.conf が無い(2012/04時点)ので、sid では /etc/X11/Xsession.d/10setxkbmap を下記のように生成して対応。\n#!/bin/sh setxkbmap -option ctl:nocaps  日本語入力 input method のインストール 日本語入力に uim-mozc を利用することに。今のところこれが一番快適。\n% sudo apt-get install uim-mozc uim-xim uim-utils mozc-utils-gui mozc-server uim-qt  /etc/X11/openbox/autostart に下記の行を追記\nif type uim-xim \u0026amp;\u0026gt; /dev/null ; then uim-xim \u0026amp; uim-toolbar-qt4 \u0026amp; fi  uim-toolbar-qt4 の preference で \u0026ldquo;オン・オフキー\u0026rdquo; 等もろもろを好みに設定。\nalsa によるサウンドの出力 昔は苦労したサウンド出力も、今ではこんなに簡単な操作で出来るようになりましたｗ\n% sudo apt-get install alsa-utils alsa-base % sudo modprobe snd-pcm-oss % sudo alsactl init % sudo alsactl store  以上です。ここまで来れば、あとは好みで設定していけばいい。もちろん Ubuntu 等の dist を使えば、こんな操作は必要無いのだけど、自分でやらないとどうも気持ちが悪 いし、万が一トラブルが起きても自分で設定していれば治せるし。\nまぁ、最近では MacBook を使うことがほとんどなのですが..。or2\n","permalink":"https://jedipunkz.github.io/post/2012/04/07/debian-sid-on-thinkpad-first-setup/","summary":"ノート PC を購入するといつも Debian Gnu/Linux sid をインストールするのだけれど も、X Window System や InputMethod をインストールして利用し始められるところま での手順っていつも忘れる。メモとしてブログに載せておきます。\nconsole 上での ctrl:caps swap 設定 取りあえずこの設定をしないと、何も操作出来ない。Caps Lock と Control キーを入 れ替える設定です。\n/etc/default/keyboard を下記のように修正\nXKBOPTIONS=\u0026quot;ctrl:swapcaps\u0026quot;  下記のコマンドで設定を反映。\n% sudo /etc/init.d/console-setup restart  sid の sources.list 設定 Debian Gnu/Linux をノート PC にインストールする時は必ず sid を入れます。新し目 のソフトウェアを使いたいから。\ndeb http://ftp.riken.jp/Linux/debian/debian/ unstable main contrib non-free deb-src http://ftp.riken.jp/Linux/debian/debian/ unstable main contrib non-free  下記のコマンドで dist-upgrade\n% sudo apt-get update % sudo apt-get dist-upgrade  iwlwifi のインストールとネットワーク設定 買うノート PC はいつも ThinkPad。大体 intel チップな Wi-Fi モジュールが搭載さ れているので、iwlwifi を使う。","title":"debian sid on thinkpad"},{"content":"長年 Gnu screen 愛用者だったのだけど完全に tmux に移行しました。\n愛用している iterm2 との相性も良く、好都合な点が幾つかあり移行する価値がありました。\nただ、サーバサイドでの利用は諦めました。問題だったコピペ問題をクリアしている tmux のバージョンが Debian sid から取得出来たのだけど、まだまだ完成度高くなく..。\nよって、Mac に tmux をインストールして作業するようになりました。インストール方法はこれ。\n予め https://github.com/mxcl/homebrew/wiki/installation に したがって homebrew をインストールする必要あり。\n% brew update % brew install tmux  インストールしたら .tmux.conf の作成に入る。prefix キーは C-t にしたかった。screen 時代から これを使っていて指がそう動くから。\n# prefix key set-option -g prefix C-t  またステータスライン周りの設定。色なども自分で選択すると良い。\n# view set -g status-interval 5 set -g status-left-length 16 set -g status-right-length 50 # status set -g status-fg white set -g status-bg black set -g status-left-length 30 set -g status-left '#[fg=white,bg=black]#H#[fg=white]:#[fg=white][#S#[fg=white]][#[default]' set -g status-right '#[fg=white,bg=red,bold] [%Y-%m-%d(%a) %H:%M]#[default]' # window-status-current setw -g window-status-current-fg white setw -g window-status-current-bg red setw -g window-status-current-attr bold#,underscore # pane-active-border set -g pane-active-border-fg black set -g pane-active-border-bg blue  UTF-8 有効化やキーバインド設定等は\u0026hellip;\n# Option set-window-option -g utf8 on #set-window-option -g mode-keys vi  私は emacs 使いなので vi モードは無効にした。これで emacs モードが有効になる。\nマウス選択を有効にすると、便利かもしれない。\n#set-option -g mouse-select-pane on #set-option -g mouse-resize-pane  が、PANE (ペイン) を横分割した際にこれらの設定を入れているとマウスでのコピーペーストが出来ない ため、私は無効にした。\nその他の設定。.tmux.conf を tmux 開いたまま設定リロードだったり、ペインの移動キーバインド等。 特に hjkl キーの vi キーバインドをペイン移動のためにアサインするため、その他のキーを下記のように 設定している。ウィンドウの移動は C-n, C-p にも追加でアサイン (default は n, p) し、ペインの移動 には hjkl をアサインした。\nbind C-r source-file ~/.tmux.conf bind C-n next-window bind C-p previous-window bind c new-window bind | split-window -h bind C-k kill-pane bind K kill-window bind i display-panes bind y copy-mode bind h select-pane -L bind j select-pane -D bind k select-pane -U bind l select-pane -R  ウィンドウの中にペインの概念が入ったことで、作業効率が非常に上がったし、Mac 上のアプリなので、Mac を 開いた瞬間に作業に入れる。(今までは debian 上の screen で生活していた) debian 上の tmux に移行する必要 があるかどうか、これから使い込んでみたいと思う。その時にまたレビューします。\n","permalink":"https://jedipunkz.github.io/post/2012/04/01/switching-screen-tmux/","summary":"長年 Gnu screen 愛用者だったのだけど完全に tmux に移行しました。\n愛用している iterm2 との相性も良く、好都合な点が幾つかあり移行する価値がありました。\nただ、サーバサイドでの利用は諦めました。問題だったコピペ問題をクリアしている tmux のバージョンが Debian sid から取得出来たのだけど、まだまだ完成度高くなく..。\nよって、Mac に tmux をインストールして作業するようになりました。インストール方法はこれ。\n予め https://github.com/mxcl/homebrew/wiki/installation に したがって homebrew をインストールする必要あり。\n% brew update % brew install tmux  インストールしたら .tmux.conf の作成に入る。prefix キーは C-t にしたかった。screen 時代から これを使っていて指がそう動くから。\n# prefix key set-option -g prefix C-t  またステータスライン周りの設定。色なども自分で選択すると良い。\n# view set -g status-interval 5 set -g status-left-length 16 set -g status-right-length 50 # status set -g status-fg white set -g status-bg black set -g status-left-length 30 set -g status-left '#[fg=white,bg=black]#H#[fg=white]:#[fg=white][#S#[fg=white]][#[default]' set -g status-right '#[fg=white,bg=red,bold] [%Y-%m-%d(%a) %H:%M]#[default]' # window-status-current setw -g window-status-current-fg white setw -g window-status-current-bg red setw -g window-status-current-attr bold#,underscore # pane-active-border set -g pane-active-border-fg black set -g pane-active-border-bg blue  UTF-8 有効化やキーバインド設定等は\u0026hellip;","title":"switching screen-\u003etmux"},{"content":"pages.github.com は github.com の WEB ホスティングサービスです。これを利用して octopress のブログを 構築する方法をメモしていきます。\nまず、github.com に \u0026ldquo;${好きな名前}.github.com\u0026rdquo; という名前のレポジトリを github.com 上で作成します。 レポジトリの作成は普通のレポジトリ作成と同じ方法で行えます。しばらくすると \u0026ldquo;${好きな名前}.github.com のページがビルド出来ました\u0026rdquo; という内容でメールが送られてきます。\npages.github.com によると、レポジトリページで \u0026ldquo;GitHub Page\u0026rdquo; にチェックを入れろと書いてありますが、情報が古いようです。 2012/03/20 現在、この操作の必要はありませんでした。\n次に octopress の環境構築。\noctopress は、jekyll ベースのブログツールです。markdown 形式で記事を書くのですが、emacs や vim 等 好きなエディタを使って記事を書けるので便利です。最近 \u0026ldquo;Blogging with Emacs\u0026rdquo; なんてブログをよく目にしたと 思うのですが、まさにソレですよね。エンジニアにとっては嬉しいブログ環境です。\nまずは、rvm の環境構築を。octopress は ruby 1.9.2 以上が必要なので用意するのですが rvm を使うと 手軽に用意出来るので、今回はその方法を記します。\n参考 URL は http://octopress.org/docs/setup/rvm/ です。\nまずは準備から。私の環境は Ubuntu Server 10.04 LTS なのですが、下記のパッケージが必用になります。\n% sudo apt-get install gcc make zlib1g-dev libssl-dev  下記のコマンドを実行すると、rvm がインストールされます。\n% bash -s stable \u0026lt; \u0026lt;(curl -s https://raw.github.com/wayneeseguin/rvm/master/binscripts/rvm-installer)  次に使っている shell に合わせて rc ファイルを設定します。\nbash なら\u0026hellip;\n% echo '[[ -s \u0026quot;$HOME/.rvm/scripts/rvm\u0026quot; ]] \u0026amp;\u0026amp; . \u0026quot;$HOME/.rvm/scripts/rvm\u0026quot; # Load RVM function' \u0026gt;\u0026gt; ~/.bash_profile % source ~/.bash_profile  zsh なら\u0026hellip;\n% echo '[[ -s $HOME/.rvm/scripts/rvm ]] \u0026amp;\u0026amp; source $HOME/.rvm/scripts/rvm' \u0026gt;\u0026gt; ~/.zshrc % source ~/.zshrc  ruby 1.9.2 をインストールします。\n% rvm install 1.9.2 \u0026amp;\u0026amp; rvm use 1.9.2 % rvm rubygems latest  次に octopress のインストールと環境設定です。\n% mkdir ${好きな名前}.github.com % cd ${好きな名前}.github.com % git clone git://github.com/imathis/octopress.git octopress % cd octopress % gem install bundler % bundle install % rake install # classic テーマのインストール  ここまで出来たら github.com へデプロイするだけです。\n% rake setup_github_pages Enter the read/write url for your repository:  と表示されるので、作成した github.com ページの情報を入力します。私の場合は\ngit@github.com:chobiwan/chobiwan.github.com.git  この操作で、.git 内の情報諸々を更新してくれます。実行した後に覗いて見て下さい。\nです。最後にブログ生成とデプロイを実行します。この作業で ${好きな名前}.github.com へのデプロイが 実行されます。\n% rake gen_deploy  暫く時間がかかりますが (数分) 、${好きな名前}.github.com のサイトにアクセスできるようになっている はずです。\n次に、必要最低限の設定を行います。_config.yml ファイル内の下記の情報を満たしていきましょう。\nurl: # For rewriting urls for RSS, etc  title = # Used in the header and title tags title = # A description used in the header simple_search: # Search engine for simple site search description = # A default meta description for your site subscribe_rss: # Url for your blog\u0026rsquo;s feed, defauts to /atom.xml subscribe_email: # Url to subscribe by email (service required) email: # Email address for the RSS feed if you want it.\nではいよいよ、最初の投稿を。\n% rake new_post[\u0026quot;title\u0026quot;] % rake new_post\\[\u0026quot;title\u0026quot;\\] # zsh の場合  source/_posts/2012-03-20-test-post.markdown という日付付きファイルが生成されるので、このファイルを 好きなエディタで編集します。\nmarkdown 形式で記すことが出来ます。上記の操作でテンプレートらしきファイルになっているので \u0026ldquo;\u0026mdash;\u0026rdquo; の配下から記事を書いていきます。また categories 等の情報は自分で修正出来ます。\n書き終わったら、ブログを生成してデプロイです。\n% rake gen_deploy  先ほど作った ${好きな名前}.github.com へアクセスしてみましょう。octopress のブログが完成している はずです。\n","permalink":"https://jedipunkz.github.io/post/2012/03/20/github-dot-com-de-octopress-gou-zhu/","summary":"pages.github.com は github.com の WEB ホスティングサービスです。これを利用して octopress のブログを 構築する方法をメモしていきます。\nまず、github.com に \u0026ldquo;${好きな名前}.github.com\u0026rdquo; という名前のレポジトリを github.com 上で作成します。 レポジトリの作成は普通のレポジトリ作成と同じ方法で行えます。しばらくすると \u0026ldquo;${好きな名前}.github.com のページがビルド出来ました\u0026rdquo; という内容でメールが送られてきます。\npages.github.com によると、レポジトリページで \u0026ldquo;GitHub Page\u0026rdquo; にチェックを入れろと書いてありますが、情報が古いようです。 2012/03/20 現在、この操作の必要はありませんでした。\n次に octopress の環境構築。\noctopress は、jekyll ベースのブログツールです。markdown 形式で記事を書くのですが、emacs や vim 等 好きなエディタを使って記事を書けるので便利です。最近 \u0026ldquo;Blogging with Emacs\u0026rdquo; なんてブログをよく目にしたと 思うのですが、まさにソレですよね。エンジニアにとっては嬉しいブログ環境です。\nまずは、rvm の環境構築を。octopress は ruby 1.9.2 以上が必要なので用意するのですが rvm を使うと 手軽に用意出来るので、今回はその方法を記します。\n参考 URL は http://octopress.org/docs/setup/rvm/ です。\nまずは準備から。私の環境は Ubuntu Server 10.04 LTS なのですが、下記のパッケージが必用になります。\n% sudo apt-get install gcc make zlib1g-dev libssl-dev  下記のコマンドを実行すると、rvm がインストールされます。","title":"github.com で octopress 構築"},{"content":"Carbon な API は排除していくべきと Apple も言っているようですし、自宅も会社も Cocoa な emacs を使うようになりました。\nその手順を書いていきます。\nソースとパッチでビルドも出来るのですが、homebrew使うとメチャ楽なので今回はそれを使います。 homebrew は公式サイトに詳しいことが書いてありますけどインストールがワンラインで済みます。 あと、事前に AppStore で Xcode を入れてください。\n% /usr/bin/ruby -e \u0026quot;$(curl -fsSL https://raw.github.com/gist/323731)\u0026quot;  だけです。\nそして Cocoa な emacs インストール。\n% brew install --cocoa emacs % sudo mv /usr/local/Cellar/emacs/24.1/Emacs.app /Applications/  以上です..。簡単すぎる。先人たちのおかげですね。\n次は時間見つけて anything.el のことを書こうかなぁと思ってます。\n参考 URL : http://mxcl.github.com/homebrew/","permalink":"https://jedipunkz.github.io/post/2012/03/07/cocoa-na-emacs-insutoru/","summary":"Carbon な API は排除していくべきと Apple も言っているようですし、自宅も会社も Cocoa な emacs を使うようになりました。\nその手順を書いていきます。\nソースとパッチでビルドも出来るのですが、homebrew使うとメチャ楽なので今回はそれを使います。 homebrew は公式サイトに詳しいことが書いてありますけどインストールがワンラインで済みます。 あと、事前に AppStore で Xcode を入れてください。\n% /usr/bin/ruby -e \u0026quot;$(curl -fsSL https://raw.github.com/gist/323731)\u0026quot;  だけです。\nそして Cocoa な emacs インストール。\n% brew install --cocoa emacs % sudo mv /usr/local/Cellar/emacs/24.1/Emacs.app /Applications/  以上です..。簡単すぎる。先人たちのおかげですね。\n次は時間見つけて anything.el のことを書こうかなぁと思ってます。\n参考 URL : http://mxcl.","title":"cocoa な emacs インストール"},{"content":"上の画像は conky というツールのキャプチャです。\nconkyは x window で使える linux マシンのステータスを文字・グラフ描画で表現してくれるツールです。\n透明にしたりグラフ表示を派手にすることも出来るのだけど、わたしは上図のようにステータスバーとして使ってます。Window Manager に openbox という素っ気ないものを使うようにしてるので、これ自体がファイラーもステータスバーも無いんです。なので conky を利用して \u0026lsquo;時間\u0026rsquo;, \u0026lsquo;バッテリ残量\u0026rsquo;, \u0026lsquo;AC アダプタ有無\u0026rsquo;, \u0026lsquo;ネットワーク使用量\u0026rsquo; 等を表示してます。\ndeviantart に rent0n86さんという方が投稿した作品があって、それをこちょこちょ自分用にいじって使ってます。\ndebian gnu/linux な GUI 環境があれば\u0026hellip;\n% sudo apt-get conky-all % cd $HOME % wget https://raw.github.com/chobiwan/dotfiles/master/.conkyrc  で、この環境を作れます。\n表示する内容は環境に合わせて修正すると楽しいです。幅は minimum_size パラメータで合わせてください。\nパラメータ一覧は、公式 Wiki サイトに正しい情報が載っています。\n話変わるけど、enlightenment 17 が完成度高くならない理由ってなんなのでしょうかね？ 16 を愛用していただけに残念。\n","permalink":"https://jedipunkz.github.io/post/2012/03/07/conky-statusbar/","summary":"上の画像は conky というツールのキャプチャです。\nconkyは x window で使える linux マシンのステータスを文字・グラフ描画で表現してくれるツールです。\n透明にしたりグラフ表示を派手にすることも出来るのだけど、わたしは上図のようにステータスバーとして使ってます。Window Manager に openbox という素っ気ないものを使うようにしてるので、これ自体がファイラーもステータスバーも無いんです。なので conky を利用して \u0026lsquo;時間\u0026rsquo;, \u0026lsquo;バッテリ残量\u0026rsquo;, \u0026lsquo;AC アダプタ有無\u0026rsquo;, \u0026lsquo;ネットワーク使用量\u0026rsquo; 等を表示してます。\ndeviantart に rent0n86さんという方が投稿した作品があって、それをこちょこちょ自分用にいじって使ってます。\ndebian gnu/linux な GUI 環境があれば\u0026hellip;\n% sudo apt-get conky-all % cd $HOME % wget https://raw.github.com/chobiwan/dotfiles/master/.conkyrc  で、この環境を作れます。\n表示する内容は環境に合わせて修正すると楽しいです。幅は minimum_size パラメータで合わせてください。\nパラメータ一覧は、公式 Wiki サイトに正しい情報が載っています。\n話変わるけど、enlightenment 17 が完成度高くならない理由ってなんなのでしょうかね？ 16 を愛用していただけに残念。","title":"conky statusbar"},{"content":"github.com は便利なのだけどプライベートなレポジトリを作るのにお金払うのはもったいないので自宅サーバに SSH 経由の Git サーバを構築した。その時の手順をメモしておきます。\ngitosis という便利なツールがあって、これを使うとあっという間に環境構築できます。私の環境は debian Gnu/Linux Squeeze なのですが apt-get で必要なモノを入れました。gitosis は git で持ってきます。\nremote% sudo apt-get update remote% sudo apt-get install git git-core python python-setuptools remote% cd $HOME/usr/src remote% git clone git://eagain.net/gitosis.git remote% cd gitosis remote% sudo python setup.py install  SSH でアクセスする先のユーザを作ります。\nremote% sudo adduser --shell /bin/sh -gecos --group \\ --disable-password --home /home/git git  作業端末で rsa な SSH 公開鍵を生成して ${remote} サーバは転送する。\nlocal% ssh-keygen -t rsa ... インタラクティブに答える local% scp .ssh/id_dsa.pub ${remote}:/tmp/  転送した鍵を元に ${remote} サーバ上で git レポジトリを初期化する。\nremote% sudo -H -u git gitosis-init \u0026lt; /tmp/id_rsa.pub  もし実行権が付いていなかったら\nremote% sudo chmod 755 /home/git/repositories/gitosis-admin.git/hooks/post-update  これで環境構築完了。ただ、このままだとローカルの作業端末からレポジトリの生成が出来ない。\nローカルの作業端末でレポジトリを生成する手順は、 gitosis-admin.git を clone してきて、gitosis.conf を修正し commit/push する。\nlocal% git clone git@${remote}:gitosis-admin.git local% vi gitosis.conf  今回はテストで test グループに test レポジトリを作るための config を書いてみる。members は複数人書ける。\n[gitosis] [group gitosis-admin] writable = gitosis-admin members = chobiwan@${local} [group test] writable = test members = chobiwan@${local}  修正した内容を ${remote} サーバへ git push して反映させる。\nlocal% git add . local% git commit -m \u0026quot;added test repo\u0026quot; local% git push origin master  これで \u0026lsquo;test\u0026rsquo; レポジトリをローカルの作業端末から作ってコミットする準備完了。 試しにファイルを add, commit, push してみる。\nlocal% mkdir ~/gitwork local% cd ~/gitwork local% touch README local% git add README local% git commit -m \u0026quot;my first commi\u0026quot; local% git remote add origin git@obi.chobiwan.me:test.git local% git push origin master  (2012/05/01 追記) 新規ユーザの追加方法は下記の通り。\nlocal% ssh-keygen -t rsa local% cd ~/gitwork local% git clone git@${remote}:gitosis-admin.git local% cp ~/.ssh/id_rsa.pub ./gitosis-admin/keydir/user@hostname.pub local% vi ./gitosis-admin/gitosis.conf \u0026lt;参加したいリポジトリの membersに user@hostname.pub を追加(スペース区切り) local% git gitosis-admin \u0026amp;\u0026amp; git add . \u0026amp;\u0026amp; git commit -m \u0026quot;added a user\u0026quot; local% git push origin master  公開鍵のアップやレポジトリ・グループの生成の仕方を覚えれば、OK なのかなぁと。 次はレポジトリのバックアップとレカバリについてまとめていきたい。リカバリできないと死ねるから。\n","permalink":"https://jedipunkz.github.io/post/2012/03/07/gitosis-ssh-plus-git-saba/","summary":"github.com は便利なのだけどプライベートなレポジトリを作るのにお金払うのはもったいないので自宅サーバに SSH 経由の Git サーバを構築した。その時の手順をメモしておきます。\ngitosis という便利なツールがあって、これを使うとあっという間に環境構築できます。私の環境は debian Gnu/Linux Squeeze なのですが apt-get で必要なモノを入れました。gitosis は git で持ってきます。\nremote% sudo apt-get update remote% sudo apt-get install git git-core python python-setuptools remote% cd $HOME/usr/src remote% git clone git://eagain.net/gitosis.git remote% cd gitosis remote% sudo python setup.py install  SSH でアクセスする先のユーザを作ります。\nremote% sudo adduser --shell /bin/sh -gecos --group \\ --disable-password --home /home/git git  作業端末で rsa な SSH 公開鍵を生成して ${remote} サーバは転送する。\nlocal% ssh-keygen -t rsa .","title":"gitosis ssh+git サーバ"},{"content":"白金台のクックパッドさんで行われた \u0026ldquo;heroku jp meet up #3\u0026rdquo; に参加してきました。\n東京マラソン参加のため来日されていた Christopher Stoltさんや Ruby コミッタの相澤さんなどの話を聞けました。\nChristopher さんからは、基本的な使い方や heroku で動作させたアプリケーションをローカル環境で動作させる foreman、また皆が意外と気にするアプリのログを tail する方法などの説明がありました。PaaS での皆の懸念点が結構解決されたんじゃないかなぁ。\n相澤さんからは NY マラソンでの実績など、比較的エンタープライズな使われ方もされ初めていると説明がありました。あと、呼び名なのですが heroku は \u0026ldquo;へろく\u0026rdquo; と発音するそうです。確かに about.heroku.com には \u0026ldquo;Heroku (pronounced her-OH-koo) is a cloud application platform\u0026rdquo; と書いてあるのだが、\u0026ldquo;へろく\u0026rdquo; が正しいそうです。w\nそのた LT が幾つあって、ちょうど気になっていた Lokkaの話があったので、自宅に帰ってから自分の heroku アカウントで lokka を動かしてみました。lokka の公式サイトに手順が書いてあって、そのままなのですが行ったのは、\n% gem install heroku bundler % git clone git://github.com/komagata/lokka.git % cd lokka % heroku create % git push heroku master % heroku rake db:setup % heroku open  すれば OK。\n私の環境では公開鍵認証がうまくいかなかったので、下記の対処をしました。\n% heroku keys:add  試しに wordpress のデータを移行してみようと思ったのですが、\u0026ldquo;Application Error\u0026rdquo; が発生。今のところうまくいっていません。コードとブログコンテンツを git で管理出来るので、今時ですよね。初めてみたい。\nまた、今回会場を提供してくださったクックパッドさんが、バレンタインデーということもあり皆に料理を提供して下さいました。感謝ー。\n本当は heroku システムがどう構成されているか？が知りたかったのですが、ユーザ視点で使われ方が分からないと何も出来ないと感じていたため、今回はとても良い機会でした。個人的にも heroku は使い続けたい、と感じたサービスでした。みなさん、当日はありがとうございました。\n","permalink":"https://jedipunkz.github.io/post/2012/03/07/heroku-jp-meetup-number-3/","summary":"白金台のクックパッドさんで行われた \u0026ldquo;heroku jp meet up #3\u0026rdquo; に参加してきました。\n東京マラソン参加のため来日されていた Christopher Stoltさんや Ruby コミッタの相澤さんなどの話を聞けました。\nChristopher さんからは、基本的な使い方や heroku で動作させたアプリケーションをローカル環境で動作させる foreman、また皆が意外と気にするアプリのログを tail する方法などの説明がありました。PaaS での皆の懸念点が結構解決されたんじゃないかなぁ。\n相澤さんからは NY マラソンでの実績など、比較的エンタープライズな使われ方もされ初めていると説明がありました。あと、呼び名なのですが heroku は \u0026ldquo;へろく\u0026rdquo; と発音するそうです。確かに about.heroku.com には \u0026ldquo;Heroku (pronounced her-OH-koo) is a cloud application platform\u0026rdquo; と書いてあるのだが、\u0026ldquo;へろく\u0026rdquo; が正しいそうです。w\nそのた LT が幾つあって、ちょうど気になっていた Lokkaの話があったので、自宅に帰ってから自分の heroku アカウントで lokka を動かしてみました。lokka の公式サイトに手順が書いてあって、そのままなのですが行ったのは、\n% gem install heroku bundler % git clone git://github.com/komagata/lokka.git % cd lokka % heroku create % git push heroku master % heroku rake db:setup % heroku open  すれば OK。","title":"Heroku JP Meetup #3"},{"content":"2012年2月6日、西新宿にある株式会社ニフティさんで行われた \u0026ldquo;OpenFlow 勉強会\u0026rdquo; に参加したので簡単なレポメモを書いておきます。\nまずは OpenFlow の基本動作。\n メッセージの切り出し ハンドシェイク コネクションの維持 スイッチから送られてくるメッセージへの応答  構成は\n OpenFlow コントローラ, スイッチから成る コントローラは通信制御 スイッチはフロールールをコントローラに問い合わせ通信を受け流し(packet/frame 転送) コントローラは L2 - L4 フィールドを見て制御する  そしてコントローラ、スイッチは各社・団体から提供されている。今月も Nicira Networks さんが自社システムの構成を抽象的ではありますが公開され、HP さんも OpenFlow 対応スイッチを12製品ほど発表されました。\nコントローラの種別、\n Beacon (Java) NOX (Python) Ryu NodeFlow Trema (C/ruby) Nicira Networks Big Switch Networks Midokura NTT Data  スイッチの種別、\n cisco nexus 3000 IBM BNT rackSwitch G8264 NEC Univerge PF5240/PF5820 Pronto Systems 3240/3290 HP 3500/5400 HP OpenFlow 化 firmware Reference Implementation (Software) Open vSwitch (Software)  NEC さんの PF ほにゃららは、GUI なインターフェースと API を持った製品で OpenFlow 1.0.0 仕様に準拠。スイッチ25台までを管理するコントローラ。価格は、コントローラ : 1000万, スイッチ : 250万 だそうです。スイッチ25台はあくまでもソフトリミットらしいです。\nTrema は開発者が日本人で OpenFlow 1.1.0 に準拠。Ruby / C を使ってフロールールが掛けるフレームワーク型のソフトウェアで、github にある Wiki を見ても分かりますが、とても簡潔なルール記述が可能です。あと、特徴なのがテストフレームワーク(シュミレータ)が付いているので、OpenFlow 対応スイッチを購入しなくても自分で書いたルールがテスト出来ます。これはとても大きな意味がある。自宅でもあれこれルールを書けて動作を知るにはとても良い機能です。\nNicira Networks の NVP (Nacira Virtual Platform) はハイパーバイザ上の OpenvSwitch やアプライアンスなスイッチを管理するコントローラから成ると今月公開されました。ただ抽象的な言葉ばかりが並んでいて ( web, datasheet 共に ) まだまだ、仕組みを理解するには情報が足らないのかなぁと感じています。特に distributed controller はどう冗長取れている？など。software designed network の ML が日本でも管理され始めて議論されていますが、ハイパーバイザ上の OpenvSwitch で、カプセル化時のオーバーヘッドをどう攻略するか？が問題じゃないかと話し合われています。\nMidokura さんも会場にいらっしゃったのですが、印象的な言葉は \u0026ldquo;OpenFlow は夢の技術ではないし、既存のネットワーク技術を置き換えるものでもない\u0026rdquo; でした。サービスを形成するサーバファーム周りで OpenFlow を使ったシステムを組むには良いですけどね。\n最後に参考 URL を記しておきます。\n sdnstudy google group trema Nicira Virtual Platform  ","permalink":"https://jedipunkz.github.io/post/2012/03/07/openflow-mian-qiang-hui/","summary":"2012年2月6日、西新宿にある株式会社ニフティさんで行われた \u0026ldquo;OpenFlow 勉強会\u0026rdquo; に参加したので簡単なレポメモを書いておきます。\nまずは OpenFlow の基本動作。\n メッセージの切り出し ハンドシェイク コネクションの維持 スイッチから送られてくるメッセージへの応答  構成は\n OpenFlow コントローラ, スイッチから成る コントローラは通信制御 スイッチはフロールールをコントローラに問い合わせ通信を受け流し(packet/frame 転送) コントローラは L2 - L4 フィールドを見て制御する  そしてコントローラ、スイッチは各社・団体から提供されている。今月も Nicira Networks さんが自社システムの構成を抽象的ではありますが公開され、HP さんも OpenFlow 対応スイッチを12製品ほど発表されました。\nコントローラの種別、\n Beacon (Java) NOX (Python) Ryu NodeFlow Trema (C/ruby) Nicira Networks Big Switch Networks Midokura NTT Data  スイッチの種別、\n cisco nexus 3000 IBM BNT rackSwitch G8264 NEC Univerge PF5240/PF5820 Pronto Systems 3240/3290 HP 3500/5400 HP OpenFlow 化 firmware Reference Implementation (Software) Open vSwitch (Software)  NEC さんの PF ほにゃららは、GUI なインターフェースと API を持った製品で OpenFlow 1.","title":"OpenFlow 勉強会"},{"content":"MS Windows なツールを使う方法だったり、vmlinuz, initrd をファイラーでコピーしたり、何故かいつもインターネットで調べると USB スティックを利用した debian のインストール方法が\u0026rsquo;面倒', \u0026lsquo;不確か\u0026rsquo; なので、忘れないようにメモ。\n手元に linux 端末用意して、USB スティック挿す。\n% wget ftp://ftp.jp.debian.org/pub/Linux/debian-cd/6.0.3/amd64/iso-cd/debian-6.0.3-amd64-netinst.iso % sudo cat debian-6.0.3-amd64-netinst.iso \u0026gt; /dev/sdb # 挿した USB スティックのデバイス名  で終わり。\nただ弱点があって、フルイメージの iso は利用できないでの、今回みたいに netinstall だったり businesscard な iso を利用しかない。 他のディストリビューションもだけど、インストールする環境はネットに繋がっていないと不都合があるって時代だからいいかなぁ。\n一方、Ubuntu Server は賢い子なので 公式サイトに行くと USB スティック用の iso がダウンロードできたり iso を焼く環境に合わせて手順まで教えてくれる\u0026hellip;。\n","permalink":"https://jedipunkz.github.io/post/2012/03/07/usb-stick-de-debian-gnu-slash-linux-insutoru/","summary":"MS Windows なツールを使う方法だったり、vmlinuz, initrd をファイラーでコピーしたり、何故かいつもインターネットで調べると USB スティックを利用した debian のインストール方法が\u0026rsquo;面倒', \u0026lsquo;不確か\u0026rsquo; なので、忘れないようにメモ。\n手元に linux 端末用意して、USB スティック挿す。\n% wget ftp://ftp.jp.debian.org/pub/Linux/debian-cd/6.0.3/amd64/iso-cd/debian-6.0.3-amd64-netinst.iso % sudo cat debian-6.0.3-amd64-netinst.iso \u0026gt; /dev/sdb # 挿した USB スティックのデバイス名  で終わり。\nただ弱点があって、フルイメージの iso は利用できないでの、今回みたいに netinstall だったり businesscard な iso を利用しかない。 他のディストリビューションもだけど、インストールする環境はネットに繋がっていないと不都合があるって時代だからいいかなぁ。\n一方、Ubuntu Server は賢い子なので 公式サイトに行くと USB スティック用の iso がダウンロードできたり iso を焼く環境に合わせて手順まで教えてくれる\u0026hellip;。","title":"USB Stick で Debian Gnu/Linux インストール"},{"content":"ブログを始めるにあたり、wordpress 環境を構築する必要が出てきました。いつもの apache2 + mysql5 + PHP じゃつまらないので、nginx と fastcgi を使って少しだけ高速化してみました。メモですけど、ここに手順を記していきます。\n※ wordpress から octopress に移行しました\u0026hellip; (2012/03/07)\nただ、今回は nginx や mysql の基本的なオペレーション手順は割愛させてもらいます。\n私の環境について\u0026hellip;\n% lsb_release -a No LSB modules are available. Distributor ID: Debian Description: Debian GNU/Linux 6.0.3 (squeeze) Release: 6.0.3 Codename: squeeze  インストールしたもの\u0026hellip; メタパッケージを指定したのでその他必要なモノはインストールされます。\n% sudo apt-get update % sudo apt-get install spawn-fcgi php5 php5-mysql php5-cgi mysql-server nginx  まずはお決まりの gzip 圧縮転送。IE の古いモノ以外は対応しているので心配なし。今回のテーマと関係無いですけど、一応入れておきます。\n% diff -u /etc/nginx/nginx.conf.org /etc/nginx/nginx.conf --- /etc/nginx/nginx.conf.org 2012-01-14 15:27:45.000000000 +0900 +++ /etc/nginx/nginx.conf 2012-01-14 15:28:58.000000000 +0900 @@ -22,6 +22,10 @@ tcp_nodelay on; gzip on; + gzip_http_version 1.0; + gzip_vary on; + gzip_comp_level 6; + gzip_types text/html text/xml text/css application/xhtml+xml application/xml application/rss+xml application/atom_xml application/x-javascript application/x-httpd-php; gzip_disable \u0026quot;MSIE [1-6]\\.(?!.*SV1)\u0026quot;; include /etc/nginx/conf.d/*.conf;  spawn-fcgi を稼働させるスクリプトを生成する。/usr/bin/php-fastcgi として下記の内容で保存する。\n#! /bin/sh /usr/bin/spawn-fcgi -a 127.0.0.1 -p 9000 -C 6 -u www-data -f /usr/bin/php5-cgi % sudo chmod 755 /usr/bin/php-fastcgi  次にこれを実行する起動スクリプトの用意と実行。/etc/init.d/php-fastcgi\n#!/bin/bash ### BEGIN INIT INFO # Required-Start: $local_fs $remote_fs $network $syslog # Required-Stop: $local_fs $remote_fs $network $syslog # Default-Start: 2 3 4 5 # Default-Stop: 0 1 6 # Short-Description: php-fastcgi script # Description: php-fastcgi script ### END INIT INFO # env SCRIPT=/usr/bin/php-fastcgi USER=www-data RETVAL=0 PIDFILE=/var/run/php5-cgi.pid # start or stop case \u0026quot;$1\u0026quot; in start) su - $USER -c $SCRIPT pidof php5-cgi \u0026gt; $PIDFILE RETVAL=$? ;; stop) killall -9 php5-cgi echo '' \u0026gt; $PIDFILE RETVAL=$? ;; restart) killall -9 php5-cgi su - $USER -c $SCRIPT pidof php5-cgi \u0026gt; $PIDFILE RETVAL=$? ;; *) echo \u0026quot;Usage: php-fastcgi {start|stop|restart}\u0026quot; exit 1 ;; esac  起動すると php-cgi のプロセスが立ち上がり localhost:9000 で LISTEN された状態になっているはずです。nginx はここへのプロキシのような動作をすることになります。 下記の手順で起動と起動スクリプトへの組み込みを行なってください。\n% sudo chmod 755 /etc/init.d/php-fastcgi % sudo update-rc.d php-fastcgi defaults % sudo service php-fastcgi start  次に nginx の virtualhost を掘ります。変数 ${FQDN_HOSTNAME}, ${DOCUMENT_ROOT}は自分 の環境情報に読み替えてください。\nserver { listen 80; server_name ${FQDN_HOSTNAME} access_log /var/log/nginx/${FQDN_HOSTNAME}.access.log; error_log /var/log/nginx/${FQDN_HOSTNAME}.error.log; location / { root ${DOCUMENT_ROOT}; index index.html index.php; } location ~ \\.php$ { include /etc/nginx/fastcgi_params; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME ${DOCUMENT_ROOT}$fastcgi_script_name; } }  nginx を stop/start してこれらの設定を有効にします。\n% sudo service nginx stop % sudo service nginx start  以上です。\nその他にも proxy cache を有効にして wordpress の静的な出力をキャッシュするチューニング方法もあるそうなので、次回試してみます。\n","permalink":"https://jedipunkz.github.io/post/2012/03/07/wordpress-wo-nginx-plus-fastcgi-degao-su-hua/","summary":"ブログを始めるにあたり、wordpress 環境を構築する必要が出てきました。いつもの apache2 + mysql5 + PHP じゃつまらないので、nginx と fastcgi を使って少しだけ高速化してみました。メモですけど、ここに手順を記していきます。\n※ wordpress から octopress に移行しました\u0026hellip; (2012/03/07)\nただ、今回は nginx や mysql の基本的なオペレーション手順は割愛させてもらいます。\n私の環境について\u0026hellip;\n% lsb_release -a No LSB modules are available. Distributor ID: Debian Description: Debian GNU/Linux 6.0.3 (squeeze) Release: 6.0.3 Codename: squeeze  インストールしたもの\u0026hellip; メタパッケージを指定したのでその他必要なモノはインストールされます。\n% sudo apt-get update % sudo apt-get install spawn-fcgi php5 php5-mysql php5-cgi mysql-server nginx  まずはお決まりの gzip 圧縮転送。IE の古いモノ以外は対応しているので心配なし。今回のテーマと関係無いですけど、一応入れておきます。\n% diff -u /etc/nginx/nginx.conf.org /etc/nginx/nginx.conf --- /etc/nginx/nginx.","title":"WordPress を nginx + fastcgi で高速化"}]