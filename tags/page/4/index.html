


    




<!DOCTYPE HTML>

<html>
    <head>
        
            <title>Tags - ジェダイさんのブログ</title>
        

        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1" />
        <meta name="generator" content="Hugo 0.53" />
        


        
        
            
        

        <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Tags"/>
<meta name="twitter:description" content=""/>

        <meta property="og:title" content="Tags" />
<meta property="og:description" content="" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://jedipunkz.github.io/tags/" />
<meta property="og:updated_time" content="2014-04-25T00:00:00&#43;00:00"/>

        
<meta itemprop="name" content="Tags">
<meta itemprop="description" content="">


        

        

        
        
            
        

        
        
            <link rel="stylesheet" href="/css/google-font.css" />
            <link rel="stylesheet" href="/css/font-awesome.min.css" />
            <link rel="stylesheet" href="/css/main.css" />
            <link rel="stylesheet" href="/css/add-on.css" />
            <link rel="stylesheet" href="/css/monokai-sublime.css">
        

        

        
        
        
            
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-30563095-1', 'auto');
	
	ga('send', 'pageview');
}
</script>

        
    </head>
    <body>

        
        <div id="wrapper">

    
<header id="header">
    
        <h2><a href="/"></i></a></h2>
    

    <nav class="links">
        <ul>
            
                <li>
                    <a href="">
                        Blog
                    </a>
                </li>
            
                <li>
                    <a href="about/index.html">
                        About
                    </a>
                </li>
            
        </ul>
    </nav>
    <nav class="main">
        <ul>
            
            <li class="search">
                <a class="fa-search" href="#search">Search</a>
                <form id="search" method="get" action="//google.com/search">
                    <input type="text" name="q" placeholder="Search" />
                    <input type="hidden" name="q" value="site:https://jedipunkz.github.io">
                </form>
            </li>
            <li class="menu">
                <a class="fa-bars" href="#menu">Menu</a>
            </li>
        </ul>
    </nav>
</header>


<section id="menu">

    
        <section>
            <form class="search" method="get" action="//google.com/search">
                <input type="text" name="q" placeholder="Search" />
                <input type="hidden" name="q" value="site:https://jedipunkz.github.io">
            </form>
        </section>

    
        <section>
            <ul class="links">
                
                    <li>
                        <a href="">
                            <h3>
                                
                                Blog
                            </h3>
                        </a>
                    </li>
                
                    <li>
                        <a href="about/index.html">
                            <h3>
                                
                                About
                            </h3>
                        </a>
                    </li>
                
            </ul>
        </section>

    
        <section>
            <ul class="links">
                <header>
                    <h3>Recent Posts</h3>
                </header>
                
                    
                

                
                    <li>
                        <a href="https://jedipunkz.github.io/blog/2018/12/31/istio/"><p>Istio, Helm を使って Getting Started 的なアプリをデプロイ</p></a>
                    </li>
                
                    <li>
                        <a href="https://jedipunkz.github.io/blog/2017/07/02/test-kitchen-cluster/"><p>Docker,Test-Kitchen,Ansible でクラスタを構成する</p></a>
                    </li>
                
                    <li>
                        <a href="https://jedipunkz.github.io/blog/2017/04/13/gke-lb/"><p>GCP ロードバランサと GKE クラスタを Terraform を使って構築する</p></a>
                    </li>
                
                    <li>
                        <a href="https://jedipunkz.github.io/blog/2017/02/12/serverless-fission/"><p>Serverless on Kubernetes : Fission を使ってみた</p></a>
                    </li>
                
                    <li>
                        <a href="https://jedipunkz.github.io/blog/2017/01/13/kubernetes-deployments/"><p>Kubernetes Deployments を使ってみた！</p></a>
                    </li>
                
            </ul>
        </section>

    
        
</section>

    
    <div id="main">
        
        
            
        

        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2014/04/25/stackforge-openstack-chef-repo-icehouse-deploy/">stackforge/openstack-chef-repo で OpenStack Icehouse デプロイ</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2014-04-25'>
            April 25, 2014</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは。<a href="https://twitter.com/jedipunkz">@jedipunkz</a> です。</p>

<p>またまた OpenStack のデプロイをどうするか？についてです。</p>

<p>今まで自分の中では Rackspace Private Cloud で使われている Rackspace 管理の
rcbops/chef-cookbooks が今現在使うならベストの選択だと思っていました。これは内
部で Chef が使われていてしかも Cookbooks が Github 上で公開されています。
Apache ライセンスで使えるので、サービス構築にも使えちゃうというモノ。</p>

<p>先日、ある OpenStack コアデベロッパーの方から「jedipunkz さん、やっぱり rcbops
がいいですか？運営とかどうなっているんでしょう？マージの規準とかどうなのかな？」
と質問受けました。確かにマージの基準は Rackspace Private Cloud がベースになり
ますし、管理しているエンジニアの一覧を見ていると Rackspace 社のエンジニアがメ
インですし、今後どうなるのか分からない&hellip;。</p>

<p>逃げ道は用意したほうが良さそう。</p>

<p>ということで、以前自分も暑かったことのある StackForge の openstack-chef-repo
を久々に使ってみました。Icehouse 構成がこの時点で既に組めるようになっていて、
以前よりだい〜ぶ完成度増した感があります。今回は nova-network 構成を作ってみた
のですが、Neutron 構成ももちろん出来そうなので後に調べてまた公開したいです。</p>

<h2 id="stackforge-とは">StackForge とは</h2>

<p>StackForge は OpenStack のデプロイ・CI の仕組みとして公式に用いられているもの。
公式サイトは下記の場所にある。</p>

<p><a href="http://ci.openstack.org/stackforge.html">http://ci.openstack.org/stackforge.html</a></p>

<p>StackForge の openstack-chef-repo は下記の場所にある。</p>

<p><a href="https://github.com/stackforge/openstack-chef-repo">https://github.com/stackforge/openstack-chef-repo</a></p>

<p>openstack-chef-repo はまだ &lsquo;stable/icehouse&rsquo; ブランチが生成されていない。が直
ちに master からブランチが切られる様子。</p>

<h2 id="目的">目的</h2>

<p>StackForge の openstack-chef-repo を用いて Icehouse リリース版の OpenStack を
デプロイするための方法を記す。今回は未だ &lsquo;stable/icehouse&rsquo; ブランチが無いので
master ブランチを用いて Icehouse リリース版 OpenStack を構築する。</p>

<h2 id="構成">構成</h2>

<p>構成はこんな感じ。</p>

<pre><code>   +-----------------+
   |    GW Router    |
+--+-----------------+
|  |
|  +-------------------+-------------------+--------------------------------------
|  |eth0               |eth0               |eth0                      VM Network (fixed)
|  +-----------------+ +-----------------+ +-----------------+ +-----------------+ 
|  | Controller Node | |  Compute Node   | |  Compute Node   | | Chef Workstation|
|  +-----------------+ +-----------------+ +-----------------+ +-----------------+ 
|  |eth1               |eth1               |eth1               |  
+--+-------------------+-------------------+-------------------+------------------
                                                                 API/Management Network
</code></pre>

<ul>
<li>Nova-Network 構成</li>
<li>今回は fixed network 用の NIC を eth0(物理 NIC) にアサイン</li>
<li>fixed network 用のネットワークをパブリックにする</li>
<li>API/Management Network 側に全ての API を出す。またここから The Internet に迂回出来るようにする</li>
<li>VM Network も GW を介して The Internet へ迂回出来るようにする</li>
<li>全ての操作は &lsquo;Chef Workstaion&rsquo; から行う</li>
<li>Compute ノードはキャパシティの許す限り何台でも可</li>
</ul>

<p>IP 一覧 (この記事での例)</p>

<ul>
<li>Controller : 10.200.9.46 (eth0), 10.200.10.46 (eth1)</li>
<li>Compute    : 10.200.9.47 (eth0), 10.200.10.47 (eth1)</li>
<li>Compute    : 10.200.9.48 (eth0), 10.200.10.48 (eth1)</li>
</ul>

<h2 id="手順">手順</h2>

<p>openstack-chef-repo をワークステーションノード上で取得する。</p>

<pre><code class="language-bash">% git clone https://github.com/stackforge/openstack-chef-repo.git
% cd openstack-chef-repo
</code></pre>

<p>Berksfile があるのでこれを用いて Chef Cookbooks を取得する。</p>

<pre><code class="language-bash">% berks install --path=./cookbooks
</code></pre>

<p>Roles, Cookbooks を Chef サーバにアップロードする。</p>

<pre><code class="language-bash">% knife cookbook upload -o cookbooks -a
% knife role from file roles/*.rb
</code></pre>

<p>1 Environment に対して 1 OpenStack クラスタである。今回構築するクラスタのため
の Environment を作成する。</p>

<p>下記を environments/icehouse-nova-network.rb として生成する。</p>

<pre><code class="language-json">name &quot;icehouse-nova-network&quot;
description &quot;separated nodes environment&quot;

override_attributes(
    &quot;release&quot; =&gt; &quot;icehouse&quot;,
    &quot;osops_networks&quot; =&gt; {
      &quot;management&quot; =&gt; &quot;10.200.10.0/24&quot;,
      &quot;public&quot; =&gt; &quot;10.200.10.0/24&quot;,
      &quot;nova&quot; =&gt; &quot;10.200.10.0/24&quot;
    },
    &quot;mysql&quot; =&gt; {
      &quot;bind_address&quot; =&gt; &quot;0.0.0.0&quot;,
      &quot;root_network_acl&quot; =&gt; &quot;%&quot;,
      &quot;allow_remote_root&quot; =&gt; true,
      &quot;server_root_password&quot; =&gt; &quot;secrete&quot;,
      &quot;server_repl_password&quot; =&gt; &quot;secrete&quot;,
      &quot;server_debian_password&quot; =&gt; &quot;secrete&quot;
    },
    &quot;nova&quot; =&gt; {
      &quot;network&quot; =&gt; {
        &quot;fixed_range&quot; =&gt; &quot;172.18.0.0/24&quot;,
        &quot;public_interface&quot; =&gt; &quot;eth0&quot;
      }
    },
    &quot;rabbitmq&quot; =&gt; {
      &quot;address&quot; =&gt; &quot;10.200.10.46&quot;,
      &quot;port&quot; =&gt; &quot;5672&quot;
    },
    &quot;openstack&quot; =&gt; {
      &quot;developer_mode&quot; =&gt; true,
      &quot;compute&quot; =&gt; {
        &quot;rabbit&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;
        },
        &quot;novnc_proxy&quot; =&gt; {
          &quot;bind_interface&quot; =&gt; &quot;eth1&quot;
        },
        &quot;libvirt&quot; =&gt; {
          &quot;bind_interface&quot; =&gt; &quot;eth1&quot;
        },
        &quot;network&quot; =&gt; {
          &quot;fixed_range&quot; =&gt; &quot;10.200.9.0/24&quot;
        },
        &quot;rabbit_server_chef_role&quot; =&gt; &quot;os-ops-messaging&quot;,
        &quot;networks&quot; =&gt; [
        {
          &quot;label&quot; =&gt; &quot;private&quot;,
          &quot;ipv4_cidr&quot; =&gt; &quot;10.200.9.0/24&quot;,
          &quot;num_networks&quot; =&gt; &quot;1&quot;,
          &quot;network_size&quot; =&gt; &quot;255&quot;,
          &quot;bridge&quot; =&gt; &quot;br200&quot;,
          &quot;bridge_dev&quot; =&gt; &quot;eth0&quot;,
          &quot;dns1&quot; =&gt; &quot;8.8.8.8&quot;,
          &quot;dns2&quot; =&gt; &quot;8.8.4.4&quot;,
          &quot;multi_host&quot; =&gt; &quot;T&quot;
        }
        ]
      },
      &quot;identity&quot; =&gt; {
        &quot;bind_interface&quot; =&gt; &quot;eth1&quot;,
        &quot;users&quot; =&gt; {
          &quot;demo&quot; =&gt; {
            &quot;password&quot; =&gt; &quot;demo&quot;,
            &quot;default_tenant&quot; =&gt; &quot;service&quot;,
            &quot;roles&quot; =&gt; {
              &quot;Member&quot; =&gt; [ &quot;Member&quot; ]
            }
          }
        }
      },
      &quot;image&quot; =&gt; {
        &quot;api&quot; =&gt; {
          &quot;bind_interface&quot; =&gt; &quot;eth1&quot;
        },
        &quot;debug&quot; =&gt; true,
        &quot;identity_service_chef_role&quot; =&gt; &quot;os-identity&quot;,
        &quot;rabbit_server_chef_role&quot; =&gt; &quot;os-ops-messaging&quot;,
        &quot;registry&quot; =&gt; {
          &quot;bind_interface&quot; =&gt; &quot;eth1&quot;
        },
        &quot;syslog&quot; =&gt; {
          &quot;use&quot; =&gt; false
        },
        &quot;upload_image&quot; =&gt; {
          &quot;cirros&quot; =&gt; &quot;http://hypnotoad/cirros-0.3.0-x86_64-disk.img&quot;,
        },
        &quot;upload_images&quot; =&gt; [
          &quot;cirros&quot;
        ]
      },
      &quot;network&quot; =&gt; {
        &quot;api&quot; =&gt; {
          &quot;bind_interface&quot; =&gt; &quot;eth1&quot;,
        }
      },
      &quot;db&quot; =&gt; {
        &quot;bind_interface&quot; =&gt; &quot;eth1&quot;,
        &quot;compute&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;
        },
        &quot;identity&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;
        },
        &quot;image&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;
        },
        &quot;network&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;
        },
        &quot;volume&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;
        },
        &quot;dashboard&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;
        }
      },
      &quot;mq&quot; =&gt; {
        &quot;bind_interface&quot; =&gt; &quot;eth1&quot;,
        &quot;host&quot; =&gt; &quot;10.200.10.46&quot;,
        &quot;user&quot; =&gt; &quot;guest&quot;,
        &quot;vhost&quot; =&gt; &quot;/nova&quot;,
        &quot;servers&quot; =&gt; &quot;10.200.10.46&quot;,
        &quot;compute&quot; =&gt; {
          &quot;service_type&quot; =&gt; &quot;rabbitmq&quot;,
          &quot;rabbit&quot; =&gt; {
            &quot;host&quot; =&gt; &quot;10.200.10.46&quot;,
            &quot;port&quot; =&gt; &quot;5672&quot;
          }
        },
        &quot;block-storage&quot; =&gt; {
          &quot;service_type&quot; =&gt; &quot;rabbitmq&quot;,
          &quot;rabbit&quot; =&gt; {
            &quot;host&quot; =&gt; &quot;10.200.10.46&quot;,
            &quot;port&quot; =&gt; &quot;5672&quot;
          }
        }
      },
      &quot;endpoints&quot; =&gt; {
        &quot;compute-api-bind&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;,
          &quot;scheme&quot; =&gt; &quot;http&quot;,
          &quot;port&quot; =&gt; &quot;8774&quot;,
          &quot;path&quot; =&gt; &quot;/v2/%(tenant_id)s&quot;
        },
        &quot;compute-api&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;,
          &quot;scheme&quot; =&gt; &quot;http&quot;,
          &quot;port&quot; =&gt; &quot;8774&quot;,
          &quot;path&quot; =&gt; &quot;/v2/%(tenant_id)s&quot;
        },
        &quot;compute-ec2-admin-bind&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;,
          &quot;scheme&quot; =&gt; &quot;http&quot;,
          &quot;port&quot; =&gt; &quot;8773&quot;,
          &quot;path&quot; =&gt; &quot;/services/Admin&quot;
        },
        &quot;compute-ec2-admin&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;,
          &quot;scheme&quot; =&gt; &quot;http&quot;,
          &quot;port&quot; =&gt; &quot;8773&quot;,
          &quot;path&quot; =&gt; &quot;/services/Admin&quot;
        },
        &quot;compute-ec2-api-bind&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;,
          &quot;scheme&quot; =&gt; &quot;http&quot;,
          &quot;port&quot; =&gt; &quot;8773&quot;,
          &quot;path&quot; =&gt; &quot;/services/Cloud&quot;
        },
        &quot;compute-ec2-api&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;,
          &quot;scheme&quot; =&gt; &quot;http&quot;,
          &quot;port&quot; =&gt; &quot;8773&quot;,
          &quot;path&quot; =&gt; &quot;/services/Cloud&quot;
        },
        &quot;compute-xvpvnc-bind&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;,
          &quot;scheme&quot; =&gt; &quot;http&quot;,
          &quot;port&quot; =&gt; &quot;6081&quot;,
          &quot;path&quot; =&gt; &quot;/console&quot;
        },
        &quot;compute-xvpvnc&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;,
          &quot;scheme&quot; =&gt; &quot;http&quot;,
          &quot;port&quot; =&gt; &quot;6081&quot;,
          &quot;path&quot; =&gt; &quot;/console&quot;
        },
        &quot;compute-novnc-bind&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;,
          &quot;scheme&quot; =&gt; &quot;http&quot;,
          &quot;port&quot; =&gt; &quot;6080&quot;,
          &quot;path&quot; =&gt; &quot;/vnc_auto.html&quot;
        },
        &quot;compute-novnc&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;,
          &quot;scheme&quot; =&gt; &quot;http&quot;,
          &quot;port&quot; =&gt; &quot;6080&quot;,
          &quot;path&quot; =&gt; &quot;/vnc_auto.html&quot;
        },
        &quot;compute-vnc&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;0.0.0.0&quot;,
          &quot;scheme&quot; =&gt; &quot;http&quot;,
          &quot;port&quot; =&gt; &quot;6080&quot;,
          &quot;path&quot; =&gt; &quot;/vnc_auto.html&quot;
        },
        &quot;image-api-bind&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;,
          &quot;scheme&quot; =&gt; &quot;http&quot;,
          &quot;port&quot; =&gt; &quot;9292&quot;,
          &quot;path&quot; =&gt; &quot;/v2&quot;
        },
        &quot;image-api&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;,
          &quot;scheme&quot; =&gt; &quot;http&quot;,
          &quot;port&quot; =&gt; &quot;9292&quot;,
          &quot;path&quot; =&gt; &quot;/v2&quot;
        },
        &quot;image-registry-bind&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;,
          &quot;scheme&quot; =&gt; &quot;http&quot;,
          &quot;port&quot; =&gt; &quot;9191&quot;,
          &quot;path&quot; =&gt; &quot;/v2&quot;
        },
        &quot;image-registry&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;,
          &quot;scheme&quot; =&gt; &quot;http&quot;,
          &quot;port&quot; =&gt; &quot;9191&quot;,
          &quot;path&quot; =&gt; &quot;/v2&quot;
        },
        &quot;identity-bind&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;,
          &quot;scheme&quot; =&gt; &quot;http&quot;,
          &quot;port&quot; =&gt; &quot;5000&quot;,
          &quot;path&quot; =&gt; &quot;/v2.0&quot;
        },
        &quot;identity-api&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;,
          &quot;scheme&quot; =&gt; &quot;http&quot;,
          &quot;port&quot; =&gt; &quot;5000&quot;,
          &quot;path&quot; =&gt; &quot;/v2.0&quot;
        },
        &quot;identity-admin&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;,
          &quot;scheme&quot; =&gt; &quot;http&quot;,
          &quot;port&quot; =&gt; &quot;35357&quot;,
          &quot;path&quot; =&gt; &quot;/v2.0&quot;
        },
        &quot;volume-api-bind&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;,
          &quot;scheme&quot; =&gt; &quot;http&quot;,
          &quot;port&quot; =&gt; &quot;8776&quot;,
          &quot;path&quot; =&gt; &quot;/v1/%(tenant_id)s&quot;
        },
        &quot;block-storage-api&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;,
          &quot;scheme&quot; =&gt; &quot;http&quot;,
          &quot;port&quot; =&gt; &quot;8776&quot;,
          &quot;path&quot; =&gt; &quot;/v1/%(tenant_id)s&quot;
        },
        &quot;telemetry-api-bind&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;,
          &quot;scheme&quot; =&gt; &quot;http&quot;,
          &quot;port&quot; =&gt; &quot;8777&quot;,
          &quot;path&quot; =&gt; &quot;/v1&quot;
        },
        &quot;telemetry-api&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;,
          &quot;scheme&quot; =&gt; &quot;http&quot;,
          &quot;port&quot; =&gt; &quot;8777&quot;,
          &quot;path&quot; =&gt; &quot;/v1&quot;
        },
        &quot;network-api-bind&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;,
          &quot;scheme&quot; =&gt; &quot;http&quot;,
          &quot;port&quot; =&gt; &quot;9696&quot;,
          &quot;path&quot; =&gt; &quot;/v2&quot;
        },
        &quot;network-api&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;,
          &quot;scheme&quot; =&gt; &quot;http&quot;,
          &quot;port&quot; =&gt; &quot;9696&quot;,
          &quot;path&quot; =&gt; &quot;/v2&quot;
        },
        &quot;orchestration-api-bind&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;,
          &quot;scheme&quot; =&gt; &quot;http&quot;,
          &quot;port&quot; =&gt; &quot;8004&quot;,
          &quot;path&quot; =&gt; &quot;/v1/%(tenant_id)s&quot;
        },
        &quot;orchestration-api&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;,
          &quot;scheme&quot; =&gt; &quot;http&quot;,
          &quot;port&quot; =&gt; &quot;8004&quot;,
          &quot;path&quot; =&gt; &quot;/v1/%(tenant_id)s&quot;
        },
        &quot;orchestration-api-cfn-bind&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;,
          &quot;scheme&quot; =&gt; &quot;http&quot;,
          &quot;port&quot; =&gt; &quot;8000&quot;,
          &quot;path&quot; =&gt; &quot;/v1&quot;
        },
        &quot;orchestration-api-cfn&quot; =&gt; {
          &quot;host&quot; =&gt; &quot;10.200.10.46&quot;,
          &quot;scheme&quot; =&gt; &quot;http&quot;,
          &quot;port&quot; =&gt; &quot;8000&quot;,
          &quot;path&quot; =&gt; &quot;/v1&quot;
        },
        &quot;mq&quot; =&gt; {
          &quot;port&quot; =&gt; &quot;5672&quot;
        }
      }
    }
)
</code></pre>

<p>生成した environment を Chef サーバにアップロードする。</p>

<pre><code class="language-bash">% knife environment from file environments/icehouse-nova-network.rb
</code></pre>

<p>Spiceweasel をインストールする。Spiceweasel は yml ファイルを元に knife の操作
を書き出して、またそれを一気に実行することが出来るツールです。</p>

<pre><code class="language-bash">% gem install spiceweasel --no-ri --no-rdoc
% rbenv rehash
</code></pre>

<p>infrastructure.yml を下記の通り修正する。</p>

<pre><code class="language-bash">berksfile:
    options: '--no-freeze --halt-on-frozen'

cookbooks:
- apache2:
- apt:
- aws:
- build-essential:
- chef_handler:
- database:
- dmg:
- erlang:
- git:
- homebrew:
- iptables:
- logrotate:
- memcached:
- mysql:
- openssl:
- openstack-block-storage:
- openstack-common:
- openstack-compute:
- openstack-dashboard:
- openstack-identity:
- openstack-image:
- openstack-network:
- openstack-object-storage:
- openstack-ops-messaging:
- openstack-ops-database:
- openstack-orchestration:
- openstack-telemetry:
- pacman:
- postgresql:
- python:
- rabbitmq:
- runit:
- selinux:
- statsd:
- windows:
- xfs:
- yum:
- yum-epel:
- yum-erlang_solutions:

roles:
- allinone-compute:
- os-compute-single-controller:
- os-base:
- os-ops-caching:
- os-ops-messaging:
- os-ops-database:
- os-block-storage:
- os-block-storage-api:
- os-block-storage-scheduler:
- os-block-storage-volume:
- os-client:
- os-compute-api:
- os-compute-api-ec2:
- os-compute-api-metadata:
- os-compute-api-os-compute:
- os-compute-cert:
- os-compute-conductor:
- os-compute-scheduler:
- os-compute-setup:
- os-compute-vncproxy:
- os-compute-worker:
- os-dashboard:
- os-identity:
- os-image:
- os-image-api:
- os-image-registry:
- os-image-upload:
- os-telemetry-agent-central:
- os-telemetry-agent-compute:
- os-telemetry-api:
- os-telemetry-collector:
- os-network:
- os-network-server:
- os-network-l3-agent:
- os-network-dhcp-agent:
- os-network-metadata-agent:
- os-network-openvswitch:
- os-object-storage:
- os-object-storage-account:
- os-object-storage-container:
- os-object-storage-management:
- os-object-storage-object:
- os-object-storage-proxy:

environments:
- icehouse-nova-network:

nodes:
- 10.200.10.46:
    run_list: role[os-compute-single-controller]
    options: -N opstall01 -E icehouse-nova-network --sudo -x thirai
- 10.200.10.47:
    run_list: role[os-compute-worker]
    options: -N opstall02 -E icehouse-nova-network --sudo -x thirai
- 10.200.10.48:
    run_list: role[os-compute-worker]
    options: -N opstall03 -E icehouse-nova-network --sudo -x thirai
</code></pre>

<p>※ nodes: 項にはデプロイしたいノードと Roles を割り当て列挙する。</p>

<p>spiceweasel を実行する。この時点ではこれから実行されるコマンドの一覧が表示され
るのみである。</p>

<pre><code class="language-bash">% spiceweasel infrastructure.yml
berks upload --no-freeze --halt-on-frozen -b ./Berksfile
knife cookbook upload apache2 apt aws build-essential chef_handler database dmg erlang git homebrew iptables logrotate memcached mysql openssl openstack-block-storage openstack-common openstack-compute openstack-dashboard openstack-identity openstack-image openstack-network openstack-object-storage openstack-ops-messaging openstack-ops-database openstack-orchestration openstack-telemetry pacman postgresql python rabbitmq runit selinux statsd windows xfs yum yum-epel yum-erlang_solutions
knife environment from file separated.rb
knife role from file allinone-compute.rb os-base.rb os-block-storage-api.rb os-block-storage-scheduler.rb os-block-storage-volume.rb os-block-storage.rb os-client.rb os-compute-api-ec2.rb os-compute-api-metadata.rb os-compute-api-os-compute.rb os-compute-api.rb os-compute-cert.rb os-compute-conductor.rb os-compute-scheduler.rb os-compute-setup.rb os-compute-single-controller.rb os-compute-vncproxy.rb os-compute-worker.rb os-dashboard.rb os-identity.rb os-image-api.rb os-image-registry.rb os-image-upload.rb os-image.rb os-network-dhcp-agent.rb os-network-l3-agent.rb os-network-metadata-agent.rb os-network-openvswitch.rb os-network-server.rb os-network.rb os-object-storage-account.rb os-object-storage-container.rb os-object-storage-management.rb os-object-storage-object.rb os-object-storage-proxy.rb os-object-storage.rb os-ops-caching.rb os-ops-database.rb os-ops-messaging.rb os-telemetry-agent-central.rb os-telemetry-agent-compute.rb os-telemetry-api.rb os-telemetry-collector.rb
knife bootstrap 10.200.10.46 -N opstall01 -E separated --sudo -x thirai -r 'role[os-compute-single-controller]'
knife bootstrap 10.200.10.47 -N opstall02 -E separated --sudo -x thirai -r 'role[os-compute-worker]'
knife bootstrap 10.200.10.48 -N opstall03 -E separated --sudo -x thirai -r 'role[os-compute-worker]'
</code></pre>

<p>-e オプションを付与すると実際にこれらのコマンドが実行される。実行してデプロイを行う。</p>

<pre><code class="language-bash">% spiceweasel -e infrastructure.yml
</code></pre>

<p>この時点で、下記の操作も行われる。</p>

<ul>
<li>Nova-Network 上に仮想ネットワークの生成</li>
<li>OS イメージのダウンロードと登録 (Environment に記したモノ)</li>
</ul>

<h2 id="cinder-の設定">Cinder の設定</h2>

<p>デプロイ完了したところで cinder-volume プロセスは稼働しているが物理ディスクの
アサインが済んでいない。これは Chef では指定出来ないので手動で行う。</p>

<p>予め Cinder 用の物理ディスクを Controller ノードに付与する。(ここでは /dev/sdb1)</p>

<pre><code class="language-bash">controller% sudo -i
controller# pvcreate /dev/sdb1
controller# vgcreate cinder-volumes /dev/sdb1
controller# service cinder-volume restart
</code></pre>

<p>これで完了。</p>

<h2 id="使ってみる">使ってみる</h2>

<p>ではデプロイした OpenStack を使って仮想マシンを作ってみる。</p>

<pre><code class="language-bash">controller% sudo -i
controller# source openrc
controller# nova keypair-add novakey01 &gt; novakey01
controller# chmod 400 novakey01
controller# nova boot --image cirros --flavor 1 --key_name novakey01 cirros01
controller# nova list 
+--------------------------------------+----------+--------+------------+-------------+--------------------+
| ID                                   | Name     | Status | Task State | Power State | Networks           |
+--------------------------------------+----------+--------+------------+-------------+--------------------+
| e6687359-1aef-4105-a8db-894600001610 | cirros01 | ACTIVE | -          | Running     | private=10.200.9.2 |
+--------------------------------------+----------+--------+------------+-------------+--------------------+
controller# ssh -i novakey01 -l cirros 10.200.9.2
vm# ping www.goo.ne.jp
</code></pre>

<p>仮想マシンが生成され The Internet に対して通信が行えたことを確認。</p>

<p>次に仮想ディスクを生成して上記で作成した仮想マシンに付与する。</p>

<pre><code class="language-bash">controller# cinder create --display-name vol01 1
+--------------------------------------+-----------+--------------+------+-------------+----------+--------------------------------------+
|                  ID                  |  Status   | Display Name | Size | Volume Type | Bootable |             Attached to              |
+--------------------------------------+-----------+--------------+------+-------------+----------+--------------------------------------+
| 0f3726a0-c2f5-45bc-bcf2-1f6eb746f5c8 | available |    vol02     |  1   |     None    |  false   | b286387c-2311-4134-ab59-850fee3e4650 |
+--------------------------------------+-----------+--------------+------+-------------+----------+--------------------------------------+
controller# nova volume-attach e6687359-1aef-4105-a8db-894600001610 0f3726a0-c2f5-45bc-bcf2-1f6eb746f5c8 auto
controller# ssh -i novakey01 -l cirros 10.200.9.2
vm# mkfs.ext4 /dev/vdb
vm# mount -t ext4 /dev/vdb /mnt
vm# df -h | grep mnt
/dev/vdb        976M  1.3M  908M   1% /mnt
</code></pre>

<p>仮想ディスクを仮想マシンに付与しマウントできることを確認出来た。</p>

<h2 id="まとめ">まとめ</h2>

<p>今回は Nova-Network 構成の Icehouse を構築出来た。Nova-Network は Havana でサポート終了との話が延期
になっていることは聞いていたが Icehouse でもしっかり動いている。また後に Neutron 構成も調査を行う。
Neutron 構成は下記の Environments を参考にすると動作するかもしれない。</p>

<p><a href="https://github.com/stackforge/openstack-chef-repo/blob/master/environments/vagrant-multi-neutron.json">https://github.com/stackforge/openstack-chef-repo/blob/master/environments/vagrant-multi-neutron.json</a></p>

<p>キーとなるのは、</p>

<pre><code class="language-json">    &quot;network&quot;: {
      &quot;debug&quot;: &quot;True&quot;,
      &quot;dhcp&quot;: {
        &quot;enable_isolated_metadata&quot;: &quot;True&quot;
      },
      &quot;metadata&quot;: {
        &quot;nova_metadata_ip&quot;: &quot;192.168.3.60&quot;
      },
      &quot;openvswitch&quot;: {
        &quot;tunnel_id_ranges&quot;: &quot;1:1000&quot;,
        &quot;enable_tunneling&quot;: &quot;True&quot;,
        &quot;tenant_network_type&quot;: &quot;gre&quot;,
        &quot;local_ip_interface&quot;: &quot;eth2&quot;
      },
      &quot;api&quot;: {
        &quot;bind_interface&quot;: &quot;eth1&quot;
      }
    },
</code></pre>

<p>この辺り。</p>

<h2 id="残っている問題点">残っている問題点</h2>

<p>VNC コンソールにアクセス出来ない。調べたのですが、environment の修正で直せる問
題ではないように見えました。</p>

<p>cookbooks/openstack-compute/templates/default/nova.conf.rb を確認すると下記の
ようになっています。</p>

<pre><code class="language-ruby">##### VNCPROXY #####
novncproxy_base_url=&lt;%= @novncproxy_base_url %&gt;
xvpvncproxy_base_url=&lt;%= @xvpvncproxy_base_url %&gt;

# This is only required on the server running xvpvncproxy
xvpvncproxy_host=&lt;%= @xvpvncproxy_bind_host %&gt;
xvpvncproxy_port=&lt;%= @xvpvncproxy_bind_port %&gt;

# This is only required on the server running novncproxy
novncproxy_host=&lt;%= @novncproxy_bind_host %&gt;
novncproxy_port=&lt;%= @novncproxy_bind_port %&gt;

vncserver_listen=&lt;%= @vncserver_listen %&gt;
vncserver_proxyclient_address=&lt;%= @vncserver_proxyclient_address %&gt;
</code></pre>

<p>vncserver_listen, vncserver_proxyclient_address はそれぞれ</p>

<ul>
<li>@vncserver_listen</li>
<li>@vncserver_proxyclient_address</li>
</ul>

<p>という変数が格納されることになっている。</p>

<p>では cookbooks/openstack-compute/recipes/nova-common.rb を確認すると、</p>

<pre><code class="language-ruby">template '/etc/nova/nova.conf' do
  source 'nova.conf.erb'
  owner node['openstack']['compute']['user']
  group node['openstack']['compute']['group']
  mode 00644
  variables(
    sql_connection: sql_connection,
    novncproxy_base_url: novnc_endpoint.to_s,
    xvpvncproxy_base_url: xvpvnc_endpoint.to_s,
    xvpvncproxy_bind_host: xvpvnc_bind.host,
    xvpvncproxy_bind_port: xvpvnc_bind.port,
    novncproxy_bind_host: novnc_bind.host,
    novncproxy_bind_port: novnc_bind.port,
    vncserver_listen: vnc_endpoint.host,
    vncserver_proxyclient_address: vnc_endpoint.host,
    以下略
</code></pre>

<p>となっている。vncserver_listen, vncserver_proxyclient_address 共に
vnc_endpoint.host が格納されることになっている。vnc_endpont.host は</p>

<pre><code class="language-ruby">vnc_endpoint = endpoint 'compute-vnc' || {}
</code></pre>

<p>となっており、Attributes の</p>

<pre><code class="language-json">    &quot;compute-vnc&quot; =&gt; {
      &quot;host&quot; =&gt; &quot;0.0.0.0&quot;,
      &quot;scheme&quot; =&gt; &quot;http&quot;,
      &quot;port&quot; =&gt; &quot;6080&quot;,
      &quot;path&quot; =&gt; &quot;/vnc_auto.html&quot;
    },
</code></pre>

<p>の設定が入ることになる。つまり上記のような Attributes だと vncserver_listen, vncserver_proxyclient_address 共に
&lsquo;0.0.0.0&rsquo; のアドレスが controller, compute ノードの双方に入ってしまい、NoVNC が正しく格納しないことになる。</p>

<p>解決したらまたここに更新版を載せたいと思いまーす！</p>

<p>以上です。</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2014/04/25/stackforge-openstack-chef-repo-icehouse-deploy/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>

        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2014/04/23/mirantis-openstack/">Mirantis OpenStack (Neutron GRE)を組んでみた！</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2014-04-23'>
            April 23, 2014</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは。<a href="https://twitter.com/jedipunkz">@jedipunkz</a> です。</p>

<p>皆さん、Mirantis OpenStack はご存知ですか？ OpenStack ディストリビューションの
1つです。以下、公式サイトです。</p>

<p><a href="http://software.mirantis.com/main/">http://software.mirantis.com/main/</a></p>

<p>この Mirantis OpenStack を使って OpenStack Havana (Neutron GRE) 構成を作ってみ
ました。その時のメモを書いていきたいと思います。</p>

<h2 id="構成は">構成は?</h2>

<p>構成は下記の通り。</p>

<p><img src="http://jedipunkz.github.io/pix/mirantis_gre.jpg"></p>

<p>※ CreativeCommon</p>

<p>特徴は</p>

<ul>
<li>Administrative Network : Fuel Node, DHCP + PXE ブート用</li>
<li>Management Network : 各コンポーネント間 API 用</li>
<li>Public/Floating IP Network : パブリック API, VM Floating IP 用</li>
<li>Storage Network : Cinder 配下ストレージ &lt;-&gt; インスタンス間用</li>
<li>要インターネット接続 : Public/Floating Networks</li>
<li>Neutron(GRE) 構成</li>
</ul>

<p>です。タグ VLAN 使って物理ネットワークの本数を減らすことも出来るはずですが、僕
の環境では何故かダメだったので上記の4つの物理ネットワークを別々に用意しました。</p>

<h2 id="fuel-ノードの構築">Fuel ノードの構築</h2>

<p>Fuel ノードとは、OpenStack の各ノードをデプロイするための管理ノードのことです。
DHCP + PXE を管理する Cobbler やデプロイツールの Puppet が内部で稼働し、
Administrative Network 上で稼働したノードを管理・その後デプロイします。</p>

<p>構築方法は&hellip;</p>

<p>下記の URL より ISO ファイルをダウンロード。</p>

<p><a href="http://software.mirantis.com/main/">http://software.mirantis.com/main/</a></p>

<p>Administrative Network に NIC を出したノードを上記の ISO から起動。</p>

<p>Grub メニューが表示されたタイミングで &ldquo;Tab&rdquo; キーを押下。</p>

<p><img src="http://jedipunkz.github.io/pix/grub.png"></p>

<p>上記画面にてカーネルオプションにて hostname, ip, gw, dns を修正。下記は例。</p>

<pre><code class="language-bash">vmlinuz initrd=initrd.img biosdevname=0 ks=cdrom:/ks.cfg ip=10.200.10.76
gw=10.200.10.1 dns1=8.8.8.8 netmask=255.255.255.0 hostname=fuel.cpi.ad.jp
showmenu=no_
</code></pre>

<p>ブラウザで <a href="http://10.200.10.76:8080">http://10.200.10.76:8080</a> (上記例)にアクセスし、新しい &lsquo;OpenStack
Environment&rsquo; を作成する。</p>

<pre><code>Name : 任意
OpenStack Release : Havana on CentOS6.4
</code></pre>

<p>なお、Ubuntu 構成も組めるが、私の環境では途中でエラーが出力した。</p>

<p>Next を押下し、ネットワーク設定を行う。今回は &lsquo;Nuetron GRE&rsquo; を選択。</p>

<p><img src="http://jedipunkz.github.io/pix/mirantis_network.png"></p>

<p>&lsquo;Save Settings&rsquo; を押下し設定を保存。この時点では &lsquo;Verify Networks&rsquo; は行えない。
少なくとも 2 ノードが必要。次のステップで2ノードの追加を行う。</p>

<h2 id="ノードの追加">ノードの追加</h2>

<p>下記の4つのネットワークセグメントに NIC を出したノードを用意し、起動する。起動
すると Administrative Network 上で稼働している Cobbler によりノードが PXE 起動
しミニマムな OS がインストールされる。これは後に Fuel ノードよりログインがされ、
各インターフェースの Mac アドレス等の情報を知るためです。ネットワークベリファ
イ等もこのミニマムな OS 越しに実施されます。</p>

<ul>
<li>Administrative Network</li>
<li>Public/Floating IP Network</li>
<li>Storage Network</li>
<li>Management Network</li>
</ul>

<p>ノードが稼働した時点で Fuel によりノードが認識されるので、ここでは2つのノード
をそれぞれ</p>

<ul>
<li>Controller ノード</li>
<li>Compute ノード</li>
</ul>

<p>として画面上で割り当てます。</p>

<h2 id="インターフェースの設定">インターフェースの設定</h2>

<p><img src="http://jedipunkz.github.io/pix/mirantis_mac.png"></p>

<p><a href="http://10.200.10.76:8000/#cluster/1/nodes">http://10.200.10.76:8000/#cluster/1/nodes</a> にログインし作成した Environment
を選択。その後、&rsquo;Nodes&rsquo; タブを押下。ノードを選択し、&rsquo;Configure Interfaces&rsquo; を
選択。各ノードのインターフェースの Mac アドレスを確認し、各々のネットワークセ
グメントを紐付ける。尚、Fuel ノードから &lsquo;root&rsquo; ユーザで SSH(22番ポート) にノン
パスフレーズで公開鍵認証ログインが可能となっている。Fuel ノードに対しては SSH
(22番ポート) にて下記のユーザにてログインが可能です。</p>

<pre><code>username : root
password : r00tme
</code></pre>

<h2 id="ネットワークの確認">ネットワークの確認</h2>

<p><a href="http://10.200.10.76:8000/#cluster/1/network">http://10.200.10.76:8000/#cluster/1/network</a> にて &lsquo;Networks&rsquo; タブを開き、&rsquo;Verify
Networks&rsquo; を押下。ネットワーク設定が正しく行われているか否かを確認。</p>

<h2 id="デプロイ">デプロイ</h2>

<p><a href="http://10.200.10.76:8000/#cluster/1/nodes">http://10.200.10.76:8000/#cluster/1/nodes</a> にて &lsquo;Deploy Changes&rsquo; を押下しデプ
ロイ開始。kickstart にて OS が自動でインストールされ puppet にて fuel ノードか
ら自動で OpenStack がインストールされます。</p>

<h2 id="openstack-へのアクセス">OpenStack へのアクセス</h2>

<p>SSH では下記のステップで OpenStack コントローラノードにログイン。</p>

<p>fuel ノード (SSH) -&gt; node-1 (OpenStack コントローラノード)(SSH)</p>

<p>ブラウザで Horizon にアクセスするには</p>

<p><a href="http://10.200.10.2">http://10.200.10.2</a></p>

<p>にアクセス。これは例。Administrative Network に接続している NIC の IP アドレス
へ HTTP でログイン。</p>

<h2 id="まとめ">まとめ</h2>

<p>Mirantis OpenStack Neutron (GRE) 構成が組めた。上記構成図を見て疑問に思ってい
た &ldquo;VM 間通信のネットワークセグメント&rdquo; であるが、Administrative Network のセグ
メントを用いている事が判った。これは利用者が意図しているとは考えづらいので、正
直、あるべき姿では無いように思える。が、上記構成図に VM ネットワークが無い理由
はこれにて判った。</p>

<p>下記はノード上で ovs-vsctl show コマンドを打った結果の抜粋です。</p>

<pre><code class="language-bash">    Bridge &quot;br-eth1&quot;
        Port &quot;br-eth1&quot;
            Interface &quot;br-eth1&quot;
                type: internal
        Port &quot;br-eth1--br-fw-admin&quot;
            trunks: [0]
            Interface &quot;br-eth1--br-fw-admin&quot;
                type: patch
                options: {peer=&quot;br-fw-admin--br-eth1&quot;}
        Port &quot;eth1&quot;
            Interface &quot;eth1&quot;
</code></pre>

<p>今回の構成は eth1 は Administrative Network に割り当てていました。</p>

<p>一昔前は OS のディストリビュータが有料サポートをビジネスにしていました。Redhat
がその代表格だと思いますが、今は OS 上で何かを実現するにもソフトウェアの完成度
が高く、エンジニアが困るシチュエーションがそれほど無くなった気がします。そこで
その OS の上で稼働する OpenStack のサポートビジネスが出てきたか！という印象で
す。OpenStack はまだまだエンジニアにとって敷居の高いソフトウェアです。自らクラ
ウドプラットフォームを構築出来るのは魅力的ですが、サポート無しに構築・運用する
には、まだ難しい技術かもしれません。こういったディストリビューションが出てくる
辺りが時代だなぁと感じます。</p>

<p>尚、ISO をダウンロードして利用するだけでしたら無償で OK です。</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2014/04/23/mirantis-openstack/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>

        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2014/04/22/geard-port-mapping/">Geard のポートマッピングについて調べてみた</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2014-04-22'>
            April 22, 2014</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは。@jedipunkz です。</p>

<p>今週 Redhat が &lsquo;Redhat Enterprise Linux Atomic Host&rsquo; しましたよね。Docker を特
徴としたミニマムな OS だとのこと。その内部で用いられている技術 Geard について
少し調べてみました。複数コンテナの関連付けが可能なようです。ここでは調べた結果
について簡単にまとめていきます。</p>

<h2 id="参考資料">参考資料</h2>

<p><a href="http://openshift.github.io/geard/deploy_with_geard.html">http://openshift.github.io/geard/deploy_with_geard.html</a></p>

<h2 id="利用方法">利用方法</h2>

<p>ここではホスト OS に Fedora20 を用意します。</p>

<p>まず Geard をインストール</p>

<pre><code class="language-bash">% sudo yum install --enablerepo=updates-testing geard
</code></pre>

<p>下記の json ファイルを作成します。ここにはコンテナをデプロイするための情報と関
連付けのための情報を記します。</p>

<pre><code class="language-json">$ ${EDITOR} rockmongo_mongodb.json
{
  &quot;containers&quot;:[
    {
      &quot;name&quot;:&quot;rockmongo&quot;,
      &quot;count&quot;:1,
      &quot;image&quot;:&quot;derekwaynecarr/rockmongo&quot;,
      &quot;publicports&quot;:[{&quot;internal&quot;:80,&quot;external&quot;:6060}],
      &quot;links&quot;:[{&quot;to&quot;:&quot;mongodb&quot;}]
    },
    {
      &quot;name&quot;:&quot;mongodb&quot;,
      &quot;count&quot;:1,
      &quot;image&quot;:&quot;ccoleman/ubuntu-mongodb&quot;,
      &quot;publicports&quot;:[{&quot;internal&quot;:27017}]
    }
  ]
}
</code></pre>

<p>上記のファイルの解説。</p>

<ul>
<li>コンテナ &lsquo;rockmongo&rsquo; と &lsquo;mongodb&rsquo; を作成</li>
<li>それぞれ1個ずつコンテナを作成</li>
<li>&lsquo;image&rsquo; パラメータにて docker イメージの指定</li>
<li>&lsquo;publicports&rsquo; パラメータにてコンテナ内部とホスト側のポートマッピングを行う</li>
<li>&lsquo;links&rsquo; パラメータで &lsquo;rockmongo&rsquo; を &lsquo;mongodb&rsquo; に関連付け</li>
</ul>

<p>では、デプロイ開始します。</p>

<pre><code class="language-bash">$ sudo gear deploy rockmongo_mongo.json
2014/04/22 07:21:12 Deploying rockmongo_mongo.json.20140422-072112
2014/04/22 07:21:12 appending 27017 on rockmongo-1: &amp;{PortPair:{Internal:27017 External:0} Target:127.0.0.1:27017} &amp;{Id:rockmongo-1 Image:derekwaynecarr/rockmongo From:rockmongo On:0xc2100bb980 Ports:[{PortPair:{Internal:80 External:6060} Target::0}] add:true remove:false container:0xc21000be00 links:[]}
2014/04/22 07:21:12 ports: Releasing
2014/04/22 07:21:12 systemd: Reloading daemon
local PortMapping: 80 -&gt; 6060
local Container rockmongo-1 is installed
2014/04/22 07:21:12 ports: Releasing
2014/04/22 07:21:12 systemd: Reloading daemon
local PortMapping: 27017 -&gt; 4000
local Container mongodb-1 is installed
linking rockmongo: 127.0.0.1:27017 -&gt; localhost:4000
local Links set on local
local Container rockmongo-1 starting
local Container mongodb-1 starting
</code></pre>

<p>ブラウザにてホスト OS に接続することで rockmongo の管理画面にアクセスが可能。</p>

<pre><code>http://&lt;ホストOSのIP:6060&gt;/
</code></pre>

<h2 id="ポートマッピング管理">ポートマッピング管理</h2>

<p>デプロイが完了して、実際に RockMongo の管理画面にアクセス出来たと思います。
関連付けが特徴と言えそうなのでその解析をしてみました。</p>

<p>Geard のコンテナ関連付けはホストとコンテナのポート管理がキモとなっていることが
解ります。これを紐解くことで geard のコンテナ管理を理解します。</p>

<pre><code>                        'rockmongo' コンテナ             'mongodb' コンテナ
+----------+        +--------------------------+        +-------------------+
|  Client  |-&gt;6060-&gt;|-&gt;80-&gt; RockMongo -&gt;27017-&gt;|-&gt;4000-&gt;|-&gt;27017-&gt; Mongodb  |
+----------+        +--------------------------+        +-------------------+
                    |-------------------- docker ホスト --------------------|
</code></pre>

<p>上記 &lsquo;gear deploy&rsquo; コマンド発行時のログと json ファイルにより上図のような構成
になっていることが理解できます。一つずつ読み解いていきましょう。</p>

<ul>
<li>&lsquo;rockmongo&rsquo; コンテナの内部でリスンしている 80 番ポートはホスト OS の 6060 番へ変換</li>
<li>&lsquo;rockmongo&rsquo; コンテナ内で稼働する RockMongo の config.php から &lsquo;127.0.0.1:27017&rsquo; でリスンしていることが解る</li>
</ul>

<p>試しに &lsquo;derekwaynecarr/rockmongo:latest&rsquo; に /bin/bash でログインし config.php を確認。</p>

<pre><code class="language-bash">$ sudo docker run -i -t derekwaynecarr/rockmongo:latest /bin/bash
bash-4.1# grep mongo_host /var/www/html/config.php
$MONGO[&quot;servers&quot;][$i][&quot;mongo_host&quot;] = &quot;127.0.0.1&quot;;//mongo host
$MONGO[&quot;servers&quot;][$i][&quot;mongo_host&quot;] = &quot;127.0.0.1&quot;;
</code></pre>

<ul>
<li>デプロイ時のログと json ファイルの &lsquo;links&rsquo; パラメータより、ホストのポートに動的に(ここでは 4000番ポート) に変換されることが解ります。</li>
</ul>

<pre><code>linking rockmongo: 127.0.0.1:27017 -&gt; localhost:4000
</code></pre>

<ul>
<li>ホストの 4000 番ポートは動的に &lsquo;mongodb&rsquo; コンテナの内部ポート 27017 にマッピングされる</li>
</ul>

<p>これらのポートマッピングによりそれぞれのコンテナの連携が取れている。</p>

<h2 id="まとめと考察">まとめと考察</h2>

<p>Geard は下記の2つを特徴とした技術だと言えるとことがわかりました。</p>

<ul>
<li>コンテナを json で管理しデプロイする仕組みを提供する</li>
<li>コンテナ間の関連付けをホスト OS のポートを動的に管理・マッピングすることで行う
<br /></li>
</ul>

<p>同じような OS に CoreOS <a href="https://coreos.com/">https://coreos.com/</a> がありますが、こちらも docker,
sytemd 等を特徴としています。さらに etcd を使ってクラスタの構成等が可能になっ
ていますが、Geard はホストのポートを動的に管理することで関連付けが可能なことが
わかりました。</p>

<p>実際に触ってみた感覚から言えば、まだまだ実用化は厳しい状況に思えますが、今後へ
の展開に期待したい技術です。</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2014/04/22/geard-port-mapping/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>

        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2014/04/04/openstack-havana-cinder-glance-ceph/">OpenStack Havana Cinder,Glance の分散ストレージ Ceph 連携</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2014-04-04'>
            April 4, 2014</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは！<a href="https://twitter.com/jedipunkz">@jedipunkz</a> です。</p>

<p>今回は Havana 版の OpenStack Glance, Cinder と分散ストレージの Ceph を連携させ
る手順を書いていきます。元ネタはこちら。下記の Ceph の公式サイトに手順です。</p>

<p><a href="https://ceph.com/docs/master/rbd/rbd-openstack/">https://ceph.com/docs/master/rbd/rbd-openstack/</a></p>

<p>この手順から下記の変更を行って、ここでは記していきます。</p>

<ul>
<li>Nova + Ceph 連携させない</li>
<li>cinder-backup は今のところ動作確認出来ていないので省く</li>
<li>諸々の手順がそのままでは実施出来ないので補足を入れていく。</li>
</ul>

<p>cinder-backup は Cinder で作成した仮想ディスクのバックアップを Ceph ストレージ
上に取ることが出来るのですが、そもそも Ceph 上にある仮想ディスクを Ceph にバッ
クアップ取っても意味が薄いですし、まだ動いていないので今回は省きます。LVM やそ
の他ストレージを使った Cinder 連携をされている方にとっては cinder-backup の
Ceph 連携は意味が大きくなってくると思います。</p>

<h2 id="構成">構成</h2>

<p>下記の通りの物理ネットワーク6つの構成です。
OpenStack, Ceph 共に最小構成を前提にします。</p>

<pre><code>                  +--------------------------------------------------------------------- external
                  |
+--------------+--(-----------+--------------+------------------------------------------ public
|              |  |           |              |
+------------+ +------------+ +------------+ +------------+ +------------+ +------------+
| controller | |  network   | |  compute   | |   ceph01   | |   ceph02   | |   ceph03   |
+------------+ +------------+ +------------+ +------------+ +------------+ +------------+
|  |           |  |           |  |  |        |  |  |        |  |  |        |  |  |
+--------------+--(-----------+--(-----------+--(--(--------+--(--(--------+--(--(------- management
   |              |              |  |           |  |           |  |           |  |
   |              +--------------+--(-----------(--(-----------(--(-----------(--(------- guest
   |                                |           |  |           |  |           |  |
   +--------------------------------+-----------+--(-----------+--(-----------+--(------- storage
                                                   |              |              |
                                                   +--------------+--------------+------- cluster
</code></pre>

<p>特徴</p>

<ul>
<li>最小構成 controller x 1 + network x 1 + compute x 1 + ceph x 3</li>
<li>OpenStack API の相互通信は management ネットワーク</li>
<li>OpenStack VM 間通信は guest ネットワーク</li>
<li>OpenStack 外部通信は public ネットワーク</li>
<li>OpenStack VM の floating-ip (グローバル) 通信は external ネットワーク</li>
<li>Ceph と OpenStack 間の通信は storage ネットワーク</li>
<li>Ceph の OSD 間通信は cluster ネットワーク</li>
<li>ここには記されていないホスト &lsquo;workstation&rsquo; から OpenStack, Ceph をデプロイ</li>
</ul>

<h2 id="前提の構成">前提の構成</h2>

<p>前提構成の OpenStack と Ceph ですが、ここでは構築方法は割愛します。OpenStack
は rcbops/chef-cookbooks を。Ceph は ceph-deploy を使って自分は構築しました。
下記の自分のブログに構築手順が載っているので参考にしてみてください。</p>

<p>OpenStack 構築方法</p>

<p><a href="http://jedipunkz.github.io/blog/2013/11/17/openstack-havana-chef-deploy/">http://jedipunkz.github.io/blog/2013/11/17/openstack-havana-chef-deploy/</a></p>

<p>Ceph 構築方法</p>

<p><a href="http://jedipunkz.github.io/blog/2014/02/27/journal-ssd-ceph-deploy/">http://jedipunkz.github.io/blog/2014/02/27/journal-ssd-ceph-deploy/</a></p>

<h2 id="openstack-ceph-連携手順">OpenStack + Ceph 連携手順</h2>

<p>では実際に連携するための手順を記していきます。</p>

<h4 id="rbd-pool-の作成">rbd pool の作成</h4>

<p>Ceph ノードの何れかで下記の手順を実施し rbd pool を作成する。</p>

<pre><code class="language-bash">ceph01% sudo ceph osd pool create volumes 128
ceph01% sudo ceph osd pool create images 128
ceph01% sudo ceph osd pool create backups 128
</code></pre>

<h4 id="ssh-鍵の配置">ssh 鍵の配置</h4>

<p>Ceph ノードから OpenStack の controller, compute ノードへ接続出来るよう鍵を配
布します。</p>

<pre><code class="language-bash">ceph01% ssh-copy-id &lt;username&gt;@&lt;controller_ip&gt;
ceph01% ssh-copy-id &lt;username&gt;@&lt;compute_ip&gt;
</code></pre>

<h4 id="sudoers-の設定">sudoers の設定</h4>

<p>controller, compute ノード上で sudoers の設定を予め実施する。
/etc/sudoers.d/<username> として保存する。</p>

<pre><code class="language-bash">&lt;username&gt; ALL = (root) NOPASSWD:ALL
</code></pre>

<h4 id="ceph-パッケージのインストール">ceph パッケージのインストール</h4>

<p>controller ノード, compute ノード上で Ceph パッケージをインストールする。</p>

<pre><code class="language-bash">controller% wget -q -O- 'https://ceph.com/git/?p=ceph.git;a=blob_plain;f=keys/release.asc' | sudo apt-key add -
controller% echo deb http://ceph.com/debian/ $(lsb_release -sc) main | sudo tee /etc/apt/sources.list.d/ceph.list
controller% sudo apt-get update &amp;&amp; sudo apt-get install -y python-ceph ceph-common

compute% wget -q -O- 'https://ceph.com/git/?p=ceph.git;a=blob_plain;f=keys/release.asc' | sudo apt-key add -
compute% echo deb http://ceph.com/debian/ $(lsb_release -sc) main | sudo tee /etc/apt/sources.list.d/ceph.list
compute% sudo apt-get update &amp;&amp; sudo apt-get install -y python-ceph ceph-common
</code></pre>

<p>controller ノード, compute ノード上でディレクトリを作成する。</p>

<pre><code class="language-bash">controller% sudo mkdir /etc/ceph
compute   % sudo mkdir /etc/ceph
</code></pre>

<p>ceph.conf を controller, compute ノードへ配布する。</p>

<pre><code class="language-bash">ceph01% sudo -i
ceph01# ssh &lt;controller_ip&gt; sudo tee /etc/ceph/ceph.conf &lt;/etc/ceph/ceph.conf
ceph01# ssh &lt;compute_ip&gt; sudo tee /etc/ceph/ceph.conf &lt;/etc/ceph/ceph.conf
</code></pre>

<h4 id="ceph-上にユーザを作成">ceph 上にユーザを作成</h4>

<p>Ceph 上に cinder, glance 用の新しいユーザを作成する。</p>

<pre><code class="language-bash">ceph auth get-or-create client.cinder mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool=volumes, allow rx pool=images'
ceph auth get-or-create client.glance mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool=images'
ceph auth get-or-create client.cinder-backup mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool=backups'
</code></pre>

<h4 id="キーリングの作成と配置">キーリングの作成と配置</h4>

<p>client.cinder, client.glance, client.cinder-backup のキーリングを作成する。また作成したキーリングを
controller ノードに配布する。</p>

<pre><code class="language-bash">ceph01% sudo ceph auth get-or-create client.glance | ssh {your-glance-api-server} sudo tee /etc/ceph/ceph.client.glance.keyring
ceph01% ssh {your-glance-api-server} sudo chown glance:glance /etc/ceph/ceph.client.glance.keyring
ceph01% sudo ceph auth get-or-create client.cinder | ssh {your-volume-server} sudo tee /etc/ceph/ceph.client.cinder.keyring
ceph01% ssh {your-cinder-volume-server} sudo chown cinder:cinder /etc/ceph/ceph.client.cinder.keyring
ceph01% sudo ceph auth get-or-create client.cinder-backup | ssh {your-cinder-backup-server} sudo tee /etc/ceph/ceph.client.cinder-backup.keyring
ceph01% ssh {your-cinder-backup-server} sudo chown cinder:cinder /etc/ceph/ceph.client.cinder-backup.keyring
</code></pre>

<p>client.cinder のキーリングを compute ノードに配置する。</p>

<pre><code class="language-bash">ceph01% sudo ceph auth get-key client.cinder | ssh {your-compute-node} tee client.cinder.key
</code></pre>

<h4 id="libvirt-への-secret-キー追加">libvirt への secret キー追加</h4>

<p>compute ノード上で secret キーを libvirt に追加する。</p>

<pre><code class="language-bash">compute% uuidgen
457eb676-33da-42ec-9a8c-9293d545c337

cat &gt; secret.xml &lt;&lt;EOF
&lt;secret ephemeral='no' private='no'&gt;
  &lt;uuid&gt;457eb676-33da-42ec-9a8c-9293d545c337&lt;/uuid&gt;
  &lt;usage type='ceph'&gt;
    &lt;name&gt;client.cinder secret&lt;/name&gt;
  &lt;/usage&gt;
&lt;/secret&gt;
EOF
compute % sudo virsh secret-define --file secret.xml
Secret 457eb676-33da-42ec-9a8c-9293d545c337 created
compute% sudo virsh secret-set-value --secret 457eb676-33da-42ec-9a8c-9293d545c337 --base64 $(cat client.cinder.key) &amp;&amp; rm client.cinder.key secret.xml
</code></pre>

<h4 id="glance-連携手順">glance 連携手順</h4>

<p>controller ノードの /etc/glance/glance-api.conf に下記を追記。
default_store=file と標準ではなっているので下記の通り rbd に書き換える。</p>

<pre><code>default_store=rbd
rbd_store_user=glance
rbd_store_pool=images
</code></pre>

<h4 id="cinder-連携手順">cinder 連携手順</h4>

<p>controller ノードの /etc/cinder/cinder.conf に下記を追記。
volume_driver は予め LVM の設定が入っていると思われるので書き換える。
また rbd_secret_uuid は先ほど生成した uuid を記す。</p>

<pre><code>volume_driver=cinder.volume.drivers.rbd.RBDDriver
rbd_pool=volumes
rbd_ceph_conf=/etc/ceph/ceph.conf
rbd_flatten_volume_from_snapshot=false
rbd_max_clone_depth=5
glance_api_version=2
rbd_user=cinder
rbd_secret_uuid=457eb676-33da-42ec-9a8c-9293d545c337
</code></pre>

<h4 id="ceph-conf-への追記">ceph.conf への追記</h4>

<p>上記で配布した ceph.conf にはキーリングのパスが記されていない。controller ノー
ド上の /etc/ceph/ceph.conf に下記の通り追記する。</p>

<p>ここは公式サイトには印されていないのでハマりました。ポイントです。</p>

<pre><code>[client.keyring]
  keyring = /etc/ceph/ceph.client.cinder.keyring
</code></pre>

<h4 id="openstack-のそれぞれのコンポーネントを再起動かける">OpenStack のそれぞれのコンポーネントを再起動かける</h4>

<pre><code class="language-bash">controller% sudo glance-control api restart
compute   % sudo service nova-compute restart
controller% sudo service cinder-volume restart
controller% sudo service cinder-backup restart
</code></pre>

<h2 id="動作確認">動作確認</h2>

<p>では動作確認を。Glance に OS イメージを登録。その後そのイメージを元にインスタ
ンスを作成。Cinder 上に仮想ディスクを作成。その後先ほど作成したインスタンスに
接続しマウント。そのマウントした仮想ディスク上で書き込みが行えるか確認をします。</p>

<p>テストで Ubuntu Precise 12.04 のイメージを glance を用いて登録する。</p>

<pre><code class="language-bash">controller% wget http://cloud-images.ubuntu.com/precise/current/precise-server-cloudimg-amd64-disk1.img
controller% glance image-create --name precise-image --is-public true --container-format bare --disk-format qcow2 &lt; precise-server-cloudimg-amd64-disk1.img
</code></pre>

<p>テスト VM を稼働する。</p>

<pre><code class="language-bash">controller% nova boot --nic net-id=&lt;net_id&gt; --flavor 2 --image precise-image --key_name novakey01 vm01
controller% cinder create --display-name vol01 1
controller% nova volume-attach &lt;instance_id&gt; &lt;volume_id&gt; auto
</code></pre>

<p>テスト VM へログインしファイルシステムを作成後、マウントする。</p>

<pre><code class="language-bash">controller% ssh -i &lt;key_file_path&gt; -l ubuntu &lt;instance_ip&gt;
vm01% sudo mkfs -t ext4 /dev/vdb
vm01% sudo mount -t ext4 /dev/vdb /mnt
</code></pre>

<p>マウントした仮想ディスク上でデータを書き込んでみる。</p>

<pre><code class="language-bash">vm01% sudo dd if=/dev/zero of=/mnt/500M bs=1M count=5000
</code></pre>

<h2 id="まとめ">まとめ</h2>

<p>Ceph, Cinder の Ceph 連携が出来ました！</p>

<p>OpenStack Grizzly 版時代に Ceph 連携は取っていたのですが Havana では</p>

<ul>
<li>cinder-bacup の Ceph 連携</li>
<li>nova の Ceph 連携</li>
</ul>

<p>が追加されていました。Nova 連携をとるとインスタンスを稼働させる際に通常は
controller ノードの /var/lib/nova 配下にファイルとしてインスタンスイメージが作
成されますが、これが Ceph 上に作成されるとのことです。Nova 連携は是非とってみ
たいのですが、今のところ動いていません。引き続き調査を行います。</p>

<p>cinder-backup も少し連携取ってみましたが backup_driver に Ceph ドライバの指定
をしているにも関わらず Swift に接続しにいってしまう有り様でした..。こちらも引
き続き調査します。またステートが &lsquo;in-use&rsquo; の場合バックアップが取れず
&lsquo;available&rsquo; なステートでないといけないようです。確かに書き込み中に操作が行われ
てしまってもバックアップの整合性が取れないですし、ここは仕方ないところですね。</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2014/04/04/openstack-havana-cinder-glance-ceph/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>

        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2014/03/16/rcbops-chef-cookbooks-keystone-havana-swift-1-10-0.deploy/">rcbops/chef-cookbooks で Keystone 2013.2.2(Havana) &#43; Swift 1.10.0 を構築</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2014-03-16'>
            March 16, 2014</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは。<a href="https://twitter.com/jedipunkz">@jedipunkz</a> です。</p>

<h4 id="追記">追記</h4>

<p>2014/03/20 : 一旦削除していた記事なのですが、無事動作が確認出来たので再度アッ
プします！</p>

<p>第17回 OpenStack 勉強会で rcbops/chef-cookbooks の話をしてきたのですが会場から
質問がありました。「Havana の Swift 構成を作る Cookbooks はどこにありますか？」
と。私が試したのが Grizzly 時代のモノで、よく rcbops/chef-cookbooks を見てみる
と Havana ブランチ又は Havana に対応したリリースタグのファイル構成に Swift が
綺麗サッパリ消えているではありませんか・・！下記の Swift の Cookbooks は幸い
github 上に残っていました。</p>

<p><a href="https://github.com/rcbops-cookbooks/swift">https://github.com/rcbops-cookbooks/swift</a></p>

<p>が rcbops/chef-cookbooks との関連付けが切れています・・。ぐあぁ。</p>

<p>ということで Havana 構成の Keystone 2013.2.2 と Swift 1.10.0 の構成を Chef で
作らねば！と思い色々試していたら結構あっさりと出来ました。今回はその方法を書い
ていきたいと思います！</p>

<h2 id="構成">構成</h2>

<p>構成は&hellip;以前の記事 <a href="http://jedipunkz.github.io/blog/2013/10/27/swift-chef/">http://jedipunkz.github.io/blog/2013/10/27/swift-chef/</a> と同じです。</p>

<pre><code>+-----------------+
|  load balancer  |
+-----------------+
|
+-------------------+-------------------+-------------------+-------------------+---------------------- proxy network
|                   |                   |                   |                   |                   
+-----------------+ +-----------------+ +-----------------+ +-----------------+ +-----------------+
|   chef server   | | chef workstation| |   swift-mange   | |  swift-proxy01  | |  swift-proxy02  | 
+-----------------+ +-----------------+ +-----------------+ +-----------------+ +-----------------+ ...&gt; scaling
|                   |                   |                   |                   |                   
+-------------------+-------------------+-------------------+-------------------+-------------------+-- storage network
|                   |                   |                   |                   |                   |
+-----------------+ +-----------------+ +-----------------+ +-----------------+ +-----------------+ +-----------------+ 
| swift-storage01 | | swift-storage02 | | swift-storage03 | | swift-account01 | | swift-account02 | | swift-account03 |
+-----------------+ +-----------------+ +-----------------+ +-----------------+ +-----------------+ +-----------------+ ..&gt; scaling
</code></pre>

<h2 id="手順">手順</h2>

<p>では早速手順を記していきますね。毎回なのですが Chef ワークステーション・Chef
サーバの環境構築については割愛します。オムニバスインストーラを使えば Chef サー
バの構築は簡単ですし、ワークステーションの構築も Ruby インストール -&gt; gem で
Chef をインストール -&gt; .chef 配下を整える、で出来ます。</p>

<p>rcbops/chef-cookbooks の取得。現在 Stable バージョンの refs/tags/v4.2.1 を用いる。</p>

<pre><code class="language-bash">% git clone https://github.com/rcbops/chef-cookbooks.git ./chef-cookbooks-4.2.1
% cd ./chef-cookbooks-4.2.1
% git checkout -b v4.2.1 refs/tags/v4.2.1
% git submodule init
% git submodule sync
% git submodule update
</code></pre>

<p>ここで本家 rcbops/chef-cookbooks と関連付けが消えている rcbops-cookbooks/swift
を cookbooks ディレクトリ配下にクローンします。あと念のため &lsquo;havana&rsquo; ブランチ
を指定します。コードを確認したところ何も変化はありませんでしたが。</p>

<pre><code class="language-bash">% git clone https://github.com/rcbops-cookbooks/swift.git cookbooks/swift
% cd cookbooks/swift
% git checkout -b havana remotes/origin/havana
% cd ../..
</code></pre>

<p>cookbooks, roles の Chef サーバへのアップロードを行います。</p>

<pre><code class="language-bash">% knife cookbook upload -o cookbooks -a
% knife role from file role/*.rb
</code></pre>

<p>今回の構成 (1クラスタ) 用の environments/swift-havana.json を作成します。json
ファイルの名前は任意です。</p>

<pre><code class="language-json">{
  &quot;name&quot;: &quot;swift-havana&quot;,
  &quot;description&quot;: &quot;&quot;,
  &quot;cookbook_versions&quot;: {
  },
  &quot;json_class&quot;: &quot;Chef::Environment&quot;,
  &quot;chef_type&quot;: &quot;environment&quot;,
  &quot;default_attributes&quot;: {
  },
  &quot;override_attributes&quot;: {
    &quot;package_component&quot;: &quot;havana&quot;,
    &quot;osops_networks&quot;: {
      &quot;management&quot;: &quot;10.200.9.0/24&quot;,
      &quot;public&quot;: &quot;10.200.10.0/24&quot;,
      &quot;swift&quot;: &quot;10.200.9.0/24&quot;
    },
    &quot;keystone&quot;: {
      &quot;pki&quot;: {
        &quot;enabled&quot;: false
      },
      &quot;admin_port&quot;: &quot;35357&quot;,
      &quot;admin_token&quot;: &quot;admin&quot;,
      &quot;admin_user&quot;: &quot;admin&quot;,
      &quot;tenants&quot;: [
        &quot;admin&quot;,
        &quot;service&quot;
      ],
      &quot;users&quot;: {
        &quot;admin&quot;: {
          &quot;password&quot;: &quot;secrete&quot;,
          &quot;roles&quot;: {
            &quot;admin&quot;: [
              &quot;admin&quot;
            ]
          }
        },
        &quot;demo&quot;: {
          &quot;password&quot;: &quot;demo&quot;,
          &quot;default_tenant&quot; : &quot;service&quot;,
          &quot;roles&quot;: {
            &quot;admin&quot;: [ &quot;admin&quot; ]
          }
        }
      },
      &quot;db&quot;: {
        &quot;password&quot;: &quot;keystone&quot;
      }
    },
    &quot;mysql&quot;: {
      &quot;root_network_acl&quot;: &quot;%&quot;,
      &quot;allow_remote_root&quot;: true,
      &quot;server_root_password&quot;: &quot;secrete&quot;,
      &quot;server_repl_password&quot;: &quot;secrete&quot;,
      &quot;server_debian_password&quot;: &quot;secrete&quot;
    },
    &quot;monitoring&quot;: {
      &quot;procmon_provider&quot;: &quot;monit&quot;,
      &quot;metric_provider&quot;: &quot;collectd&quot;
    },
    &quot;vips&quot;: {
      &quot;keystone-admin-api&quot;: &quot;10.200.9.11&quot;,
      &quot;keystone-service-api&quot;: &quot;10.200.9.11&quot;,
      &quot;keystone-internal-api&quot;: &quot;10.200.9.11&quot;,
      &quot;swift-proxy&quot;: &quot;10.200.9.11&quot;,
      &quot;config&quot;: {
        &quot;10.200.9.112&quot;: {
          &quot;vrid&quot;: 12,
          &quot;network&quot;: &quot;management&quot;
        }
      }
    },
    &quot;developer_mode&quot;: false,
    &quot;swift&quot;: {
      &quot;swift_hash&quot;: &quot;307c0568ea84&quot;,
      &quot;authmode&quot;: &quot;keystone&quot;,
      &quot;authkey&quot;: &quot;20281b71-ce89-4b27-a2ad-ad873d3f2760&quot;
    }
  }
}
</code></pre>

<p>作成した environment ファイル environments/swift-havana.json を chef-server へアップ
ロードする。</p>

<pre><code class="language-bash">% knife environment from file environments/swift-havana.json
</code></pre>

<h2 id="cookbooks-の修正">Cookbooks の修正</h2>

<p>swift cookbooks を修正します。havana からは keystone を扱うクライアントは
keystone.middleware.auth_token でなく keystoneclient.middleware.auth_token を
使うように変更掛かっていますので、下記のように修正しました。</p>

<pre><code class="language-bash">% cd cookbooks/swift/templates/default
% diff -u proxy-server.conf.erb.org proxy-server.conf.erb
--- proxy-server.conf.erb.org   2014-03-20 16:28:28.000000000 +0900
+++ proxy-server.conf.erb       2014-03-20 16:28:13.000000000 +0900
@@ -243,7 +243,8 @@
 use = egg:swift#keystoneauth
 
 [filter:authtoken]
-paste.filter_factory = keystone.middleware.auth_token:filter_factory
+#paste.filter_factory = keystone.middleware.auth_token:filter_factory
+paste.filter_factory = keystoneclient.middleware.auth_token:filter_factory
 auth_host = &lt;%= @keystone_api_ipaddress %&gt;
 auth_port = &lt;%= @keystone_admin_port %&gt;
 auth_protocol = &lt;%= @keystone_admin_protocol %&gt;
% cd ../../../..
</code></pre>

<h2 id="デプロイ">デプロイ</h2>

<p>かきのとおり knife bootstrap する。</p>

<pre><code class="language-bash">% knife bootstrap &lt;manage_ip_addr&gt; -N swift-manage -r 'role[base]','role[mysql-master]','role[keystone]','role[swift-management-server]' -E swift-havana --sudo -x thirai
% knife bootstrap &lt;proxy01_ip_addr&gt; -N swift-proxy01 -r &quot;role[base]&quot;,&quot;role[swift-proxy-server]&quot;,'role[swift-setup]','role[openstack-ha]' -E swift-havana --sudo -x thirai
% knife bootstrap &lt;proxy02_ip_addr&gt; -N swift-proxy02 -r &quot;role[base]&quot;,&quot;role[swift-proxy-server]&quot;,'role[openstack-ha]' -E swift-havana --sudo -x thirai
% knife bootstrap &lt;storage01_ip_addr&gt; -N swift-storage01 -r 'role[base]','role[swift-object-server]' -E swift-havana --sudo -x thirai
% knife bootstrap &lt;storage02_ip_addr&gt; -N swift-storage02 -r 'role[base]','role[swift-object-server]' -E swift-havana --sudo -x thirai
% knife bootstrap &lt;storage03_ip_addr&gt; -N swift-storage03 -r 'role[base]','role[swift-object-server]' -E swift-havana --sudo -x thirai
% knife bootstrap &lt;account01_ip_addr&gt; -N swift-account01 -r 'role[base]','role[swift-account-server]','role[swift-container-server]' -E swift-havana --sudo -x thirai
% knife bootstrap &lt;account02_ip_addr&gt; -N swift-account02 -r 'role[base]','role[swift-account-server]','role[swift-container-server]' -E swift-havana --sudo -x thirai
% knife bootstrap &lt;account03_ip_addr&gt; -N swift-account03 -r 'role[base]','role[swift-account-server]','role[swift-container-server]' -E swift-havana --sudo -x thirai
</code></pre>

<p>ここでバグ対策。Swift 1.10.0 にはバグがあるので下記の通り対処します。</p>

<p>keystone.middleware.s3_token に既知のバグがあり、下記のように対処します。この
状態ではバグにより swift-proxy が稼働してない状態ですが後の各ノードでの
chef-client 実行時に稼働する予定です。</p>

<pre><code class="language-python">% diff /usr/lib/python2.7/dist-packages/keystone/exception.py.org /usr/lib/python2.7/dist-packages/keystone/exception.py 
--- exception.py.org    2014-03-12 16:45:00.181420694 +0900
+++ exception.py        2014-03-12 16:44:47.173177081 +0900
@@ -18,6 +18,7 @@

 from keystone.common import config
 from keystone.openstack.common import log as logging
+from keystone.openstack.common.gettextutils import _
 from keystone.openstack.common import strutils
</code></pre>

<p>上記のバグ報告は下記の URL にあります。</p>

<p><a href="https://bugs.launchpad.net/ubuntu/+source/swift/+bug/1231339">https://bugs.launchpad.net/ubuntu/+source/swift/+bug/1231339</a></p>

<p>zone 番号を付与します。</p>

<pre><code class="language-bash">% knife exec -E &quot;nodes.find(:name =&gt; 'swift-storage01') {|n| n.set['swift']['zone'] = '1'; n.save }&quot;
% knife exec -E &quot;nodes.find(:name =&gt; 'swift-account01') {|n| n.set['swift']['zone'] = '1'; n.save }&quot;
% knife exec -E &quot;nodes.find(:name =&gt; 'swift-storage02') {|n| n.set['swift']['zone'] = '2'; n.save }&quot;
% knife exec -E &quot;nodes.find(:name =&gt; 'swift-account02') {|n| n.set['swift']['zone'] = '2'; n.save }&quot;
% knife exec -E &quot;nodes.find(:name =&gt; 'swift-storage03') {|n| n.set['swift']['zone'] = '3'; n.save }&quot;
% knife exec -E &quot;nodes.find(:name =&gt; 'swift-account03') {|n| n.set['swift']['zone'] = '3'; n.save }&quot;
</code></pre>

<p>zone 番号が付与されたこと下記の通りを確認します</p>

<p>account-server の確認</p>

<pre><code class="language-bash">% knife exec -E 'search(:node,&quot;role:swift-account-server&quot;) \
  { |n| z=n[:swift][:zone]||&quot;not defined&quot;; puts &quot;#{n.name} has the role \
  [swift-account-server] and is in swift zone #{z}&quot;; }'
swift-account01 has the role       [swift-account-server] and is in swift zone 1
swift-account02 has the role       [swift-account-server] and is in swift zone 2
swift-account03 has the role       [swift-account-server] and is in swift zone 3
</code></pre>

<p>container-server の確認</p>

<pre><code class="language-bash">% knife exec -E 'search(:node,&quot;role:swift-container-server&quot;) \
  { |n| z=n[:swift][:zone]||&quot;not defined&quot;; puts &quot;#{n.name} has the role \
  [swift-container-server] and is in swift zone #{z}&quot;; }'
swift-account01 has the role       [swift-container-server] and is in swift zone 1
swift-account02 has the role       [swift-container-server] and is in swift zone 2
swift-account03 has the role       [swift-container-server] and is in swift zone 3
</code></pre>

<p>object-server の確認</p>

<pre><code class="language-bash">% knife exec -E 'search(:node,&quot;role:swift-object-server&quot;) \
  { |n| z=n[:swift][:zone]||&quot;not defined&quot;; puts &quot;#{n.name} has the role \
  [swift-object-server] and is in swift zone #{z}&quot;; }'
swift-storage01 has the role   [swift-object-server] and is in swift zone 1
swift-storage02 has the role   [swift-object-server] and is in swift zone 2
swift-storage03 has the role   [swift-object-server] and is in swift zone 3
</code></pre>

<p>Chef が各々のノードに搭載された Disk を検知出来るか否かを確認する。</p>

<pre><code class="language-ruby">% knife exec -E \
  'search(:node,&quot;role:swift-object-server OR \
  role:swift-account-server \
  OR role:swift-container-server&quot;) \
  { |n| puts &quot;#{n.name}&quot;; \
  begin; n[:swift][:state][:devs].each do |d| \
  puts &quot;\tdevice #{d[1][&quot;device&quot;]}&quot;; \
  end; rescue; puts \
  &quot;no candidate drives found&quot;; end; }'
    swift-storage02
            device sdb1
    swift-storage03
            device sdb1
    swift-account01
            device sdb1
    swift-account02
            device sdb1
    swift-account03
            device sdb1
    swift-storage01
            device sdb1
</code></pre>

<p>swift-manage ノードにて chef-client を実行し
/etc/swift/ring-workspace/generate-rings.sh を更新します。</p>

<pre><code class="language-bash">swift-manage% sudo chef-client
</code></pre>

<p>generate-rings.sh の &lsquo;exit 0&rsquo; 行をコメントアウトし実行します。</p>

<pre><code class="language-bash">swift-manage% sudo ${EDITOR} /etc/swift/ring-workspace/generage-rings.sh
swift-manage% sudo /etc/swift/ring-workspace/generate-rings.sh
</code></pre>

<p>この操作で /etc/swift/ring-workspace/rings 配下に account, container, object
用の Rings ファイル群が生成されたことを確認出来るはずです。これらを
swift-manage 上で既に稼働している git サーバに push し管理します。</p>

<pre><code class="language-bash">swift-manage# cd /etc/swift/ring-workspace/rings
swift-manage# git add account.builder container.builder object.builder
swift-manage# git add account.ring.gz container.ring.gz object.ring.gz
swift-manage# git commit -m &quot;initial commit&quot;
swift-manage# git push
</code></pre>

<p>各々のノードにて chef-client を実行することで git サーバ上の Rings ファイル群
を取得し、swift プロセスを稼働させます。</p>

<pre><code class="language-bash">swift-proxy01# chef-client
swift-proxy02# chef-client
swift-storage01# chef-client
swift-storage02# chef-client
swift-storage03# chef-client
swift-account01# chef-client
swift-account02# chef-client
swift-account03# chef-client
</code></pre>

<p>3台のノードが登録されたかどうかを下記の通り確認行います。</p>

<pre><code class="language-bash">swift-proxy01% sudo swift-recon --md5
[sudo] password for thirai:
===============================================================================
--&gt; Starting reconnaissance on 3 hosts
===============================================================================
[2013-10-18 11:14:43] Checking ring md5sums
3/3 hosts matched, 0 error[s] while checking hosts.
===============================================================================
</code></pre>

<h2 id="動作確認">動作確認</h2>

<p>構築が出来ました！ということで動作の確認をしてみましょう。</p>

<p>テストコンテナ &lsquo;container01&rsquo; にテストファイル &lsquo;test&rsquo; をアップロードしてみる。</p>

<pre><code class="language-bash">swift-storage01% swift -V 2 -A http://&lt;ip_addr_keystone&gt;:5000/v2.0/ -U admin:admin -K secrete stat
swift-storage01% swift -V 2 -A http://&lt;ip_addr_keystone&gt;:5000/v2.0/ -U admin:admin -K secrete post container01
swift-storage01% echo &quot;test&quot; &gt; test
swift-storage01% swift -V 2 -A http://&lt;ip_addr_keystone&gt;:5000/v2.0/ -U admin:admin -K secrete upload container01 test
swift-storage01% swift -V 2 -A http://&lt;ip_addr_keystone&gt;:5000/v2.0/ -U admin:admin -K secrete list
container01
swift-storage01% swift -V 2 -A http://&lt;ip_addr_keystone&gt;:5000/v2.0/ -U admin:admin -K secrete list container01 test
</code></pre>

<h2 id="まとめ">まとめ</h2>

<p>前回「実用的な Swift 構成を Chef でデプロイ」の記事で記した内容とほぼ手順は変
わりませんでした。rcbops-cookbooks/rcbops-utils 内にソフトウェアの取得先レポジ
トリを記すレシピが下記の場所にあります。</p>

<p><a href="https://github.com/rcbops-cookbooks/osops-utils/blob/master/recipes/packages.rb">https://github.com/rcbops-cookbooks/osops-utils/blob/master/recipes/packages.rb</a></p>

<p>そして havana ブランチの attributes を確認すると Ubuntu Cloud Archive の URL
が記されていることが確認出来ます。下記のファイルです。</p>

<p><a href="https://github.com/rcbops-cookbooks/osops-utils/blob/havana/attributes/repos.rb">https://github.com/rcbops-cookbooks/osops-utils/blob/havana/attributes/repos.rb</a></p>

<p>ファイルの中身の抜粋です。</p>

<pre><code class="language-json">    &quot;havana&quot; =&gt; {
      &quot;uri&quot; =&gt; &quot;http://ubuntu-cloud.archive.canonical.com/ubuntu&quot;,
      &quot;distribution&quot; =&gt; &quot;precise-updates/havana&quot;,
      &quot;components&quot; =&gt; [&quot;main&quot;],
      &quot;keyserver&quot; =&gt; &quot;hkp://keyserver.ubuntu.com:80&quot;,
      &quot;key&quot; =&gt; &quot;EC4926EA&quot;
    },
</code></pre>

<p>これらのことより、rcbops-utils の attibutes で havana (実際には
&lsquo;havana-proposed&rsquo;) をレポジトリ指定するように Cookbooks 構成を管理してあげれば
Havana 構成の Keystone, Swift が構築出来ることになります。ちなみに
havana-proposed で Swift や Keystone のどのバージョンがインストールされるかは、
下記の Packages ファイルを確認すると判断出来ます。</p>

<p><a href="http://ubuntu-cloud.archive.canonical.com/ubuntu/dists/precise-proposed/havana/main/binary-amd64/Packages">http://ubuntu-cloud.archive.canonical.com/ubuntu/dists/precise-proposed/havana/main/binary-amd64/Packages</a></p>

<p>以上です。</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2014/03/16/rcbops-chef-cookbooks-keystone-havana-swift-1-10-0.deploy/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>

        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2014/03/05/sensu-chef-openstack-fog-autoscaler/">Sensu,Chef,OpenStack,Fog を使ったオレオレオートスケーラを作ってみた！</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2014-03-05'>
            March 5, 2014</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは。@jedipunkz です。</p>

<p>今まで監視システムの Sensu やクラウドプラットフォームの OpenStack、コンフィギュ
レーションマネージメントツールの Chef やクラウドライブラリの Fog 等使ってきま
したが、これらを組み合わせるとオレオレオートスケーラ作れるんじゃないか？と思い、
ちょろっと作ってみました。</p>

<p>ちなみに自分はインフラエンジニアでしかも運用の出身なので Ruby に関しては初心者
レベルです。Chef で扱っているのと Rails アプリを作った経験はありますが、その程
度。Fog というクラウドライブラリにコントリビュートしたことはアリますが..。ちな
みに Fog のコントリビュート内容は OpenStack Neutron(当時 Quantum) の仮想ルータ
の操作を行う実装です。</p>

<p>そんな自分ですが&hellip;設計1周間・実装1周間でマネージャと CLI が出来ました。
また暫く放置していたマネージャと CLI に WebUI くっつけようかなぁ？と思って
sinatra の学習を始めたのですが、学習を初めて 1.5 日で WebUI が動くところまで行
きました。何故か？Ruby には有用な技術が揃っているから・・！(´；ω；｀)ﾌﾞﾜｯ</p>

<h2 id="オレオレオートスケーラ-sclman-の置き場所">オレオレオートスケーラ &lsquo;sclman&rsquo; の置き場所</h2>

<p><a href="https://github.com/jedipunkz/sclman">https://github.com/jedipunkz/sclman</a></p>

<p>スクリーンショット
+++</p>

<p><img src="https://raw.github.com/jedipunkz/sclman/master/pix/sclman.png" width="600"></p>

<h2 id="構成は">構成は？</h2>

<pre><code>+-------------- public network                  +-------------+
|                                               |sclman-api.rb|
+----+----+---+                                 |  sclman.rb  |
| vm | vm |.. |                                 |sclman-cli.rb|
+-------------+ +-------------+ +-------------+ +-------------+
|  openstack  | | chef server | | sensu server| | workstation |
+-------------+ +-------------+ +-------------+ +-------------+
|               |               |               |
+---------------+---------------+---------------+--------------- management network
</code></pre>

<p>&lsquo;sclman&rsquo; っていう名前です。上図の workstation ノードで稼働します。処理の流れは</p>

<h2 id="処理の流れ">処理の流れ</h2>

<ul>
<li>1) sclman-cli.rb もしくは WebUI から HTTP クラスタのセットを OpenStack 上に生成</li>
<li>2) 生成された VM に対して Chef で nginx をインストール</li>
<li>3) Chef の Roles である &lsquo;LB&rsquo; と &lsquo;Web&rdquo; が同一 Envrionment になるようにブートストラップ</li>
<li>4) LB VM のバックエンドに Web VM が指し示される</li>
<li>5) bootstrap と同時に sensu-client をインストール</li>
<li>6) Web VM の load を sensu で監視</li>
<li>7) sclman.rb (マネージャ) は Sensu AP を定期的に叩いて Web VM の load を監視</li>
<li>8) load が高い environment があれが該当 environment の HTTP クラスタに VM を追加</li>
<li>9) LB VM は追加された VM へのリクエストを追加<br /></li>
<li>10) 引き続き監視し一定期間 load が上がらなけれ Web VM を削除</li>
<li>11) LB VM は削除された VM へのリクエストを削除</li>
</ul>

<p>といった感じです。要約すると CLI/WebUI から HA クラスタを作成。その時に LB,
Web ミドルウェアと同時に sensu クライアントを VM に放り込む。監視を継続して負
荷が上昇すれば Web インスタンスの数を増加させ LB のリクエスト振り先にもその追
加した VM のアドレスを追加。逆に負荷が下がれば VM 削除と共にリクエスト振り先も
削除。この間、人の手を介さずに処理してくれる。です。</p>

<h2 id="使い方">使い方</h2>

<p>詳細な使い方は github の README.md を見て下さい。ここには簡単な説明だけ書いて
おきます。</p>

<ul>
<li>sclman を github から取得して bundler で必要な gems をインストールします。</li>
<li>chef-repo に移動して Berkshelf で必要な cookbooks をインストールします。</li>
<li>cookbooks を用いて sensu をデプロイします。</li>
<li>Omnibus インストーラーを使って chef サーバをインストールします。</li>
<li>OpenStack をインストールします。</li>
<li>sclman.conf を環境に合わせて修正します。</li>
<li>sclman.rb (マネージャ) を稼働します。</li>
<li>sclman-api.rb (WebUI/API) を稼働します。</li>
<li>sclman-cli.rb (CLI) もしくは WebUI から最小構成の HTTP クラスタを稼働します。</li>
<li>この状態で &lsquo;Web&rsquo; Role のインスタンスに負荷が掛かると &lsquo;Web&rsquo; Role のインスタンスの数が増加します。</li>
<li>また逆に負荷が下がるとインスタンス数が減ります。</li>
</ul>

<p>負荷の増減のシビアさは sclman.conf のパラメータ &lsquo;man_sensitivity&rsquo; で決定します。
値が長ければ長いほど増減のし易さは低下します。</p>

<p>まとめ
+++</p>

<p>こんな僕でも Ruby の周辺技術使ってなんとなくの形が出来ましたー。ただまだまだ課
題はあって、インフラを制御するアプリってエラーハンドリングが難しいっていうこと
です。帰ってくるエラーの一覧などがクラウドプラットフォーム・クラウドライブラリ
のドキュメントにあればいいのだけど、まだそこまで行ってない。Fog もまだまだ絶賛
開発中と言うかクラウドプラットフォームの進化に必死で追いついている状態なので、
僕らがアプリを作るときには自分でエラーを全部洗い出す等の作業が必要になるかもし
れない。大変だけど面白い所でもありますよね！これからも楽しみです。</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2014/03/05/sensu-chef-openstack-fog-autoscaler/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>

        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2014/02/27/journal-ssd-ceph-deploy/">Journal 用 SSD を用いた Ceph 構成の構築</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2014-02-27'>
            February 27, 2014</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは、@jedipunkz です。</p>

<p>前回、&rsquo;Ceph のプロセス配置ベストプラクティス&rsquo; というタイトルで記事を書きました。</p>

<p><a href="http://jedipunkz.github.io/blog/2014/01/29/ceph-process-best-practice/">http://jedipunkz.github.io/blog/2014/01/29/ceph-process-best-practice/</a></p>

<p>今回はこの記事にあるポリシに従って下記のような特徴を持った構成を作る手順を具体
的に書いていきたいと思います。</p>

<ul>
<li>ceph01 - ceph04 の4台構成</li>
<li>ノードに HDD 2台搭載されていることを前提 (/dev/sdb, /dev/sdc)</li>
<li>ノードに Journal 用 SSD 1台搭載されていることを前提 (/dev/ssd)</li>
<li>ceph04 は mds サービス稼働</li>
<li>ceph05 は ceph-deploy を実行するためのワークステーション</li>
<li>最終的に ceph04 から Ceph をマウントする</li>
<li>mon は ノード単位で稼働</li>
<li>osd は HDD 単位で稼働</li>
<li>mds は ceph04 に稼働</li>
</ul>

<h2 id="構成-ハードウェアとノードとネットワークの関係">構成 : ハードウェアとノードとネットワークの関係</h2>

<pre><code>                                                                                      public network
         +-------------------+-------------------+-------------------+-------------------+---------
         |                   |                   |                   |                   |
+--------+--------+ +--------+--------+ +--------+--------+ +--------+--------+ +--------+--------+
|      ceph01     | |      ceph02     | |      ceph03     | |      ceph04     | |      ceph05     |
| +-----+ +-----+ | | +-----+ +-----+ | | +-----+ +-----+ | |                 | |                 |
| | sdb | | sdc | | | | sdb | | sdc | | | | sdb | | sdc | | |                 | |                 |
| +-----+-+-----+ | | +-----+-+-----+ | | +-----+-+-----+ | |                 | |                 |
| |     ssd     | | | |     ssd     | | | |     ssd     | | |                 | |                 |
| +-------------+ | | +-------------+ | | +-------------+ | |                 | |                 |
+--------+--------+ +--------+--------+ +--------+--------+ +-----------------+ +-----------------+
         |                   |                   |                                    cluster network
         +-------------------+-------------------+-------------------------------------------------
</code></pre>

<h2 id="構成-プロセスとノードとネットワークの関係">構成 : プロセスとノードとネットワークの関係</h2>

<pre><code>                                                                                      public network
         +-------------------+-------------------+-------------------+-------------------+---------
         |                   |                   |                   |                   |
+--------+--------+ +--------+--------+ +--------+--------+ +--------+--------+ +--------+--------+
|      ceph01     | |      ceph02     | |      ceph03     | |      ceph04     | |      ceph05     |
| +-----+ +-----+ | | +-----+ +-----+ | | +-----+ +-----+ | | +-------------+ | |                 |
| | osd | | osd | | | | osd | | osd | | | | osd | | osd | | | |     mds     | | |                 |
| +-----+-+-----+ | | +-----+-+-----+ | | +-----+-+-----+ | | +-------------+ | |                 |
| |     mon     | | | |     mon     | | | |     mon     | | |                 | |                 |
| +-------------+ | | +-------------+ | | +-------------+ | |                 | |                 |
+--------+--------+ +--------+--------+ +--------+--------+ +-----------------+ +-----------------+
         |                   |                   |                                    cluster network
         +-------------------+-------------------+-------------------------------------------------
</code></pre>

<p>注意 : 上記の図だと ssd : mon が対に見えますが、そうではありません。</p>

<p>では構築方法について書いていきます。</p>

<h2 id="作業用ホストの準備">作業用ホストの準備</h2>

<p>ノンパスフレーズの SSH 公開鍵・秘密鍵を生成する。</p>

<pre><code class="language-bash">% ssh-keygen
</code></pre>

<p>公開鍵をターゲットホスト (ceph01-03) に配置</p>

<pre><code class="language-bash">% ssh-copy-id ceph@ceph01
% ssh-copy-id ceph@ceph02
% ssh-copy-id ceph@ceph03
</code></pre>

<p>ceph-deploy の取得を行う。</p>

<pre><code class="language-bash">% git clone https://github.com/ceph/ceph-deploy.git ~/ceph-deploy
</code></pre>

<p>&lsquo;python-virtualenv&rsquo; パッケージをインストールする。</p>

<pre><code class="language-bash">% sudo apt-get update ; sudo apt-get -y install python-virtualenv
</code></pre>

<p>ceph-deploy をブートストラップする</p>

<pre><code class="language-bash">% cd ~/ceph-deploy
% ./bootstrap
</code></pre>

<p>PATH を通す。下記は例。</p>

<pre><code class="language-bash">% ${EDITOR} ~/.zshrc
export PATH=$HOME/ceph-deploy:$PATH
</code></pre>

<p>ホスト名の解決を行う。IP アドレスは例。</p>

<pre><code class="language-bash">% sudo ${EDITOR} /etc/hosts
10.200.10.11    ceph01
10.200.10.12    ceph02
10.200.10.13    ceph03
10.200.10.14    ceph04
10.200.10.15    ceph05
</code></pre>

<h2 id="上記の構成の構築方法">上記の構成の構築方法</h2>

<p>以前の記事同様に ceph-deploy をデプロイツールとして用いる。</p>

<p>ceph-deploy に関しては下記の URL を参照のこと。</p>

<p><a href="https://github.com/ceph/ceph-deploy">https://github.com/ceph/ceph-deploy</a></p>

<p>下記の手順でコンフィギュレーションと鍵の生成を行う。またこれからの操作はすべて public network
上の ceph-deploy 専用 node からの操作とする。</p>

<pre><code class="language-bash">% mkdir ~/ceph-work
% cd ~/ceph-work
% ceph-deploy --cluster cluster01 new ceph01 ceph02 ceph03 ceph04
</code></pre>

<p>~/ceph-work ディレクトリ上に cluster01.conf が生成されているので下記の通り
cluster network を扱う形へと追記を行う。</p>

<pre><code class="language-bash">public network = &lt;public_network_addr&gt;/&lt;netmask&gt;
cluster network = &lt;cluster_network_addr&gt;/&lt;netmask&gt;

[mon.a]
    host = ceph01
    mon addr = &lt;ceph01_ip_addr&gt;:6789

[mon.b]
    host = ceph02
    mon addr = &lt;ceph02_ip_addr&gt;:6789

[mon.c]
    host = ceph03
    mon addr = &lt;ceph03_ip_addr&gt;:6789

[osd.0]
    public addr = &lt;ceph01_public_ip_addr&gt;
    cluster addr = &lt;ceph01_cluster_ip_addr&gt;

[osd.1]
    public addr = &lt;ceph01_public_ip_addr&gt;
    cluster addr = &lt;ceph01_cluster_ip_addr&gt;

[osd.2]
    public addr = &lt;ceph01_public_ip_addr&gt;
    cluster addr = &lt;ceph01_cluster_ip_addr&gt;

[mds.a]
    host = ceph04
</code></pre>

<p>ceph の各 nodes へのインストールを行う。ceph はワークステーションである ceph05 にも
インストールしておきます。後に Ceph ストレージをマウントするためです。</p>

<pre><code class="language-bash">% ceph-deploy --cluster cluster01 install ceph01 ceph02 ceph03 ceph04 ceph04
</code></pre>

<p>mon プロセスを各 nodes で稼働する。</p>

<pre><code class="language-bash">% ceph-deploy --cluster cluster01 mon create ceph01 ceph02 ceph03
</code></pre>

<p>鍵の配布を各 nodes に行う。</p>

<pre><code class="language-bash">% ceph-deploy --cluster cluster01 gatherkeys ceph01 ceph02 ceph03 ceph04 ceph05
</code></pre>

<p>disk のリストを確認。</p>

<p>各 node 毎に用いることが可能は disk の一覧を確認する。</p>

<pre><code class="language-bash">% ceph-deploy --cluster cluster01 disk list ceph01
% ceph-deploy --cluster cluster01 disk list ceph02
% ceph-deploy --cluster cluster01 disk list ceph03
</code></pre>

<p>disk の初期化を行う。この作業を行うと指定ディスク上のデータは消去される。</p>

<pre><code class="language-bash">% ceph-deploy --cluster cluster01 disk zap ceph01:/dev/sdb ceph01:/dev/sdc
% ceph-deploy --cluster cluster01 disk zap ceph02:/dev/sdb ceph02:/dev/sdc
% ceph-deploy --cluster cluster01 disk zap ceph03:/dev/sdb ceph03:/dev/sdc
</code></pre>

<p>journal 用の ssd のパーティションを切る。ここでは 10GB 毎に切った
/dev/ssd1, /dev/ssd2 が存在することを前提に記す。ceph と同時にインストールされた
gdisk を用いる。</p>

<pre><code class="language-bash">% sudo gdisk /dev/ssd
</code></pre>

<p>(注意) 下記の公式ドキュメントでは osd prepare, osc activate の手順が掲載されて
いるがその後の osd create のコマンドにて prepare が実行されるようでこれら2つの
手順を行うと正常に osd create コマンドが実行できなかった。よってこのタイミング
にて osd create を行うものとする。</p>

<ul>
<li><a href="http://ceph.com/docs/master/rados/deployment/ceph-deploy-osd/#prepare-osds">http://ceph.com/docs/master/rados/deployment/ceph-deploy-osd/#prepare-osds</a></li>
<li><a href="http://ceph.com/docs/dumpling/start/quick-ceph-deploy/">http://ceph.com/docs/dumpling/start/quick-ceph-deploy/</a></li>
</ul>

<p>2 つの disk に対してそれぞれ osd を稼働させる。</p>

<pre><code class="language-bash">% ceph-deploy --cluster cluster01 osd create ceph01:sdb:/dev/ssd1 ceph02:sdb:/dev/ssd1 ceph03:sdb:/dev/ssd1
% ceph-deploy --cluster cluster01 osd create ceph01:sdc:/dev/ssd2 ceph02:sdc:/dev/ssd2 ceph03:sdc:/dev/ssd2
</code></pre>

<p>mds の稼働を行う。ここでは1号機にのみ稼働を行う。</p>

<pre><code class="language-bash">% ceph-deploy --cluster cluster01 mds create ceph04
</code></pre>

<h2 id="クライアントからのマウント方法各種">クライアントからのマウント方法各種</h2>

<p>上記で構築した Ceph ストレージを利用する方法を3つ説明する。先に述べたように
POSIX 互換 filesystem として利用が可能。それぞれ mds が稼働しているホストに対
して接続を行う。</p>

<h4 id="block-device-としてマウントする方法">Block Device としてマウントする方法</h4>

<p>ストレージ上に block device を生成しそれをマウントする</p>

<pre><code class="language-bash">cephclient% rbd create foo --size 4096
cephclient% sudo modprobe rbd
cephclient% sudo rbd map foo --pool rbd --name client.admin
cephclient% sudo mkfs.ext4 -m0 /dev/rbd/rbd/foo
cephclient% sudo mkdir /mnt/myrbd
cephclinet% sudo mount /dev/rbd/rbd/foo /mnt/myrbd
</code></pre>

<h4 id="kernel-driver-を用いてマウントする方法">Kernel Driver を用いてマウントする方法</h4>

<p>kernel Driver を用いてストレージをマウントする</p>

<pre><code class="language-bash">cephclient% sudo mkdir /mnt/mycephfs
cephclient% sudo mount -t ceph 10.200.10.26:6789:/ /mnt/mycephfs -o \
            name=admin,secret=`sudo ceph-authtool -p /etc/ceph/cluster01.client.admin.keyring`
</code></pre>

<h4 id="fuse-driver-ユーザランド-を用いてマウントする方法">Fuse Driver (ユーザランド) を用いてマウントする方法</h4>

<p>ユーザランドソフトウェア FUSE を用いてマウントする方法</p>

<pre><code class="language-bash">cephclient% sudo mkdir /home/&lt;username&gt;/cephfs
cephclient% sudo ceph-fuse -m 10.200.10.26:6789 /home/&lt;username&gt;/cephfs
</code></pre>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2014/02/27/journal-ssd-ceph-deploy/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>

        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2014/01/29/ceph-process-best-practice/">Ceph のプロセス配置ベストプラクティス</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2014-01-29'>
            January 29, 2014</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>Ceph はブロック型の分散ストレージファイルシステムです。POSIX のファイルシステ
ムとしてマウント出来ます。Linux の Kernel ドライバや FUSE ドライバを用いてマウ
ントします。またブロックデバイスとしてマウントする方法もあります。</p>

<p>だいぶ前ですが、Ceph に関する記事を以前下記の通り書きました。</p>

<ul>
<li><a href="http://jedipunkz.github.io/blog/2013/05/25/ceph-cluster-network/">http://jedipunkz.github.io/blog/2013/05/25/ceph-cluster-network/</a></li>
<li><a href="http://jedipunkz.github.io/blog/2013/05/11/ceph-deploy/">http://jedipunkz.github.io/blog/2013/05/11/ceph-deploy/</a></li>
</ul>

<p>Ceph の構築方法について記したブログだったのですが、今まで mon, osd, mds の各プ
ロセスをそれぞれ何台のノードに対して配置し、またそれぞれのプロセス幾つを何に対
して配置するのか？という疑問が付きまとわっていました。node, disk, process のそ
れぞれの数の関係について知りたいなぁと思っていました。幾つかのドキュメントを読
んでいて、ぼんやり見えてきた事があるのでそれを今回はまとめたいと思います。</p>

<p>また、皆さん気になるトコロだと思われる容量設計についても軽く触れたいと思います。</p>

<h2 id="参考資料">参考資料</h2>

<ul>
<li><a href="http://ceph.com/docs/master/rados/configuration/mon-config-ref/">http://ceph.com/docs/master/rados/configuration/mon-config-ref/</a></li>
<li><a href="http://www.sebastien-han.fr/blog/2013/12/02/ceph-performance-interesting-things-going-on/">http://www.sebastien-han.fr/blog/2013/12/02/ceph-performance-interesting-things-going-on/</a></li>
</ul>

<h2 id="各要素の数の関係">各要素の数の関係</h2>

<p>ハードウェア要素である node, disk(hdd), ssd そしてソフトウェア要素である mon,
osd, mds の数の関係はどのようにするべきか？基本となる関係は</p>

<ul>
<li>1 mds process / node</li>
<li>1 mon process / node</li>
<li>1 osd process / disk</li>
<li>n jornal ssd device / disk / node</li>
</ul>

<p>だと考えられます。僕が今のところ理想かなぁと思っている構成をまとめたいと思いま
す。</p>

<p>下記の図がそれです。</p>

<pre><code>+------------------------+
|         client         |
+------------------------+
|
+--------------------------+--------------------------+-------------------------------+-------------------------
|                          |                          |                               |            public network
+------------------------+ +------------------------+ +------------------------+      +------------------------+
|          mon           | |          mon           | |          mon           |      |          mds           |
+------+ +------+ +------+ +------+ +------+ +------+ +------+ +------+ +------+      +------------------------+
| osd  | | osd  | | osd  | | osd  | | osd  | | osd  | | osd  | | osd  | | osd  |      |                        |
+------+ +------+ +------+ +------+ +------+ +------+ +------+ +------+ +------+      |                        |
| disk | | disk | | disk | | disk | | disk | | disk | | disk | | disk | | disk |....&gt; |                        |
+------+ +------+ +------+ +------+ +------+ +------+ +------+ +------+ +------+scale |          node          |
|          ssd           | |          ssd           | |          ssd           |      |                        |
+------------------------+ +------------------------+ +------------------------+      |                        |
|          node          | |          node          | |          node          |      |                        |
+------------------------+ +------------------------+ +------------------------+      +------------------------+
|                          |                          |                               |
+--------------------------+--------------------------+-------------------------------+-------------------------
                                                                                                  cluster network
</code></pre>

<h4 id="mds-と-node-の関係">mds と node の関係</h4>

<p>mds はリモートクライアントへのファイルシステムサービスの提供を行うことや特性が
全く異なることから別ノードに切り出しました。また mds は幾つかのノードで稼働さ
せる事も可能。が、mds はそれぞれのサービスを HA 組む仕組みは持っていないので
どれか一方の mds をクライアントは指し示す必要があり、その mds が停止すれば直
ちに障害に発展します。</p>

<h4 id="mon-と-node-の関係">mon と node の関係</h4>

<p>mon は比較的少量のリソースで稼働します。今回 osd と同じノードの搭載しましたが
別ノードに切り出すことも勿論可能です。mon は CRUSH Maps アルゴリズムの元に連携
が取れますので複数のプロセスを稼働することが推奨されていることからも、比較的少
ないノード数のクラスタの場合は osd と同ノードに搭載するのが容易かなと考えまし
た。</p>

<h4 id="osd-と-node-の関係">osd と node の関係</h4>

<p>1 osd プロセスに対して 1 disk が基本となります。osd は実データのレプリケーショ
ンを行うことからコンフィギュレーションに対して上図の様にクラスタ用のネットワー
クを紐付け、高トラヒックに対応する必要があります。また osd 用の disk device で
すが RAID を組まないことが推奨されています。CEPH 自体に HA の仕組みがあること、
また RAID 構成にもよりますがディスクアクセスが遅くなることは Ceph にとってボト
ルネックを早く招くことになりますし、小さいディスク容量しか扱えなくなることは
Ceph にとって不利になると思います。</p>

<h4 id="journal-用の-ssd-device-と-disk-node-の関係">journal 用の ssd device と disk, node の関係</h4>

<p>現在の Stable Release 版の Ceph は journal を用いてメタデータを管理します。各
osd の disk 単位に journal 用の disk device を指定出来るようになっています。メ
タデータですので実データ用の disk よりだいぶ小さな容量で構わないこと、また比較
的高速なアクセスを要求されることからも SSD を選択することが推奨されつつあるよ
うです。実際にストアされるデータの特性にもよりますが 1 node に対して 1 ssd
device を配置すれば十分かと思います。また osd のプロセスの数 (disk の数) に対
して一つのパーティションを切ることで対応出来るかと思います。</p>

<p>設定方法の例を記します。ここに ceph01-03 の3台のノードがありそれぞれ 2 disk, 1
ssd が搭載されているとします。/dev/ssd は gdisk 等を用いて2つ以上のパーティショ
ンに切り分けます。</p>

<p>下記のように /dev/sdb (hdd) に対して /dev/ssd1 (ssd), /dev/sdc (hdd) に対して /dev/ssd2 (ssd)
を割り当てることが出来ます。</p>

<pre><code class="language-bash">% ceph-deploy --cluster cluster01 osd create ceph01:sdb:/dev/ssd1 ceph02:sdb:/dev/ssd1 ceph03:sdb:/dev/ssd1
% ceph-deploy --cluster cluster01 osd create ceph01:sdc:/dev/ssd2 ceph02:sdc:/dev/ssd2 ceph03:sdc:/dev/ssd2
</code></pre>

<h2 id="ceph-ストレージ全体の容量設計と-mon-の-ratio-の関係">Ceph ストレージ全体の容量設計と mon の ratio の関係</h2>

<p>3TB のディスクを持ったノードが 33 台並んでいるとします。各ノードには osd プロ
セスが1つ稼働します。合計容量は 99 TB となりますが mon が持っているコンフィギュ
レーションである full ratio というパラメータがありデフォルト値が 0.95 となって
います。よってこのクラスタで扱える全体のディスク容量は 95TB となります。</p>

<p>また、ラックに数台のノードを積むのが通常ですが、電源故障等で一気にラック単位で
障害が発生したとします。この場合 Ceph はすべてのデータに関してレプリカを取り復
旧作業を行います。しかしながら停止するノード数によってはストレージ全体の扱える
容量をオーバーすることも懸念されます。これに対応するために先ほど登場した ratio
パラメータを調整することが出来ます。</p>

<pre><code>[global]

mon osd full ratio = .80
mon osd nearfull ratio = .70
</code></pre>

<p>上記の例では full ステートの ratio が 0.80, nearfull ステートの ratio が 0.70
となります。想定の障害ノード数を考慮し ratio パラメータにてその台数分を減算す
れば良いわけです。</p>

<h2 id="まとめ">まとめ</h2>

<p>前述した通り上図は比較的少ないノード数のクラスタを組む場合を想定しています。ノー
ド数が増える場合は mon は mds, osd とも必要とするリソースの特性が異なりますの
で別ノードに切り出すことも考えたほうが良さそうです。2014年の2月には Firefly と
いう新しいリリース版が出ます。ここでのブループリント(設計書)を確認すると&hellip;</p>

<p><a href="http://wiki.ceph.com/Planning/Blueprints/Firefly/osd%3A_new_key%2F%2Fvalue_backend">http://wiki.ceph.com/Planning/Blueprints/Firefly/osd%3A_new_key%2F%2Fvalue_backend</a></p>

<p>journal に変わる新たなメタデータ管理方法として KVS データベースを扱うことも視
野に入っているようです。上記の URL 見る限りでは Facebook がオープンソースにし
た rocksdb や fusionio の nvmkv, seagate の kinetic 等が挙がっています。2月に
期待しましょう！</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2014/01/29/ceph-process-best-practice/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>

        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2014/01/21/17th-openstack-study/">第17回 OpenStack 勉強会で話してきました</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2014-01-21'>
            January 21, 2014</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは。<a href="https://twitter.com/jedipunkz">@jedipunkz</a> です。</p>

<p>昨晩、第17回 OpenStack 勉強会が開催されました</p>

<p><a href="http://connpass.com/event/4545/">http://connpass.com/event/4545/</a></p>

<p>ここで発表をしてきましたぁ！発表タイトルは &ldquo;rcbops/chef-cookbooks&rdquo; です。</p>

<script async class="speakerdeck-embed"
data-id="27a2739063d601314bce6a232911c4f0" data-ratio="1.33333333333333"
src="//speakerdeck.com/assets/embed.js"></script>

<p>何を発表したかと言うと詳しくは上記のスライドを見ていただくとして、簡単に言うと
&ldquo;RackSpace 社のエンジニアが管理している Chef Cookbooks でOpenStack 構成を作ろ
う&rdquo; ってことです。</p>

<p>今日知ったのですがどうも昨晩は初心者向けの勉強会という位置付けだったらしく..少
しだけディープな話題を話してしまったかもしれません！すいません！＞＜</p>

<p>でもとても楽しく発表出来ましたし、逆に質問のコーナーで最新の情報も教えてもらえ
たり！なんと Havana 対応の v4.2.0 以降では Swift の Cookbooks が消えてしまった
とか！&hellip; 皆 Swift 好きくないの？&hellip;; ;</p>

<p>rcbops/chef-cookbooks はずっと追っていますが、ものすごいスピードで開発進んでい
るので、今後ぜひみなさん使ってみて下さいー。</p>

<p>最後に詳しい利用方法を記した僕のブログの URL を貼り付けておきます。</p>

<ul>
<li>OpenStack Havana を Chef でデプロイ</li>
</ul>

<p><a href="http://jedipunkz.github.io/blog/2013/11/17/openstack-havana-chef-deploy/">http://jedipunkz.github.io/blog/2013/11/17/openstack-havana-chef-deploy/</a></p>

<ul>
<li>Swift HA 構成を Chef でデプロイ</li>
</ul>

<p><a href="http://jedipunkz.github.io/blog/2013/07/26/swift-ha-chef-deploy/">http://jedipunkz.github.io/blog/2013/07/26/swift-ha-chef-deploy/</a></p>

<ul>
<li>実用的な Swift 構成を Chef でデプロイ</li>
</ul>

<p><a href="http://jedipunkz.github.io/blog/2013/10/27/swift-chef/">http://jedipunkz.github.io/blog/2013/10/27/swift-chef/</a></p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2014/01/21/17th-openstack-study/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Categories
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
        <li><a href='/categories/report'>report</a></li>
    
</ul>

    </footer>
</article>

        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2013/12/09/chef-autonoumous-cluster/">Chef で自律的クラスタを考える</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2013-12-09'>
            December 9, 2013</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは。<a href="https://twitter.com/jedipunkz">@jedipunkz</a> です。</p>

<p>Serf の登場があったり、ここ最近オーケストレーションについて考える人が増えた気
がします。システムをデプロイしてその後各ノード間の連結だったりも同じ Chef,
Puppet 等のコンフィギュレーションツールで行うのか？全く別のツールで？..</p>

<p>最近 Serf というツールの登場がありました。</p>

<p>僕も Serf を触ってつい先日ブログに書きました。有用なツールだと思います。シ
ンプルだからこそ応用が効きますし、リアルタイム性もあり、将来的に異なるネットワー
クセグメント上のノードとも連結出来るようになりそうですし、とても期待です。</p>

<p>話が少し飛びますが..</p>

<p>いつも Rebuild.fm を楽しく聞いているのですが Immutable Infrastructure の話題の
時にオーケストレーションの話題になって、どうも &lsquo;Chef でも自律的なクラスタを組
むことが認知されていないのでは？&rsquo; と思うようになりました。もちろん Chef でやる
べき！とは言い切りませんし、今後どうなるかわかりません。Opscode の中の人も &lsquo;オー
ケストレーションは自分でやってね&rsquo; というスタンスだったとずいぶん前ですが聞きま
した。Serf を等のオーケストレーションツールを使う使わないの話は今回はしないの
ですが Chef でも自律的クラスタを組むことは出来ますよ〜というのが今回の話題。</p>

<p>まえがきが長くなりました。</p>

<p>今回は Chef で自律的クラスタを構成する方法を記したいと思います。</p>

<p>haproxy 等を利用すれば尚良いと思いますが、よりクラスタを組むのが簡単な nginx
を今回は利用したいと思います。</p>

<p><a href="https://github.com/opscode-cookbooks/nginx">https://github.com/opscode-cookbooks/nginx</a></p>

<h2 id="構成">構成</h2>

<p>&lsquo;web&rsquo; という Role 名と &lsquo;lb&rsquo; という Role 名で単純な HTTP サーバとしての nginx
ノードを複数台と、ロードバランサとしての nginx ノードを1台でクラスタを構成しま
す。また共に environment 名は同じものを利用します。別の environment 名の場合は
別クラスタという区切りです。</p>

<ul>
<li>&lsquo;lb&rsquo; node x 1 + &lsquo;web&rsquo; node x n (&lsquo;foo&rsquo; environment)</li>
<li>&lsquo;lb&rsquo; node x 1 + &lsquo;web&rsquo; node x n (&lsquo;bar&rsquo; environment)</li>
</ul>

<h2 id="lb-nginx-ロードバランサのレシピ">&lsquo;lb&rsquo; nginx ロードバランサのレシピ</h2>

<p>下記が &lsquo;lb&rsquo; Role の recipes/cmomnos_conf.rb の修正した内容です。</p>

<pre><code class="language-ruby">environment = node.chef_environment
webservers = search(:node, &quot;role:web AND chef_environment:#{environment}&quot;)
 
template &quot;#{node['nginx']['dir']}/sites-available/default&quot; do
  source &quot;default-site.erb&quot;
  owner &quot;root&quot;
  group &quot;root&quot;
  mode 00644
  notifies :reload, 'service[nginx]'
    variables ({
      :webservers =&gt; webservers
    })
end
</code></pre>

<p>何をやっているかと言うと、environment という変数に自ノードの environment 名を。
webservers という変数に role 名が &lsquo;web&rsquo; で尚且つ自ノードと同じ environment 名
が付いたノード名を入れています。これで自分と同じ environment に所属している
&lsquo;web&rsquo; Role なノードを Chef サーバに対して検索しています。また、template 内で
webservers という変数をそのまま利用できるように variables で渡しています。</p>

<h2 id="lb-nginx-ロードバランサのテンプレート">&lsquo;lb&rsquo; nginx ロードバランサのテンプレート</h2>

<p>下記が webservers 変数を受け取った後の template 内の処理です。</p>

<pre><code class="language-ruby">&lt;% if @webservers and ( @webservers != [] ) %&gt;
upstream backend {
&lt;% @webservers.each do |hostname| -%&gt;
  server &lt;%= hostname['ipaddr'] -%&gt;;
&lt;% end -%&gt;
}
&lt;% end %&gt;
  
server {
  listen   80;
  server_name  &lt;%= node['hostname'] %&gt;;
    
  access_log  &lt;%= node['nginx']['log_dir'] %&gt;/localhost.access.log;
    
  location / {
    &lt;% if @webservers and ( @webservers != [] ) %&gt;
    proxy_pass http://backend;
    &lt;% else %&gt;
    root   /var/www/nginx-default;
    index  index.html index.htm;
    &lt;% end %&gt;
  }
}
</code></pre>

<p>upstream backend { &hellip; は皆さん見慣れた記述だと思うのですが、バックエンドの
HTTP サーバの IP アドレスを一覧化します。each で回しているので台数分だけ
server <ip_addr>; の記述が入ります。</p>

<p>chef-client をデーモン稼働しておけば、新規に Chef サーバに登録された &lsquo;web&rsquo;
Role の HTTP サーバを自動で &lsquo;lb&rsquo; Role のロードバランサが組み込む、つまり自律的
なクラスタが組めることになります。もちろんこの間の手作業は一切ありません。</p>

<p>ちなみに chef-client をデーモン稼働するには</p>

<pre><code>recipe[chef-client::service]
</code></pre>

<p>というレシピをノードに割り当てることで可能です。</p>

<h2 id="まとめ">まとめ</h2>

<p>Chef でも自律的なクラスタが組めました。もちろん chef-client の稼働間隔があるの
でリアルタイム性はありません。chef-client の稼働間隔は &lsquo;chef-client&rsquo; レシピの
attributes で調整出来ます。その点は serf のほうが確実に勝っていると見るべきで
しょう。冒頭に記したようにこの辺りの操作を Chef で行うのか別のツールを使うのか
はまだまだ模索が必要そう。ただ、私がいつも使っている &lsquo;OpenStack を Chef で構成
する Cookbooks&rsquo; 等は複数台構成を Chef で構成しています。なので僕にとってはこの
辺りの話は当たり前だと思っていたのだけど、どうも勉強会に出たりすると &ldquo;Chef は
複数台構成を作るのが苦手だ&rdquo; って話があがってくるので気になっていました。</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2013/12/09/chef-autonoumous-cluster/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>

        

        
<ul class="actions pagination">
    
        <li><a href="/tags/page/3/"
                class="button big previous">Previous Page</a></li>
    

    
        <li><a href="/tags/page/5/"
                class="button big next">Next Page</a></li>
    
</ul>

    </div>
    
<section id="sidebar">

    
        <section id="intro">
            
            
            
            <ul class="icons">
                
                    <li><a href="https://jedipunkz.github.io/tags/index.xml" type="application/rss+xml"
                        target="_blank" title="RSS" class="fa fa-rss"></a></li>
                
                
            </ul>
        </section>

    
        <section id="recent-posts">
            <ul class="posts">
                <header>
                    <h3>Recent Posts</h3>
                </header>
                
                    
                

                
                    
                

                
                    <li>
                        <article>
                            <header>
                                <h3><a href="https://jedipunkz.github.io/blog/2018/12/31/istio/">Istio, Helm を使って Getting Started 的なアプリをデプロイ</a></h3>
                                
                                    
                                
                                <time class="published" datetime=
                                    '2018-12-31'>
                                    December 31, 2018</time>
                            </header>
                        </article>
                    </li>
                
                    <li>
                        <article>
                            <header>
                                <h3><a href="https://jedipunkz.github.io/blog/2017/07/02/test-kitchen-cluster/">Docker,Test-Kitchen,Ansible でクラスタを構成する</a></h3>
                                
                                    
                                
                                <time class="published" datetime=
                                    '2017-07-02'>
                                    July 2, 2017</time>
                            </header>
                        </article>
                    </li>
                
                    <li>
                        <article>
                            <header>
                                <h3><a href="https://jedipunkz.github.io/blog/2017/04/13/gke-lb/">GCP ロードバランサと GKE クラスタを Terraform を使って構築する</a></h3>
                                
                                    
                                
                                <time class="published" datetime=
                                    '2017-04-13'>
                                    April 13, 2017</time>
                            </header>
                        </article>
                    </li>
                
                    <li>
                        <article>
                            <header>
                                <h3><a href="https://jedipunkz.github.io/blog/2017/02/12/serverless-fission/">Serverless on Kubernetes : Fission を使ってみた</a></h3>
                                
                                    
                                
                                <time class="published" datetime=
                                    '2017-02-12'>
                                    February 12, 2017</time>
                            </header>
                        </article>
                    </li>
                
                    <li>
                        <article>
                            <header>
                                <h3><a href="https://jedipunkz.github.io/blog/2017/01/13/kubernetes-deployments/">Kubernetes Deployments を使ってみた！</a></h3>
                                
                                    
                                
                                <time class="published" datetime=
                                    '2017-01-13'>
                                    January 13, 2017</time>
                            </header>
                        </article>
                    </li>
                

                
                    <li>
                        <ul class="actions">
                            <li><a href=
                            
                                "/post/"
                            
                            class="button">View more posts</a></li>
                        </ul>
                    </li>
                
            </ul>
        </section>

    
    
    
    
        <section id="categories">
            <ul class="posts">
                <header>
                    <h3><a href="/categories/">Categories</a></h3>
                </header>

                
                    
                

                
                    <li>
                        <article>
                            <header>
                                <a href="/categories/infrastructure/">infrastructure</a>
                                <span style="float:right;">110</span>
                            </header>
                        </article>
                    </li>
                
                    <li>
                        <article>
                            <header>
                                <a href="/categories/report/">report</a>
                                <span style="float:right;">9</span>
                            </header>
                        </article>
                    </li>
                
                    <li>
                        <article>
                            <header>
                                <a href="/categories/tools/">tools</a>
                                <span style="float:right;">11</span>
                            </header>
                        </article>
                    </li>
                
            </ul>
        </section>
    

    
        

    
        <section id="footer">
            <ul class="icons">
                
                    <li><a href="https://jedipunkz.github.io/tags/index.xml" type="application/rss+xml"
                        target="_blank" title="RSS" class="fa fa-rss"></a></li>
                
                
            </ul>

            <p class="copyright">&copy; ジェダイさんのブログ. テーマデザインは <a href="//github.com/jpescador" target="_blank">Julio Pescador</a>さんによるものです。 </p>
        </section>

</section>

            </div>
        <a id="back-to-top" href="#" class="fa fa-arrow-up fa-border fa-2x"></a>
        

        
        
            
        

        
        
            <script src="/js/jquery.min.js"></script>
            <script src="/js/skel.min.js"></script>
            <script src="/js/util.js"></script>
            <script src="/js/main.js"></script>
            <script src="/js/backToTop.js"></script>
            <script src="/js/highlight.pack.js"></script>
        

        

            
            <script>hljs.initHighlightingOnLoad();</script>
            
    </body>
</html>

