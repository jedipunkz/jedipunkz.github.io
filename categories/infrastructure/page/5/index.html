


    
        
    




<!DOCTYPE HTML>

<html>
    <head>
        
            <title>Infrastructure Posts - ジェダイさんのブログ</title>
        

        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1" />
        <meta name="generator" content="Hugo 0.53" />
        


        
        
            
        

        <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Infrastructure"/>
<meta name="twitter:description" content=""/>

        <meta property="og:title" content="Infrastructure" />
<meta property="og:description" content="" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://jedipunkz.github.io/categories/infrastructure/" />
<meta property="og:updated_time" content="2013-11-27T00:00:00&#43;00:00"/>

        
<meta itemprop="name" content="Infrastructure">
<meta itemprop="description" content="">


        

        

        
        
            
        

        
        
            <link rel="stylesheet" href="/css/google-font.css" />
            <link rel="stylesheet" href="/css/font-awesome.min.css" />
            <link rel="stylesheet" href="/css/main.css" />
            <link rel="stylesheet" href="/css/add-on.css" />
            <link rel="stylesheet" href="/css/monokai-sublime.css">
        

        

        
        
        
            
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-30563095-1', 'auto');
	
	ga('send', 'pageview');
}
</script>

        
    </head>
    <body>

        
        <div id="wrapper">

    
<header id="header">
    
        <h2><a href="/"></i></a></h2>
    

    <nav class="links">
        <ul>
            
                <li>
                    <a href="">
                        Blog
                    </a>
                </li>
            
                <li>
                    <a href="about/index.html">
                        About
                    </a>
                </li>
            
        </ul>
    </nav>
    <nav class="main">
        <ul>
            
            <li class="search">
                <a class="fa-search" href="#search">Search</a>
                <form id="search" method="get" action="//google.com/search">
                    <input type="text" name="q" placeholder="Search" />
                    <input type="hidden" name="q" value="site:https://jedipunkz.github.io">
                </form>
            </li>
            <li class="menu">
                <a class="fa-bars" href="#menu">Menu</a>
            </li>
        </ul>
    </nav>
</header>


<section id="menu">

    
        <section>
            <form class="search" method="get" action="//google.com/search">
                <input type="text" name="q" placeholder="Search" />
                <input type="hidden" name="q" value="site:https://jedipunkz.github.io">
            </form>
        </section>

    
        <section>
            <ul class="links">
                
                    <li>
                        <a href="">
                            <h3>
                                
                                Blog
                            </h3>
                        </a>
                    </li>
                
                    <li>
                        <a href="about/index.html">
                            <h3>
                                
                                About
                            </h3>
                        </a>
                    </li>
                
            </ul>
        </section>

    
        <section>
            <ul class="links">
                <header>
                    <h3>Recent Posts</h3>
                </header>
                
                    
                

                
                    <li>
                        <a href="https://jedipunkz.github.io/blog/2018/12/31/istio/"><p>Istio, Helm を使って Getting Started 的なアプリをデプロイ</p></a>
                    </li>
                
                    <li>
                        <a href="https://jedipunkz.github.io/blog/2017/07/02/test-kitchen-cluster/"><p>Docker,Test-Kitchen,Ansible でクラスタを構成する</p></a>
                    </li>
                
                    <li>
                        <a href="https://jedipunkz.github.io/blog/2017/04/13/gke-lb/"><p>GCP ロードバランサと GKE クラスタを Terraform を使って構築する</p></a>
                    </li>
                
                    <li>
                        <a href="https://jedipunkz.github.io/blog/2017/02/12/serverless-fission/"><p>Serverless on Kubernetes : Fission を使ってみた</p></a>
                    </li>
                
                    <li>
                        <a href="https://jedipunkz.github.io/blog/2017/01/13/kubernetes-deployments/"><p>Kubernetes Deployments を使ってみた！</p></a>
                    </li>
                
            </ul>
        </section>

    
        
</section>

    
    <div id="main">
        <h1>Infrastructure</h1>
        
        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2013/11/27/sensu-chef-deploy-2/">sensu-chef で監視システム Sensu を管理 #2</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2013-11-27'>
            November 27, 2013</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは。<a href="https://twitter.com/jedipunkz">@jedipunkz</a> です。</p>

<p>以前、Sensu を Chef で管理する方法について書きました。</p>

<p><a href="http://jedipunkz.github.io/blog/2013/06/20/sensu-chef-controll/">http://jedipunkz.github.io/blog/2013/06/20/sensu-chef-controll/</a></p>

<p>これは今年(2013)の6月頃の記事ですが、この時はまだ sensu-chef を include して使う別の Chef
Cookbook が必要でした。また Redis 周りの Cookbooks が完成度あまく、またこれも
公式とは別の Cookbooks を改修して再利用する形でした。この作業は結構しんどかっ
た記憶があるのですが、最近 GlideNote さんのブログを読んで( ﾟдﾟ)ﾊｯ!と思い、
sensu-chef を再確認したのですが、だいぶ更新されていました。</p>

<p>下記が sensu-chef です。</p>

<p><a href="https://github.com/sensu/sensu-chef">https://github.com/sensu/sensu-chef</a></p>

<p>この Chef Cookbook 単体で利用できる形に更新されていて、plugins, checks 等は
Recipe に追記することで対応可能になっていました。早速利用してみたので簡単に使
い方を書いていきます。</p>

<p>下記が Sensu の管理画面です。最終的にこの画面に監視対象のアラートが上がってきます。</p>

<p>{% img /pix/sensu.png %}</p>

<h2 id="使い方">使い方</h2>

<p>sensu-chef を取得する。chef-repo になっています。</p>

<pre><code class="language-bash">% git clone https://github.com/sensu/sensu-chef.git ~/chef-repo-sensu
</code></pre>

<p>bundle にて Gemfile に記述の在る gem パッケージをインストールします。</p>

<pre><code class="language-bash">% cd ~/chef-repo-sensu
% bundle install
</code></pre>

<p>.chef/ 配下の設定は割愛します。chef サーバの情報に合わせて設定します。</p>

<p>ssl 鍵を生成して data bags に投入します。</p>

<pre><code class="language-bash">% cd examples/ssl
% ./ssl_certs.sh generate
% cd ../../
% knife data bag create sensu
% knife data bag from file sensu examples/ssl/ssl.json
% ./examples/ssl/ssl_certs.sh clean
</code></pre>

<p>Roles を作成します。chef-repo なのに何も入っていませんでした&hellip;汗
ここで面白いのは &lsquo;sensu-client&rsquo; は sensu で言う subscribers の名前にそのまま利
用されるところです。つまり &lsquo;sensu-client&rsquo; の名前が記された sensu サーバ上の監
視項目 (checks) がこの sensu クライアントに割り当てられます。</p>

<pre><code class="language-ruby">% mkdir roles
% ${EDITOR} roles/sensu-server.rb
name &quot;sensu-server&quot;
description &quot;role applied to sensu server.&quot;
run_list 'recipe[sensu::default]',
         'recipe[sensu::rabbitmq]',
         'recipe[sensu::redis]',
         'recipe[sensu::server_service]',
         'recipe[sensu::api_service]',
         'recipe[sensu::dashboard_service]',
         'recipe[chef-client::service]'

% ${EDITOR} roles/sensu-client.rb
name &quot;sensu-client&quot;
description &quot;role applied to sensu client.&quot;
run_list 'recipe[sensu::default]',
         'recipe[sensu::client_service]',
         'recipe[chef-client::service]'
</code></pre>

<p>Cheffile に &lsquo;chef-client&rsquo; の Cookbook 名を追記します。</p>

<pre><code class="language-ruby">site 'http://community.opscode.com/api/v1'

cookbook 'sensu', path: './'
cookbook 'sensu-test', path: './test/cookbooks/sensu-test'
cookbook 'chef-client' # &lt;---- 追記
</code></pre>

<p>librarian-chef を実行して Cookbooks を取得します。</p>

<pre><code class="language-bash">% librarian-chef install
</code></pre>

<p>Rabbitmq, Redis, API の IP アドレスを設定します。IP アドレスは例です。</p>

<pre><code class="language-bash">% ${EDITOR} cookbooks/sensu/attributes/default.rb
default.sensu.rabbitmq.host = &quot;172.24.19.11&quot;
default.sensu.redis.host = &quot;172.24.19.11&quot;
default.sensu.api.host = &quot;172.24.19.11&quot;
</code></pre>

<p>sensu-server 用の Recipe に監視項目を追記します。ここでは cron デーモンの稼働
状況を監視してみました。</p>

<pre><code class="language-bash">% ${EDITOR} cookbooks/sensu/recipes/server_service.rb # 下記を追記
</code></pre>

<pre><code class="language-ruby">sensu_check &quot;cron_process&quot; do
  command &quot;check-procs.rb -p cron -C 1&quot;
  handlers [&quot;default&quot;]
  subscribers [&quot;sensu-client&quot;]
  interval 30
  additional(:notification =&gt; &quot;Cron is not running&quot;, :occurrences =&gt; 5)
end
</code></pre>

<p>sensu-client 用の Recipe に client.json (クライアント用設定ファイル) の記述と
上記監視項目に合った plugins 設定の追記を行います。</p>

<pre><code class="language-bash">% ${EDITOR} cookbooks/sensu/recipes/client_service.rb # 下記を追記
</code></pre>

<pre><code class="language-ruby">sensu_client node.name do
  address node.ipaddress
  subscriptions node.roles + [&quot;all&quot;]
end

cookbook_file 'check-procs.rb' do
  source 'check-procs.rb'
  mode 0755
  path '/etc/sensu/plugins/check-procs.rb'
end
</code></pre>

<p>上記 check-procs.rb は community サイトからダウンロードする必要があるのですが
cookbook_file で対応したので files/ ディレクトリ配下に置いておきます。</p>

<pre><code class="language-bash">% mkdir -p cookbooks/sensu/files/default
% wget -o cookbooks/sensu/files/default/check-procs.rb \
  https://github.com/sensu/sensu-community-plugins/raw/master/plugins/processes/check-procs.rb
</code></pre>

<p>上記の check-procs.rb は行頭に &lsquo;#!/usr/bin/env ruby&rsquo; が記されているのですが
Sensu インストール時に入る ruby は /opt/sensu/embedded/bin/ruby にあるので行頭
の1行を書き換えます。</p>

<pre><code class="language-diff">% diff cookbooks/sensu/files/default/check-procs.rb.org cookbooks/sensu/files/default/check-procs.rb
- #!/usr/bin/env ruby
+ #!/opt/sensu/embedded/bin/ruby
</code></pre>

<p>Roles, Cookbooks を Chef サーバにアップロードします。</p>

<pre><code class="language-bash">% knife cookbook upload -o ./cookbooks -a
% knife role from file roles/*.rb
</code></pre>

<p>いよいよブートストラップします!</p>

<p>まずは sensu-server を。</p>

<pre><code class="language-bash">% knife bootstrap &lt;server_ip_addr&gt; -N sensu-server -r 'role[sensu-server]' \
  -x ubuntu --sudo
</code></pre>

<p>次に監視対象である sensu-client を。</p>

<pre><code class="language-bash">% knife bootstrap &lt;client_ip_addr&gt; -N sensu-client01 -r 'role[sensu-client]' \
  -x ubuntu --sudo
</code></pre>

<p>http://<sensu-server の IP アドレス>:8080/ にアクセスすると Sensu の管理画面が
表示されます。認証アカウントは cookbooks/sensu/attributes/default.rb に記述が
ありますので確認して下さい。</p>

<h2 id="まとめ">まとめ</h2>

<p>インフラリソースのフルオートメーション化について情報リサーチしていますが監視も
重要なインフラリソースの一部です。Sensu サーバ・クライアントの自動デプロイが出
来たのでこれで一つパーツが揃ったことに。Sensu は API を持っていますのでアプリ
から検知することも簡単に出来ますよ。API については下記を参照してください。</p>

<p><a href="http://sensuapp.org/docs/0.12/api">http://sensuapp.org/docs/0.12/api</a></p>

<p>また以前抱えていた問題もスッキリクリアになって、これでまた前進できた感があります。</p>

<h2 id="参考サイト">参考サイト</h2>

<ul>
<li><a href="http://sensuapp.org/docs/0.12">http://sensuapp.org/docs/0.12</a></li>
<li><a href="http://blog.glidenote.com/blog/2013/11/26/sensu/">http://blog.glidenote.com/blog/2013/11/26/sensu/</a></li>
<li><a href="https://github.com/sensu/sensu-chef">https://github.com/sensu/sensu-chef</a></li>
</ul>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2013/11/27/sensu-chef-deploy-2/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>


        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2013/11/26/vyatta-timemachine-netatalk/">Vyatta で Mac 用 TimeMachine サーバ兼ファイルサーバを構築！</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2013-11-26'>
            November 26, 2013</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは。<a href="https://twitter.com/jedipunkz">@jedipunkz</a> です。</p>

<p>自宅ルータを Vyatta で運用しているのですが、諸電力な筐体に交換した際に HDD ス
ロットが余っていたので HDD を一本さしてみました。もったいないので Netatalk を
インストールして Mac 用の TimeMachine サーバにするか！そんでもってファイルサー
バ兼務にしよう！と思い立って作業したら簡単に出来たので共有します。</p>

<p>Vyatta はご存知の通り Debian Gnu/Linux がベースになっているのでパッケージレポ
ジトリを追加してちょちょいで設定出来ます。</p>

<h2 id="手順">手順</h2>

<p>電源を通して Disk を追加します。その後起動。私は 3TB Disk が余っていたのでそれ
を挿しました。</p>

<p>debian wheezy のパッケージレポジトリを Vyatta に追記します。</p>

<pre><code class="language-bash">% configure
# set system package repository debian url http://ftp.jp.debian.org/debian
# set system package repository debian distribution wheezy
# set system package repository debian components &quot;main contrib&quot;
# commit
# save
# exit 
</code></pre>

<p>netatalk, avahi をインストールする。その際に libgcrypt11 のバージョン 1.5.0 が
必要になるのでインストールすること。</p>

<pre><code class="language-bash">% sudo apt-get update
% sudo apt-get install netatalk avahi-daemon avahi-utils libgcrypt11
</code></pre>

<p>ディスクのパーティショニング・フォーマットを行うために e2fsprogs, gdisk を入れ
る。2TB オーバーな disk が一般的になったので fdisk ではなく gdisk を使う。</p>

<pre><code class="language-bash">% sudo apt-get install e2fsprogs gdisk
</code></pre>

<p>パーティショニングを行う。私は1つの大きなパーティションを作りました。その後ファ
イルシステム ext4 にてフォーマットを行います。</p>

<pre><code class="language-bash">% sudo gdisk /dev/sdb # パーティショニング方法は割愛
% sudo mkfs.ext4 /dev/sdb1
</code></pre>

<p>/mnt ディレクトリにマウントします。/mnt/storage をファイルサーバ用,
/mnt/timemachine を Mac の TimeMachine 用として稼働させるためにディレクトリを
作成します。</p>

<pre><code class="language-bash">% sudo mount -t ext4 /dev/sdb1 /mnt
% sudo vi /etc/fstab # /mnt の記述追加
% sudo mkdir /mnt/storage ; sudo chown &lt;userid&gt;:users /mnt/storage
% sudo mkdir /mnt/timemachine ; sudo chown &lt;userid&gt;:users /mnt/timemachine
</code></pre>

<p>/etc/netatalk/apfd.conf を修正します。</p>

<pre><code class="language-diff">% diff /etc/netatalk/apfd.conf.org /etc/netatalk/afpd.conf
- # - -tcp -noddp -uamlist uams_dhx.so,uams_dhx2.so -nosavepassword
+ - -tcp -noddp -uamlist uams_guest.so,uams_dhx.so,uams_dhx2.so -nosavepassword
</code></pre>

<p>TimeMachine 用のディレクトリパスに下記のファイルを touch します。中身は空で OK
です。</p>

<pre><code class="language-bash">% touch /mnt/timemachine/.com.apple.timemachine.supported
</code></pre>

<p>/etc/netatalk/AppleVolumes.default ファイルにファイルサーバ, TimeMachine 用の
設定を投入します。TimeMachine 用は下記の通りオプションが必要になります。</p>

<pre><code class="language-bash">% sudo vi /etc/netatalk/AppleVolumes.default # 下記を追記
/mnt/storage &quot;Storage&quot;
/mnt/timemachine &quot;TimeMachine&quot; cnidscheme:dbd options:usedots,upriv,tm
</code></pre>

<p>netatalk を再起動します。</p>

<pre><code class="language-bash">% sudo /etc/init.d/netatalk restart
</code></pre>

<h2 id="timemachine-を設定する-mac-側の設定">TimeMachine を設定する Mac 側の設定</h2>

<p>今回の様に Apple 公式の TimeCupsule 以外のマシンを TimeMachine のバックアップ
先として利用する場合、Mac 側で下記の設定が必要になります。</p>

<pre><code class="language-bash">% defaults write com.apple.systempreferences TMShowUnsupportedNetworkVolumes 1
</code></pre>

<p>あとは &lsquo;環境設定&rsquo; -&gt; &lsquo;TimeMachine&rsquo; にて Vyatta を TimeMachine のバックアップ先
に指定すれば OK です。</p>

<h2 id="まとめ">まとめ</h2>

<p>パッケージレポジトリは Wheezy の main, contrib を設定しましたが他のモノを入れ
ることも勿論可能。最近の Netatalk は TimeMachine 用の設定が簡単にできるように
なっていました。</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2013/11/26/vyatta-timemachine-netatalk/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>


        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2013/11/17/openstack-havana-chef-deploy/">OpenStack Havana を Chef でデプロイ</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2013-11-17'>
            November 17, 2013</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは。<a href="https://twitter.com/jedipunkz">@jedipunkz</a> です。</p>

<p>毎度お馴染みになった OpenStack の Chef によるデプロイですが、今回は OpenStack
Havana 構成を Chef でデプロイする方法についてです。使用するのは今回も
rcbops/chef-cookbooks です。ブランチは &lsquo;havana&rsquo; を用います。</p>

<p>早速ですが構成について。4.1.2 辺りからだと思うのですが構成の前提が物理ネットワー
ク4つを前提にし始めました。public, external (VM) を別ける必要が出てきました。
通信の特性も異なるので (public は public API を。external は VM 用) 、別けるの
が得策かもしれません。</p>

<h2 id="構成">構成</h2>

<pre><code>                  +--------------+------------------------------------------------------- external
                  |              |
+--------------+--(-----------+--(-----------+--------------+---------------------------- public
|              |  |           |  |           |              |
+------------+ +------------+ +------------+ +------------+ +------------+ +------------+
| controller | |  network   | |  network   | |  compute   | |  compute   | | workstation|
+------------+ +------------+ +------------+ +------------+ +------------+ +------------+
|              |  |           |  |           |  |           |  |           |
+--------------+--(-----------+--(-----------+--(-----------+--(-----------+------------- management
                  |              |              |              |
                  +--------------+--------------+--------------+------------------------- guest
</code></pre>

<h4 id="上記の構成の特徴">上記の構成の特徴</h4>

<ul>
<li>4つの物理ネットワークを前提</li>
<li>public ネットワーク : 外部 API 用ネットワーク</li>
<li>external ネットワーク : インスタンス外部接続用ネットワーク</li>
<li>guest ネットワーク : インスタンス内部用ネットワーク</li>
<li>management ネットワーク : 各コンポーネント接続用ネットワーク</li>
<li>public, external のみグローバルネットワーク</li>
<li>controller : 2 nics, network : 4 nics, compute : 3nics の構成</li>
<li>controller はシングル構成</li>
<li>network ノードは台数拡張可能, agent 単位でノード間移動可能</li>
<li>compute ノードも台数拡張可能</li>
<li>workstation は chef-repo の所在地, management ネットワークに所属</li>
</ul>

<h2 id="各ノードの準備">各ノードの準備</h2>

<p>OS インストール後、各ノードのネットワークインターフェースの設定を下記の通り行っ
てください。また LVM を使うのであれば cinder ボリュームの設定も必要になってきます。</p>

<h4 id="controller-ノード">controller ノード</h4>

<p>2 nics を下記の通り設定します。</p>

<ul>
<li>eth0 を public ネットワークに。gateway をこのインターフェースに設定</li>
<li>eth1 を guest ネットワークに。gateway はなし</li>
</ul>

<p>/etc/network/interfaces の例</p>

<pre><code>auto eth0
iface eth0 inet static
    address &lt;ip_addr&gt;
    netmask &lt;netmask&gt;
    gateway &lt;gateway&gt;
    dns-nameservers &lt;dns_cache_server&gt;
    dns-search &lt;domain&gt;

auto eth1
iface eth1 inet static
    address &lt;ip_addr&gt;
    netmask &lt;netmask&gt;
</code></pre>

<p>/dev/sdb を cinder 用ディスクデバイスとします。</p>

<pre><code class="language-bash">% sudo pvcreate /dev/sdb
% sudo vgcreate cinder-volumes /dev/sdb
</code></pre>

<h4 id="network-ノード">network ノード</h4>

<p>4 nics を下記の通り設定します。up route で仮想ネットワークへのルーティングを書
いてあげると、network ノードから直接仮想ネットワーク上のインスタンスへ通信する
ことが可能です。最初は書かなくても OK です。</p>

<pre><code>auto eth0
iface eth0 inet static
    address &lt;ip_addr&gt;
    netmask &lt;netmask&gt;
    dns-nameservers &lt;dns_cache_server&gt;
    dns-search &lt;domain&gt;

auto eth1
iface eth1 inet static
    address &lt;ip_addr&gt;
    netmask &lt;netmask&gt;

auto eth2
iface eth2 inet static
    address &lt;ip_addr&gt;
    netmask &lt;netmask&gt;
    gateway 10.200.10.1
    dns-nameservers 8.8.8.8 8.8.4.4
    dns-search cpi.ad.jp

auto eth3
iface eth3 inet static
    address &lt;ip_addr&gt;
    netmask &lt;netmask&gt;
    # 仮想ネットワークへのルーティング
    # up route add -net &lt;virtual_net_cidr&gt; gw &lt;neutron_gw&gt;
    # up route add -net &lt;virtual_net_cidr&gt; gw &lt;neutron_gw&gt;
</code></pre>

<h4 id="compute-ノード">compute ノード</h4>

<p>3 nics を下記の通り設定します。</p>

<pre><code>auto eth0
iface eth0 inet static
    address &lt;ip_addr&gt;
    netmask &lt;netmask&gt;
    dns-nameservers &lt;dns_cache_server&gt;
    dns-search &lt;domain&gt;

auto eth1
iface eth1 inet static
    address &lt;ip_addr&gt;
    netmask &lt;netmask&gt;
    
auto eth2
iface eth2 inet static
    address &lt;ip_addr&gt;
    netmask &lt;netmask&gt;
    gateway &lt;gateway&gt;
    dns-nameservers &lt;dns_cache_server&gt;
    dns-search &lt;domain&gt;
</code></pre>

<h2 id="cookbooks-roles-environments-等の準備">Cookbooks, Roles, Environments 等の準備</h2>

<p>下記の操作は全て workstation ノードからの操作です。
また chef のインストールや chef サーバの構築方法については割愛します。</p>

<p>rcbops/chef-cookbooks を取得します。</p>

<pre><code class="language-bash">% git clone https://github.com/rcbops/chef-cookbooks.git
% cd chef-cookbooks
% git checkout -b havana remotes/origin/havana
% # .chef 配下の準備割愛。各 Chef サーバ環境に合わせる
% git submodule init
% git submodule sync
% git submodule update
% knife cookbook upload -o cookbooks -a
% knife role from file roles/*.rb
</code></pre>

<p>今回の構成用の environment を生成します。それぞれの環境に合わせて作成してくだ
さい。生成のコツは各 Cookbooks の attributes を見ながら設定することです。
生成した json ファイルを environments ディレクトリ配下に配置します。</p>

<pre><code class="language-json">{
  &quot;name&quot;: &quot;havana-neutron&quot;,
  &quot;description&quot;: &quot;&quot;,
  &quot;cookbook_versions&quot;: {
  },
  &quot;json_class&quot;: &quot;Chef::Environment&quot;,
  &quot;chef_type&quot;: &quot;environment&quot;,
  &quot;default_attributes&quot;: {
  },
  &quot;override_attributes&quot;: {
    &quot;package_component&quot;: &quot;havana&quot;,
    &quot;osops_networks&quot;: {
      &quot;management&quot;: &quot;10.200.10.0/24&quot;,
      &quot;public&quot;: &quot;10.200.9.0/24&quot;,
      &quot;nova&quot;: &quot;10.200.10.0/24&quot;
    },
    &quot;nova&quot;: {
      &quot;config&quot;: {
        &quot;use_single_default_gateway&quot;: false,
        &quot;ram_allocation_ratio&quot;: &quot;1&quot;,
        &quot;cpu_allocation_ratio&quot;: &quot;16&quot;
      },
      &quot;network&quot;: {
        &quot;provider&quot;: &quot;neutron&quot;,
        &quot;network_type&quot;: &quot;vlan&quot;
      },
      &quot;apply_patches&quot;: true,
      &quot;libvirt&quot;: {
        &quot;vncserver_listen&quot;: &quot;0.0.0.0&quot;,
        &quot;virt_type&quot;: &quot;kvm&quot;
      },
      &quot;db&quot;: {
        &quot;password&quot;: &quot;nova&quot;
      },
      &quot;services&quot;: {
        &quot;novnc-proxy&quot;: {
          &quot;network&quot;: &quot;public&quot;
        },
        &quot;ec2-admin&quot;: {
          &quot;network&quot;: &quot;management&quot;
        }
      }
    },
    &quot;cinder&quot;: {
      &quot;db&quot;: {
        &quot;password&quot;: &quot;cinder&quot;
      }
    },
    &quot;keystone&quot;: {
      &quot;admin_user&quot;: &quot;admin&quot;,
      &quot;tenants&quot;: [
        &quot;admin&quot;,
        &quot;service&quot;
      ],
      &quot;users&quot;: {
        &quot;admin&quot;: {
          &quot;password&quot;: &quot;secrete&quot;,
          &quot;roles&quot;: {
            &quot;admin&quot;: [
              &quot;admin&quot;
            ]
          }
        },
        &quot;demo&quot;: {
          &quot;password&quot;: &quot;demo&quot;,
          &quot;default_tenant&quot; : &quot;service&quot;,
          &quot;roles&quot;: {
            &quot;service&quot;: [ &quot;service&quot; ]
          }
        }
      },
      &quot;db&quot;: {
        &quot;password&quot;: &quot;keystone&quot;
      }
    },
    &quot;horizon&quot;: {
      &quot;theme&quot;: &quot;Rackspace&quot;,
      &quot;db&quot;: {
        &quot;password&quot;: &quot;horizon&quot;
      },
      &quot;endpoint_type&quot; : &quot;publicURL&quot;,
      &quot;endpoint_scheme&quot; : &quot;http&quot;
    },
    &quot;mysql&quot;: {
      &quot;root_network_acl&quot;: &quot;%&quot;,
      &quot;allow_remote_root&quot;: true,
      &quot;server_root_password&quot;: &quot;secrete&quot;,
      &quot;server_repl_password&quot;: &quot;secrete&quot;,
      &quot;server_debian_password&quot;: &quot;secrete&quot;
    },
    &quot;monitoring&quot;: {
      &quot;procmon_provider&quot;: &quot;monit&quot;,
      &quot;metric_provider&quot;: &quot;collectd&quot;
    },
    &quot;glance&quot;: {
      &quot;images&quot;: [
        &quot;precise&quot;, &quot;cirros&quot;
      ],
      &quot;image&quot;: {
      },
      &quot;image_upload&quot;: false,
      &quot;db&quot;: {
        &quot;password&quot;: &quot;glance&quot;
      }
    },
    &quot;neutron&quot;: {
      &quot;service_pass&quot;: &quot;neutron&quot;,
      &quot;db&quot;: {
        &quot;password&quot;: &quot;neutron&quot;
      }
    },
    &quot;developer_mode&quot;: false
  }
}
</code></pre>

<p>environment ファイルの chef サーバへのアップロード。</p>

<pre><code class="language-bash">% knife environment from file environments/havana-neutron.json
</code></pre>

<h2 id="knife-bootstrap-によるデプロイ">knife bootstrap によるデプロイ</h2>

<p>下記の通り knife bootstrap することで各ノードをデプロイします。</p>

<p>controller ノードのデプロイ。</p>

<pre><code class="language-bash">% knife bootstrap &lt;controller_ipaddr&gt; -N &lt;controller_name&gt; \
  -r 'role[single-controller]','role[cinder-volume]' \
  -E havana-neutron -x &lt;username&gt; --sudo
</code></pre>

<p>network ノードのデプロイ。台数分デプロイしてください。</p>

<pre><code class="language-bash">% knife bootstrap &lt;network_ipaddr&gt; -N &lt;network_name&gt; \
  -r 'role[single-network-node]','recipe[nova-network::neutron-l3-agent]' \
  -E neutron-havana -x &lt;username&gt; --sudo
</code></pre>

<p>compute ノードのデプロイ。台数分デプロイしてください。</p>

<pre><code class="language-bash">% knife bootstrap &lt;compute_ipaddr&gt; -N &lt;compute_name&gt; \
  -r 'role[single-compute]' \
  -E havana-neutron -x &lt;username&gt; --sudo
</code></pre>

<h2 id="openvswitch-の物理-nic-とブリッジインタフェースマッピング作業">openvswitch の物理 NIC とブリッジインタフェースマッピング作業</h2>

<p>各ノードで下記の通り物理 NIC とブリッジのマッピング作業を行ってください。操作
は必ず management ネットワークを介して行うようにしましょう。その他のネットワー
クから操作すると通信が途絶える可能性があります。</p>

<p>network ノード</p>

<pre><code class="language-bash">% sudo ovs-vsctl add-port br-eth1 eth1
% sudo ovs-vsctl add-port br-ex eth3
</code></pre>

<p>compute ノード</p>

<pre><code class="language-bash">% sudo ovs-vsctl add-port br-eth1 eth1
</code></pre>

<h2 id="まとめと考察">まとめと考察</h2>

<p>havana の roles にはコアプロジェクトのコンポーネントが入っています。つまり
ceilometer と heat も上記のデプロイで入ってきます。ceilometer に関して公式のド
キュメントではデータ格納用 DB として mongodb が記されていますが、ここでは
mysqld 上に格納する構成になっていました。これは個人的には非常に助かります。
また、compute ノードから controller ノードの public api を叩く必要が出てきて、
compute ノードにも public ネットワーク用の NIC を足しましたが、こうするべきな
のかどうかは今後考えてみます。理想は management ネットワークを介することですが
attributes の調整でけではうまくいきませんでした。&rsquo;havana&rsquo; ブランチはまだ開発が
進んでいるので改善されるかもしれません。コントリビュートするのも手だと思います。</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2013/11/17/openstack-havana-chef-deploy/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>


        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2013/11/13/elasticsearch-second-study-report/">第2回 Elasticsearch 勉強会参加レポート</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2013-11-13'>
            November 13, 2013</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは。<a href="https://twitter.com/jedipunkz">@jedipunkz</a> です。</p>

<p>第2回 Elasticsearch 勉強会に参加してきました。箇条書きですが参加レポートを記し
ておきます。</p>

<pre><code>開催日 : 2013/11/12
場所 : 東京駅 グラントウキョウサウスタワー リクルートテクノロジーズさま
URL : http://elasticsearch.doorkeeper.jp/events/6532
</code></pre>

<h2 id="routing-周りの話">Routing 周りの話</h2>

<pre><code>株式会社シーマーク　大谷純さん (@johtani)
</code></pre>

<h4 id="index-構成">Index 構成</h4>

<ul>
<li>cluster の中に index -&gt; type が作成される</li>
<li>index は shard という部分的な index の集まり</li>
<li>shard 数は生成時のみ指定可能</li>
<li>node ごとに replica, primary を別ける</li>
<li>replica 数は後に変えられる</li>
<li>doc -&gt; hash 値を shard 数で割って replica, primary に登録</li>
<li>doc の id の ハッシュ値を利用</li>
<li>type も含める場合はかの設定を true に</li>
<li>クライアントはどのノードに対してクエリを投げても OK</li>
</ul>

<h4 id="routing">routing</h4>

<ul>
<li>id の代わりに routing (URL パラメータ) で登録</li>
<li>url リクエストパラメータとして登録時にルーティングパラメータを登録</li>
<li>id の代わりにパラメータで指定された値のハッシュ値を計算して利用</li>
<li>検索時 routing 指定で関係のある shard のみを指定出来る</li>
</ul>

<h4 id="スケールアウト">スケールアウト</h4>

<ul>
<li>sharding によるスケールアウト数 = インデックス作成時に指定</li>
<li>shard によるインデックスの分割以外にインデックス自体を複数持つことによるスケール</li>
<li>複数のドキュメントをエイリアス書けることが可能</li>
</ul>

<h4 id="所感">所感</h4>

<p>個人的には非常に興味のあるところでした。mongodb のような sharding をイメージし
てよいのか？そうでないのか？すら理解出来ていなかったので。sharding を理解する
前提知識の話もあって非常に参考になりました。</p>

<h2 id="elasticsearchを使ったbaas基盤の開発">ElasticSearchを使ったBaaS基盤の開発</h2>

<pre><code>株式会社富士通ソフトウェアテクノロジーズ 滝田聖己さん（@pisatoshi）
</code></pre>

<p>運用の話。これは貴重&hellip;。</p>

<ul>
<li>shard 数 : 10 , replica : 1 で運用している</li>
<li>データは業務データ , トラッキングデータ</li>
<li>マルチテナント, 更新の即時反映, ルーティングによる性能向上 が要件</li>
<li>登録更新 -&gt; 検索結果反映までタイムラグがある -&gt; replica 完了まで待つので高コスト</li>
<li>replica 数を減らすことで性能向上</li>
</ul>

<h4 id="routing-id-を指定することで">routing id を指定することで&hellip;</h4>

<ul>
<li>doc id の has から shard 自動選択がデフォルト</li>
<li>-&gt; hash key を指定して格納シャードを制御出来る</li>
<li>-&gt; doc 登録性能向上</li>
<li>-&gt; 検索対象の shard を絞りこみえる</li>
<li>-&gt; 不可を軽減</li>
</ul>

<h4 id="バックアップ-リストア">バックアップ/リストア</h4>

<ul>
<li>mysql にもデータ格納 -&gt; backup</li>
<li>elasticsearch の index はバックアップ取らず -&gt; bug を踏みたくないのが理由</li>
</ul>

<h4 id="dynamic-mapping-の問題">dynamic mapping の問題</h4>

<ul>
<li>入力データから型を推測 -&gt; 自動マッピング登録</li>
<li>-&gt; マッピング定義が肥大化</li>
<li>-&gt; データ型のコンフリクト</li>
</ul>

<h4 id="mapping-の肥大化">mapping の肥大化</h4>

<ul>
<li>mapping 定義は type, field の数に応じてサイズが増加</li>
<li>-&gt; index 性能低下</li>
<li>-&gt; mapping を伴う doc 登録 3sec (mapping size 80MB)</li>
<li>-&gt; 各 node への mapping 定義の同期</li>
<li>大量のデータを一気に登録するときは 1 node が速い</li>
<li>-&gt; その後シャード再配置したことがある</li>
</ul>

<p>=&gt; dynamic mapping を使うのをやめた -&gt; app で指定</p>

<h4 id="よく利用するツール">よく利用するツール</h4>

<ul>
<li>bigdesk</li>
<li>elasticsearch-head</li>
<li>sense (chrome extention)</li>
</ul>

<h4 id="unit-test">unit test</h4>

<p>unit test はどうする？</p>

<ul>
<li>nodeClient テスト開始時に起動</li>
<li>テストデータを配置して起動, 終了時に削除</li>
<li>memoryIndex で高速に実行</li>
<li>elasticsearch-test が便利</li>
</ul>

<h4 id="所感-1">所感</h4>

<p>運用の話はどの技術でも重要!!! 実際に困った話、トラブル等聞けて貴重な時間でした。
よく使うツールの話も意外と参考になるので聞けてよかった。</p>

<h2 id="kibana">Kibana</h2>

<pre><code>Cookpad 水戸祐介さん (@y_310)
</code></pre>

<ul>
<li>作った dashboard は elasticsearch に保存される</li>
<li>db 要らず , web サーバのみ, クライアントサイドの技術のみで実装されている</li>
<li>term, trends, map, table, column&hellip; それぞれ図形式がある
title = &ldquo;test&rdquo;, -title</li>
<li>1つの index に異なるスキーマを持つデータを入れられる</li>
<li>1つの index に入れることでグラフを重ねて比較出来る</li>
<li>やはり dynamic mapping は使わないほうが良い</li>
<li>m1.large x 1 : 1日のインデックスサイズが10GB 超えるあたりで ES が詰まる</li>
</ul>

<p>下記が当日の資料です。</p>

<script async class="speakerdeck-embed"
data-id="3d0776802ddb0131fdd042970ce72c15" data-ratio="1.33333333333333" src="//speakerdeck.com/assets/embed.js"></script>

<h4 id="所感-2">所感</h4>

<p>Cookpad さんで実際に利用されているとか。運用の話と同じくやはり dynamic routing
でのトラブルの話があった。また Cookpad さんでのトラブル例の話もあってよかった。</p>

<h2 id="elasticsearch-kibana-v3を利用する際の運用ノウハウ">ElasticSearch＋Kibana v3を利用する際の運用ノウハウ</h2>

<pre><code>株式会社リブセンス Y.Kentaro さん (@yoshi_ken)
</code></pre>

<p>下記が発表資料。</p>

<p><iframe src="http://www.slideshare.net/slideshow/embed_code/28165189"
width="597" height="486" frameborder="0" marginwidth="0" marginheight="0"
scrolling="no" style="border:1px solid #CCC;border-width:1px 1px
0;margin-bottom:5px" allowfullscreen> </iframe> <div
style="margin-bottom:5px"> <strong> <a
href="https://www.slideshare.net/y-ken/elasticsearch-kibnana-fluentd-management-tips"
title="ElasticSearch+Kibanaでログデータの検索と視覚化を実現するテクニックと運
用ノウハウ" target="_blank">ElasticSearch+Kibanaでログデータの検索と視覚化を実
現するテクニックと運用ノウハウ</a> </strong> from <strong><a
href="http://www.slideshare.net/y-ken" target="_blank">Kentaro
Yoshida</a></strong> </div></p>

<ul>
<li>nginx の basic 認証等でアクセス制限を掛ける例</li>
</ul>

<p>等など&hellip; ごめんなさい！メモが取れてなかった！&hellip;汗</p>

<h2 id="fluentd-as-a-kibana">Fluentd as a Kibana</h2>

<pre><code>@repeatedly さん
</code></pre>

<p>fluentd-plugin-kibana-server  というkibana をダウンロードしてくるのがめんどい
ので fluentd の中で kibana を動かすプラグインを作った話。下記が発表資料。</p>

<p><a href="https://gist.github.com/repeatedly/7427856">https://gist.github.com/repeatedly/7427856</a></p>

<ul>
<li>fluentd input plugin として実装</li>
<li>logstash 版もあるらしい</li>
</ul>

<h4 id="所感-3">所感</h4>

<p>さすが&hellip;。確かにサーバをいちいち作るのめんどいです。fluentd plugin の中で
kibana を動かすって発想が&hellip;。</p>

<h2 id="auth-プラグインでアクセスコントロール">auth プラグインでアクセスコントロール</h2>

<pre><code>株式会社エヌツーエスエム 菅谷信介さん (@shinsuke_sugaya)
</code></pre>

<ul>
<li>elasticsearch のアクセス制御をしたい</li>
<li>rest api をアクセス制御する</li>
<li>ユーザ管理, REST API のアクセス管理, ログイン・ログアウト・トーケン</li>
<li>ユーザ管理は elasticsearch 内に格納</li>
<li>path, http method, role の組み合わせで制御</li>
</ul>

<p>インストールは下記の通り elasticsearch plugin コマンドで。</p>

<pre><code>% ./bin/plugin --install org.codelibs/elasticsearch-auth/1.0.0
</code></pre>

<ul>
<li>ユーザ作成は POST で可能</li>
</ul>

<h2 id="所感とまとめ">所感とまとめ</h2>

<p>箇条書きのざっくりしたまとめでしたが&hellip;。個人的には sharding の話はとても聞き
たかったので貴重な時間でした。また、運用の話もとてもおもしろかったし plugin 作っ
てみた話も「すごい..」って感じでした。僕はこれからアプリも書くけどインフラエン
ジニアなのでアーキテクチャをまず知りたいかなぁという印象。と言うかスケールさせ
るときにも横に並べて shard するだけの様な話に聞こえましたが、まだぼんやり感が
あります。特に shard 周り。mongo の sharding と同じ様なものを想像していますが。
またモチベーション上がったので週末やってみますー！</p>

<p>kibana3 は一回使いました。javascript, html で実装されているのでクライアントか
ら elasticsearch まで直接アクセス出来なければならないとなって、構成的にどうな
のかな？と思っていたら菅谷さんの &lsquo;auth プラグイン&rsquo; の話があったりしたので、良
かったです。使ってみます！</p>

<p>主催の方々、発表者の方々、当日はありがとうございましたー。また参加させてもらい
ます。</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2013/11/13/elasticsearch-second-study-report/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>


        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2013/11/10/serf/">Serf を使ってみた</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2013-11-10'>
            November 10, 2013</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは。<a href="https://twitter.com/jedipunkz">@jedipunkz</a> です。</p>

<p>僕は Chef 使いなのですが、Chef はオーケストレーションまで踏み込んだツールでは
ないように思います。せいぜいインテグレーションが出来る程度なのかなぁと。
しかもインテグレーションするにも Cookbooks の工夫が必要です。以前聞いたことの
ある話ですが Opscode 社のエンジニア曰く「オーケストレーション等へのアプローチ
はそれぞれ好きにやってね」だそうです。</p>

<p>個人的にオーケストレーションをテーマに色々調べようかと考えているのですが、
Serf という面白いツールが出てきました。&rsquo;Serf&rsquo; はオーケストレーションを手助けし
てくれるシンプルなツールになっています。</p>

<p>もう既にいろんな方が Serf について調べていますが、どのような動きをするのかを自
分なりに理解した点を記しておこうと思います。</p>

<h2 id="参考にしたサイト">参考にしたサイト</h2>

<ul>
<li>公式サイト <a href="http://www.serfdom.io/">http://www.serfdom.io/</a></li>
<li>クラスメソッド開発者ブログ <a href="http://dev.classmethod.jp/cloud/aws/serf_on_ec2/">http://dev.classmethod.jp/cloud/aws/serf_on_ec2/</a></li>
<li>Glidenote さん <a href="http://blog.glidenote.com/blog/2013/10/30/serf-haproxy/">http://blog.glidenote.com/blog/2013/10/30/serf-haproxy/</a></li>
</ul>

<h2 id="serf-とは">Serf とは</h2>

<p>Serf は gossip protocol をクラスタにブロードキャストする。gossip protocol は
SWIM : Scalable Weakly-consistent Infecton-style process Group Membership
Protocol” をベースとして形成されている。</p>

<h2 id="swim-protocol-概略">SWIM Protocol 概略</h2>

<p>serf は新しいクラスタとして稼働するか、既存のクラスタに ‘join’ する形で稼働
するかのどちらかで起動する。</p>

<p>新しいメンバは TCP で状態を &lsquo;full state sync&rsquo; され既存のクラスタ内にて
‘gossipin (噂)される。この ’gosiping’ は UDP で通信されこれはネットワーク使
用量はノード数に比例することになる。</p>

<p>ランダムなノードとの &lsquo;full state sync&rsquo; は TCP で行われるけどこれは
‘gossiping’ に比べて少い</p>

<p>ある一定期間、あるノードが fails 状態の場合、迂回経路を使ってそのノードに対し
てチェックを行う。両方の経路にて fails な場合ノードが ‘suspiciou&rsquo; (容疑者)
状態になる。もし迂回経路のチェックが fails しなかった場合ネットワーク障害として
判断される。</p>

<h2 id="serf-の-swim-protocol-からの改修点">Serf の SWIM Protocol からの改修点</h2>

<p>Serf は SWIM Protocol をベースにしていると記しましたが、どのような点を改修した
のかまとめました。</p>

<ul>
<li>&lsquo;full state sync&rsquo; の TCP 化</li>
</ul>

<p>serf は ‘full state sync’ を TCP にて行う。これにより serf はネットワーセグ
メントが分離されている状態でも通信出来る。</p>

<ul>
<li>gossip レイヤの分離</li>
</ul>

<p>serf は failure detection protocol から完全に gossip レイヤを分離。これでより
高速にデータ増殖が行える。</p>

<ul>
<li>dead node の扱い
<br /></li>
</ul>

<p>serf は デッドノードに対して回数を記憶している。これにより full state sync が
実施された場合、逆に死んだノードの情報を受取る。SWIM では full state sync を行
わないので死んだノードにを削除してしまう。これはクラスタに対する情報一点集中化
に役立つ。</p>

<h2 id="使ってみる">使ってみる</h2>

<p>早速使ってみます。下記の URL にある demo 用スクリプトを少し改修して使ってみま
した。</p>

<p><a href="https://github.com/hashicorp/serf/tree/master/demo/web-load-balancer">https://github.com/hashicorp/serf/tree/master/demo/web-load-balancer</a></p>

<ul>
<li>2つノードを立ち上げる</li>
</ul>

<p>AWS でも OpenStack でもベアメタルでも何でも良いのでノードを2台用意する。</p>

<ul>
<li>下記のスクリプトを置いて両ノードで実行する。</li>
</ul>

<pre><code class="language-bash">#!/bin/sh
set -e

export SERF_ROLE=role01

sudo apt-get install -y unzip

# Download and install Serf
cd /tmp
until wget -O serf.zip https://dl.bintray.com/mitchellh/serf/0.2.1_linux_amd64.zip; do
sleep 1
done
unzip serf.zip
sudo mv serf /usr/local/bin/serf

# The member join script is invoked when a member joins the Serf cluster.
# Our join script simply adds the node to the load balancer.
cat &lt;&lt;EOF &gt;/tmp/join.sh
#!/bin/sh
echo 'member joined' &gt;&gt; /tmp/serf_join.log
EOF
sudo mv /tmp/join.sh /usr/local/bin/serf_member_join.sh
chmod +x /usr/local/bin/serf_member_join.sh

# Configure the agent
cat &lt;&lt;EOF &gt;/tmp/agent.conf
description &quot;Serf agent&quot;

start on runlevel [2345]
stop on runlevel [!2345]

exec /usr/local/bin/serf agent \\
-event-handler &quot;member-join=/usr/local/bin/serf_member_join.sh&quot; \\
-role=${SERF_ROLE} &gt;&gt;/var/log/serf.log 2&gt;&amp;1
EOF
sudo mv /tmp/agent.conf /etc/init/serf.conf

# Start the agent!
sudo start serf
</code></pre>

<p>実行すると &lsquo;role01&rsquo; という Role 名で serf agent が稼働しているはず。またスクリ
プトを見てわかると思うが &ndash;event-handler &ldquo;member-join=&lt;スクリプト&gt;&rdquo; としている。
これでクラスタに新しいメンバが join すると /tmp/serf_join.log に &lsquo;member
joined&rsquo; というメッセージが出力されるはずだ。実際に実行してみる。</p>

<pre><code class="language-bash">% sudo serf join &lt;もう片系ノードの IP&gt;
% sudo serf members
  vm01    10.0.2.1    alive    role01
  vm02    10.0.2.3    alive    role01
% tail /tmp/serf_join.log
  ‘member joined
  ‘member joined
</code></pre>

<p>イベントハンドラにはユーザ指定のものも扱える。</p>

<pre><code>user:deploy=foo.sh
</code></pre>

<p>この場合のハンドラは下記のコマンドで発生出来る。</p>

<pre><code class="language-bash">% serf event deploy
</code></pre>

<h2 id="コンフィギュレーションファイル">コンフィギュレーションファイル</h2>

<p>今回は init 起動スクリプト内でイベントハンドラ発生時のスクリプト指定等を行った
が、json 形式のコンフィギュレーションファイルにて記述することも可能。</p>

<pre><code class="language-json">{
  &quot;role&quot;: &quot;load-balancer&quot;,

  &quot;event_handlers&quot;: [
    &quot;member_join.sh&quot;,
    &quot;user:deploy=foo.sh&quot;
  ]
}
</code></pre>

<p>上記ファイルを serf.conf とした場合、このコンフィグの指定は
&ndash;config-file=serf.conf で行える。</p>

<h2 id="ロードマップ">ロードマップ</h2>

<p>最後に Serf の今後のロードマップについて記してあったので、まとめてみた。</p>

<ul>
<li>コンフィギュレーションファイル</li>
</ul>

<p>よりカスタマイズ可能なコンフィギュレーションファイルを扱えるようにする。</p>

<ul>
<li>SIGHUP</li>
</ul>

<p>SIGHUP 信号でリロード出来ないので、これに対応する。</p>

<ul>
<li>イベントハンドラライブラリ</li>
</ul>

<p>イベントハンドラを自作・シェアを容易に行えるようプラグイン化を進める。</p>

<h2 id="まとめと考察">まとめと考察</h2>

<p>冒頭にオーケストレーションについて触れましたが、このツールを使うだけでは自分の
考えているオーケストレーションにはならないと思いました。当たり前ですね。。複数
ノードを束ねて構成を形成する、またそれをトリガするのが人間であればそれはインテ
グレーションの範囲かなぁと。その先にオーケストレーションがあるとすればトリガ自
身もソフトウェアにさせる必要があるのでそのアルゴリズムを人間が書く必要があるの
かなぁと。もちろんそのためにこのSerf は貴重なパーツとなってくれそう。</p>

<p>あと、ロードマップにも記しましたが開発が盛んにされているようなので今後が楽しみ
です。</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2013/11/10/serf/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>


        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2013/10/27/swift-chef/">実用的な Swift 構成を Chef でデプロイ</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2013-10-27'>
            October 27, 2013</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは。<a href="https://twitter.com/jedipunkz">@jedipunkz</a> です。</p>

<p>以前、&rdquo;Swift HA 構成を Chef でデプロイ&rdquo; というタイトルで記事を書きました。</p>

<p><a href="http://jedipunkz.github.io/blog/2013/07/26/swift-ha-chef-deploy/">http://jedipunkz.github.io/blog/2013/07/26/swift-ha-chef-deploy/</a></p>

<p>こちらですが、Swift-Proxy, MySQL, Keystone をそれぞれ haproxy, keepalived で
HA 組みました。ですが、これは実用的なのかどうか自分でずっと考えていました。</p>

<p>MySQL と KeepAlived はできればシングル構成にしたいのと、Swift-Proxy は HA で組
みたい。MySQL は Master/Master レプリケーション構成になり、どちらかのノードが
障害を起こし万が一復旧が難しくなった時、構築し直しがしんどくなります。かと言っ
て Swift-Proxy をシングル構成にすると今度はノード追加・削除の作業時にサービス
断が発生します。Swift-Proxy を再起動書ける必要があるからです。なので
Swift-Proxy は引き続き HA 構成にしたい。</p>

<p>もう一点、見直したいと思っていました。</p>

<p>日経コンピュータから出版されている &ldquo;仮想化大全 2014&rdquo; の記事を読んでいて
気がついたのですが。Swift には下記の通りそれぞれのサーバがあります。</p>

<ul>
<li>swift-proxy-server</li>
<li>swift-account-server</li>
<li>swift-container-server</li>
<li>swift-object-server</li>
</ul>

<p>Swift には下記のような特徴がある事がわかりました。</p>

<ul>
<li>swift-object</li>
</ul>

<p>swift-object は swift-accout, swift-container とは物理リソースの扱いに全く異な
る特性を持っています。swift-account, swift-container はクライアントからのリクエ
ストに対して &ldquo;アカウントの存在を確認&rdquo;, &ldquo;ACL 情報の確認&rdquo; 等を行うサーバであるの
に対して swift-object はストレージ上のオブジェクトをクライアントに提供、または
逆に格納するサーバです。よって、Disk I/O の利用特性として swift-account,
container は SSD 等、高スループットの Disk を利用するケースが推奨されるのに対
して swift-object はオブジェクトの実体を格納する必要があるため Disk 容量の大き
なストレージを要する。</p>

<p>よって今回は下記の構成変更をしてより実用的な構成を Chef でデプロイする方法を記
そうと思います。</p>

<ul>
<li>swift-object を swift-account, swift-container とノードを分離する</li>
<li>MySQL/Keystone の HA を行わず Swift-Proxy のみ HA 化する</li>
</ul>

<h2 id="参考資料">参考資料</h2>

<ul>
<li><a href="http://www.rackspace.com/knowledge_center/article/managing-openstack-object-storage-with-chef">http://www.rackspace.com/knowledge_center/article/managing-openstack-object-storage-with-chef</a></li>
<li><a href=http://www.amazon.co.jp/%E3%81%99%E3%81%B9%E3%81%A6%E3%82%8F%E3%81%8B%E3%82%8B%E4%BB%AE%E6%83%B3%E5%8C%96%E5%A4%A7%E5%85%A82014-%E6%97%A5%E7%B5%8CBP%E3%83%A0%E3%83%83%E3%82%AF-%E6%97%A5%E7%B5%8C%E3%82%B3%E3%83%B3%E3%83%94%E3%83%A5%E3%83%BC%E3%82%BF/dp/4822262871>仮想化大全</a></li>
</ul>

<h2 id="構成">構成</h2>

<pre><code>+-----------------+
|  load balancer  |
+-----------------+
|
+-------------------+-------------------+-------------------+-------------------+---------------------- proxy network
|                   |                   |                   |                   |                   
+-----------------+ +-----------------+ +-----------------+ +-----------------+ +-----------------+
|   chef server   | | chef workstation| |   swift-mange   | |  swift-proxy01  | |  swift-proxy02  | 
+-----------------+ +-----------------+ +-----------------+ +-----------------+ +-----------------+ ...&gt; scaling
|                   |                   |                   |                   |                   
+-------------------+-------------------+-------------------+-------------------+-------------------+-- storage network
|                   |                   |                   |                   |                   |
+-----------------+ +-----------------+ +-----------------+ +-----------------+ +-----------------+ +-----------------+ 
| swift-storage01 | | swift-storage02 | | swift-storage03 | | swift-account01 | | swift-account02 | | swift-account03 |
+-----------------+ +-----------------+ +-----------------+ +-----------------+ +-----------------+ +-----------------+ ..&gt; scaling
</code></pre>

<h4 id="特徴">特徴</h4>

<ul>
<li>load balancer は一般的なハードウェア、ソフトウェアで用意 (今回は割愛)</li>
<li>swift-proxy は同等の機能のモノを並列に</li>
<li>swift-storage と &lsquo;swift-account, container&rsquo; はそれぞれの zone 毎に並べるため同数</li>
<li>swift-account は swift-account, swift-container が稼働</li>
<li>swift-manage は mysql, keystone, git server を搭載</li>
<li>chef server は外部に配置しても構わないが到達出来る箇所に</li>
<li>chef workstation は全ての操作を行う端末。全てのノードに到達出来る箇所に</li>
</ul>

<h2 id="それぞれのノードでの-disk-の準備">それぞれのノードでの disk の準備</h2>

<p>object, account, container 共に OS 領域とは別の disk を必要とするため /dev/sdb
等のディスクを追加し下記の通り gdisk でパーティションを切る。</p>

<pre><code class="language-bash">% sudo apt-get update; sudo apt-get -y install gdisk
% sudo gdisk /dev/sdb # デバイス名は例
</code></pre>

<p>対象ノード : swift-object0[1-3], swift-account0[1-3]</p>

<h2 id="デプロイ準備">デプロイ準備</h2>

<p>下記は全て chef-workstation での操作</p>

<p>rcpops 管理の chef-repo を取得する。</p>

<pre><code class="language-bash">% git clone https://github.com/rcbops/chef-cookbooks.git
% cd chef-cookbooks
</code></pre>

<p>現時点で v4.1.2 がリリースされているので tag を利用して checkout する。</p>

<pre><code class="language-bash">% git tag -l
1.0.0
release-2011.3-d5
release-v2.0
v2.9.0
v2.9.1
v2.9.2
v2.9.3
v2.9.4
v2.9.5
v2.9.6
v2.9.7
v2.9.8
v3.0.0
v3.0.1
v3.1.0
v4.1.0
v4.1.1
v4.1.2
% git checkout -b v4.1.2 refs/tags/v4.1.2
</code></pre>

<p>git submodule 化されている cookbooks を取得する。</p>

<pre><code class="language-bash">% git submodule init
% git submodule sync
% git submodule upate
</code></pre>

<p>cookbooks, roles の chef-server へのアップロードを行う。</p>

<pre><code class="language-bash">% knife cookbook upload -o cookbook -a
% knife role from file role/*.rb
</code></pre>

<p>今回の構成一式を収容する chef environment を作成する。</p>

<pre><code class="language-json">{
  &quot;name&quot;: &quot;swift&quot;,
  &quot;description&quot;: &quot;&quot;,
  &quot;cookbook_versions&quot;: {
  },
  &quot;json_class&quot;: &quot;Chef::Environment&quot;,
  &quot;chef_type&quot;: &quot;environment&quot;,
  &quot;default_attributes&quot;: {
  },
  &quot;override_attributes&quot;: {
    &quot;package_component&quot;: &quot;grizzly&quot;,
    &quot;osops_networks&quot;: {
      &quot;management&quot;: &quot;10.200.9.0/24&quot;,
      &quot;public&quot;: &quot;10.200.9.0/24&quot;,
      &quot;nova&quot;: &quot;10.200.9.0/24&quot;,
      &quot;swift&quot;: &quot;10.200.9.0/24&quot;
    },
    &quot;keystone&quot;: {
      &quot;admin_user&quot;: &quot;admin&quot;,
      &quot;tenants&quot;: [
        &quot;admin&quot;,
        &quot;service&quot;
      ],
      &quot;users&quot;: {
        &quot;admin&quot;: {
          &quot;password&quot;: &quot;secrete&quot;,
          &quot;roles&quot;: {
            &quot;admin&quot;: [
              &quot;admin&quot;
            ]
          }
        },
        &quot;demo&quot;: {
          &quot;password&quot;: &quot;demo&quot;,
          &quot;default_tenant&quot; : &quot;service&quot;,
          &quot;roles&quot;: {
            &quot;admin&quot;: [ &quot;admin&quot; ]
          }
        }
      },
      &quot;db&quot;: {
        &quot;password&quot;: &quot;keystone&quot;
      }
    },
    &quot;mysql&quot;: {
      &quot;root_network_acl&quot;: &quot;%&quot;,
      &quot;allow_remote_root&quot;: true,
      &quot;server_root_password&quot;: &quot;secrete&quot;,
      &quot;server_repl_password&quot;: &quot;secrete&quot;,
      &quot;server_debian_password&quot;: &quot;secrete&quot;
    },
    &quot;monitoring&quot;: {
      &quot;procmon_provider&quot;: &quot;monit&quot;,
      &quot;metric_provider&quot;: &quot;collectd&quot;
    },
    &quot;vips&quot;: {
      &quot;keystone-admin-api&quot;: &quot;10.200.9.112&quot;,
      &quot;keystone-service-api&quot;: &quot;10.200.9.112&quot;,
      &quot;keystone-internal-api&quot;: &quot;10.200.9.112&quot;,
      &quot;swift-proxy&quot;: &quot;10.200.9.112&quot;,
      &quot;config&quot;: {
        &quot;10.200.9.112&quot;: {
          &quot;vrid&quot;: 12,
          &quot;network&quot;: &quot;management&quot;
        }
      }
    },
    &quot;developer_mode&quot;: false,
    &quot;swift&quot;: {
      &quot;swift_hash&quot;: &quot;107c0568ea84&quot;,
      &quot;authmode&quot;: &quot;keystone&quot;,
      &quot;authkey&quot;: &quot;3f281b71-ce89-4b27-a2ad-ad873d3f2760&quot;
    }
  }
}
</code></pre>

<p>作成した environment ファイル environments/swift.json を chef-server へアップ
ロードする。</p>

<pre><code class="language-bash">% knife environment from file environments/swift.json
</code></pre>

<h2 id="デプロイ">デプロイ</h2>

<p>かきのとおり knife bootstrap する。</p>

<pre><code class="language-bash">% knife bootstrap &lt;manage_ip_addr&gt; -N swift-manage -r 'role[base]','role[mysql-master]','role[keystone]','role[swift-management-server]' -E swift --sudo -x thirai
% knife bootstrap &lt;proxy01_ip_addr&gt; -N swift-proxy01 -r &quot;role[base]&quot;,&quot;role[swift-proxy-server]&quot;,'role[swift-setup]','role[openstack-ha]' -E swift --sudo -x thirai
% knife bootstrap &lt;proxy02_ip_addr&gt; -N swift-proxy02 -r &quot;role[base]&quot;,&quot;role[swift-proxy-server]&quot;,'role[openstack-ha]' -E swift --sudo -x thirai
% knife bootstrap &lt;storage01_ip_addr&gt; -N swift-storage01 -r 'role[base]','role[swift-object-server]' -E swift --sudo -x thirai
% knife bootstrap &lt;storage02_ip_addr&gt; -N swift-storage02 -r 'role[base]','role[swift-object-server]' -E swift --sudo -x thirai
% knife bootstrap &lt;storage03_ip_addr&gt; -N swift-storage03 -r 'role[base]','role[swift-object-server]' -E swift --sudo -x thirai
% knife bootstrap &lt;account01_ip_addr&gt; -N swift-account01 -r 'role[base]','role[swift-account-server]','role[swift-container-server]' -E swift --sudo -x thirai
% knife bootstrap &lt;account02_ip_addr&gt; -N swift-account02 -r 'role[base]','role[swift-account-server]','role[swift-container-server]' -E swift --sudo -x thirai
% knife bootstrap &lt;account03_ip_addr&gt; -N swift-account03 -r 'role[base]','role[swift-account-server]','role[swift-container-server]' -E swift --sudo -x thirai
</code></pre>

<p>zone 番号を付与する。</p>

<pre><code class="language-bash">% knife exec -E &quot;nodes.find(:name =&gt; 'swift-storage01') {|n| n.set['swift']['zone'] = '1'; n.save }&quot;
% knife exec -E &quot;nodes.find(:name =&gt; 'swift-account01') {|n| n.set['swift']['zone'] = '1'; n.save }&quot;
% knife exec -E &quot;nodes.find(:name =&gt; 'swift-storage02') {|n| n.set['swift']['zone'] = '2'; n.save }&quot;
% knife exec -E &quot;nodes.find(:name =&gt; 'swift-account02') {|n| n.set['swift']['zone'] = '2'; n.save }&quot;
% knife exec -E &quot;nodes.find(:name =&gt; 'swift-storage03') {|n| n.set['swift']['zone'] = '3'; n.save }&quot;
% knife exec -E &quot;nodes.find(:name =&gt; 'swift-account03') {|n| n.set['swift']['zone'] = '3'; n.save }&quot;
</code></pre>

<p>zone 番号が付与されたことを確認する。</p>

<p>account-server の確認</p>

<pre><code class="language-bash">% knife exec -E 'search(:node,&quot;role:swift-account-server&quot;) \
  { |n| z=n[:swift][:zone]||&quot;not defined&quot;; puts &quot;#{n.name} has the role \
  [swift-account-server] and is in swift zone #{z}&quot;; }'
swift-account01 has the role       [swift-account-server] and is in swift zone 1
swift-account02 has the role       [swift-account-server] and is in swift zone 2
swift-account03 has the role       [swift-account-server] and is in swift zone 3
</code></pre>

<p>container-server の確認</p>

<pre><code class="language-bash">% knife exec -E 'search(:node,&quot;role:swift-container-server&quot;) \
  { |n| z=n[:swift][:zone]||&quot;not defined&quot;; puts &quot;#{n.name} has the role \
  [swift-container-server] and is in swift zone #{z}&quot;; }'
swift-account01 has the role       [swift-container-server] and is in swift zone 1
swift-account02 has the role       [swift-container-server] and is in swift zone 2
swift-account03 has the role       [swift-container-server] and is in swift zone 3
</code></pre>

<p>object-server の確認</p>

<pre><code class="language-bash">% knife exec -E 'search(:node,&quot;role:swift-object-server&quot;) \
  { |n| z=n[:swift][:zone]||&quot;not defined&quot;; puts &quot;#{n.name} has the role \
  [swift-object-server] and is in swift zone #{z}&quot;; }'
swift-storage01 has the role   [swift-object-server] and is in swift zone 1
swift-storage02 has the role   [swift-object-server] and is in swift zone 2
swift-storage03 has the role   [swift-object-server] and is in swift zone 3
</code></pre>

<p>Chef が各々のノードに搭載された Disk を検知出来るか否かを確認する。</p>

<pre><code class="language-bash">% knife exec -E \
  'search(:node,&quot;role:swift-object-server OR \
  role:swift-account-server \
  OR role:swift-container-server&quot;) \
  { |n| puts &quot;#{n.name}&quot;; \
  begin; n[:swift][:state][:devs].each do |d| \
  puts &quot;\tdevice #{d[1][&quot;device&quot;]}&quot;; \
  end; rescue; puts \
  &quot;no candidate drives found&quot;; end; }'
    swift-storage02
            device sdb1
    swift-storage03
            device sdb1
    swift-account01
            device sdb1
    swift-account02
            device sdb1
    swift-account03
            device sdb1
    swift-storage01
            device sdb1
</code></pre>

<p>swift-manage ノードにて chef-client を実行し
/etc/swift/ring-workspace/generate-rings.sh を更新する。</p>

<pre><code class="language-bash">swift-manage% sudo chef-client
</code></pre>

<p>generate-rings.sh の &lsquo;exit 0&rsquo; 行をコメントアウトし実行</p>

<pre><code class="language-bash">swift-manage% sudo ${EDITOR} /etc/swift/ring-workspace/generage-rings.sh
swift-manage% sudo /etc/swift/ring-workspace/generate-rings.sh
</code></pre>

<p>この操作で /etc/swift/ring-workspace/rings 配下に account, container, object
用の Rings ファイル群が生成されたことを確認出来るはずである。これらを
swift-manage 上で既に稼働している git サーバに push し管理する。</p>

<pre><code class="language-bash">swift-manage# cd /etc/swift/ring-workspace/rings
swift-manage# git add account.builder container.builder object.builder
swift-manage# git add account.ring.gz container.ring.gz object.ring.gz
swift-manage# git commit -m &quot;initial commit&quot;
swift-manage# git push
</code></pre>

<p>各々のノードにて chef-client を実行することで git サーバ上の Rings ファイル群
を取得し、swift プロセスを稼働させる。</p>

<pre><code class="language-bash">swift-proxy01# chef-client
swift-proxy02# chef-client
swift-storage01# chef-client
swift-storage02# chef-client
swift-storage03# chef-client
swift-account01# chef-client
swift-account02# chef-client
swift-account03# chef-client
</code></pre>

<p>3台のノードが登録されたかどうかを下記の通り確認行う。</p>

<pre><code class="language-bash">swift-proxy01% sudo swift-recon --md5
[sudo] password for thirai:
===============================================================================
--&gt; Starting reconnaissance on 3 hosts
===============================================================================
[2013-10-18 11:14:43] Checking ring md5sums
3/3 hosts matched, 0 error[s] while checking hosts.
===============================================================================
</code></pre>

<h2 id="動作確認">動作確認</h2>

<pre><code class="language-bash">swift-storage01# source swift-openrc
swift-storage01# swift post container01
swift-storage01# echo &quot;test&quot; &gt; test
swift-storage01# swift upload container01 test
swift-storage01# swift list
swift-storage01# swift list container01
</code></pre>

<h2 id="ノードの追加方法">ノードの追加方法</h2>

<p>次にノードの追加方法を記します。</p>

<p>gdisk にて /dev/sdb1 等のパーティションを作成する。</p>

<p>下記の通り knife bootstrap する。</p>

<pre><code class="language-bash">% knife bootstrap &lt;storage04_ip_addr&gt; -N swift-storage04 -r 'role[base]','role[swift-object-server]' -E swift --sudo -x thirai
% knife bootstrap &lt;account04_ip_addr&gt; -N swift-account04 -r 'role[base]','role[swift-account-server]','role[swift-container-server]' -E swift --sudo -x thirai
</code></pre>

<p>zone 番号を付与する。</p>

<pre><code class="language-bash">% knife exec -E &quot;nodes.find(:name =&gt; 'swift-storage04') {|n| n.set['swift']['zone'] = '4'; n.save }&quot;
% knife exec -E &quot;nodes.find(:name =&gt; 'swift-account04') {|n| n.set['swift']['zone'] = '4'; n.save }&quot;
</code></pre>

<p>zone 番号が付与されたことを確認する。</p>

<p>account-server の確認</p>

<pre><code class="language-bash">% knife exec -E 'search(:node,&quot;role:swift-account-server&quot;) \
  { |n| z=n[:swift][:zone]||&quot;not defined&quot;; puts &quot;#{n.name} has the role \
  [swift-account-server] and is in swift zone #{z}&quot;; }'
swift-account01 has the role       [swift-account-server] and is in swift zone 1
swift-account02 has the role       [swift-account-server] and is in swift zone 2
swift-account03 has the role       [swift-account-server] and is in swift zone 3
swift-account04 has the role       [swift-account-server] and is in swift zone 4
</code></pre>

<p>container-server の確認</p>

<pre><code class="language-bash">% knife exec -E 'search(:node,&quot;role:swift-container-server&quot;) \
  { |n| z=n[:swift][:zone]||&quot;not defined&quot;; puts &quot;#{n.name} has the role \
  [swift-container-server] and is in swift zone #{z}&quot;; }'
swift-account01 has the role       [swift-container-server] and is in swift zone 1
swift-account02 has the role       [swift-container-server] and is in swift zone 2
swift-account03 has the role       [swift-container-server] and is in swift zone 3
swift-account04 has the role       [swift-container-server] and is in swift zone 4
</code></pre>

<p>object-server の確認</p>

<pre><code class="language-bash">% knife exec -E 'search(:node,&quot;role:swift-object-server&quot;) \
  { |n| z=n[:swift][:zone]||&quot;not defined&quot;; puts &quot;#{n.name} has the role \
  [swift-object-server] and is in swift zone #{z}&quot;; }'
swift-storage01 has the role   [swift-object-server] and is in swift zone 1
swift-storage02 has the role   [swift-object-server] and is in swift zone 2
swift-storage03 has the role   [swift-object-server] and is in swift zone 3
swift-storage04 has the role   [swift-object-server] and is in swift zone 4
</code></pre>

<p>swift-manage ノードにて chef-client を実行し
/etc/swift/ring-workspace/generate-rings.sh を更新する。</p>

<pre><code class="language-bash">swift-manage% sudo chef-client
</code></pre>

<p>generate-rings.sh の &lsquo;exit 0&rsquo; 行をコメントアウトし実行</p>

<pre><code class="language-bash">swift-manage% sudo ${EDITOR} /etc/swift/ring-workspace/generage-rings.sh
swift-manage% sudo /etc/swift/ring-workspace/generate-rings.sh
</code></pre>

<p>この操作で /etc/swift/ring-workspace/rings 配下に account, container, object
用の Rings ファイル群が生成されたことを確認出来るはずである。これらを
swift-manage 上で既に稼働している git サーバに push し管理する。</p>

<pre><code class="language-bash">swift-manage# cd /etc/swift/ring-workspace/rings
swift-manage# git add account.builder container.builder object.builder
swift-manage# git add account.ring.gz container.ring.gz object.ring.gz
swift-manage# git commit -m &quot;added zone 4 nodes&quot;
swift-manage# git push
</code></pre>

<p>各々のノードにて chef-client を実行することで git サーバ上の Rings ファイル群
を取得し、swift プロセスを稼働させる。</p>

<pre><code class="language-bash">swift-proxy01# chef-client
swift-proxy02# chef-client
swift-storage01# chef-client
swift-storage02# chef-client
swift-storage03# chef-client
swift-storage04# chef-client
swift-account01# chef-client
swift-account02# chef-client
swift-account03# chef-client
swift-account04# chef-client
</code></pre>

<p>4台のノードが登録されたかどうかを下記の通り確認行う。</p>

<pre><code class="language-bash">swift-proxy01% sudo swift-recon --md5
[sudo] password for thirai:
===============================================================================
--&gt; Starting reconnaissance on 4 hosts
===============================================================================
[2013-10-18 11:14:43] Checking ring md5sums
4/4 hosts matched, 0 error[s] while checking hosts.
===============================================================================
</code></pre>

<p>proxy ノードの swift プロセスを停止・起動する</p>

<pre><code class="language-bash">swift-proxy01# swift-init all stop
swift-proxy01# swift-init all start
swift-proxy02# swift-init all stop
swift-proxy02# swift-init all start
</code></pre>

<h2 id="ノード削除方法">ノード削除方法</h2>

<p>次にノードの削除方法を記します。</p>

<p>disk 障害等のzone4 のノード swift-storage04, swift-account04 を削除する前提で記す。</p>

<pre><code class="language-bash">swift-manage# swift-ring-builder /etc/swift/ring-workspace/rings/account.builder remove 10.200.9.109
swift-manage# swift-ring-builder /etc/swift/ring-workspace/rings/container.builder remove 10.200.9.109
swift-manage# swift-ring-builder /etc/swift/ring-workspace/rings/object.builder remove 10.200.9.110
</code></pre>

<p>swift-manage 上で既に稼働している git サーバに push し管理する。</p>

<pre><code class="language-bash">swift-manage# cd /etc/swift/ring-workspace/rings
swift-manage# git add account.builder container.builder object.builder
swift-manage# git add account.ring.gz container.ring.gz object.ring.gz
swift-manage# git commit -m &quot;added zone 4 nodes&quot;
swift-manage# git push
</code></pre>

<p>全てのノードで chef-client を実行する。この操作で rings ファイルの配布が行える。</p>

<pre><code class="language-bash">swift-proxy01# chef-client
swift-proxy02# chef-client
swift-storage01# chef-client
swift-storage02# chef-client
swift-storage03# chef-client
swift-storage04# chef-client
swift-account01# chef-client
swift-account02# chef-client
swift-account03# chef-client
swift-account04# chef-client
</code></pre>

<p>proxy ノードの swift プロセスを停止・起動する</p>

<pre><code class="language-bash">swift-proxy01# swift-init all stop
swift-proxy01# swift-init all start
swift-proxy02# swift-init all stop
swift-proxy02# swift-init all start
</code></pre>

<h2 id="まとめと考察">まとめと考察</h2>

<p>MySQL のリカバリは単純にダンプからのリストアが良かったのでシングル構成に出来て
よかった。また、通常のシングルスター・マルチスレーブの構成に変えても OK かと思
いますが、Cookbooks を修正する必要がありそう。まぁ Keystone のデータのみを管理
している MySQL なので負荷は無いですしシングル構成が良いと個人的には思います。
また Swift-Proxy は 2 台構成の HA になります。VRRP プロトコルの仕組み、また
haproxy の構成上 3 台以上でも OK でありますが、Cookbooks 的に NG でした。これ
は修正する価値が大いにありそう。</p>

<p>またこの構成を Chef で管理し始めると大量のノードで Chef-Client を実行すること
になるので Gnu Prallel や pssh を使った方が良さそう&hellip;。</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2013/10/27/swift-chef/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>


        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2013/10/20/test-kitchen-openstack-chef-cookbooks-test-2/">test-kitchen と OpenStack で Chef Cookbooks テスト (後篇)</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2013-10-20'>
            October 20, 2013</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは。<a href="https://twitter.com/jedipunkz">@jedipunkz</a> です。</p>

<p>前回、OpenStack と test-kitchen を使った環境構築方法を書きました。下記の記事で
す。</p>

<p><a href="http://jedipunkz.github.io/blog/2013/10/13/test-kitchn-openstack-chef-cookbooks-test/">http://jedipunkz.github.io/blog/2013/10/13/test-kitchn-openstack-chef-cookbooks-test/</a></p>

<p>今回は実際にテストを書く方法を記していたい思います。</p>

<p>今回使用するテストツールは下記の2つです。</p>

<ul>
<li>rspec と serverspec</li>
<li>busser-bats</li>
</ul>

<h2 id="参考資料">参考資料</h2>

<p>Creationline lab さんの資料を参考にさせて頂きました。</p>

<p><a href="http://www.creationline.com/lab/2933">http://www.creationline.com/lab/2933</a></p>

<h2 id="用意するモノ達">用意するモノ達</h2>

<ul>
<li>OpenStack にアクセスするためのユーザ・パスワード</li>
<li>Keystone の AUTH_URL</li>
<li>テストに用いる OS イメージの Image ID</li>
<li>テナント ID</li>
<li>nova 管理のキーペアの作成</li>
</ul>

<p>これらは OpenStack を普段から利用されている方なら馴染みのモノかと思います。</p>

<h2 id="kitchen-yml-ファイルの作成">.kitchen.yml ファイルの作成</h2>

<p>下記の通り .kitchen.yml ファイルを test-kitchen のルートディレクトリで作成しま
す。今後の操作は全てこのディレクトリで作業行います。</p>

<p>&ldquo;&lt;&gt;&rdquo; で括った箇所が環境に合わせた設定になります。</p>

<p>また、ここでは前回同様に &lsquo;ntp&rsquo; の Cookbook をテストする前提で記します。</p>

<pre><code class="language-yaml">+++
driver_plugin: openstack

suites:
- name: default
  run_list:
  - recipe[ntp::default]
  attributes: {}

platforms:
- name: ubuntu-12.04
  driver_config:
    openstack_username: &lt;openstack_username&gt;
    openstack_api_key: &lt;openstack_password&gt;
    openstack_auth_url: http://&lt;openstack_ip_addr&gt;:5000/v2.0/tokens
    image_ref: &lt;image_id&gt;
    flavor_ref: 1
    key_name: &lt;key_name&gt;
    openstack_tenant: &lt;tenant_name&gt;
    username: &lt;ssh_username&gt;
    private_key_path: &lt;path_to_secretkey&gt;

- name: centos-64
  driver_config:
    openstack_username: &lt;openstack_username&gt;
    openstack_api_key: &lt;openstack_password&gt;
    openstack_auth_url: http://&lt;openstack_ip_addr&gt;:5000/v2.0/tokens
    image_ref: &lt;image_id&gt;
    flavor_ref: 1
    key_name: &lt;key_name&gt;
    openstack_tenant: &lt;tenant_name&gt;
    username: &lt;ssh_username&gt;
    private_key_path: &lt;path_to_secretkey&gt;
</code></pre>

<p>busser-bats テスト
+++</p>

<p>まずはじめに busser-bats のテストを記します。</p>

<h4 id="ディレクトリ作成">ディレクトリ作成</h4>

<p>kitchen init を行うことでもこの操作は可能なのですが kitchen-openstack を利用すること
を想定しない形で成形されてしまうため、下記の通り実行する。</p>

<pre><code class="language-bash">% cd $TEST-KITCHEN-ROOT/
% mkdir -p test/integration/default/bats
</code></pre>

<h4 id="ディレクトリ構成">ディレクトリ構成</h4>

<p>ディレクトリ構成は</p>

<pre><code>test/integration/${SUITE_NAME}/${BUSSER_NAME}/${TEST_NAME}
</code></pre>

<p>となっています。</p>

<h4 id="テスト作成">テスト作成</h4>

<p>test/integration/default/bats/test.bats として下記のファイルを作成します。</p>

<pre><code class="language-ruby">@test &quot;ntp must be installed&quot; {
    which ntpd
}

@test &quot;ntp.conf must be exist&quot; {
    cat /etc/ntp.conf | grep &quot;server 0.pool.ntp.org iburst&quot;
}
</code></pre>

<p>一項目と二項目の説明を書いておきます。</p>

<ul>
<li>一項目の @test</li>
</ul>

<p>ntpd コマンドが存在するか否かで package &lsquo;ntp&rsquo; がインストール
されていることを確認。</p>

<ul>
<li>二項目の @test</li>
</ul>

<p>特定の文字列が /etc/ntp.conf に記述あるか否かでファイルの存在を確認。</p>

<h2 id="rspec-serverspec-によるテスト実施">rspec serverspec によるテスト実施</h2>

<p>次に rspec + serverspec のテストの記述方法を記します。</p>

<h4 id="ディレクトリ作成-1">ディレクトリ作成</h4>

<p>ディレクトリを作成します。</p>

<pre><code class="language-bash">% mkdir test/integration/default/rspec
</code></pre>

<h3 id="gemfile-追記">Gemfile 追記</h3>

<p>下記の通り test/integration/default/rspec/Gemfile を作成します。</p>

<pre><code class="language-ruby">source 'https://rubygems.org'
gem 'serverspec'
</code></pre>

<h4 id="serverspec-init-の実行">serverspec-init の実行</h4>

<p>serverspec-init コマンドにより初期化を行います。</p>

<pre><code class="language-bash">% cd test/integration/default/rspec
% serverspec-init
Select OS type:

  1) UN*X
  2) Windows

Select number: 1

Select a backend type:

  1) SSH
  2) Exec (local)

Select number: 2

 + spec/
 + spec/localhost/
 + spec/localhost/httpd_spec.rb
 + spec/spec_helper.rb
 + Rakefile
</code></pre>

<p>上記の通り spec ディレクトリ・ファイルが作成されることを確認します。</p>

<h4 id="ntp-spec-rb-を作成">ntp_spec.rb を作成</h4>

<p>下記の通り test/integration/default/rspec/spec/localhost/ntp_spec.rb を作成し
ます。</p>

<pre><code class="language-ruby">require 'spec_helper'

describe package('ntp') do
  it { should be_installed }
end

describe service('ntp') do
  it { should be_enabled   }
  it { should be_running   }
end

describe port(123) do
  it { should be_listening }
end
   
describe file('/etc/ntp.conf') do
  it { should be_file }
  it { should contain &quot;server 0.pool.ntp.org iburst&quot; }
end
</code></pre>

<p>テスト内容の解説は&hellip;</p>

<ul>
<li>1) &lsquo;ntp&rsquo; パッケージがインストールされていることを確認</li>
<li>2) &lsquo;ntp&rsquo; サービスが再起動時有効になっていること・起動していることを確認</li>
<li>3) 123 番ポートで Listen していることを確認</li>
<li>4) /etc/ntp.conf に特定の文字列が存在することを確認</li>
</ul>

<h2 id="テスト実行">テスト実行</h2>

<p>下記の通りテストを実行する。</p>

<pre><code class="language-bash">% cd $TEST-KITCHEN-ROOT
% bundle exec kitchen create # &lt;---- OpenStack 上にインスタンス作成
% bundle exec kitchen setup  # &lt;---- Chef Cookbooks の実行
% bundle exec kitchen verify # &lt;---- テストの実施
</code></pre>

<p>またこれらの操作は下記の一つのコマンドで実施出来る。</p>

<pre><code class="language-bash">% bundle exec kitchen test
</code></pre>

<h2 id="テスト結果">テスト結果</h2>

<h4 id="busser-bats">busser-bats</h4>

<p>下記の通り2つのテストが実施され &lsquo;ok&rsquo; ステータスが帰って来たことを確認する。</p>

<pre><code>-----&gt; Running bats test suite
       1..2
       ok 1 ntp must be installed
       ok 2 ntp.conf must be exist
</code></pre>

<h4 id="serverspec">serverspec</h4>

<p>下記の通り 6 個のテスト全てが通り failure 0 個であることを確認する。</p>

<pre><code>-----&gt; Running rspec test suite
       [2013-10-17T02:27:19+00:00] INFO: Run List is []
       [2013-10-17T02:27:19+00:00] INFO: Run List expands to []
Recipe: (chef-apply cookbook)::(chef-apply recipe)
  * execute[bundle install --local || bundle install] action run       [2013-10-17T02:27:19+00:00] INFO: Processing execute[bundle install --local || bundle install] action run ((chef-apply cookbook)::(chef-apply recipe) line 42)
Resolving dependencies...
Using diff-lcs (1.2.4)
Using highline (1.6.20)
Using net-ssh (2.7.0)
Using rspec-core (2.14.6)
Using rspec-expectations (2.14.3)
Using rspec-mocks (2.14.4)
Using rspec (2.14.1)
Using serverspec (0.10.6)
Using bundler (1.3.5)
       Your bundle is complete!
       Use `bundle show [gemname]` to see where a bundled gem is installed.
       [2013-10-17T02:27:19+00:00] INFO: execute[bundle install --local || bundle install] ran successfully

           - execute bundle install --local || bundle install
</code></pre>

<p>&hellip;&hellip;</p>

<pre><code>   Finished in 0.0567 seconds
</code></pre>

<p>6 examples, 0 failures
       Finished verifying <default-ubuntu-1204> (0m7.44s).
```</p>

<h2 id="まとめ">まとめ</h2>

<p>busser-bats はまだ地道なテストの記述が必要らしい。それに比べて serverspec は既
に実用的と言えるかもしれない。どちらのテストツールも Cookbook でデプロイされた
環境の <em>状態</em> をテストするモノであって Cookbook, Recipe のテストとは違う。これ
はある意味都合が良い。Cookbooks のテストは ChefSpec 等のテストツールでテストを
行い、完成された Cookbooks を実際に複数の OS 上にデプロイしてテストするのが今
回紹介したモノとなる。</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2013/10/20/test-kitchen-openstack-chef-cookbooks-test-2/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>


        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2013/10/13/test-kitchn-openstack-chef-cookbooks-test/">test-kitchen と OpenStack で Chef Cookbooks テスト(前篇)</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2013-10-13'>
            October 13, 2013</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは。<a href="https://twitter.com/jedipunkz">@jedipunkz</a> です。</p>

<p>test-kitchen + Vagrant を利用して複数環境で Chef Cookbooks のテストを行う方法は
結構皆さん利用されていると思うのですが Vagrant だと手元のマシンに仮想マシンが
バシバシ立ち上げるので僕はあまり好きではないです。そこで、OpenStack のインスタ
ンスをその代替で使えればいいなぁと結構前から思っていたのですが、今回うまくいっ
たのでその方法を記します。</p>

<h2 id="用意するモノ">用意するモノ</h2>

<ul>
<li>OpenStack 環境一式</li>
<li>Chef がインストールされた OS イメージとその ID</li>
<li>test-kitchen を実行するワークステーション (お手持ちの Macbook 等)</li>
</ul>

<p>OS イメージの作成ですが Veewee などで自動構築できますし、インスタンス上で Chef
のインストールを行った後にスナップショットを作成してそれを利用しても構いません。</p>

<h2 id="test-kitchen-のインストール">test-kitchen のインストール</h2>

<p>test-kitchen をインストールします。versoin 1.0.0 はまだリリースされていないの
で github から master ブランチを取得してビルドします。直近で OpenStack に関連
する不具合の修正等が入っているのでこの方法を取ります。</p>

<pre><code class="language-bash">% git clone https://github.com/opscode/test-kitchen.git
% cd test-kitchen
% bundle install
% rake build # &lt;--- gem をビルド
% gen install ./pkg/test-kitchen-1.0.0.dev.gem
</code></pre>

<p>現時点 (2013/10/13) で berkshelf の利用しているソフトウェアと衝突を起こす問題
があるので bundle で解決します。下記のように Gemfile に gem
&lsquo;kitchen-openstack&rsquo; と記述します。</p>

<pre><code class="language-ruby">source 'https://rubygems.org'
gemspec

gem 'kitchen-openstack' # &lt;--- 追記

group :guard do
  gem 'rb-inotify', :require =&gt; false
  gem 'rb-fsevent', :require =&gt; false
  gem 'rb-fchange', :require =&gt; false
  gem 'guard-minitest', '~&gt; 1.3'
  gem 'guard-cucumber', '~&gt; 1.4'
end
</code></pre>

<h2 id="kitchen-openstack-のインストール">kitchen-openstack のインストール</h2>

<p>kitchen-openstack をインストールします。こちらも gem をビルドしてインストール
します。</p>

<pre><code class="language-bash">% git clone https://github.com/RoboticCheese/kitchen-openstack.git
% cd kitchen-openstack
% bundle insatll #&lt;---- 関連ソフトウェアインストール
% rake build     #&lt;---- gem をビルド
% gem install ./pkg/kitchen-openstack-0.5.1.gem
</code></pre>

<h2 id="kitchen-yml-の作成">.kitchen.yml の作成</h2>

<p>.kitchen.yml を用意します。test-kitchen のディレクトリに移動し .kitchen.yml を
下記の例に従って作成します。今回は Ubuntu OS, CentOS にてテストを実行します。</p>

<pre><code class="language-bash">% cd /path/to/test-kitchen
% ${EDITOR} .kitchen.yml
</code></pre>

<pre><code class="language-ruby">+++
driver_plugin: openstack

suites:
- name: default
  run_list:
  - recipe[ntp::default]
  attributes: {}

platforms:
- name: ubuntu-12.04
  driver_config:
    openstack_username: &lt;openstac_username&gt;
    openstack_api_key: &lt;openstack_password&gt;
    openstack_auth_url: http://&lt;openstack_ip_addr&gt;:5000/v2.0/tokens
    image_ref: &lt;image_id&gt;
    flavor_ref: 1
    key_name: &lt;key_name&gt;
    openstack_tenant: service
    username: ubuntu
    private_key_path: &lt;path_to_secretkey&gt;

- name: centos-64
  driver_config:
    openstack_username: &lt;openstac_username&gt;
    openstack_api_key: &lt;openstack_password&gt;
    openstack_auth_url: http://&lt;openstack_ip_addr&gt;:5000/v2.0/tokens
    image_ref: &lt;image_id&gt;
    flavor_ref: 1
    key_name: &lt;key_name&gt;
    openstack_tenant: service
    username: root
    private_key_path: &lt;path_to_secretkey&gt;
</code></pre>

<p>ファイルの内容について解説します。</p>

<ul>
<li>suites:</li>
</ul>

<p>実行したい Chef Cookbooks のレシピ名を指定します。attriutes などをここで上書き
することも出来ます。</p>

<ul>
<li>platforms:</li>
</ul>

<p>テストに用いたい OS を列挙していけます。ここでは例として Ubuntu, CentOS を記し
ました。</p>

<ul>
<li>openstack_username, openstack_api_key</li>
</ul>

<p>OpenStack にログインするためのユーザ名とパスワードです。keystone で作成します。</p>

<ul>
<li>openstack_auth_url</li>
</ul>

<p>Keystone の URL です。最後に /tokens と付けるのを忘れずに。</p>

<ul>
<li>image_ref</li>
</ul>

<p>それぞれの OS イメージの ID を記します。前述したとおりインスタンスでオペレーショ
ン後にスナップショットを作成しそれを記すことも可能です。</p>

<ul>
<li>flavor_ref</li>
</ul>

<p>Flavor ID を記します。ここでは一番小さい Flavor である m1.tiny を記しました。</p>

<ul>
<li>key_name</li>
</ul>

<p>インスタンス作成時に選択する Nova のキーペア名です。OpenStack コマンドラインで
言う &ndash;key_name です。</p>

<ul>
<li>openstack_tenant</li>
</ul>

<p>どのテナントにインスタンスを作成するか？を記します。</p>

<ul>
<li>username</li>
</ul>

<p>インスタンスにログインする際のユーザ名を記します。</p>

<ul>
<li>private_key_path</li>
</ul>

<p>インスタンスにログインするための SSH 秘密鍵のパスを記します。ここではノンパス
ワードでログイン出来るよう鍵を生成してあげる必要があります。</p>

<h2 id="cookbooks-の配置">Cookbooks の配置</h2>

<p>カレントディレクトリ配下に &lsquo;cookbooks&rsquo; という名前のディレクトリを用意し
テストで用いたい Cookbooks を配置します。Berkshelf を用いれば簡単です。が、
現時点で Berkshelf の用いているソフトウェア test-kitchen のそれが衝突を起こす
のでテスト実行前には Berkshelf ファイルを退避してください。</p>

<pre><code class="language-bash">% ${EDITOR} Berksfile
site :opscode
    
cookbook 'ntp;
% berks install --path=./cookbooks
% mv Berksfile Berksfile.old
</code></pre>

<h2 id="テスト実行">テスト実行</h2>

<p>いよいよテストを実行します。上記の例では Ubuntu OS, Debian OS に対して ntp の
Chef Cookbooks を実際にデプロイしテストを行います。</p>

<pre><code class="language-bash">% bundle exec kitchen test
</code></pre>

<p>&lsquo;test&rsquo; を引数で渡すと</p>

<ul>
<li>インスタンス作成</li>
<li>Chef のインストール</li>
<li>Cookbooks のアップロード</li>
<li>chef-solo の実行</li>
<li>Cookbooks 中に &lsquo;test&rsquo; ディレクトリがある場合はテスト実行</li>
</ul>

<p>を行ってくれます。それぞれ別々に実行したい場合は</p>

<pre><code class="language-bash">% bundle exec kitchen create   # &lt;---- インスタンスの作成
% bundle exec kitchen setup    # &lt;---- chef のインストールと初回の converge
% bundle exec kitchen converge # &lt;---- chef-solo を再度実行
% bundle exec kitchen verify   # &lt;---- 'test' ディレクトリに従いテスト実行
% bundle exec kitchen destroy  # &lt;---- インスタンスの削除
</code></pre>

<p>と行えば良いです。converge, verify は何度でも繰り返し実行が可能。</p>

<h2 id="まとめ">まとめ</h2>

<p>前述したとおり Vagrant を使うと手元のマシンのリソースを大量に消費してしまうの
で OpenStack を利用する価値は結構あるのかなぁと思っています。バージョン 1.0.0
がリリースされる時期も近いと思うので今のうちに知っておくと良いかと思います。</p>

<p>また、テストと言っても test-kitchen の場合2つの意味があると思います。実際に
Cookbooks をインスタンスにインストールするテスト、と Cookbooks 自体のテストと
いう意味です。後者についてはまた後ほど記したいと思います。</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2013/10/13/test-kitchn-openstack-chef-cookbooks-test/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>


        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2013/10/12/glusterfs-install/">GlusterFS の各クラスタタイプ構築</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2013-10-12'>
            October 12, 2013</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは。@jedipunkz です。</p>

<p>GlusterFS をちょっと前に調べてました。何故かと言うと OpenStack Havana がもうす
ぐリリースされるのですが、Havana から GlusterFS がサポートされる予定だからです。</p>

<p>この辺りに色々情報が載っています。</p>

<p><a href="http://www.gluster.org/category/openstack/">http://www.gluster.org/category/openstack/</a></p>

<p>その前に GlusterFS を構築出来ないといけないので、今回はその方法を書いていきま
す。各クラスタタイプ毎に特徴や構築方法が異なるのでその辺りを重点的に。</p>

<h2 id="環境">環境</h2>

<ul>
<li>Ubuntu Server 12.04.3 LTS</li>
<li>PPA レポジトリ利用</li>
<li>/dev/sdb を OS 領域とは別の disk としてサーバに追加する</li>
</ul>

<h2 id="用いる-ppa-レポジトリ">用いる PPA レポジトリ</h2>

<p>Ubuntu 12.04.3 LTS の GlusterFS バージョンは 3.2 です。3.4 系が今回使いたかっ
たので下記の PPA レポジトリを利用させてもらいます。ちゃんと構築するなら自分で
パッケージを作成することをオススメします。</p>

<p><a href="https://launchpad.net/~semiosis/+archive/ubuntu-glusterfs-3.4">https://launchpad.net/~semiosis/+archive/ubuntu-glusterfs-3.4</a></p>

<h1 id="準備">準備</h1>

<p>ここからの手順は全てのサーバで操作します。</p>

<h2 id="レポジトリの利用方法">レポジトリの利用方法</h2>

<pre><code class="language-bash">% sudo aptitude install python-software-properties
% sudo add-apt-repository ppa:semiosis/ubuntu-glusterfs-3.4
% sudo apt-get update
</code></pre>

<h2 id="glusterfs3-4-のインストール">GlusterFS3.4 のインストール</h2>

<pre><code class="language-bash">% sudo apt-get install glusterfs-server gluserfs-client
</code></pre>

<h2 id="xfsprogs-のインストール">xfsprogs のインストール</h2>

<p>glusterfs は xfs を扱うため xfsprogs をインストールする。</p>

<pre><code class="language-bash">% sudo apt-get install xfsprogs
</code></pre>

<h2 id="ディスクの準備">ディスクの準備</h2>

<pre><code class="language-bash">% sudo mkfs.xfs -i size=512 /dev/sdb
% sudo mkdir -p /export/brick1
% sudo vim /etc/fstab
/dev/sdb /export/brick1 xfs defaults 1 2 # &lt;- 追記
% sudo mount -a
% mount
</code></pre>

<h1 id="各クラスタタイプでのマウント方法">各クラスタタイプでのマウント方法</h1>

<p>ここからの手順はどこか一台のノードで行えば OK です。</p>

<h2 id="distributed-タイプ">distributed タイプ</h2>

<p>まずはデフォルトとなる distributed のマウント方法。</p>

<pre><code class="language-bash">% sudo gluster peer probe &lt;other_node_ip_addr&gt; ...
% sudo gluster volume create gv0 &lt;mine_ip_addr&gt;:/export/brick1/sdb &lt;another_node_ip_addr&gt;:/export/brick1/sdb
% sudo gluster volume start gv0
% sudo gluster volume info
Volume Name: gv0
Type: Distribute
Volume ID: 803fdd46-4735-444a-99c8-83d1cee172e6
Status: Started
Number of Bricks: 2
Transport-type: tcp
Bricks:
Brick1: &lt;mine_ip_addr&gt;:/export/brick1/sdb
Brick2: &lt;other_ip_addr&gt;:/export/brick1/sdb
</code></pre>

<p>マウント行う。どちらのノードに対して行うことが出来る。</p>

<pre><code class="language-bash">% sudo mount -t glusterfs &lt;mine_ip_addr&gt;:/gv0 /mnt
</code></pre>

<h2 id="その他のクラスタタイプのマウント方法">その他のクラスタタイプのマウント方法</h2>

<p>replicated の場合&hellip;</p>

<pre><code class="language-bash">% sudo gluster volume create gv0 replica 2 &lt;mine_ip_addr&gt;:/export/brick1/sdb \
  &lt;other_node_ip_addr&gt;:/export/brick1/sdb
</code></pre>

<p>striped の場合&hellip;</p>

<pre><code class="language-bash">% sudo gluster volume create gv0 stripe 2 &lt;mine_ip_addr&gt;:/export/brick1/sdb \
  &lt;other_node_ip_addr&gt;:/export/brick1/sdb
</code></pre>

<p>distributed replicated の場合&hellip;</p>

<pre><code class="language-bash">% sudo gluster volume create dist-repl replica 2 \
  &lt;mine_ip_addr&gt;:/export/brick1/sdb \
  &lt;other_ip_addr&gt;:/export/brick1/sdb \
  &lt;other_ip_addr&gt;:/export/brick1/sdb \
  &lt;other_ip_addr&gt;:/export/brick1/sdb
</code></pre>

<p>striped replicated の場合&hellip;</p>

<pre><code class="language-bash">% sudo gluster volume create strip-repl stripe 2 replica 2 \
  &lt;mine_ip_addr&gt;:/export/brick1/sdb \
  &lt;other_ip_addr&gt;:/export/brick1/sdb \
  &lt;other_ip_addr&gt;:/export/brick1/sdb \
  &lt;other_ip_addr&gt;:/export/brick1/sdb
</code></pre>

<p>distributed striped replicated の場合&hellip;</p>

<pre><code class="language-bash">% sudo gluster volume create dist-strip-repl stripe 2 replica 2 \
  &lt;mine_ip_addr&gt;:/export/brick1/sdb \
  &lt;other_ip_addr&gt;:/export/brick1/sdb \
  &lt;other_ip_addr&gt;:/export/brick1/sdb \
  &lt;other_ip_addr&gt;:/export/brick1/sdb
  &lt;other_ip_addr&gt;:/export/brick1/sdb \
  &lt;other_ip_addr&gt;:/export/brick1/sdb \
  &lt;other_ip_addr&gt;:/export/brick1/sdb \
  &lt;other_ip_addr&gt;:/export/brick1/sdb
</code></pre>

<h2 id="各クラスタタイプの特徴">各クラスタタイプの特徴</h2>

<ul>
<li>distributed</li>
</ul>

<p>1つのオブジェクトを GlusterFS 上に保管するとオブジェクト単位でどれかのノードに
対して保管する。よって &lsquo;ノード上の disk x ノード数&rsquo; が合計容量として扱うことが
出来る。ノード障害の場合にはその該当ノード上の disk に保管されているオブジェク
トの読み書きは NG 。その他のノード上 disk に保管されているオブジェクトに対して
は読み書きが正常に行える。</p>

<ul>
<li>stripe</li>
</ul>

<p>1つのオブジェクトをブロック単位で分割し各ノードに保管する。扱える合計容量は
distributed と同じく &lsquo;ノード上の disk x ノード数&rsquo; となる。障害がどこかのノード
で発生した場合、全てのオブジェクトの読み書きが行えなくなる。</p>

<ul>
<li>replicated</li>
</ul>

<p>1つのオブジェクトを 2 台のノードに対してミラーリングを行い保管する。障害系の対
応が可能になる。その分、扱える合計容量は distrubuted/stripe に比べ半分となる。</p>

<ul>
<li>distributed replicated</li>
</ul>

<p>構成イメージ図&hellip;</p>

<pre><code>+--------+
| client |
+--------+
|
+---------------------+
|                     |
+----------+          +----------+
|          |          |          |
+--------+ +--------+ +--------+ +--------+
| node1  | | node2  | | node3  | | node4  |
+--------+ +--------+ +--------+ +--------+
</code></pre>

<p>node1, 2 と 3, 4 にて replicated しそれぞれに対して distributed を組む。扱える
disk 容量は node の持っている disk 容量 x ノード数 / 2 となる。distributed は
実質、障害系の対応が良くないので distributed を扱うのであれば、この volume
type が推奨されるものと思われる。</p>

<ul>
<li>striped replicated</li>
</ul>

<p>distributed replicated と同等の構成だがブロック単位でのオブジェクトの保管とな
る。</p>

<h2 id="まとめ">まとめ</h2>

<p>GlusterFS には一貫性の問題 (disk 容量の一貫性を保つ必要がある) と思っていたが、
昔の話しらしい。容量のことなる disk をノードに追加しても、それをうまく合計容量
に合算されるのを確認した。また分散ファイルシステムの美味しいところと冗長性を兼
ねた構成を組むのが良いと思うので distributed replicated もしくは striped
replicated を選択するのがオススメ。今回は TCP で扱ったがバージョン 3.4 からは
Infiniband と組み合わせて RDMA を扱うことが可能。下記の URL が参考になる。</p>

<p><a href="http://gluster.org/community/documentation/index.php/Gluster_3.2:_Configuring_GlusterFS_to_work_over_InfiniBand">http://gluster.org/community/documentation/index.php/Gluster_3.2:_Configuring_GlusterFS_to_work_over_InfiniBand</a></p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2013/10/12/glusterfs-install/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>


        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2013/10/01/methos-architecture-number-2-docker-on-mesos/">Methos アーキテクチャ #2 (Docker on Mesos)</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2013-10-01'>
            October 1, 2013</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは。<a href="https://twitter.com/jedipunkz">@jedipunkz</a> です。</p>

<p>Mesos アーキテクチャについて2つめの記事です。</p>

<p><a href="http://jedipunkz.github.io/blog/2013/09/28/mesos-architecture-number-1/">http://jedipunkz.github.io/blog/2013/09/28/mesos-architecture-number-1/</a></p>

<p>上記の前回の記事で Mesos 自体のアーキテクチャについて触れましたが、今回は
Mesos + Marathon + Docker の構成について理解したことを書いていこうと思います。</p>

<p>mesos クラスタは 幾つかの mesos masters と沢山の mesos slaves から成っており、
mesos slaves の上では docker を操作する executor が稼働している。marathon は
mesos master の上で稼働する mesos framework である。init や upstart の様な存在
であることが言え、REST API を持ち container の動作を制御する。marathon には
ruby の client 等も存在する。下記がそれ。</p>

<p><a href="https://github.com/mesosphere/marathon_client">https://github.com/mesosphere/marathon_client</a></p>

<h2 id="構成">構成</h2>

<pre><code>+-----------------+
| docker registry | index.docker.io (もしくは local registry)
+-----------------+
|
+----------------+
|                |
+--------------+ +--------------+
| mesos master | | mesos master |
+--------------+ +--------------+
|                |
|----------------+-----------------------------------|

+--------------+ +--------------+     +--------------+
| mesos slave  | | mesos slave  | ... | mesos slave  | 
+--------------+ +--------------+     +--------------+
|                |                    |
+--------------+ +--------------+     +--------------+
| mesos slave  | | mesos slave  | ... | mesos slave  | 
+--------------+ +--------------+     +--------------+
.                .                    .
.                .                    .
.                .                    .
+--------------+ +--------------+     +--------------+
| mesos slave  | | mesos slave  | ... | mesos slave  |
+--------------+ +--------------+     +--------------+
</code></pre>

<h2 id="オファから-docker-が稼働するまでの流れ">オファから docker が稼働するまでの流れ</h2>

<p>上記の構成の図を見ながら理解していきましょう。</p>

<ul>
<li>HTTP API もしくは web UI で marathon がリクエストを受ける</li>
<li>marathon はリソースリクエストを作成しオファが受け付けられるのを待つ</li>
<li>オファが受け付けられた後、mesos master は slave に task の仕様を送信する</li>
<li>slave では docker コマンドラインツールを実行する mesos docker を mesos slave デーモンが呼び出す</li>
<li>docker コマンドラインツールはローカルの docker デーモンと image cache, lxc ツールにより通信する</li>
<li>もし image cache が存在すればそれを、無ければ docker registry から pull する。</li>
<li>その時、index.docker.io の代わりにローカルの docker registry を稼働させることも可能</li>
<li>docker デーモンが container を稼働させる</li>
</ul>

<h2 id="marathon-のクラスタとしての動作">marathon のクラスタとしての動作</h2>

<p>marathon は init や upstart のようなモノだと上記で説明しましたが、図を交えて説明して
いきましょう。</p>

<p>marathon が &lsquo;serarch service&rsquo; と &lsquo;docker&rsquo; を稼働させている状態だとする。</p>

<pre><code>+----------+ +----------+ +----------+
|          | |          | |          | 
| |search| | | |docker| | | |docker| |
+----------+ +----------+ +----------+

+----------+ +----------+ +----------+
|          | |          | |          | 
| |search| | | |docker| | | |docker| |
+----------+ +----------+ +----------+

+----------+ +----------+ +----------+
|          | |          | |          | 
| |search| | | |docker| | | |docker| |
+----------+ +----------+ +----------+
</code></pre>

<p>サービスの状況によりオファが立て込んでくると下記のように docker をスケールアウ
トが発生する。</p>

<pre><code>+----------+ +----------+ +----------+
| |docker| | | |docker| | |          | 
| |search| | | |docker| | | |docker| |
+----------+ +----------+ +----------+

+----------+ +----------+ +----------+
|          | | |docker  | | |docker| | 
| |search| | | |docker| | | |docker| |
+----------+ +----------+ +----------+

+----------+ +----------+ +----------+
| |docker| | | |docker| | | |docker| |
| |search| | | |docker| | | |docker| |
+----------+ +----------+ +----------+
</code></pre>

<p>システムに異常がありノードが落ちた場合、下記のように marathon は serach
service と docker をノード間で移動させる処置を行う。</p>

<pre><code>             +----------+ +----------+
             | |docker| | | |search| | 
             | |docker| | | |docker| |
             +----------+ +----------+

+----------+ +----------+ +----------+
| |docker|   | |docker| | | |docker| | 
| |search| | | |docker| | | |docker| |
+----------+ +----------+ +----------+

+----------+ +----------+ +----------+
| |docker| | | |docker| | | |docker| |
| |search| | | |docker| | | |docker| |
+----------+ +----------+ +----------+
</code></pre>

<h2 id="まとめ">まとめ</h2>

<p>mesos と docker, marathon の関係について記していきました。今度、実際にこの構成
を組んでみて障害系のテストしてみたいです。あとは framework について理解してい
く必要がありそう。あとは chronos についても。chronos については下記の URL が公
式らしい。これは cron 代替な仕組みらしいです。</p>

<p><a href="https://github.com/airbnb/chronos">https://github.com/airbnb/chronos</a></p>

<p>まだまだ理解できていないことだらけだ&hellip;。</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2013/10/01/methos-architecture-number-2-docker-on-mesos/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>


        

        
<ul class="actions pagination">
    
        <li><a href="/categories/infrastructure/page/4/"
                class="button big previous">Previous Page</a></li>
    

    
        <li><a href="/categories/infrastructure/page/6/"
                class="button big next">Next Page</a></li>
    
</ul>

    </div>
    
<section id="sidebar">

    
        <section id="intro">
            
            
            
            <ul class="icons">
                
                    <li><a href="https://jedipunkz.github.io/categories/infrastructure/index.xml" type="application/rss+xml"
                        target="_blank" title="RSS" class="fa fa-rss"></a></li>
                
                
            </ul>
        </section>

    
        <section id="recent-posts">
            <ul class="posts">
                <header>
                    <h3>Recent Posts</h3>
                </header>
                
                    
                

                
                    
                

                
                    <li>
                        <article>
                            <header>
                                <h3><a href="https://jedipunkz.github.io/blog/2018/12/31/istio/">Istio, Helm を使って Getting Started 的なアプリをデプロイ</a></h3>
                                
                                    
                                
                                <time class="published" datetime=
                                    '2018-12-31'>
                                    December 31, 2018</time>
                            </header>
                        </article>
                    </li>
                
                    <li>
                        <article>
                            <header>
                                <h3><a href="https://jedipunkz.github.io/blog/2017/07/02/test-kitchen-cluster/">Docker,Test-Kitchen,Ansible でクラスタを構成する</a></h3>
                                
                                    
                                
                                <time class="published" datetime=
                                    '2017-07-02'>
                                    July 2, 2017</time>
                            </header>
                        </article>
                    </li>
                
                    <li>
                        <article>
                            <header>
                                <h3><a href="https://jedipunkz.github.io/blog/2017/04/13/gke-lb/">GCP ロードバランサと GKE クラスタを Terraform を使って構築する</a></h3>
                                
                                    
                                
                                <time class="published" datetime=
                                    '2017-04-13'>
                                    April 13, 2017</time>
                            </header>
                        </article>
                    </li>
                
                    <li>
                        <article>
                            <header>
                                <h3><a href="https://jedipunkz.github.io/blog/2017/02/12/serverless-fission/">Serverless on Kubernetes : Fission を使ってみた</a></h3>
                                
                                    
                                
                                <time class="published" datetime=
                                    '2017-02-12'>
                                    February 12, 2017</time>
                            </header>
                        </article>
                    </li>
                
                    <li>
                        <article>
                            <header>
                                <h3><a href="https://jedipunkz.github.io/blog/2017/01/13/kubernetes-deployments/">Kubernetes Deployments を使ってみた！</a></h3>
                                
                                    
                                
                                <time class="published" datetime=
                                    '2017-01-13'>
                                    January 13, 2017</time>
                            </header>
                        </article>
                    </li>
                

                
                    <li>
                        <ul class="actions">
                            <li><a href=
                            
                                "/post/"
                            
                            class="button">View more posts</a></li>
                        </ul>
                    </li>
                
            </ul>
        </section>

    
    
    
    
        <section id="categories">
            <ul class="posts">
                <header>
                    <h3><a href="/categories/">Categories</a></h3>
                </header>

                
                    
                

                
                    <li>
                        <article>
                            <header>
                                <a href="/categories/infrastructure/">infrastructure</a>
                                <span style="float:right;">110</span>
                            </header>
                        </article>
                    </li>
                
                    <li>
                        <article>
                            <header>
                                <a href="/categories/report/">report</a>
                                <span style="float:right;">9</span>
                            </header>
                        </article>
                    </li>
                
                    <li>
                        <article>
                            <header>
                                <a href="/categories/tools/">tools</a>
                                <span style="float:right;">11</span>
                            </header>
                        </article>
                    </li>
                
            </ul>
        </section>
    

    
        

    
        <section id="footer">
            <ul class="icons">
                
                    <li><a href="https://jedipunkz.github.io/categories/infrastructure/index.xml" type="application/rss+xml"
                        target="_blank" title="RSS" class="fa fa-rss"></a></li>
                
                
            </ul>

            <p class="copyright">&copy; ジェダイさんのブログ. テーマデザインは <a href="//github.com/jpescador" target="_blank">Julio Pescador</a>さんによるものです。 </p>
        </section>

</section>

            </div>
        <a id="back-to-top" href="#" class="fa fa-arrow-up fa-border fa-2x"></a>
        

        
        
            
        

        
        
            <script src="/js/jquery.min.js"></script>
            <script src="/js/skel.min.js"></script>
            <script src="/js/util.js"></script>
            <script src="/js/main.js"></script>
            <script src="/js/backToTop.js"></script>
            <script src="/js/highlight.pack.js"></script>
        

        

            
            <script>hljs.initHighlightingOnLoad();</script>
            
    </body>
</html>

