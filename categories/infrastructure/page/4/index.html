


    
        
    




<!DOCTYPE HTML>

<html>
    <head>
        
            <title>Infrastructure Posts - ジェダイさんのブログ</title>
        

        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1" />
        <meta name="generator" content="Hugo 0.53" />
        


        
        
            
        

        <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Infrastructure"/>
<meta name="twitter:description" content=""/>

        <meta property="og:title" content="Infrastructure" />
<meta property="og:description" content="" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://jedipunkz.github.io/categories/infrastructure/" />
<meta property="og:updated_time" content="2014-04-22T00:00:00&#43;00:00"/>

        
<meta itemprop="name" content="Infrastructure">
<meta itemprop="description" content="">


        

        

        
        
            
        

        
        
            <link rel="stylesheet" href="/css/google-font.css" />
            <link rel="stylesheet" href="/css/font-awesome.min.css" />
            <link rel="stylesheet" href="/css/main.css" />
            <link rel="stylesheet" href="/css/add-on.css" />
            <link rel="stylesheet" href="/css/monokai-sublime.css">
        

        

        
        
        
            
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-30563095-1', 'auto');
	
	ga('send', 'pageview');
}
</script>

        
    </head>
    <body>

        
        <div id="wrapper">

    
<header id="header">
    
        <h2><a href="/"></i></a></h2>
    

    <nav class="links">
        <ul>
            
                <li>
                    <a href="">
                        Blog
                    </a>
                </li>
            
                <li>
                    <a href="about/index.html">
                        About
                    </a>
                </li>
            
        </ul>
    </nav>
    <nav class="main">
        <ul>
            
            <li class="search">
                <a class="fa-search" href="#search">Search</a>
                <form id="search" method="get" action="//google.com/search">
                    <input type="text" name="q" placeholder="Search" />
                    <input type="hidden" name="q" value="site:https://jedipunkz.github.io">
                </form>
            </li>
            <li class="menu">
                <a class="fa-bars" href="#menu">Menu</a>
            </li>
        </ul>
    </nav>
</header>


<section id="menu">

    
        <section>
            <form class="search" method="get" action="//google.com/search">
                <input type="text" name="q" placeholder="Search" />
                <input type="hidden" name="q" value="site:https://jedipunkz.github.io">
            </form>
        </section>

    
        <section>
            <ul class="links">
                
                    <li>
                        <a href="">
                            <h3>
                                
                                Blog
                            </h3>
                        </a>
                    </li>
                
                    <li>
                        <a href="about/index.html">
                            <h3>
                                
                                About
                            </h3>
                        </a>
                    </li>
                
            </ul>
        </section>

    
        <section>
            <ul class="links">
                <header>
                    <h3>Recent Posts</h3>
                </header>
                
                    
                

                
                    <li>
                        <a href="https://jedipunkz.github.io/blog/2018/12/31/istio/"><p>Istio, Helm を使って Getting Started 的なアプリをデプロイ</p></a>
                    </li>
                
                    <li>
                        <a href="https://jedipunkz.github.io/blog/2017/07/02/test-kitchen-cluster/"><p>Docker,Test-Kitchen,Ansible でクラスタを構成する</p></a>
                    </li>
                
                    <li>
                        <a href="https://jedipunkz.github.io/blog/2017/04/13/gke-lb/"><p>GCP ロードバランサと GKE クラスタを Terraform を使って構築する</p></a>
                    </li>
                
                    <li>
                        <a href="https://jedipunkz.github.io/blog/2017/02/12/serverless-fission/"><p>Serverless on Kubernetes : Fission を使ってみた</p></a>
                    </li>
                
                    <li>
                        <a href="https://jedipunkz.github.io/blog/2017/01/13/kubernetes-deployments/"><p>Kubernetes Deployments を使ってみた！</p></a>
                    </li>
                
            </ul>
        </section>

    
        
</section>

    
    <div id="main">
        <h1>Infrastructure</h1>
        
        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2014/04/22/geard-port-mapping/">Geard のポートマッピングについて調べてみた</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2014-04-22'>
            April 22, 2014</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは。@jedipunkz です。</p>

<p>今週 Redhat が &lsquo;Redhat Enterprise Linux Atomic Host&rsquo; しましたよね。Docker を特
徴としたミニマムな OS だとのこと。その内部で用いられている技術 Geard について
少し調べてみました。複数コンテナの関連付けが可能なようです。ここでは調べた結果
について簡単にまとめていきます。</p>

<h2 id="参考資料">参考資料</h2>

<p><a href="http://openshift.github.io/geard/deploy_with_geard.html">http://openshift.github.io/geard/deploy_with_geard.html</a></p>

<h2 id="利用方法">利用方法</h2>

<p>ここではホスト OS に Fedora20 を用意します。</p>

<p>まず Geard をインストール</p>

<pre><code class="language-bash">% sudo yum install --enablerepo=updates-testing geard
</code></pre>

<p>下記の json ファイルを作成します。ここにはコンテナをデプロイするための情報と関
連付けのための情報を記します。</p>

<pre><code class="language-json">$ ${EDITOR} rockmongo_mongodb.json
{
  &quot;containers&quot;:[
    {
      &quot;name&quot;:&quot;rockmongo&quot;,
      &quot;count&quot;:1,
      &quot;image&quot;:&quot;derekwaynecarr/rockmongo&quot;,
      &quot;publicports&quot;:[{&quot;internal&quot;:80,&quot;external&quot;:6060}],
      &quot;links&quot;:[{&quot;to&quot;:&quot;mongodb&quot;}]
    },
    {
      &quot;name&quot;:&quot;mongodb&quot;,
      &quot;count&quot;:1,
      &quot;image&quot;:&quot;ccoleman/ubuntu-mongodb&quot;,
      &quot;publicports&quot;:[{&quot;internal&quot;:27017}]
    }
  ]
}
</code></pre>

<p>上記のファイルの解説。</p>

<ul>
<li>コンテナ &lsquo;rockmongo&rsquo; と &lsquo;mongodb&rsquo; を作成</li>
<li>それぞれ1個ずつコンテナを作成</li>
<li>&lsquo;image&rsquo; パラメータにて docker イメージの指定</li>
<li>&lsquo;publicports&rsquo; パラメータにてコンテナ内部とホスト側のポートマッピングを行う</li>
<li>&lsquo;links&rsquo; パラメータで &lsquo;rockmongo&rsquo; を &lsquo;mongodb&rsquo; に関連付け</li>
</ul>

<p>では、デプロイ開始します。</p>

<pre><code class="language-bash">$ sudo gear deploy rockmongo_mongo.json
2014/04/22 07:21:12 Deploying rockmongo_mongo.json.20140422-072112
2014/04/22 07:21:12 appending 27017 on rockmongo-1: &amp;{PortPair:{Internal:27017 External:0} Target:127.0.0.1:27017} &amp;{Id:rockmongo-1 Image:derekwaynecarr/rockmongo From:rockmongo On:0xc2100bb980 Ports:[{PortPair:{Internal:80 External:6060} Target::0}] add:true remove:false container:0xc21000be00 links:[]}
2014/04/22 07:21:12 ports: Releasing
2014/04/22 07:21:12 systemd: Reloading daemon
local PortMapping: 80 -&gt; 6060
local Container rockmongo-1 is installed
2014/04/22 07:21:12 ports: Releasing
2014/04/22 07:21:12 systemd: Reloading daemon
local PortMapping: 27017 -&gt; 4000
local Container mongodb-1 is installed
linking rockmongo: 127.0.0.1:27017 -&gt; localhost:4000
local Links set on local
local Container rockmongo-1 starting
local Container mongodb-1 starting
</code></pre>

<p>ブラウザにてホスト OS に接続することで rockmongo の管理画面にアクセスが可能。</p>

<pre><code>http://&lt;ホストOSのIP:6060&gt;/
</code></pre>

<h2 id="ポートマッピング管理">ポートマッピング管理</h2>

<p>デプロイが完了して、実際に RockMongo の管理画面にアクセス出来たと思います。
関連付けが特徴と言えそうなのでその解析をしてみました。</p>

<p>Geard のコンテナ関連付けはホストとコンテナのポート管理がキモとなっていることが
解ります。これを紐解くことで geard のコンテナ管理を理解します。</p>

<pre><code>                        'rockmongo' コンテナ             'mongodb' コンテナ
+----------+        +--------------------------+        +-------------------+
|  Client  |-&gt;6060-&gt;|-&gt;80-&gt; RockMongo -&gt;27017-&gt;|-&gt;4000-&gt;|-&gt;27017-&gt; Mongodb  |
+----------+        +--------------------------+        +-------------------+
                    |-------------------- docker ホスト --------------------|
</code></pre>

<p>上記 &lsquo;gear deploy&rsquo; コマンド発行時のログと json ファイルにより上図のような構成
になっていることが理解できます。一つずつ読み解いていきましょう。</p>

<ul>
<li>&lsquo;rockmongo&rsquo; コンテナの内部でリスンしている 80 番ポートはホスト OS の 6060 番へ変換</li>
<li>&lsquo;rockmongo&rsquo; コンテナ内で稼働する RockMongo の config.php から &lsquo;127.0.0.1:27017&rsquo; でリスンしていることが解る</li>
</ul>

<p>試しに &lsquo;derekwaynecarr/rockmongo:latest&rsquo; に /bin/bash でログインし config.php を確認。</p>

<pre><code class="language-bash">$ sudo docker run -i -t derekwaynecarr/rockmongo:latest /bin/bash
bash-4.1# grep mongo_host /var/www/html/config.php
$MONGO[&quot;servers&quot;][$i][&quot;mongo_host&quot;] = &quot;127.0.0.1&quot;;//mongo host
$MONGO[&quot;servers&quot;][$i][&quot;mongo_host&quot;] = &quot;127.0.0.1&quot;;
</code></pre>

<ul>
<li>デプロイ時のログと json ファイルの &lsquo;links&rsquo; パラメータより、ホストのポートに動的に(ここでは 4000番ポート) に変換されることが解ります。</li>
</ul>

<pre><code>linking rockmongo: 127.0.0.1:27017 -&gt; localhost:4000
</code></pre>

<ul>
<li>ホストの 4000 番ポートは動的に &lsquo;mongodb&rsquo; コンテナの内部ポート 27017 にマッピングされる</li>
</ul>

<p>これらのポートマッピングによりそれぞれのコンテナの連携が取れている。</p>

<h2 id="まとめと考察">まとめと考察</h2>

<p>Geard は下記の2つを特徴とした技術だと言えるとことがわかりました。</p>

<ul>
<li>コンテナを json で管理しデプロイする仕組みを提供する</li>
<li>コンテナ間の関連付けをホスト OS のポートを動的に管理・マッピングすることで行う
<br /></li>
</ul>

<p>同じような OS に CoreOS <a href="https://coreos.com/">https://coreos.com/</a> がありますが、こちらも docker,
sytemd 等を特徴としています。さらに etcd を使ってクラスタの構成等が可能になっ
ていますが、Geard はホストのポートを動的に管理することで関連付けが可能なことが
わかりました。</p>

<p>実際に触ってみた感覚から言えば、まだまだ実用化は厳しい状況に思えますが、今後へ
の展開に期待したい技術です。</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2014/04/22/geard-port-mapping/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>


        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2014/04/04/openstack-havana-cinder-glance-ceph/">OpenStack Havana Cinder,Glance の分散ストレージ Ceph 連携</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2014-04-04'>
            April 4, 2014</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは！<a href="https://twitter.com/jedipunkz">@jedipunkz</a> です。</p>

<p>今回は Havana 版の OpenStack Glance, Cinder と分散ストレージの Ceph を連携させ
る手順を書いていきます。元ネタはこちら。下記の Ceph の公式サイトに手順です。</p>

<p><a href="https://ceph.com/docs/master/rbd/rbd-openstack/">https://ceph.com/docs/master/rbd/rbd-openstack/</a></p>

<p>この手順から下記の変更を行って、ここでは記していきます。</p>

<ul>
<li>Nova + Ceph 連携させない</li>
<li>cinder-backup は今のところ動作確認出来ていないので省く</li>
<li>諸々の手順がそのままでは実施出来ないので補足を入れていく。</li>
</ul>

<p>cinder-backup は Cinder で作成した仮想ディスクのバックアップを Ceph ストレージ
上に取ることが出来るのですが、そもそも Ceph 上にある仮想ディスクを Ceph にバッ
クアップ取っても意味が薄いですし、まだ動いていないので今回は省きます。LVM やそ
の他ストレージを使った Cinder 連携をされている方にとっては cinder-backup の
Ceph 連携は意味が大きくなってくると思います。</p>

<h2 id="構成">構成</h2>

<p>下記の通りの物理ネットワーク6つの構成です。
OpenStack, Ceph 共に最小構成を前提にします。</p>

<pre><code>                  +--------------------------------------------------------------------- external
                  |
+--------------+--(-----------+--------------+------------------------------------------ public
|              |  |           |              |
+------------+ +------------+ +------------+ +------------+ +------------+ +------------+
| controller | |  network   | |  compute   | |   ceph01   | |   ceph02   | |   ceph03   |
+------------+ +------------+ +------------+ +------------+ +------------+ +------------+
|  |           |  |           |  |  |        |  |  |        |  |  |        |  |  |
+--------------+--(-----------+--(-----------+--(--(--------+--(--(--------+--(--(------- management
   |              |              |  |           |  |           |  |           |  |
   |              +--------------+--(-----------(--(-----------(--(-----------(--(------- guest
   |                                |           |  |           |  |           |  |
   +--------------------------------+-----------+--(-----------+--(-----------+--(------- storage
                                                   |              |              |
                                                   +--------------+--------------+------- cluster
</code></pre>

<p>特徴</p>

<ul>
<li>最小構成 controller x 1 + network x 1 + compute x 1 + ceph x 3</li>
<li>OpenStack API の相互通信は management ネットワーク</li>
<li>OpenStack VM 間通信は guest ネットワーク</li>
<li>OpenStack 外部通信は public ネットワーク</li>
<li>OpenStack VM の floating-ip (グローバル) 通信は external ネットワーク</li>
<li>Ceph と OpenStack 間の通信は storage ネットワーク</li>
<li>Ceph の OSD 間通信は cluster ネットワーク</li>
<li>ここには記されていないホスト &lsquo;workstation&rsquo; から OpenStack, Ceph をデプロイ</li>
</ul>

<h2 id="前提の構成">前提の構成</h2>

<p>前提構成の OpenStack と Ceph ですが、ここでは構築方法は割愛します。OpenStack
は rcbops/chef-cookbooks を。Ceph は ceph-deploy を使って自分は構築しました。
下記の自分のブログに構築手順が載っているので参考にしてみてください。</p>

<p>OpenStack 構築方法</p>

<p><a href="http://jedipunkz.github.io/blog/2013/11/17/openstack-havana-chef-deploy/">http://jedipunkz.github.io/blog/2013/11/17/openstack-havana-chef-deploy/</a></p>

<p>Ceph 構築方法</p>

<p><a href="http://jedipunkz.github.io/blog/2014/02/27/journal-ssd-ceph-deploy/">http://jedipunkz.github.io/blog/2014/02/27/journal-ssd-ceph-deploy/</a></p>

<h2 id="openstack-ceph-連携手順">OpenStack + Ceph 連携手順</h2>

<p>では実際に連携するための手順を記していきます。</p>

<h4 id="rbd-pool-の作成">rbd pool の作成</h4>

<p>Ceph ノードの何れかで下記の手順を実施し rbd pool を作成する。</p>

<pre><code class="language-bash">ceph01% sudo ceph osd pool create volumes 128
ceph01% sudo ceph osd pool create images 128
ceph01% sudo ceph osd pool create backups 128
</code></pre>

<h4 id="ssh-鍵の配置">ssh 鍵の配置</h4>

<p>Ceph ノードから OpenStack の controller, compute ノードへ接続出来るよう鍵を配
布します。</p>

<pre><code class="language-bash">ceph01% ssh-copy-id &lt;username&gt;@&lt;controller_ip&gt;
ceph01% ssh-copy-id &lt;username&gt;@&lt;compute_ip&gt;
</code></pre>

<h4 id="sudoers-の設定">sudoers の設定</h4>

<p>controller, compute ノード上で sudoers の設定を予め実施する。
/etc/sudoers.d/<username> として保存する。</p>

<pre><code class="language-bash">&lt;username&gt; ALL = (root) NOPASSWD:ALL
</code></pre>

<h4 id="ceph-パッケージのインストール">ceph パッケージのインストール</h4>

<p>controller ノード, compute ノード上で Ceph パッケージをインストールする。</p>

<pre><code class="language-bash">controller% wget -q -O- 'https://ceph.com/git/?p=ceph.git;a=blob_plain;f=keys/release.asc' | sudo apt-key add -
controller% echo deb http://ceph.com/debian/ $(lsb_release -sc) main | sudo tee /etc/apt/sources.list.d/ceph.list
controller% sudo apt-get update &amp;&amp; sudo apt-get install -y python-ceph ceph-common

compute% wget -q -O- 'https://ceph.com/git/?p=ceph.git;a=blob_plain;f=keys/release.asc' | sudo apt-key add -
compute% echo deb http://ceph.com/debian/ $(lsb_release -sc) main | sudo tee /etc/apt/sources.list.d/ceph.list
compute% sudo apt-get update &amp;&amp; sudo apt-get install -y python-ceph ceph-common
</code></pre>

<p>controller ノード, compute ノード上でディレクトリを作成する。</p>

<pre><code class="language-bash">controller% sudo mkdir /etc/ceph
compute   % sudo mkdir /etc/ceph
</code></pre>

<p>ceph.conf を controller, compute ノードへ配布する。</p>

<pre><code class="language-bash">ceph01% sudo -i
ceph01# ssh &lt;controller_ip&gt; sudo tee /etc/ceph/ceph.conf &lt;/etc/ceph/ceph.conf
ceph01# ssh &lt;compute_ip&gt; sudo tee /etc/ceph/ceph.conf &lt;/etc/ceph/ceph.conf
</code></pre>

<h4 id="ceph-上にユーザを作成">ceph 上にユーザを作成</h4>

<p>Ceph 上に cinder, glance 用の新しいユーザを作成する。</p>

<pre><code class="language-bash">ceph auth get-or-create client.cinder mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool=volumes, allow rx pool=images'
ceph auth get-or-create client.glance mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool=images'
ceph auth get-or-create client.cinder-backup mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool=backups'
</code></pre>

<h4 id="キーリングの作成と配置">キーリングの作成と配置</h4>

<p>client.cinder, client.glance, client.cinder-backup のキーリングを作成する。また作成したキーリングを
controller ノードに配布する。</p>

<pre><code class="language-bash">ceph01% sudo ceph auth get-or-create client.glance | ssh {your-glance-api-server} sudo tee /etc/ceph/ceph.client.glance.keyring
ceph01% ssh {your-glance-api-server} sudo chown glance:glance /etc/ceph/ceph.client.glance.keyring
ceph01% sudo ceph auth get-or-create client.cinder | ssh {your-volume-server} sudo tee /etc/ceph/ceph.client.cinder.keyring
ceph01% ssh {your-cinder-volume-server} sudo chown cinder:cinder /etc/ceph/ceph.client.cinder.keyring
ceph01% sudo ceph auth get-or-create client.cinder-backup | ssh {your-cinder-backup-server} sudo tee /etc/ceph/ceph.client.cinder-backup.keyring
ceph01% ssh {your-cinder-backup-server} sudo chown cinder:cinder /etc/ceph/ceph.client.cinder-backup.keyring
</code></pre>

<p>client.cinder のキーリングを compute ノードに配置する。</p>

<pre><code class="language-bash">ceph01% sudo ceph auth get-key client.cinder | ssh {your-compute-node} tee client.cinder.key
</code></pre>

<h4 id="libvirt-への-secret-キー追加">libvirt への secret キー追加</h4>

<p>compute ノード上で secret キーを libvirt に追加する。</p>

<pre><code class="language-bash">compute% uuidgen
457eb676-33da-42ec-9a8c-9293d545c337

cat &gt; secret.xml &lt;&lt;EOF
&lt;secret ephemeral='no' private='no'&gt;
  &lt;uuid&gt;457eb676-33da-42ec-9a8c-9293d545c337&lt;/uuid&gt;
  &lt;usage type='ceph'&gt;
    &lt;name&gt;client.cinder secret&lt;/name&gt;
  &lt;/usage&gt;
&lt;/secret&gt;
EOF
compute % sudo virsh secret-define --file secret.xml
Secret 457eb676-33da-42ec-9a8c-9293d545c337 created
compute% sudo virsh secret-set-value --secret 457eb676-33da-42ec-9a8c-9293d545c337 --base64 $(cat client.cinder.key) &amp;&amp; rm client.cinder.key secret.xml
</code></pre>

<h4 id="glance-連携手順">glance 連携手順</h4>

<p>controller ノードの /etc/glance/glance-api.conf に下記を追記。
default_store=file と標準ではなっているので下記の通り rbd に書き換える。</p>

<pre><code>default_store=rbd
rbd_store_user=glance
rbd_store_pool=images
</code></pre>

<h4 id="cinder-連携手順">cinder 連携手順</h4>

<p>controller ノードの /etc/cinder/cinder.conf に下記を追記。
volume_driver は予め LVM の設定が入っていると思われるので書き換える。
また rbd_secret_uuid は先ほど生成した uuid を記す。</p>

<pre><code>volume_driver=cinder.volume.drivers.rbd.RBDDriver
rbd_pool=volumes
rbd_ceph_conf=/etc/ceph/ceph.conf
rbd_flatten_volume_from_snapshot=false
rbd_max_clone_depth=5
glance_api_version=2
rbd_user=cinder
rbd_secret_uuid=457eb676-33da-42ec-9a8c-9293d545c337
</code></pre>

<h4 id="ceph-conf-への追記">ceph.conf への追記</h4>

<p>上記で配布した ceph.conf にはキーリングのパスが記されていない。controller ノー
ド上の /etc/ceph/ceph.conf に下記の通り追記する。</p>

<p>ここは公式サイトには印されていないのでハマりました。ポイントです。</p>

<pre><code>[client.keyring]
  keyring = /etc/ceph/ceph.client.cinder.keyring
</code></pre>

<h4 id="openstack-のそれぞれのコンポーネントを再起動かける">OpenStack のそれぞれのコンポーネントを再起動かける</h4>

<pre><code class="language-bash">controller% sudo glance-control api restart
compute   % sudo service nova-compute restart
controller% sudo service cinder-volume restart
controller% sudo service cinder-backup restart
</code></pre>

<h2 id="動作確認">動作確認</h2>

<p>では動作確認を。Glance に OS イメージを登録。その後そのイメージを元にインスタ
ンスを作成。Cinder 上に仮想ディスクを作成。その後先ほど作成したインスタンスに
接続しマウント。そのマウントした仮想ディスク上で書き込みが行えるか確認をします。</p>

<p>テストで Ubuntu Precise 12.04 のイメージを glance を用いて登録する。</p>

<pre><code class="language-bash">controller% wget http://cloud-images.ubuntu.com/precise/current/precise-server-cloudimg-amd64-disk1.img
controller% glance image-create --name precise-image --is-public true --container-format bare --disk-format qcow2 &lt; precise-server-cloudimg-amd64-disk1.img
</code></pre>

<p>テスト VM を稼働する。</p>

<pre><code class="language-bash">controller% nova boot --nic net-id=&lt;net_id&gt; --flavor 2 --image precise-image --key_name novakey01 vm01
controller% cinder create --display-name vol01 1
controller% nova volume-attach &lt;instance_id&gt; &lt;volume_id&gt; auto
</code></pre>

<p>テスト VM へログインしファイルシステムを作成後、マウントする。</p>

<pre><code class="language-bash">controller% ssh -i &lt;key_file_path&gt; -l ubuntu &lt;instance_ip&gt;
vm01% sudo mkfs -t ext4 /dev/vdb
vm01% sudo mount -t ext4 /dev/vdb /mnt
</code></pre>

<p>マウントした仮想ディスク上でデータを書き込んでみる。</p>

<pre><code class="language-bash">vm01% sudo dd if=/dev/zero of=/mnt/500M bs=1M count=5000
</code></pre>

<h2 id="まとめ">まとめ</h2>

<p>Ceph, Cinder の Ceph 連携が出来ました！</p>

<p>OpenStack Grizzly 版時代に Ceph 連携は取っていたのですが Havana では</p>

<ul>
<li>cinder-bacup の Ceph 連携</li>
<li>nova の Ceph 連携</li>
</ul>

<p>が追加されていました。Nova 連携をとるとインスタンスを稼働させる際に通常は
controller ノードの /var/lib/nova 配下にファイルとしてインスタンスイメージが作
成されますが、これが Ceph 上に作成されるとのことです。Nova 連携は是非とってみ
たいのですが、今のところ動いていません。引き続き調査を行います。</p>

<p>cinder-backup も少し連携取ってみましたが backup_driver に Ceph ドライバの指定
をしているにも関わらず Swift に接続しにいってしまう有り様でした..。こちらも引
き続き調査します。またステートが &lsquo;in-use&rsquo; の場合バックアップが取れず
&lsquo;available&rsquo; なステートでないといけないようです。確かに書き込み中に操作が行われ
てしまってもバックアップの整合性が取れないですし、ここは仕方ないところですね。</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2014/04/04/openstack-havana-cinder-glance-ceph/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>


        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2014/03/16/rcbops-chef-cookbooks-keystone-havana-swift-1-10-0.deploy/">rcbops/chef-cookbooks で Keystone 2013.2.2(Havana) &#43; Swift 1.10.0 を構築</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2014-03-16'>
            March 16, 2014</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは。<a href="https://twitter.com/jedipunkz">@jedipunkz</a> です。</p>

<h4 id="追記">追記</h4>

<p>2014/03/20 : 一旦削除していた記事なのですが、無事動作が確認出来たので再度アッ
プします！</p>

<p>第17回 OpenStack 勉強会で rcbops/chef-cookbooks の話をしてきたのですが会場から
質問がありました。「Havana の Swift 構成を作る Cookbooks はどこにありますか？」
と。私が試したのが Grizzly 時代のモノで、よく rcbops/chef-cookbooks を見てみる
と Havana ブランチ又は Havana に対応したリリースタグのファイル構成に Swift が
綺麗サッパリ消えているではありませんか・・！下記の Swift の Cookbooks は幸い
github 上に残っていました。</p>

<p><a href="https://github.com/rcbops-cookbooks/swift">https://github.com/rcbops-cookbooks/swift</a></p>

<p>が rcbops/chef-cookbooks との関連付けが切れています・・。ぐあぁ。</p>

<p>ということで Havana 構成の Keystone 2013.2.2 と Swift 1.10.0 の構成を Chef で
作らねば！と思い色々試していたら結構あっさりと出来ました。今回はその方法を書い
ていきたいと思います！</p>

<h2 id="構成">構成</h2>

<p>構成は&hellip;以前の記事 <a href="http://jedipunkz.github.io/blog/2013/10/27/swift-chef/">http://jedipunkz.github.io/blog/2013/10/27/swift-chef/</a> と同じです。</p>

<pre><code>+-----------------+
|  load balancer  |
+-----------------+
|
+-------------------+-------------------+-------------------+-------------------+---------------------- proxy network
|                   |                   |                   |                   |                   
+-----------------+ +-----------------+ +-----------------+ +-----------------+ +-----------------+
|   chef server   | | chef workstation| |   swift-mange   | |  swift-proxy01  | |  swift-proxy02  | 
+-----------------+ +-----------------+ +-----------------+ +-----------------+ +-----------------+ ...&gt; scaling
|                   |                   |                   |                   |                   
+-------------------+-------------------+-------------------+-------------------+-------------------+-- storage network
|                   |                   |                   |                   |                   |
+-----------------+ +-----------------+ +-----------------+ +-----------------+ +-----------------+ +-----------------+ 
| swift-storage01 | | swift-storage02 | | swift-storage03 | | swift-account01 | | swift-account02 | | swift-account03 |
+-----------------+ +-----------------+ +-----------------+ +-----------------+ +-----------------+ +-----------------+ ..&gt; scaling
</code></pre>

<h2 id="手順">手順</h2>

<p>では早速手順を記していきますね。毎回なのですが Chef ワークステーション・Chef
サーバの環境構築については割愛します。オムニバスインストーラを使えば Chef サー
バの構築は簡単ですし、ワークステーションの構築も Ruby インストール -&gt; gem で
Chef をインストール -&gt; .chef 配下を整える、で出来ます。</p>

<p>rcbops/chef-cookbooks の取得。現在 Stable バージョンの refs/tags/v4.2.1 を用いる。</p>

<pre><code class="language-bash">% git clone https://github.com/rcbops/chef-cookbooks.git ./chef-cookbooks-4.2.1
% cd ./chef-cookbooks-4.2.1
% git checkout -b v4.2.1 refs/tags/v4.2.1
% git submodule init
% git submodule sync
% git submodule update
</code></pre>

<p>ここで本家 rcbops/chef-cookbooks と関連付けが消えている rcbops-cookbooks/swift
を cookbooks ディレクトリ配下にクローンします。あと念のため &lsquo;havana&rsquo; ブランチ
を指定します。コードを確認したところ何も変化はありませんでしたが。</p>

<pre><code class="language-bash">% git clone https://github.com/rcbops-cookbooks/swift.git cookbooks/swift
% cd cookbooks/swift
% git checkout -b havana remotes/origin/havana
% cd ../..
</code></pre>

<p>cookbooks, roles の Chef サーバへのアップロードを行います。</p>

<pre><code class="language-bash">% knife cookbook upload -o cookbooks -a
% knife role from file role/*.rb
</code></pre>

<p>今回の構成 (1クラスタ) 用の environments/swift-havana.json を作成します。json
ファイルの名前は任意です。</p>

<pre><code class="language-json">{
  &quot;name&quot;: &quot;swift-havana&quot;,
  &quot;description&quot;: &quot;&quot;,
  &quot;cookbook_versions&quot;: {
  },
  &quot;json_class&quot;: &quot;Chef::Environment&quot;,
  &quot;chef_type&quot;: &quot;environment&quot;,
  &quot;default_attributes&quot;: {
  },
  &quot;override_attributes&quot;: {
    &quot;package_component&quot;: &quot;havana&quot;,
    &quot;osops_networks&quot;: {
      &quot;management&quot;: &quot;10.200.9.0/24&quot;,
      &quot;public&quot;: &quot;10.200.10.0/24&quot;,
      &quot;swift&quot;: &quot;10.200.9.0/24&quot;
    },
    &quot;keystone&quot;: {
      &quot;pki&quot;: {
        &quot;enabled&quot;: false
      },
      &quot;admin_port&quot;: &quot;35357&quot;,
      &quot;admin_token&quot;: &quot;admin&quot;,
      &quot;admin_user&quot;: &quot;admin&quot;,
      &quot;tenants&quot;: [
        &quot;admin&quot;,
        &quot;service&quot;
      ],
      &quot;users&quot;: {
        &quot;admin&quot;: {
          &quot;password&quot;: &quot;secrete&quot;,
          &quot;roles&quot;: {
            &quot;admin&quot;: [
              &quot;admin&quot;
            ]
          }
        },
        &quot;demo&quot;: {
          &quot;password&quot;: &quot;demo&quot;,
          &quot;default_tenant&quot; : &quot;service&quot;,
          &quot;roles&quot;: {
            &quot;admin&quot;: [ &quot;admin&quot; ]
          }
        }
      },
      &quot;db&quot;: {
        &quot;password&quot;: &quot;keystone&quot;
      }
    },
    &quot;mysql&quot;: {
      &quot;root_network_acl&quot;: &quot;%&quot;,
      &quot;allow_remote_root&quot;: true,
      &quot;server_root_password&quot;: &quot;secrete&quot;,
      &quot;server_repl_password&quot;: &quot;secrete&quot;,
      &quot;server_debian_password&quot;: &quot;secrete&quot;
    },
    &quot;monitoring&quot;: {
      &quot;procmon_provider&quot;: &quot;monit&quot;,
      &quot;metric_provider&quot;: &quot;collectd&quot;
    },
    &quot;vips&quot;: {
      &quot;keystone-admin-api&quot;: &quot;10.200.9.11&quot;,
      &quot;keystone-service-api&quot;: &quot;10.200.9.11&quot;,
      &quot;keystone-internal-api&quot;: &quot;10.200.9.11&quot;,
      &quot;swift-proxy&quot;: &quot;10.200.9.11&quot;,
      &quot;config&quot;: {
        &quot;10.200.9.112&quot;: {
          &quot;vrid&quot;: 12,
          &quot;network&quot;: &quot;management&quot;
        }
      }
    },
    &quot;developer_mode&quot;: false,
    &quot;swift&quot;: {
      &quot;swift_hash&quot;: &quot;307c0568ea84&quot;,
      &quot;authmode&quot;: &quot;keystone&quot;,
      &quot;authkey&quot;: &quot;20281b71-ce89-4b27-a2ad-ad873d3f2760&quot;
    }
  }
}
</code></pre>

<p>作成した environment ファイル environments/swift-havana.json を chef-server へアップ
ロードする。</p>

<pre><code class="language-bash">% knife environment from file environments/swift-havana.json
</code></pre>

<h2 id="cookbooks-の修正">Cookbooks の修正</h2>

<p>swift cookbooks を修正します。havana からは keystone を扱うクライアントは
keystone.middleware.auth_token でなく keystoneclient.middleware.auth_token を
使うように変更掛かっていますので、下記のように修正しました。</p>

<pre><code class="language-bash">% cd cookbooks/swift/templates/default
% diff -u proxy-server.conf.erb.org proxy-server.conf.erb
--- proxy-server.conf.erb.org   2014-03-20 16:28:28.000000000 +0900
+++ proxy-server.conf.erb       2014-03-20 16:28:13.000000000 +0900
@@ -243,7 +243,8 @@
 use = egg:swift#keystoneauth
 
 [filter:authtoken]
-paste.filter_factory = keystone.middleware.auth_token:filter_factory
+#paste.filter_factory = keystone.middleware.auth_token:filter_factory
+paste.filter_factory = keystoneclient.middleware.auth_token:filter_factory
 auth_host = &lt;%= @keystone_api_ipaddress %&gt;
 auth_port = &lt;%= @keystone_admin_port %&gt;
 auth_protocol = &lt;%= @keystone_admin_protocol %&gt;
% cd ../../../..
</code></pre>

<h2 id="デプロイ">デプロイ</h2>

<p>かきのとおり knife bootstrap する。</p>

<pre><code class="language-bash">% knife bootstrap &lt;manage_ip_addr&gt; -N swift-manage -r 'role[base]','role[mysql-master]','role[keystone]','role[swift-management-server]' -E swift-havana --sudo -x thirai
% knife bootstrap &lt;proxy01_ip_addr&gt; -N swift-proxy01 -r &quot;role[base]&quot;,&quot;role[swift-proxy-server]&quot;,'role[swift-setup]','role[openstack-ha]' -E swift-havana --sudo -x thirai
% knife bootstrap &lt;proxy02_ip_addr&gt; -N swift-proxy02 -r &quot;role[base]&quot;,&quot;role[swift-proxy-server]&quot;,'role[openstack-ha]' -E swift-havana --sudo -x thirai
% knife bootstrap &lt;storage01_ip_addr&gt; -N swift-storage01 -r 'role[base]','role[swift-object-server]' -E swift-havana --sudo -x thirai
% knife bootstrap &lt;storage02_ip_addr&gt; -N swift-storage02 -r 'role[base]','role[swift-object-server]' -E swift-havana --sudo -x thirai
% knife bootstrap &lt;storage03_ip_addr&gt; -N swift-storage03 -r 'role[base]','role[swift-object-server]' -E swift-havana --sudo -x thirai
% knife bootstrap &lt;account01_ip_addr&gt; -N swift-account01 -r 'role[base]','role[swift-account-server]','role[swift-container-server]' -E swift-havana --sudo -x thirai
% knife bootstrap &lt;account02_ip_addr&gt; -N swift-account02 -r 'role[base]','role[swift-account-server]','role[swift-container-server]' -E swift-havana --sudo -x thirai
% knife bootstrap &lt;account03_ip_addr&gt; -N swift-account03 -r 'role[base]','role[swift-account-server]','role[swift-container-server]' -E swift-havana --sudo -x thirai
</code></pre>

<p>ここでバグ対策。Swift 1.10.0 にはバグがあるので下記の通り対処します。</p>

<p>keystone.middleware.s3_token に既知のバグがあり、下記のように対処します。この
状態ではバグにより swift-proxy が稼働してない状態ですが後の各ノードでの
chef-client 実行時に稼働する予定です。</p>

<pre><code class="language-python">% diff /usr/lib/python2.7/dist-packages/keystone/exception.py.org /usr/lib/python2.7/dist-packages/keystone/exception.py 
--- exception.py.org    2014-03-12 16:45:00.181420694 +0900
+++ exception.py        2014-03-12 16:44:47.173177081 +0900
@@ -18,6 +18,7 @@

 from keystone.common import config
 from keystone.openstack.common import log as logging
+from keystone.openstack.common.gettextutils import _
 from keystone.openstack.common import strutils
</code></pre>

<p>上記のバグ報告は下記の URL にあります。</p>

<p><a href="https://bugs.launchpad.net/ubuntu/+source/swift/+bug/1231339">https://bugs.launchpad.net/ubuntu/+source/swift/+bug/1231339</a></p>

<p>zone 番号を付与します。</p>

<pre><code class="language-bash">% knife exec -E &quot;nodes.find(:name =&gt; 'swift-storage01') {|n| n.set['swift']['zone'] = '1'; n.save }&quot;
% knife exec -E &quot;nodes.find(:name =&gt; 'swift-account01') {|n| n.set['swift']['zone'] = '1'; n.save }&quot;
% knife exec -E &quot;nodes.find(:name =&gt; 'swift-storage02') {|n| n.set['swift']['zone'] = '2'; n.save }&quot;
% knife exec -E &quot;nodes.find(:name =&gt; 'swift-account02') {|n| n.set['swift']['zone'] = '2'; n.save }&quot;
% knife exec -E &quot;nodes.find(:name =&gt; 'swift-storage03') {|n| n.set['swift']['zone'] = '3'; n.save }&quot;
% knife exec -E &quot;nodes.find(:name =&gt; 'swift-account03') {|n| n.set['swift']['zone'] = '3'; n.save }&quot;
</code></pre>

<p>zone 番号が付与されたこと下記の通りを確認します</p>

<p>account-server の確認</p>

<pre><code class="language-bash">% knife exec -E 'search(:node,&quot;role:swift-account-server&quot;) \
  { |n| z=n[:swift][:zone]||&quot;not defined&quot;; puts &quot;#{n.name} has the role \
  [swift-account-server] and is in swift zone #{z}&quot;; }'
swift-account01 has the role       [swift-account-server] and is in swift zone 1
swift-account02 has the role       [swift-account-server] and is in swift zone 2
swift-account03 has the role       [swift-account-server] and is in swift zone 3
</code></pre>

<p>container-server の確認</p>

<pre><code class="language-bash">% knife exec -E 'search(:node,&quot;role:swift-container-server&quot;) \
  { |n| z=n[:swift][:zone]||&quot;not defined&quot;; puts &quot;#{n.name} has the role \
  [swift-container-server] and is in swift zone #{z}&quot;; }'
swift-account01 has the role       [swift-container-server] and is in swift zone 1
swift-account02 has the role       [swift-container-server] and is in swift zone 2
swift-account03 has the role       [swift-container-server] and is in swift zone 3
</code></pre>

<p>object-server の確認</p>

<pre><code class="language-bash">% knife exec -E 'search(:node,&quot;role:swift-object-server&quot;) \
  { |n| z=n[:swift][:zone]||&quot;not defined&quot;; puts &quot;#{n.name} has the role \
  [swift-object-server] and is in swift zone #{z}&quot;; }'
swift-storage01 has the role   [swift-object-server] and is in swift zone 1
swift-storage02 has the role   [swift-object-server] and is in swift zone 2
swift-storage03 has the role   [swift-object-server] and is in swift zone 3
</code></pre>

<p>Chef が各々のノードに搭載された Disk を検知出来るか否かを確認する。</p>

<pre><code class="language-ruby">% knife exec -E \
  'search(:node,&quot;role:swift-object-server OR \
  role:swift-account-server \
  OR role:swift-container-server&quot;) \
  { |n| puts &quot;#{n.name}&quot;; \
  begin; n[:swift][:state][:devs].each do |d| \
  puts &quot;\tdevice #{d[1][&quot;device&quot;]}&quot;; \
  end; rescue; puts \
  &quot;no candidate drives found&quot;; end; }'
    swift-storage02
            device sdb1
    swift-storage03
            device sdb1
    swift-account01
            device sdb1
    swift-account02
            device sdb1
    swift-account03
            device sdb1
    swift-storage01
            device sdb1
</code></pre>

<p>swift-manage ノードにて chef-client を実行し
/etc/swift/ring-workspace/generate-rings.sh を更新します。</p>

<pre><code class="language-bash">swift-manage% sudo chef-client
</code></pre>

<p>generate-rings.sh の &lsquo;exit 0&rsquo; 行をコメントアウトし実行します。</p>

<pre><code class="language-bash">swift-manage% sudo ${EDITOR} /etc/swift/ring-workspace/generage-rings.sh
swift-manage% sudo /etc/swift/ring-workspace/generate-rings.sh
</code></pre>

<p>この操作で /etc/swift/ring-workspace/rings 配下に account, container, object
用の Rings ファイル群が生成されたことを確認出来るはずです。これらを
swift-manage 上で既に稼働している git サーバに push し管理します。</p>

<pre><code class="language-bash">swift-manage# cd /etc/swift/ring-workspace/rings
swift-manage# git add account.builder container.builder object.builder
swift-manage# git add account.ring.gz container.ring.gz object.ring.gz
swift-manage# git commit -m &quot;initial commit&quot;
swift-manage# git push
</code></pre>

<p>各々のノードにて chef-client を実行することで git サーバ上の Rings ファイル群
を取得し、swift プロセスを稼働させます。</p>

<pre><code class="language-bash">swift-proxy01# chef-client
swift-proxy02# chef-client
swift-storage01# chef-client
swift-storage02# chef-client
swift-storage03# chef-client
swift-account01# chef-client
swift-account02# chef-client
swift-account03# chef-client
</code></pre>

<p>3台のノードが登録されたかどうかを下記の通り確認行います。</p>

<pre><code class="language-bash">swift-proxy01% sudo swift-recon --md5
[sudo] password for thirai:
===============================================================================
--&gt; Starting reconnaissance on 3 hosts
===============================================================================
[2013-10-18 11:14:43] Checking ring md5sums
3/3 hosts matched, 0 error[s] while checking hosts.
===============================================================================
</code></pre>

<h2 id="動作確認">動作確認</h2>

<p>構築が出来ました！ということで動作の確認をしてみましょう。</p>

<p>テストコンテナ &lsquo;container01&rsquo; にテストファイル &lsquo;test&rsquo; をアップロードしてみる。</p>

<pre><code class="language-bash">swift-storage01% swift -V 2 -A http://&lt;ip_addr_keystone&gt;:5000/v2.0/ -U admin:admin -K secrete stat
swift-storage01% swift -V 2 -A http://&lt;ip_addr_keystone&gt;:5000/v2.0/ -U admin:admin -K secrete post container01
swift-storage01% echo &quot;test&quot; &gt; test
swift-storage01% swift -V 2 -A http://&lt;ip_addr_keystone&gt;:5000/v2.0/ -U admin:admin -K secrete upload container01 test
swift-storage01% swift -V 2 -A http://&lt;ip_addr_keystone&gt;:5000/v2.0/ -U admin:admin -K secrete list
container01
swift-storage01% swift -V 2 -A http://&lt;ip_addr_keystone&gt;:5000/v2.0/ -U admin:admin -K secrete list container01 test
</code></pre>

<h2 id="まとめ">まとめ</h2>

<p>前回「実用的な Swift 構成を Chef でデプロイ」の記事で記した内容とほぼ手順は変
わりませんでした。rcbops-cookbooks/rcbops-utils 内にソフトウェアの取得先レポジ
トリを記すレシピが下記の場所にあります。</p>

<p><a href="https://github.com/rcbops-cookbooks/osops-utils/blob/master/recipes/packages.rb">https://github.com/rcbops-cookbooks/osops-utils/blob/master/recipes/packages.rb</a></p>

<p>そして havana ブランチの attributes を確認すると Ubuntu Cloud Archive の URL
が記されていることが確認出来ます。下記のファイルです。</p>

<p><a href="https://github.com/rcbops-cookbooks/osops-utils/blob/havana/attributes/repos.rb">https://github.com/rcbops-cookbooks/osops-utils/blob/havana/attributes/repos.rb</a></p>

<p>ファイルの中身の抜粋です。</p>

<pre><code class="language-json">    &quot;havana&quot; =&gt; {
      &quot;uri&quot; =&gt; &quot;http://ubuntu-cloud.archive.canonical.com/ubuntu&quot;,
      &quot;distribution&quot; =&gt; &quot;precise-updates/havana&quot;,
      &quot;components&quot; =&gt; [&quot;main&quot;],
      &quot;keyserver&quot; =&gt; &quot;hkp://keyserver.ubuntu.com:80&quot;,
      &quot;key&quot; =&gt; &quot;EC4926EA&quot;
    },
</code></pre>

<p>これらのことより、rcbops-utils の attibutes で havana (実際には
&lsquo;havana-proposed&rsquo;) をレポジトリ指定するように Cookbooks 構成を管理してあげれば
Havana 構成の Keystone, Swift が構築出来ることになります。ちなみに
havana-proposed で Swift や Keystone のどのバージョンがインストールされるかは、
下記の Packages ファイルを確認すると判断出来ます。</p>

<p><a href="http://ubuntu-cloud.archive.canonical.com/ubuntu/dists/precise-proposed/havana/main/binary-amd64/Packages">http://ubuntu-cloud.archive.canonical.com/ubuntu/dists/precise-proposed/havana/main/binary-amd64/Packages</a></p>

<p>以上です。</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2014/03/16/rcbops-chef-cookbooks-keystone-havana-swift-1-10-0.deploy/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>


        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2014/03/05/sensu-chef-openstack-fog-autoscaler/">Sensu,Chef,OpenStack,Fog を使ったオレオレオートスケーラを作ってみた！</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2014-03-05'>
            March 5, 2014</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは。@jedipunkz です。</p>

<p>今まで監視システムの Sensu やクラウドプラットフォームの OpenStack、コンフィギュ
レーションマネージメントツールの Chef やクラウドライブラリの Fog 等使ってきま
したが、これらを組み合わせるとオレオレオートスケーラ作れるんじゃないか？と思い、
ちょろっと作ってみました。</p>

<p>ちなみに自分はインフラエンジニアでしかも運用の出身なので Ruby に関しては初心者
レベルです。Chef で扱っているのと Rails アプリを作った経験はありますが、その程
度。Fog というクラウドライブラリにコントリビュートしたことはアリますが..。ちな
みに Fog のコントリビュート内容は OpenStack Neutron(当時 Quantum) の仮想ルータ
の操作を行う実装です。</p>

<p>そんな自分ですが&hellip;設計1周間・実装1周間でマネージャと CLI が出来ました。
また暫く放置していたマネージャと CLI に WebUI くっつけようかなぁ？と思って
sinatra の学習を始めたのですが、学習を初めて 1.5 日で WebUI が動くところまで行
きました。何故か？Ruby には有用な技術が揃っているから・・！(´；ω；｀)ﾌﾞﾜｯ</p>

<h2 id="オレオレオートスケーラ-sclman-の置き場所">オレオレオートスケーラ &lsquo;sclman&rsquo; の置き場所</h2>

<p><a href="https://github.com/jedipunkz/sclman">https://github.com/jedipunkz/sclman</a></p>

<p>スクリーンショット
+++</p>

<p><img src="https://raw.github.com/jedipunkz/sclman/master/pix/sclman.png" width="600"></p>

<h2 id="構成は">構成は？</h2>

<pre><code>+-------------- public network                  +-------------+
|                                               |sclman-api.rb|
+----+----+---+                                 |  sclman.rb  |
| vm | vm |.. |                                 |sclman-cli.rb|
+-------------+ +-------------+ +-------------+ +-------------+
|  openstack  | | chef server | | sensu server| | workstation |
+-------------+ +-------------+ +-------------+ +-------------+
|               |               |               |
+---------------+---------------+---------------+--------------- management network
</code></pre>

<p>&lsquo;sclman&rsquo; っていう名前です。上図の workstation ノードで稼働します。処理の流れは</p>

<h2 id="処理の流れ">処理の流れ</h2>

<ul>
<li>1) sclman-cli.rb もしくは WebUI から HTTP クラスタのセットを OpenStack 上に生成</li>
<li>2) 生成された VM に対して Chef で nginx をインストール</li>
<li>3) Chef の Roles である &lsquo;LB&rsquo; と &lsquo;Web&rdquo; が同一 Envrionment になるようにブートストラップ</li>
<li>4) LB VM のバックエンドに Web VM が指し示される</li>
<li>5) bootstrap と同時に sensu-client をインストール</li>
<li>6) Web VM の load を sensu で監視</li>
<li>7) sclman.rb (マネージャ) は Sensu AP を定期的に叩いて Web VM の load を監視</li>
<li>8) load が高い environment があれが該当 environment の HTTP クラスタに VM を追加</li>
<li>9) LB VM は追加された VM へのリクエストを追加<br /></li>
<li>10) 引き続き監視し一定期間 load が上がらなけれ Web VM を削除</li>
<li>11) LB VM は削除された VM へのリクエストを削除</li>
</ul>

<p>といった感じです。要約すると CLI/WebUI から HA クラスタを作成。その時に LB,
Web ミドルウェアと同時に sensu クライアントを VM に放り込む。監視を継続して負
荷が上昇すれば Web インスタンスの数を増加させ LB のリクエスト振り先にもその追
加した VM のアドレスを追加。逆に負荷が下がれば VM 削除と共にリクエスト振り先も
削除。この間、人の手を介さずに処理してくれる。です。</p>

<h2 id="使い方">使い方</h2>

<p>詳細な使い方は github の README.md を見て下さい。ここには簡単な説明だけ書いて
おきます。</p>

<ul>
<li>sclman を github から取得して bundler で必要な gems をインストールします。</li>
<li>chef-repo に移動して Berkshelf で必要な cookbooks をインストールします。</li>
<li>cookbooks を用いて sensu をデプロイします。</li>
<li>Omnibus インストーラーを使って chef サーバをインストールします。</li>
<li>OpenStack をインストールします。</li>
<li>sclman.conf を環境に合わせて修正します。</li>
<li>sclman.rb (マネージャ) を稼働します。</li>
<li>sclman-api.rb (WebUI/API) を稼働します。</li>
<li>sclman-cli.rb (CLI) もしくは WebUI から最小構成の HTTP クラスタを稼働します。</li>
<li>この状態で &lsquo;Web&rsquo; Role のインスタンスに負荷が掛かると &lsquo;Web&rsquo; Role のインスタンスの数が増加します。</li>
<li>また逆に負荷が下がるとインスタンス数が減ります。</li>
</ul>

<p>負荷の増減のシビアさは sclman.conf のパラメータ &lsquo;man_sensitivity&rsquo; で決定します。
値が長ければ長いほど増減のし易さは低下します。</p>

<p>まとめ
+++</p>

<p>こんな僕でも Ruby の周辺技術使ってなんとなくの形が出来ましたー。ただまだまだ課
題はあって、インフラを制御するアプリってエラーハンドリングが難しいっていうこと
です。帰ってくるエラーの一覧などがクラウドプラットフォーム・クラウドライブラリ
のドキュメントにあればいいのだけど、まだそこまで行ってない。Fog もまだまだ絶賛
開発中と言うかクラウドプラットフォームの進化に必死で追いついている状態なので、
僕らがアプリを作るときには自分でエラーを全部洗い出す等の作業が必要になるかもし
れない。大変だけど面白い所でもありますよね！これからも楽しみです。</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2014/03/05/sensu-chef-openstack-fog-autoscaler/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>


        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2014/02/27/journal-ssd-ceph-deploy/">Journal 用 SSD を用いた Ceph 構成の構築</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2014-02-27'>
            February 27, 2014</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは、@jedipunkz です。</p>

<p>前回、&rsquo;Ceph のプロセス配置ベストプラクティス&rsquo; というタイトルで記事を書きました。</p>

<p><a href="http://jedipunkz.github.io/blog/2014/01/29/ceph-process-best-practice/">http://jedipunkz.github.io/blog/2014/01/29/ceph-process-best-practice/</a></p>

<p>今回はこの記事にあるポリシに従って下記のような特徴を持った構成を作る手順を具体
的に書いていきたいと思います。</p>

<ul>
<li>ceph01 - ceph04 の4台構成</li>
<li>ノードに HDD 2台搭載されていることを前提 (/dev/sdb, /dev/sdc)</li>
<li>ノードに Journal 用 SSD 1台搭載されていることを前提 (/dev/ssd)</li>
<li>ceph04 は mds サービス稼働</li>
<li>ceph05 は ceph-deploy を実行するためのワークステーション</li>
<li>最終的に ceph04 から Ceph をマウントする</li>
<li>mon は ノード単位で稼働</li>
<li>osd は HDD 単位で稼働</li>
<li>mds は ceph04 に稼働</li>
</ul>

<h2 id="構成-ハードウェアとノードとネットワークの関係">構成 : ハードウェアとノードとネットワークの関係</h2>

<pre><code>                                                                                      public network
         +-------------------+-------------------+-------------------+-------------------+---------
         |                   |                   |                   |                   |
+--------+--------+ +--------+--------+ +--------+--------+ +--------+--------+ +--------+--------+
|      ceph01     | |      ceph02     | |      ceph03     | |      ceph04     | |      ceph05     |
| +-----+ +-----+ | | +-----+ +-----+ | | +-----+ +-----+ | |                 | |                 |
| | sdb | | sdc | | | | sdb | | sdc | | | | sdb | | sdc | | |                 | |                 |
| +-----+-+-----+ | | +-----+-+-----+ | | +-----+-+-----+ | |                 | |                 |
| |     ssd     | | | |     ssd     | | | |     ssd     | | |                 | |                 |
| +-------------+ | | +-------------+ | | +-------------+ | |                 | |                 |
+--------+--------+ +--------+--------+ +--------+--------+ +-----------------+ +-----------------+
         |                   |                   |                                    cluster network
         +-------------------+-------------------+-------------------------------------------------
</code></pre>

<h2 id="構成-プロセスとノードとネットワークの関係">構成 : プロセスとノードとネットワークの関係</h2>

<pre><code>                                                                                      public network
         +-------------------+-------------------+-------------------+-------------------+---------
         |                   |                   |                   |                   |
+--------+--------+ +--------+--------+ +--------+--------+ +--------+--------+ +--------+--------+
|      ceph01     | |      ceph02     | |      ceph03     | |      ceph04     | |      ceph05     |
| +-----+ +-----+ | | +-----+ +-----+ | | +-----+ +-----+ | | +-------------+ | |                 |
| | osd | | osd | | | | osd | | osd | | | | osd | | osd | | | |     mds     | | |                 |
| +-----+-+-----+ | | +-----+-+-----+ | | +-----+-+-----+ | | +-------------+ | |                 |
| |     mon     | | | |     mon     | | | |     mon     | | |                 | |                 |
| +-------------+ | | +-------------+ | | +-------------+ | |                 | |                 |
+--------+--------+ +--------+--------+ +--------+--------+ +-----------------+ +-----------------+
         |                   |                   |                                    cluster network
         +-------------------+-------------------+-------------------------------------------------
</code></pre>

<p>注意 : 上記の図だと ssd : mon が対に見えますが、そうではありません。</p>

<p>では構築方法について書いていきます。</p>

<h2 id="作業用ホストの準備">作業用ホストの準備</h2>

<p>ノンパスフレーズの SSH 公開鍵・秘密鍵を生成する。</p>

<pre><code class="language-bash">% ssh-keygen
</code></pre>

<p>公開鍵をターゲットホスト (ceph01-03) に配置</p>

<pre><code class="language-bash">% ssh-copy-id ceph@ceph01
% ssh-copy-id ceph@ceph02
% ssh-copy-id ceph@ceph03
</code></pre>

<p>ceph-deploy の取得を行う。</p>

<pre><code class="language-bash">% git clone https://github.com/ceph/ceph-deploy.git ~/ceph-deploy
</code></pre>

<p>&lsquo;python-virtualenv&rsquo; パッケージをインストールする。</p>

<pre><code class="language-bash">% sudo apt-get update ; sudo apt-get -y install python-virtualenv
</code></pre>

<p>ceph-deploy をブートストラップする</p>

<pre><code class="language-bash">% cd ~/ceph-deploy
% ./bootstrap
</code></pre>

<p>PATH を通す。下記は例。</p>

<pre><code class="language-bash">% ${EDITOR} ~/.zshrc
export PATH=$HOME/ceph-deploy:$PATH
</code></pre>

<p>ホスト名の解決を行う。IP アドレスは例。</p>

<pre><code class="language-bash">% sudo ${EDITOR} /etc/hosts
10.200.10.11    ceph01
10.200.10.12    ceph02
10.200.10.13    ceph03
10.200.10.14    ceph04
10.200.10.15    ceph05
</code></pre>

<h2 id="上記の構成の構築方法">上記の構成の構築方法</h2>

<p>以前の記事同様に ceph-deploy をデプロイツールとして用いる。</p>

<p>ceph-deploy に関しては下記の URL を参照のこと。</p>

<p><a href="https://github.com/ceph/ceph-deploy">https://github.com/ceph/ceph-deploy</a></p>

<p>下記の手順でコンフィギュレーションと鍵の生成を行う。またこれからの操作はすべて public network
上の ceph-deploy 専用 node からの操作とする。</p>

<pre><code class="language-bash">% mkdir ~/ceph-work
% cd ~/ceph-work
% ceph-deploy --cluster cluster01 new ceph01 ceph02 ceph03 ceph04
</code></pre>

<p>~/ceph-work ディレクトリ上に cluster01.conf が生成されているので下記の通り
cluster network を扱う形へと追記を行う。</p>

<pre><code class="language-bash">public network = &lt;public_network_addr&gt;/&lt;netmask&gt;
cluster network = &lt;cluster_network_addr&gt;/&lt;netmask&gt;

[mon.a]
    host = ceph01
    mon addr = &lt;ceph01_ip_addr&gt;:6789

[mon.b]
    host = ceph02
    mon addr = &lt;ceph02_ip_addr&gt;:6789

[mon.c]
    host = ceph03
    mon addr = &lt;ceph03_ip_addr&gt;:6789

[osd.0]
    public addr = &lt;ceph01_public_ip_addr&gt;
    cluster addr = &lt;ceph01_cluster_ip_addr&gt;

[osd.1]
    public addr = &lt;ceph01_public_ip_addr&gt;
    cluster addr = &lt;ceph01_cluster_ip_addr&gt;

[osd.2]
    public addr = &lt;ceph01_public_ip_addr&gt;
    cluster addr = &lt;ceph01_cluster_ip_addr&gt;

[mds.a]
    host = ceph04
</code></pre>

<p>ceph の各 nodes へのインストールを行う。ceph はワークステーションである ceph05 にも
インストールしておきます。後に Ceph ストレージをマウントするためです。</p>

<pre><code class="language-bash">% ceph-deploy --cluster cluster01 install ceph01 ceph02 ceph03 ceph04 ceph04
</code></pre>

<p>mon プロセスを各 nodes で稼働する。</p>

<pre><code class="language-bash">% ceph-deploy --cluster cluster01 mon create ceph01 ceph02 ceph03
</code></pre>

<p>鍵の配布を各 nodes に行う。</p>

<pre><code class="language-bash">% ceph-deploy --cluster cluster01 gatherkeys ceph01 ceph02 ceph03 ceph04 ceph05
</code></pre>

<p>disk のリストを確認。</p>

<p>各 node 毎に用いることが可能は disk の一覧を確認する。</p>

<pre><code class="language-bash">% ceph-deploy --cluster cluster01 disk list ceph01
% ceph-deploy --cluster cluster01 disk list ceph02
% ceph-deploy --cluster cluster01 disk list ceph03
</code></pre>

<p>disk の初期化を行う。この作業を行うと指定ディスク上のデータは消去される。</p>

<pre><code class="language-bash">% ceph-deploy --cluster cluster01 disk zap ceph01:/dev/sdb ceph01:/dev/sdc
% ceph-deploy --cluster cluster01 disk zap ceph02:/dev/sdb ceph02:/dev/sdc
% ceph-deploy --cluster cluster01 disk zap ceph03:/dev/sdb ceph03:/dev/sdc
</code></pre>

<p>journal 用の ssd のパーティションを切る。ここでは 10GB 毎に切った
/dev/ssd1, /dev/ssd2 が存在することを前提に記す。ceph と同時にインストールされた
gdisk を用いる。</p>

<pre><code class="language-bash">% sudo gdisk /dev/ssd
</code></pre>

<p>(注意) 下記の公式ドキュメントでは osd prepare, osc activate の手順が掲載されて
いるがその後の osd create のコマンドにて prepare が実行されるようでこれら2つの
手順を行うと正常に osd create コマンドが実行できなかった。よってこのタイミング
にて osd create を行うものとする。</p>

<ul>
<li><a href="http://ceph.com/docs/master/rados/deployment/ceph-deploy-osd/#prepare-osds">http://ceph.com/docs/master/rados/deployment/ceph-deploy-osd/#prepare-osds</a></li>
<li><a href="http://ceph.com/docs/dumpling/start/quick-ceph-deploy/">http://ceph.com/docs/dumpling/start/quick-ceph-deploy/</a></li>
</ul>

<p>2 つの disk に対してそれぞれ osd を稼働させる。</p>

<pre><code class="language-bash">% ceph-deploy --cluster cluster01 osd create ceph01:sdb:/dev/ssd1 ceph02:sdb:/dev/ssd1 ceph03:sdb:/dev/ssd1
% ceph-deploy --cluster cluster01 osd create ceph01:sdc:/dev/ssd2 ceph02:sdc:/dev/ssd2 ceph03:sdc:/dev/ssd2
</code></pre>

<p>mds の稼働を行う。ここでは1号機にのみ稼働を行う。</p>

<pre><code class="language-bash">% ceph-deploy --cluster cluster01 mds create ceph04
</code></pre>

<h2 id="クライアントからのマウント方法各種">クライアントからのマウント方法各種</h2>

<p>上記で構築した Ceph ストレージを利用する方法を3つ説明する。先に述べたように
POSIX 互換 filesystem として利用が可能。それぞれ mds が稼働しているホストに対
して接続を行う。</p>

<h4 id="block-device-としてマウントする方法">Block Device としてマウントする方法</h4>

<p>ストレージ上に block device を生成しそれをマウントする</p>

<pre><code class="language-bash">cephclient% rbd create foo --size 4096
cephclient% sudo modprobe rbd
cephclient% sudo rbd map foo --pool rbd --name client.admin
cephclient% sudo mkfs.ext4 -m0 /dev/rbd/rbd/foo
cephclient% sudo mkdir /mnt/myrbd
cephclinet% sudo mount /dev/rbd/rbd/foo /mnt/myrbd
</code></pre>

<h4 id="kernel-driver-を用いてマウントする方法">Kernel Driver を用いてマウントする方法</h4>

<p>kernel Driver を用いてストレージをマウントする</p>

<pre><code class="language-bash">cephclient% sudo mkdir /mnt/mycephfs
cephclient% sudo mount -t ceph 10.200.10.26:6789:/ /mnt/mycephfs -o \
            name=admin,secret=`sudo ceph-authtool -p /etc/ceph/cluster01.client.admin.keyring`
</code></pre>

<h4 id="fuse-driver-ユーザランド-を用いてマウントする方法">Fuse Driver (ユーザランド) を用いてマウントする方法</h4>

<p>ユーザランドソフトウェア FUSE を用いてマウントする方法</p>

<pre><code class="language-bash">cephclient% sudo mkdir /home/&lt;username&gt;/cephfs
cephclient% sudo ceph-fuse -m 10.200.10.26:6789 /home/&lt;username&gt;/cephfs
</code></pre>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2014/02/27/journal-ssd-ceph-deploy/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>


        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2014/01/29/ceph-process-best-practice/">Ceph のプロセス配置ベストプラクティス</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2014-01-29'>
            January 29, 2014</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>Ceph はブロック型の分散ストレージファイルシステムです。POSIX のファイルシステ
ムとしてマウント出来ます。Linux の Kernel ドライバや FUSE ドライバを用いてマウ
ントします。またブロックデバイスとしてマウントする方法もあります。</p>

<p>だいぶ前ですが、Ceph に関する記事を以前下記の通り書きました。</p>

<ul>
<li><a href="http://jedipunkz.github.io/blog/2013/05/25/ceph-cluster-network/">http://jedipunkz.github.io/blog/2013/05/25/ceph-cluster-network/</a></li>
<li><a href="http://jedipunkz.github.io/blog/2013/05/11/ceph-deploy/">http://jedipunkz.github.io/blog/2013/05/11/ceph-deploy/</a></li>
</ul>

<p>Ceph の構築方法について記したブログだったのですが、今まで mon, osd, mds の各プ
ロセスをそれぞれ何台のノードに対して配置し、またそれぞれのプロセス幾つを何に対
して配置するのか？という疑問が付きまとわっていました。node, disk, process のそ
れぞれの数の関係について知りたいなぁと思っていました。幾つかのドキュメントを読
んでいて、ぼんやり見えてきた事があるのでそれを今回はまとめたいと思います。</p>

<p>また、皆さん気になるトコロだと思われる容量設計についても軽く触れたいと思います。</p>

<h2 id="参考資料">参考資料</h2>

<ul>
<li><a href="http://ceph.com/docs/master/rados/configuration/mon-config-ref/">http://ceph.com/docs/master/rados/configuration/mon-config-ref/</a></li>
<li><a href="http://www.sebastien-han.fr/blog/2013/12/02/ceph-performance-interesting-things-going-on/">http://www.sebastien-han.fr/blog/2013/12/02/ceph-performance-interesting-things-going-on/</a></li>
</ul>

<h2 id="各要素の数の関係">各要素の数の関係</h2>

<p>ハードウェア要素である node, disk(hdd), ssd そしてソフトウェア要素である mon,
osd, mds の数の関係はどのようにするべきか？基本となる関係は</p>

<ul>
<li>1 mds process / node</li>
<li>1 mon process / node</li>
<li>1 osd process / disk</li>
<li>n jornal ssd device / disk / node</li>
</ul>

<p>だと考えられます。僕が今のところ理想かなぁと思っている構成をまとめたいと思いま
す。</p>

<p>下記の図がそれです。</p>

<pre><code>+------------------------+
|         client         |
+------------------------+
|
+--------------------------+--------------------------+-------------------------------+-------------------------
|                          |                          |                               |            public network
+------------------------+ +------------------------+ +------------------------+      +------------------------+
|          mon           | |          mon           | |          mon           |      |          mds           |
+------+ +------+ +------+ +------+ +------+ +------+ +------+ +------+ +------+      +------------------------+
| osd  | | osd  | | osd  | | osd  | | osd  | | osd  | | osd  | | osd  | | osd  |      |                        |
+------+ +------+ +------+ +------+ +------+ +------+ +------+ +------+ +------+      |                        |
| disk | | disk | | disk | | disk | | disk | | disk | | disk | | disk | | disk |....&gt; |                        |
+------+ +------+ +------+ +------+ +------+ +------+ +------+ +------+ +------+scale |          node          |
|          ssd           | |          ssd           | |          ssd           |      |                        |
+------------------------+ +------------------------+ +------------------------+      |                        |
|          node          | |          node          | |          node          |      |                        |
+------------------------+ +------------------------+ +------------------------+      +------------------------+
|                          |                          |                               |
+--------------------------+--------------------------+-------------------------------+-------------------------
                                                                                                  cluster network
</code></pre>

<h4 id="mds-と-node-の関係">mds と node の関係</h4>

<p>mds はリモートクライアントへのファイルシステムサービスの提供を行うことや特性が
全く異なることから別ノードに切り出しました。また mds は幾つかのノードで稼働さ
せる事も可能。が、mds はそれぞれのサービスを HA 組む仕組みは持っていないので
どれか一方の mds をクライアントは指し示す必要があり、その mds が停止すれば直
ちに障害に発展します。</p>

<h4 id="mon-と-node-の関係">mon と node の関係</h4>

<p>mon は比較的少量のリソースで稼働します。今回 osd と同じノードの搭載しましたが
別ノードに切り出すことも勿論可能です。mon は CRUSH Maps アルゴリズムの元に連携
が取れますので複数のプロセスを稼働することが推奨されていることからも、比較的少
ないノード数のクラスタの場合は osd と同ノードに搭載するのが容易かなと考えまし
た。</p>

<h4 id="osd-と-node-の関係">osd と node の関係</h4>

<p>1 osd プロセスに対して 1 disk が基本となります。osd は実データのレプリケーショ
ンを行うことからコンフィギュレーションに対して上図の様にクラスタ用のネットワー
クを紐付け、高トラヒックに対応する必要があります。また osd 用の disk device で
すが RAID を組まないことが推奨されています。CEPH 自体に HA の仕組みがあること、
また RAID 構成にもよりますがディスクアクセスが遅くなることは Ceph にとってボト
ルネックを早く招くことになりますし、小さいディスク容量しか扱えなくなることは
Ceph にとって不利になると思います。</p>

<h4 id="journal-用の-ssd-device-と-disk-node-の関係">journal 用の ssd device と disk, node の関係</h4>

<p>現在の Stable Release 版の Ceph は journal を用いてメタデータを管理します。各
osd の disk 単位に journal 用の disk device を指定出来るようになっています。メ
タデータですので実データ用の disk よりだいぶ小さな容量で構わないこと、また比較
的高速なアクセスを要求されることからも SSD を選択することが推奨されつつあるよ
うです。実際にストアされるデータの特性にもよりますが 1 node に対して 1 ssd
device を配置すれば十分かと思います。また osd のプロセスの数 (disk の数) に対
して一つのパーティションを切ることで対応出来るかと思います。</p>

<p>設定方法の例を記します。ここに ceph01-03 の3台のノードがありそれぞれ 2 disk, 1
ssd が搭載されているとします。/dev/ssd は gdisk 等を用いて2つ以上のパーティショ
ンに切り分けます。</p>

<p>下記のように /dev/sdb (hdd) に対して /dev/ssd1 (ssd), /dev/sdc (hdd) に対して /dev/ssd2 (ssd)
を割り当てることが出来ます。</p>

<pre><code class="language-bash">% ceph-deploy --cluster cluster01 osd create ceph01:sdb:/dev/ssd1 ceph02:sdb:/dev/ssd1 ceph03:sdb:/dev/ssd1
% ceph-deploy --cluster cluster01 osd create ceph01:sdc:/dev/ssd2 ceph02:sdc:/dev/ssd2 ceph03:sdc:/dev/ssd2
</code></pre>

<h2 id="ceph-ストレージ全体の容量設計と-mon-の-ratio-の関係">Ceph ストレージ全体の容量設計と mon の ratio の関係</h2>

<p>3TB のディスクを持ったノードが 33 台並んでいるとします。各ノードには osd プロ
セスが1つ稼働します。合計容量は 99 TB となりますが mon が持っているコンフィギュ
レーションである full ratio というパラメータがありデフォルト値が 0.95 となって
います。よってこのクラスタで扱える全体のディスク容量は 95TB となります。</p>

<p>また、ラックに数台のノードを積むのが通常ですが、電源故障等で一気にラック単位で
障害が発生したとします。この場合 Ceph はすべてのデータに関してレプリカを取り復
旧作業を行います。しかしながら停止するノード数によってはストレージ全体の扱える
容量をオーバーすることも懸念されます。これに対応するために先ほど登場した ratio
パラメータを調整することが出来ます。</p>

<pre><code>[global]

mon osd full ratio = .80
mon osd nearfull ratio = .70
</code></pre>

<p>上記の例では full ステートの ratio が 0.80, nearfull ステートの ratio が 0.70
となります。想定の障害ノード数を考慮し ratio パラメータにてその台数分を減算す
れば良いわけです。</p>

<h2 id="まとめ">まとめ</h2>

<p>前述した通り上図は比較的少ないノード数のクラスタを組む場合を想定しています。ノー
ド数が増える場合は mon は mds, osd とも必要とするリソースの特性が異なりますの
で別ノードに切り出すことも考えたほうが良さそうです。2014年の2月には Firefly と
いう新しいリリース版が出ます。ここでのブループリント(設計書)を確認すると&hellip;</p>

<p><a href="http://wiki.ceph.com/Planning/Blueprints/Firefly/osd%3A_new_key%2F%2Fvalue_backend">http://wiki.ceph.com/Planning/Blueprints/Firefly/osd%3A_new_key%2F%2Fvalue_backend</a></p>

<p>journal に変わる新たなメタデータ管理方法として KVS データベースを扱うことも視
野に入っているようです。上記の URL 見る限りでは Facebook がオープンソースにし
た rocksdb や fusionio の nvmkv, seagate の kinetic 等が挙がっています。2月に
期待しましょう！</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2014/01/29/ceph-process-best-practice/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>


        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2014/01/21/17th-openstack-study/">第17回 OpenStack 勉強会で話してきました</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2014-01-21'>
            January 21, 2014</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは。<a href="https://twitter.com/jedipunkz">@jedipunkz</a> です。</p>

<p>昨晩、第17回 OpenStack 勉強会が開催されました</p>

<p><a href="http://connpass.com/event/4545/">http://connpass.com/event/4545/</a></p>

<p>ここで発表をしてきましたぁ！発表タイトルは &ldquo;rcbops/chef-cookbooks&rdquo; です。</p>

<script async class="speakerdeck-embed"
data-id="27a2739063d601314bce6a232911c4f0" data-ratio="1.33333333333333"
src="//speakerdeck.com/assets/embed.js"></script>

<p>何を発表したかと言うと詳しくは上記のスライドを見ていただくとして、簡単に言うと
&ldquo;RackSpace 社のエンジニアが管理している Chef Cookbooks でOpenStack 構成を作ろ
う&rdquo; ってことです。</p>

<p>今日知ったのですがどうも昨晩は初心者向けの勉強会という位置付けだったらしく..少
しだけディープな話題を話してしまったかもしれません！すいません！＞＜</p>

<p>でもとても楽しく発表出来ましたし、逆に質問のコーナーで最新の情報も教えてもらえ
たり！なんと Havana 対応の v4.2.0 以降では Swift の Cookbooks が消えてしまった
とか！&hellip; 皆 Swift 好きくないの？&hellip;; ;</p>

<p>rcbops/chef-cookbooks はずっと追っていますが、ものすごいスピードで開発進んでい
るので、今後ぜひみなさん使ってみて下さいー。</p>

<p>最後に詳しい利用方法を記した僕のブログの URL を貼り付けておきます。</p>

<ul>
<li>OpenStack Havana を Chef でデプロイ</li>
</ul>

<p><a href="http://jedipunkz.github.io/blog/2013/11/17/openstack-havana-chef-deploy/">http://jedipunkz.github.io/blog/2013/11/17/openstack-havana-chef-deploy/</a></p>

<ul>
<li>Swift HA 構成を Chef でデプロイ</li>
</ul>

<p><a href="http://jedipunkz.github.io/blog/2013/07/26/swift-ha-chef-deploy/">http://jedipunkz.github.io/blog/2013/07/26/swift-ha-chef-deploy/</a></p>

<ul>
<li>実用的な Swift 構成を Chef でデプロイ</li>
</ul>

<p><a href="http://jedipunkz.github.io/blog/2013/10/27/swift-chef/">http://jedipunkz.github.io/blog/2013/10/27/swift-chef/</a></p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2014/01/21/17th-openstack-study/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Categories
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
        <li><a href='/categories/report'>report</a></li>
    
</ul>

    </footer>
</article>


        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2013/12/09/chef-autonoumous-cluster/">Chef で自律的クラスタを考える</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2013-12-09'>
            December 9, 2013</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは。<a href="https://twitter.com/jedipunkz">@jedipunkz</a> です。</p>

<p>Serf の登場があったり、ここ最近オーケストレーションについて考える人が増えた気
がします。システムをデプロイしてその後各ノード間の連結だったりも同じ Chef,
Puppet 等のコンフィギュレーションツールで行うのか？全く別のツールで？..</p>

<p>最近 Serf というツールの登場がありました。</p>

<p>僕も Serf を触ってつい先日ブログに書きました。有用なツールだと思います。シ
ンプルだからこそ応用が効きますし、リアルタイム性もあり、将来的に異なるネットワー
クセグメント上のノードとも連結出来るようになりそうですし、とても期待です。</p>

<p>話が少し飛びますが..</p>

<p>いつも Rebuild.fm を楽しく聞いているのですが Immutable Infrastructure の話題の
時にオーケストレーションの話題になって、どうも &lsquo;Chef でも自律的なクラスタを組
むことが認知されていないのでは？&rsquo; と思うようになりました。もちろん Chef でやる
べき！とは言い切りませんし、今後どうなるかわかりません。Opscode の中の人も &lsquo;オー
ケストレーションは自分でやってね&rsquo; というスタンスだったとずいぶん前ですが聞きま
した。Serf を等のオーケストレーションツールを使う使わないの話は今回はしないの
ですが Chef でも自律的クラスタを組むことは出来ますよ〜というのが今回の話題。</p>

<p>まえがきが長くなりました。</p>

<p>今回は Chef で自律的クラスタを構成する方法を記したいと思います。</p>

<p>haproxy 等を利用すれば尚良いと思いますが、よりクラスタを組むのが簡単な nginx
を今回は利用したいと思います。</p>

<p><a href="https://github.com/opscode-cookbooks/nginx">https://github.com/opscode-cookbooks/nginx</a></p>

<h2 id="構成">構成</h2>

<p>&lsquo;web&rsquo; という Role 名と &lsquo;lb&rsquo; という Role 名で単純な HTTP サーバとしての nginx
ノードを複数台と、ロードバランサとしての nginx ノードを1台でクラスタを構成しま
す。また共に environment 名は同じものを利用します。別の environment 名の場合は
別クラスタという区切りです。</p>

<ul>
<li>&lsquo;lb&rsquo; node x 1 + &lsquo;web&rsquo; node x n (&lsquo;foo&rsquo; environment)</li>
<li>&lsquo;lb&rsquo; node x 1 + &lsquo;web&rsquo; node x n (&lsquo;bar&rsquo; environment)</li>
</ul>

<h2 id="lb-nginx-ロードバランサのレシピ">&lsquo;lb&rsquo; nginx ロードバランサのレシピ</h2>

<p>下記が &lsquo;lb&rsquo; Role の recipes/cmomnos_conf.rb の修正した内容です。</p>

<pre><code class="language-ruby">environment = node.chef_environment
webservers = search(:node, &quot;role:web AND chef_environment:#{environment}&quot;)
 
template &quot;#{node['nginx']['dir']}/sites-available/default&quot; do
  source &quot;default-site.erb&quot;
  owner &quot;root&quot;
  group &quot;root&quot;
  mode 00644
  notifies :reload, 'service[nginx]'
    variables ({
      :webservers =&gt; webservers
    })
end
</code></pre>

<p>何をやっているかと言うと、environment という変数に自ノードの environment 名を。
webservers という変数に role 名が &lsquo;web&rsquo; で尚且つ自ノードと同じ environment 名
が付いたノード名を入れています。これで自分と同じ environment に所属している
&lsquo;web&rsquo; Role なノードを Chef サーバに対して検索しています。また、template 内で
webservers という変数をそのまま利用できるように variables で渡しています。</p>

<h2 id="lb-nginx-ロードバランサのテンプレート">&lsquo;lb&rsquo; nginx ロードバランサのテンプレート</h2>

<p>下記が webservers 変数を受け取った後の template 内の処理です。</p>

<pre><code class="language-ruby">&lt;% if @webservers and ( @webservers != [] ) %&gt;
upstream backend {
&lt;% @webservers.each do |hostname| -%&gt;
  server &lt;%= hostname['ipaddr'] -%&gt;;
&lt;% end -%&gt;
}
&lt;% end %&gt;
  
server {
  listen   80;
  server_name  &lt;%= node['hostname'] %&gt;;
    
  access_log  &lt;%= node['nginx']['log_dir'] %&gt;/localhost.access.log;
    
  location / {
    &lt;% if @webservers and ( @webservers != [] ) %&gt;
    proxy_pass http://backend;
    &lt;% else %&gt;
    root   /var/www/nginx-default;
    index  index.html index.htm;
    &lt;% end %&gt;
  }
}
</code></pre>

<p>upstream backend { &hellip; は皆さん見慣れた記述だと思うのですが、バックエンドの
HTTP サーバの IP アドレスを一覧化します。each で回しているので台数分だけ
server <ip_addr>; の記述が入ります。</p>

<p>chef-client をデーモン稼働しておけば、新規に Chef サーバに登録された &lsquo;web&rsquo;
Role の HTTP サーバを自動で &lsquo;lb&rsquo; Role のロードバランサが組み込む、つまり自律的
なクラスタが組めることになります。もちろんこの間の手作業は一切ありません。</p>

<p>ちなみに chef-client をデーモン稼働するには</p>

<pre><code>recipe[chef-client::service]
</code></pre>

<p>というレシピをノードに割り当てることで可能です。</p>

<h2 id="まとめ">まとめ</h2>

<p>Chef でも自律的なクラスタが組めました。もちろん chef-client の稼働間隔があるの
でリアルタイム性はありません。chef-client の稼働間隔は &lsquo;chef-client&rsquo; レシピの
attributes で調整出来ます。その点は serf のほうが確実に勝っていると見るべきで
しょう。冒頭に記したようにこの辺りの操作を Chef で行うのか別のツールを使うのか
はまだまだ模索が必要そう。ただ、私がいつも使っている &lsquo;OpenStack を Chef で構成
する Cookbooks&rsquo; 等は複数台構成を Chef で構成しています。なので僕にとってはこの
辺りの話は当たり前だと思っていたのだけど、どうも勉強会に出たりすると &ldquo;Chef は
複数台構成を作るのが苦手だ&rdquo; って話があがってくるので気になっていました。</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2013/12/09/chef-autonoumous-cluster/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>


        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2013/12/09/coreos-etcd-cluster/">CoreOS etcd のクラスタとその応用性</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2013-12-09'>
            December 9, 2013</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは。<a href="https://twitter.com/jedipunkz">@jedipunkz</a> です。</p>

<p>皆さん CoreOS は利用されたことありますか？CoreOS は軽量な docker との相性の良
い OS です。下記が公式サイト。</p>

<p><a href="http://coreos.com/">http://coreos.com/</a></p>

<p>特徴としては下記の3つがあります。</p>

<ul>
<li>etcd</li>
<li>systemd</li>
<li>docker</li>
</ul>

<p>ここではこの中の etcd について注目していきたいと思います。etcd はクラスタエイ
ブルな KVS データベースです。コンフィギュレーションをクラスタ間で共有すること
がなので、オーケストレーションの分野でも期待出来るのでは？と個人的に感じていま
す。今回は etcd のクラスタ構成構築の手順とその基本動作の確認、またどう応用出来
るのか？について記していきたいと思います。</p>

<h2 id="参考-url">参考 URL</h2>

<ul>
<li><a href="http://coreos.com/using-coreos/etcd/">http://coreos.com/using-coreos/etcd/</a></li>
<li><a href="https://github.com/coreos/etcd">https://github.com/coreos/etcd</a></li>
</ul>

<h2 id="ビルド">ビルド</h2>

<p>go 1.1 or later をインストールして etcd のコンパイル準備を行います。Ubuntu
Saucy のパッケージを用いると容易に行えます。</p>

<pre><code class="language-bash">% apt-get -y install golang
</code></pre>

<p>coreos/etcd を取得しビルド</p>

<pre><code class="language-bash">% git clone https://github.com/coreos/etcd
% cd coreos
% ./build
% ./etcd --version
v0.2.0-rc1-60-g73f04d5
</code></pre>

<h2 id="coreos-の用意">CoreOS の用意</h2>

<p>ここではたまたま手元にあった OpenStack を用いて CoreOS のイメージを登録してい
みます。ベアメタルでも可能ですのでその場合は手順を読み替えて作業してみてくださ
い。OpenStack 等クラウドプラットフォームを利用する場合は metadata サービスが必
須となるので注意してください。</p>

<pre><code class="language-bash">% wget http://storage.core-os.net/coreos/amd64-generic/dev-channel/coreos_production_openstack_image.img.bz2
% bunzip2 coreos_production_openstack_image.img.bz2
% glance image-create --name coreos-image --container-format ovf \
  --disk-format qcow2 --file coreos_production_openstack_image.img
</code></pre>

<p>nova boot にて CoreOS を起動します。(下記は例)</p>

<pre><code class="language-bash">% nova keypair-add testkey01 &gt; testkey01.pem
% nova boot --nic net-id .... --image coreos-image --flavor 1 --key_name testkey01 coreos01
</code></pre>

<h2 id="coreos-上での-etcd-クラスタ起動">CoreOS 上での etcd クラスタ起動</h2>

<p>上記でコンパイルした etcd のバイナリを起動したインスタンス (CoreOS) に転送しま
す。scp 等で転送してください。</p>

<p>ここでは 1 node 上で複数のポート番号を用いて 3 つの etcd を稼働することでクラ
スタを構築します。</p>

<p>7002 番ポートを peer addr として master を起動。listen ポートは 4002</p>

<pre><code class="language-bash">% ./etcd -peer-addr 127.0.0.1:7002 -addr 127.0.0.1:4002 -data-dir machines/machine1 -name machine1
</code></pre>

<p>上記の master を参照する slaves (残り2台) を起動。</p>

<pre><code class="language-bash">% ./etcd -peer-addr 127.0.0.1:7003 -addr 127.0.0.1:4003 -peers 127.0.0.1:7002 -data-dir machines/machine2 -name machine2
% ./etcd -peer-addr 127.0.0.1:7004 -addr 127.0.0.1:4004 -peers 127.0.0.1:7002 -data-dir machines/machine3 -name machine3
</code></pre>

<p>クラスタ構成内のノード情報を確認する。</p>

<pre><code class="language-bash">% curl -L http://127.0.0.1:4002/v2/machines
[etcd] Dec  4 03:46:44.153 INFO      | URLs: machine1 / machine1 (http://127.0.0.1:4002,http://127.0.0.1:4003,http://127.0.0.1:4004)
http://127.0.0.1:4002, http://127.0.0.1:4003, http://127.0.0.1:4004
</code></pre>

<p>leader (master) 情報を確認する。</p>

<pre><code class="language-bash">% curl -L http://127.0.0.1:4002/v2/leader
http://127.0.0.1:7002
</code></pre>

<p>上記で起動した master プロセスが leader (master) になっていることを確認出来る
と思います。</p>

<h2 id="キーの投入と参照">キーの投入と参照</h2>

<p>テストでキーと値を入力してみましょう。&rsquo;foo&rsquo; キーに &lsquo;bar&rsquo; という値を投入てくだ
さい。</p>

<pre><code class="language-bash">% curl -L http://127.0.0.1:4002/v2/keys/foo -XPUT -d value=bar
</code></pre>

<p>クラスタ内全てのプロセスから上記のキーの取得できることを確認します。</p>

<pre><code class="language-bash">% curl -L http://127.0.0.1:4002/v2/keys/foo
{&quot;action&quot;:&quot;get&quot;,&quot;node&quot;:{&quot;key&quot;:&quot;/foo&quot;,&quot;value&quot;:&quot;bar&quot;,&quot;modifiedIndex&quot;:4,&quot;createdIndex&quot;:4}}
% curl -L http://127.0.0.1:4003/v2/keys/foo
{&quot;action&quot;:&quot;get&quot;,&quot;node&quot;:{&quot;key&quot;:&quot;/foo&quot;,&quot;value&quot;:&quot;bar&quot;,&quot;modifiedIndex&quot;:4,&quot;createdIndex&quot;:4}}
% curl -L http://127.0.0.1:4004/v2/keys/foo
{&quot;action&quot;:&quot;get&quot;,&quot;node&quot;:{&quot;key&quot;:&quot;/foo&quot;,&quot;value&quot;:&quot;bar&quot;,&quot;modifiedIndex&quot;:4,&quot;createdIndex&quot;:4}}
</code></pre>

<h2 id="master-のシャットダウンと-master-選挙後の動作確認">master のシャットダウンと master 選挙後の動作確認</h2>

<p>テストで master のプロセスをシャットダウンしてみます。</p>

<p>master プロセスのシャットダウン</p>

<pre><code class="language-bash">% kill &lt;master プロセスの ID&gt;
</code></pre>

<p>その他 2 つのプロセスから &lsquo;foo&rsquo; キーの確認を行う。</p>

<pre><code class="language-bash">% curl -L http://127.0.0.1:4004/v2/keys/foo
{&quot;action&quot;:&quot;get&quot;,&quot;node&quot;:{&quot;key&quot;:&quot;/foo&quot;,&quot;value&quot;:&quot;bar&quot;,&quot;modifiedIndex&quot;:4,&quot;createdIndex&quot;:4}}
% curl -L http://127.0.0.1:4003/v2/keys/foo
{&quot;action&quot;:&quot;get&quot;,&quot;node&quot;:{&quot;key&quot;:&quot;/foo&quot;,&quot;value&quot;:&quot;bar&quot;,&quot;modifiedIndex&quot;:4,&quot;createdIndex&quot;:4}}
</code></pre>

<p>勿論、旧 master からは確認出来ない。</p>

<pre><code class="language-bash">% curl -L http://127.0.0.1:4002/v2/keys/foo
 curl: (7) Failed connect to 127.0.0.1:4002; Connection refused
</code></pre>

<p>新 master の確認を行う。選挙の結果 3 つ目のプロセスが master に昇格しているこ
とが確認出来る。</p>

<pre><code class="language-bash">% curl -L http://127.0.0.1:4003/v2/leader
http://127.0.0.1:7004
</code></pre>

<h2 id="考察とその応用性について">考察とその応用性について</h2>

<p>とてもシンプルな KVS ではあるけど大きな可能性を秘めていると思っています。オー
ケストレーション等への応用です。お互いのノード (今回はプロセス) 間で情報をやり
とりできるので自律的なクラスタの構築も可能になるのでは？と思っています。</p>

<p>&lsquo;etcenv&rsquo; という @mattn さんが開発したツールを見てみましょう。</p>

<p><a href="https://github.com/mattn/etcdenv">https://github.com/mattn/etcdenv</a></p>

<p>下記、README から引用。</p>

<pre><code class="language-bash">$ curl http://127.0.0.1:4001/v1/keys/app/db -d value=&quot;newdb&quot;
$ curl http://127.0.0.1:4001/v1/keys/app/cache -d value=&quot;new cache&quot;

$ curl http://localhost:4001/v1/keys/app
[{&quot;action&quot;:&quot;GET&quot;,&quot;key&quot;:&quot;/app/db&quot;,&quot;value&quot;:&quot;newdb&quot;,&quot;index&quot;:4},{&quot;action&quot;:&quot;GET&quot;,&quot;key&quot;:&quot;/app/cache&quot;,&quot;value&quot;:&quot;new cache&quot;,&quot;index&quot;:4}]

$ etcdenv -key=/app/
DB=newdb
CACHE=new cache

$ etcdenv -key=/app/ ruby web.rb
</code></pre>

<p>クラスタ間の情報を環境変数に落としこむツールです。自ノードの環境変数まで落ちれ
ば、クラスタ構築も色々想像出来るのではないでしょうか？</p>

<p>軽量で docker との相性も良くて etcd 等の仕組みも持っている CoreOS にはこれから
も期待です。</p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2013/12/09/coreos-etcd-cluster/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>


        
            <article class="post">
    <header>
    <div class="title">
        
            <h2><a href="https://jedipunkz.github.io/blog/2013/12/05/ironic-openstack-beremetal/">Ironic でベアメタル OpenStack ！..の一歩手前</a></h2>
        
        
    </div>
    <div class="meta">
        
            
        

        <time class="published"
            datetime='2013-12-05'>
            December 5, 2013</time>
        <span class="author"></span>
        
        
    </div>
</header>

    

    
    <p><p>こんにちは。<a href="https://twitter.com/jedipunkz">@jedipunkz</a> です。</p>

<p>アドベントカレンダーの季節がやって参りました。</p>

<p>Ironic を使って OpenStack でベアメタルサーバを扱いたい！ということで色々とやっ
ている最中 (今週から始めました..) なのですが、まだまだ incubator プロジェクト
ということもあって実装が追い付いていなかったりドキュメントも揃っていなかったり
とシンドい状況ｗ ここ2日程で集めた情報を整理するためにも 2013年 OpenStack アド
ベントカレンダーに参加させてもらいますー。</p>

<h2 id="参考資料のまとめ">参考資料のまとめ</h2>

<p>まずは公式 wiki ページ。逆に言うとここに記されている以上の情報は無いんじゃ？あ
とはコード読め！の世界かも..。</p>

<p><a href="https://wiki.openstack.org/wiki/Ironic">https://wiki.openstack.org/wiki/Ironic</a></p>

<p>devtest_undercloud です。上の資料の中でも手順の中で度々こちらにジャンプしている。
同じく incubator プロジェクトの TrippleO のデベロッパ用ドキュメントになっている。
上記の公式 wiki の情報を合わせ読むことで Ironic を使ったデプロイの手順に仕上がります。</p>

<p><a href="http://docs.openstack.org/developer/tripleo-incubator/devtest_undercloud.html">http://docs.openstack.org/developer/tripleo-incubator/devtest_undercloud.html</a></p>

<p>ソースコードとドキュメント。あとでドキュメント作成方法を記しますが、こちらを取
得して作成します。</p>

<p><a href="https://github.com/openstack/ironic">https://github.com/openstack/ironic</a></p>

<p>ドキュメントサイト。まだ情報が揃っていません。よって上の github から取得したモ
ノからドキュメントを作る方法を後で書きます。</p>

<p><a href="http://docs.openstack.org/developer/ironic/">http://docs.openstack.org/developer/ironic/</a></p>

<p>launchpad サイト。全てのバグ情報やブループリント等が閲覧出来ます。まだ絶賛開発
中なので読む必要があると思います。</p>

<p><a href="https://launchpad.net/ironic">https://launchpad.net/ironic</a></p>

<p>ドキュメントを作る
+++</p>

<p>公式 ドキュメントサイトは一応、上記の通りあるのですが、ドキュメントも絶賛執筆
中ということで所々抜けがあります。また公式ドキュメントサイトがどのスパンで更新
されているか分からないので、いち早く情報をゲットしたい場合ドキュメントを作る必
要があると思います。ということで、その作り方を記していきます。尚、公式 wiki サ
イトにも手順が載っていますが Vagrant と Apache を用いた方法になっているので、
普通に Ubuntu サーバが手元にある環境を想定して読み替えながら説明していきます。</p>

<p>必要なパッケージのインストールを行います。</p>

<pre><code class="language-bash">% sudo apt-get update
% sudo apt-get install -y git python-dev swig libssl-dev python-pip \
  libmysqlclient-dev libxml2-dev libxslt-dev libxslt1-dev python-mysqldb \
  libpq-dev
% sudo pip install virtualenv setuptools-git flake8 tox
% sudo easy_install nose
</code></pre>

<p>ソースコード・ドキュメントを取得します。</p>

<pre><code class="language-bash">% git clone git://github.com/openstack/ironic.git
</code></pre>

<p>Sphinx で構成されているのでビルドします。</p>

<pre><code class="language-bash">% cd ironic
% tox -evenv -- echo 'done'
% source .tox/venv/bin/activate
&gt; python setup.ph build_sphinx
&gt; deactivate
</code></pre>

<p>ironic/doc/build/html ディレクトリ配下に HTML のドキュメントが生成されたはずで
す。これを手元の端末に持ってきて開けばブラウザで最新のドキュメントが閲覧出来ま
す。</p>

<h2 id="ironic-を有効にした-devstack-による構築">ironic を有効にした devstack による構築</h2>

<p>devstack を使って ironic を機能させていきます。私は下記の localrc を用いて
ironic の試験をしていました。またブランチは &lsquo;master&rsquo; を使います。</p>

<pre><code class="language-bash">% git clone https://github.com/openstack-dev/devstack.git
% cd devstack
% ${EDITOR} localrc # 下記の通り

HOST_IP=&lt;your_machine_ip_addr&gt;

LOGFILE=stack.sh.log

ADMIN_PASSWORD=nomoresecrete
MYSQL_PASSWORD=$ADMIN_PASSWORD
RABBIT_PASSWORD=$ADMIN_PASSWORD
SERVICE_PASSWORD=$ADMIN_PASSWORD
SERVICE_TOKEN=admintoken

disable_service n-obj

# ironic
enable_service ir-api
enable_service ir-cond

# use neutron
disable_service n-net
enable_service q-svc
enable_service q-agt
enable_service q-dhcp
enable_service q-l3
enable_service q-meta
enable_service q-lbaas
enable_service neutron

ENABLE_TENANT_TUNNELS=True

# heat
ENABLED_SERVICES+=,heat,h-api,h-api-cfn,h-api-cw,h-eng
## It would also be useful to automatically download and register VM images that Heat can launch.
# 64bit image (~660MB)
IMAGE_URLS+=&quot;,http://fedorapeople.org/groups/heat/prebuilt-jeos-images/F17-x86_64-cfntools.qcow2&quot;
# 32bit image (~640MB)
IMAGE_URLS+=&quot;,http://fedorapeople.org/groups/heat/prebuilt-jeos-images/F17-i386-cfntools.qcow2&quot;

# syslog
SYSLOG=True
SYSLOG_HOST=$HOST_IP
SYSLOG_PORT=514
</code></pre>

<p>stack.sh を実行します。</p>

<pre><code class="language-bash">% ./stack.sh
</code></pre>

<p>Ironic を有効にした OpenStack 環境が出来上がったはずです。</p>

<h2 id="diskimage-builder-を使ったイメージ作成">diskimage-builder を使ったイメージ作成</h2>

<p>ベアメタルサーバをデプロイするためのイメージを作成します。元々は TrippleO のプ
ロジェクト内に存在していましたが、現在は git レポジトリが別れています。</p>

<p>先ほど devstack を導入したホストでイメージを作ります。作成には結構時間が掛かります。</p>

<pre><code class="language-bash">% cd ~     
% git clone https://github.com/openstack/diskimage-builder.git
% git clone https://github.com/openstack/tripleo-incubator.git
% git clone https://github.com/openstack/tripleo-image-elements.git
% git clone https://github.com/openstack/tripleo-heat-templates.git
% export UNDERCLOUD_DIB_EXTRA_ARGS='ironic-api ironic-conductor'
% export ELEMENTS_PATH=/home/thirai/tripleo-image-elements/elements
% ./diskimage-builder/bin/disk-image-create -a amd64 -o ~/undercloud boot-stack \
  nova-baremetal os-collect-config stackuser dhcp-all-interfaces \
  neutron-dhcp-agent ${UNDERCLOUD_DIB_EXTRA_ARGS:-} ubuntu 2&gt;&amp;1 | tee /tmp/undercloud.log 
</code></pre>

<p>イメージが ~/undercloud.qcow2 が生成されたはずです。作成したイメージを Glance に登録します。</p>

<pre><code class="language-bash">% ~/tripleo-incubator/scripts/load-image undercloud.qcow2
</code></pre>

<p>undercloud.yaml と ironic.yaml をマージします。</p>

<pre><code class="language-bash">% cd ~/tripleo-heat-templates
% make undercloud-vm-ironic.yaml
</code></pre>

<p>パスワードの生成と環境変数への読み込みを行います。</p>

<pre><code class="language-bash">% cd ~/tripleo-incubator/scripts/
% export PATH=$PATH:.
% ./setup-undercloud-passwords
% source tripleo-undercloud-passwords
</code></pre>

<p>UNDERCLOUD_IRONIC_PASSWORD 環境変数にも読み込みます。</p>

<pre><code class="language-bash">% export UNDERCLOUD_IRONIC_PASSWORD=$(~/tripleo-incubator/scripts/os-make-password)
</code></pre>

<p>さて、イメージを利用したベアメタルへの稼働ですが、&hellip;</p>

<pre><code class="language-bash">if [ &quot;$DHCP_DRIVER&quot; = &quot;bm-dnsmasq&quot; ]; then
    UNDERCLOUD_NATIVE_PXE=&quot;&quot;
else
    UNDERCLOUD_NATIVE_PXE=&quot;;NeutronNativePXE=True&quot;
fi

heat stack-create -f ./tripleo-heat-templates/undercloud-vm-ironic.yaml \
-P &quot;PowerUserName=$(whoami);\
AdminToken=${UNDERCLOUD_ADMIN_TOKEN};\
AdminPassword=${UNDERCLOUD_ADMIN_PASSWORD};\
GlancePassword=${UNDERCLOUD_GLANCE_PASSWORD};\
HeatPassword=${UNDERCLOUD_HEAT_PASSWORD};\
NeutronPassword=${UNDERCLOUD_NEUTRON_PASSWORD};\
NovaPassword=${UNDERCLOUD_NOVA_PASSWORD};\
BaremetalArch=${NODE_ARCH}$UNDERCLOUD_NATIVE_PXE&quot; \
IronicPassword=${UNDERCLOUD_IRONIC_PASSWORD}&quot; \
undercloud
</code></pre>

<p>コケました。&hellip;エラーは下記の通り。</p>

<pre><code>TRACE heat.engine.resource Error: Creation of server teststack01-WikiDatabase-5nyiqluilnxn failed: No valid host was found. Exceeded max
scheduling attempts 3 for instance 733b69df-2b54-44ae-9d61-de766746f21a (500)#0122013-12-03 16:04:14.513 10022 TRACE heat.engine.resource
</code></pre>

<p>No Valid Host とな&hellip;。うーん。確かに IPMI を積んだベアメタルマシンの情報って
どこにも記していないんだよなぁ。しかも heat のテンプレート
(underclound-vm-ironic.yaml) 見てもよく理解していない自分がいる&hellip;(´・ω・`)</p>

<p>というか手順にはその周りのこと何も書いていないのでぇすがぁ！&hellip;</p>

<p>ということで、まだまだコントリビュートするチャンス満載の状態なので、よかったら
皆さん参加されませんか !?!?</p>

<p><a href="https://wiki.openstack.org/wiki/HowToContribute">https://wiki.openstack.org/wiki/HowToContribute</a></p>
</p>

    <footer>
        <ul class="actions">
            <li><a href="https://jedipunkz.github.io/blog/2013/12/05/ironic-openstack-beremetal/" class="button big">Continue Reading</a></li>
        </ul>
        <ul class="stats">
    
        

        
        
            <li>
                
                
                    

                    
                        Category
                    
                
            </li>
        
    

    
    
        <li><a href='/categories/infrastructure'>infrastructure</a></li>
    
</ul>

    </footer>
</article>


        

        
<ul class="actions pagination">
    
        <li><a href="/categories/infrastructure/page/3/"
                class="button big previous">Previous Page</a></li>
    

    
        <li><a href="/categories/infrastructure/page/5/"
                class="button big next">Next Page</a></li>
    
</ul>

    </div>
    
<section id="sidebar">

    
        <section id="intro">
            
            
            
            <ul class="icons">
                
                    <li><a href="https://jedipunkz.github.io/categories/infrastructure/index.xml" type="application/rss+xml"
                        target="_blank" title="RSS" class="fa fa-rss"></a></li>
                
                
            </ul>
        </section>

    
        <section id="recent-posts">
            <ul class="posts">
                <header>
                    <h3>Recent Posts</h3>
                </header>
                
                    
                

                
                    
                

                
                    <li>
                        <article>
                            <header>
                                <h3><a href="https://jedipunkz.github.io/blog/2018/12/31/istio/">Istio, Helm を使って Getting Started 的なアプリをデプロイ</a></h3>
                                
                                    
                                
                                <time class="published" datetime=
                                    '2018-12-31'>
                                    December 31, 2018</time>
                            </header>
                        </article>
                    </li>
                
                    <li>
                        <article>
                            <header>
                                <h3><a href="https://jedipunkz.github.io/blog/2017/07/02/test-kitchen-cluster/">Docker,Test-Kitchen,Ansible でクラスタを構成する</a></h3>
                                
                                    
                                
                                <time class="published" datetime=
                                    '2017-07-02'>
                                    July 2, 2017</time>
                            </header>
                        </article>
                    </li>
                
                    <li>
                        <article>
                            <header>
                                <h3><a href="https://jedipunkz.github.io/blog/2017/04/13/gke-lb/">GCP ロードバランサと GKE クラスタを Terraform を使って構築する</a></h3>
                                
                                    
                                
                                <time class="published" datetime=
                                    '2017-04-13'>
                                    April 13, 2017</time>
                            </header>
                        </article>
                    </li>
                
                    <li>
                        <article>
                            <header>
                                <h3><a href="https://jedipunkz.github.io/blog/2017/02/12/serverless-fission/">Serverless on Kubernetes : Fission を使ってみた</a></h3>
                                
                                    
                                
                                <time class="published" datetime=
                                    '2017-02-12'>
                                    February 12, 2017</time>
                            </header>
                        </article>
                    </li>
                
                    <li>
                        <article>
                            <header>
                                <h3><a href="https://jedipunkz.github.io/blog/2017/01/13/kubernetes-deployments/">Kubernetes Deployments を使ってみた！</a></h3>
                                
                                    
                                
                                <time class="published" datetime=
                                    '2017-01-13'>
                                    January 13, 2017</time>
                            </header>
                        </article>
                    </li>
                

                
                    <li>
                        <ul class="actions">
                            <li><a href=
                            
                                "/post/"
                            
                            class="button">View more posts</a></li>
                        </ul>
                    </li>
                
            </ul>
        </section>

    
    
    
    
        <section id="categories">
            <ul class="posts">
                <header>
                    <h3><a href="/categories/">Categories</a></h3>
                </header>

                
                    
                

                
                    <li>
                        <article>
                            <header>
                                <a href="/categories/infrastructure/">infrastructure</a>
                                <span style="float:right;">110</span>
                            </header>
                        </article>
                    </li>
                
                    <li>
                        <article>
                            <header>
                                <a href="/categories/report/">report</a>
                                <span style="float:right;">9</span>
                            </header>
                        </article>
                    </li>
                
                    <li>
                        <article>
                            <header>
                                <a href="/categories/tools/">tools</a>
                                <span style="float:right;">11</span>
                            </header>
                        </article>
                    </li>
                
            </ul>
        </section>
    

    
        

    
        <section id="footer">
            <ul class="icons">
                
                    <li><a href="https://jedipunkz.github.io/categories/infrastructure/index.xml" type="application/rss+xml"
                        target="_blank" title="RSS" class="fa fa-rss"></a></li>
                
                
            </ul>

            <p class="copyright">&copy; ジェダイさんのブログ. テーマデザインは <a href="//github.com/jpescador" target="_blank">Julio Pescador</a>さんによるものです。 </p>
        </section>

</section>

            </div>
        <a id="back-to-top" href="#" class="fa fa-arrow-up fa-border fa-2x"></a>
        

        
        
            
        

        
        
            <script src="/js/jquery.min.js"></script>
            <script src="/js/skel.min.js"></script>
            <script src="/js/util.js"></script>
            <script src="/js/main.js"></script>
            <script src="/js/backToTop.js"></script>
            <script src="/js/highlight.pack.js"></script>
        

        

            
            <script>hljs.initHighlightingOnLoad();</script>
            
    </body>
</html>

