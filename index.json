[{"content":"こんにちは。jedipunkz🚀 です。\nTerraform を運用しているとたまに Plan 結果がどうしても出てしまう事があります。また Terraform を GitHub で実行する環境を運用していると Plan 結果をうまく扱って自動化したいモチベーションも湧いてきます。この場合に Plan の差分をうまく処理してくれる GitHub Action があればなと思って作ってみました。\nGitHub Actions 作成した GitHub Action は下記のレポジトリで公開しています。\nhttps://github.com/jedipunkz/tf-plan-parser\n入力・オプション設定 この GitHub Action では2つの入力オプションが利用可能です：\nterraform-plan (必須) パース対象となる Terraform Plan の出力結果を指定します。通常は前のステップで実行した terraform plan コマンドの標準出力を渡します。\nignore-resources (オプション) 無視したいリソースタイプや特定のリソースを配列形式で指定します。デフォルトは空の配列 [] です。\n指定方法の例：\nリソースタイプ全体を無視:\nignore-resources: \u0026#39;[\u0026#34;null_resource\u0026#34;, \u0026#34;local_file\u0026#34;]\u0026#39; 特定のリソースインスタンスを無視:\nignore-resources: \u0026#39;[\u0026#34;null_resource.temporary\u0026#34;, \u0026#34;local_file.cache\u0026#34;]\u0026#39; リソースタイプとインスタンスの混在:\nignore-resources: \u0026#39;[\u0026#34;null_resource\u0026#34;, \u0026#34;aws_s3_bucket.temp\u0026#34;, \u0026#34;local_file\u0026#34;]\u0026#39; 出力される情報 この Action は以下の出力を提供します。また下記は ignore-resources オプションの指定に沿って結果を出力してくれます。\ndiff-bool: 変更があるかどうかの真偽値（true または false） diff-count: 変更されるリソースの数 diff-resources: 変更されるリソースのアドレス一覧（カンマ区切り） diff-raw: 生の差分データ これらの出力を使って、後続のステップで条件分岐や通知の制御が可能です。\n使い方の例 基本的な使い方 - name: Terraform Plan id: plan run: terraform plan -no-color - name: Parse Plan id: parse uses: jedipunkz/tf-plan-parser@v1 with: terraform-plan: ${{ steps.plan.outputs.stdout }} ignore-resources: \u0026#39;[\u0026#34;null_resource\u0026#34;, \u0026#34;random_id\u0026#34;]\u0026#39; - name: Check results run: | echo \u0026#34;Changes detected: ${{ steps.parse.outputs.diff-bool }}\u0026#34; echo \u0026#34;Resource count: ${{ steps.parse.outputs.diff-count }}\u0026#34; Pull Request への自動コメント - name: Terraform Plan id: plan run: terraform plan -out=tfplan -no-color - name: Parse Plan id: parse uses: jedipunkz/tf-plan-parser@v1 with: terraform-plan: ${{ steps.plan.outputs.stdout }} ignore-resources: \u0026#39;[\u0026#34;null_resource\u0026#34;, \u0026#34;local_file\u0026#34;]\u0026#39; - name: Comment on PR uses: actions/github-script@v7 with: script: | const diffBool = \u0026#39;${{ steps.parse.outputs.diff-bool }}\u0026#39;; const diffCount = \u0026#39;${{ steps.parse.outputs.diff-count }}\u0026#39;; const resources = JSON.parse(\u0026#39;${{ steps.parse.outputs.diff-resources }}\u0026#39;); const diffRaw = `${{ steps.parse.outputs.diff-raw }}`; let body = \u0026#39;## Terraform Plan Analysis\\n\\n\u0026#39;; // Summary if (diffBool === \u0026#39;true\u0026#39;) { body += `✅ **Changes detected** affecting ${diffCount} resources:\\n\\n`; body += \u0026#39;### Changed Resources\\n```\\n\u0026#39;; for (const resource of resources) { body += `${resource}\\n`; } body += \u0026#39;```\\n\\n\u0026#39;; } else { body += \u0026#39;✅ **No changes detected**\\n\\n\u0026#39;; } // All outputs body += \u0026#39;### Outputs\\n\u0026#39;; body += `- **diff-bool**: \\`${diffBool}\\`\\n`; body += `- **diff-count**: \\`${diffCount}\\`\\n`; body += `- **diff-resources**: \\`${JSON.stringify(resources)}\\`\\n\\n`; // Raw plan if (diffRaw \u0026amp;\u0026amp; diffRaw.trim()) { body += \u0026#39;### Raw Terraform Plan Output\\n\u0026#39;; body += \u0026#39;```\\n\u0026#39; + diffRaw + \u0026#39;\\n```\\n\\n\u0026#39;; } body += \u0026#39;---\\n*Generated by Terraform Plan Parser*\u0026#39;; github.rest.issues.createComment({ issue_number: context.issue.number, owner: context.repo.owner, repo: context.repo.repo, body: body }) 条件分岐でのデプロイ制御 - name: Terraform Plan id: plan run: terraform plan -out=tfplan -no-color - name: Parse Plan id: parse uses: jedipunkz/tf-plan-parser@v1 with: terraform-plan: ${{ steps.plan.outputs.stdout }} ignore-resources: \u0026#39;[\u0026#34;null_resource\u0026#34;, \u0026#34;random_id\u0026#34;, \u0026#34;local_file\u0026#34;]\u0026#39; - name: Skip deployment if no changes if: steps.parseoutputs.diff-bool == \u0026#39;false\u0026#39; run: echo \u0026#34;No infrastructure changes detected. Skipping deployment.\u0026#34; - name: Proceed with deployment if: steps.parseoutputs.diff-bool == \u0026#39;true\u0026#39; run: | echo \u0026#34;Infrastructure changes detected: ${{ steps.parseoutputs.diff-count }} resources\u0026#34; terraform apply -auto-approve tfplan - name: Notify on major changes if: steps.parseoutputs.diff-count \u0026gt; 10 run: | echo \u0026#34;⚠️ Large number of changes detected (${{ steps.parseoutputs.diff-count }} resources)\u0026#34; echo \u0026#34;Manual review recommended\u0026#34; まとめ・今後の機能追加について 今回作成したツールにより、Terraform Plan の差分解析が効率化されるかもしれないと思っています。特に下記の点で効果を期待しています。\n意図しない差分と重要な差分の区別が指定出来るようになった Plan 結果差分に反応して通知する仕組み等に応用が効く チーム全体で差分の判断基準を統一指定出来る 今後の機能追加として、以下を検討しています：\n差分の変更の種類（create/update/delete）ごとの分類 入力・出力の拡張 ぜひ皆さんの Terraform 運用でも活用してみてください！\n","permalink":"https://jedipunkz.github.io/post/tf-plan-analyzer/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://x.com/jedipunkz\"\u003ejedipunkz🚀\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003eTerraform を運用しているとたまに Plan 結果がどうしても出てしまう事があります。また Terraform を GitHub で実行する環境を運用していると Plan 結果をうまく扱って自動化したいモチベーションも湧いてきます。この場合に Plan の差分をうまく処理してくれる GitHub Action があればなと思って作ってみました。\u003c/p\u003e\n\u003ch2 id=\"github-actions\"\u003eGitHub Actions\u003c/h2\u003e\n\u003cp\u003e作成した GitHub Action は下記のレポジトリで公開しています。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/jedipunkz/tf-plan-parser\"\u003ehttps://github.com/jedipunkz/tf-plan-parser\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"入力オプション設定\"\u003e入力・オプション設定\u003c/h2\u003e\n\u003cp\u003eこの GitHub Action では2つの入力オプションが利用可能です：\u003c/p\u003e\n\u003ch3 id=\"terraform-plan-必須\"\u003e\u003ccode\u003eterraform-plan\u003c/code\u003e (必須)\u003c/h3\u003e\n\u003cp\u003eパース対象となる Terraform Plan の出力結果を指定します。通常は前のステップで実行した \u003ccode\u003eterraform plan\u003c/code\u003e コマンドの標準出力を渡します。\u003c/p\u003e\n\u003ch3 id=\"ignore-resources-オプション\"\u003e\u003ccode\u003eignore-resources\u003c/code\u003e (オプション)\u003c/h3\u003e\n\u003cp\u003e無視したいリソースタイプや特定のリソースを配列形式で指定します。デフォルトは空の配列 \u003ccode\u003e[]\u003c/code\u003e です。\u003c/p\u003e\n\u003cp\u003e指定方法の例：\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eリソースタイプ全体を無視:\u003c/strong\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-yaml\" data-lang=\"yaml\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eignore-resources\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;[\u0026#34;null_resource\u0026#34;, \u0026#34;local_file\u0026#34;]\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cstrong\u003e特定のリソースインスタンスを無視:\u003c/strong\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-yaml\" data-lang=\"yaml\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eignore-resources\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;[\u0026#34;null_resource.temporary\u0026#34;, \u0026#34;local_file.cache\u0026#34;]\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cstrong\u003eリソースタイプとインスタンスの混在:\u003c/strong\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-yaml\" data-lang=\"yaml\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eignore-resources\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;[\u0026#34;null_resource\u0026#34;, \u0026#34;aws_s3_bucket.temp\u0026#34;, \u0026#34;local_file\u0026#34;]\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"出力される情報\"\u003e出力される情報\u003c/h2\u003e\n\u003cp\u003eこの Action は以下の出力を提供します。また下記は ignore-resources オプションの指定に沿って結果を出力してくれます。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e\u003ccode\u003ediff-bool\u003c/code\u003e\u003c/strong\u003e: 変更があるかどうかの真偽値（\u003ccode\u003etrue\u003c/code\u003e または \u003ccode\u003efalse\u003c/code\u003e）\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e\u003ccode\u003ediff-count\u003c/code\u003e\u003c/strong\u003e: 変更されるリソースの数\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e\u003ccode\u003ediff-resources\u003c/code\u003e\u003c/strong\u003e: 変更されるリソースのアドレス一覧（カンマ区切り）\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e\u003ccode\u003ediff-raw\u003c/code\u003e\u003c/strong\u003e: 生の差分データ\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eこれらの出力を使って、後続のステップで条件分岐や通知の制御が可能です。\u003c/p\u003e","title":"Terraform Plan 差分をパースする GitHub Actions を作った"},{"content":"\nこんにちは @jedipunkz 🚀 です。\n元インフラエンジニア・クラウドエンジニアで最近は AWS, GCP を扱う SRE として働いています。ソフトウェアでインフラの課題を解決すべく勉強していきます。学んだことをこのブログに記せたらいいなと思っています。\n小学生の頃からプログラミングをしていたり今でも自作 PC を組んでゲームをしていたりと、仕事・プライベート共にコンピュータに触れながら生活しています。これからのコンピュータが発展していく未来に期待して学んでゆきます。\n","permalink":"https://jedipunkz.github.io/about/profile/","summary":"\u003cp\u003e\u003cimg alt=\"icon\" loading=\"lazy\" src=\"../../pix/jedipunkz_banner.png\"\u003e\u003c/p\u003e\n\u003cp\u003eこんにちは \u003ca href=\"https://x.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e 🚀 です。\u003c/p\u003e\n\u003cp\u003e元インフラエンジニア・クラウドエンジニアで最近は AWS, GCP を扱う SRE として働いています。ソフトウェアでインフラの課題を解決すべく勉強していきます。学んだことをこのブログに記せたらいいなと思っています。\u003c/p\u003e\n\u003cp\u003e小学生の頃からプログラミングをしていたり今でも自作 PC を組んでゲームをしていたりと、仕事・プライベート共にコンピュータに触れながら生活しています。これからのコンピュータが発展していく未来に期待して学んでゆきます。\u003c/p\u003e","title":"自己紹介"},{"content":"自分は Go の初学者なのですがクリーンアーキテクチャを学ぶために幾つかの書籍を読んでみたもののなかなかしっくりと理解が出来ていない状態でした。 そこで AI に極力シンプルなコードを書かせて理解するという事をやってみたのですが、なかなかいい感じに理解が進んだのでここで記事にしたいと思っています。\nクリーンアーキテクチャとは？ クリーンアーキテクチャは、Robert C. Martin によって提唱されたソフトウェア設計原則で最も重要な特徴は依存関係の方向性にあります。\n従来のアーキテクチャとは異なり、内側の層は外側の層を知らないという原則に基づいています。これにより以下のメリットが得られます。\nテスタビリティ: ビジネスロジックを単体でテスト可能 柔軟性: データベースや Web フレームワークの変更が容易 保守性: 関心の分離により変更の影響範囲を限定 実装するシステムの全体像 今回 AI に実装させたのは RESTful API を提供するユーザー管理システムです。\nコードは下記のレポジトリにあります。\nhttps://github.com/jedipunkz/go-clean-architecture-playground\nシステムは以下の4つの層で構成されています：\nEntity レイヤ - ビジネスの核となるルール Interface レイヤ - 抽象化による疎結合 Use Case レイヤ - ビジネスロジックの実装 Infrastructure レイヤ - 外部システムとの連携実装 Controller レイヤ - HTTP APIの提供 各層の詳細実装 1. Entity レイヤ - ビジネスの核となるルール 最も内側の層から実装を始めます。エンティティ層は他のどの層にも依存しない純粋なビジネスロジックです。\n// entity/user.go package entity import ( \u0026#34;errors\u0026#34; \u0026#34;time\u0026#34; ) type User struct { ID int `json:\u0026#34;id\u0026#34;` Name string `json:\u0026#34;name\u0026#34;` Email string `json:\u0026#34;email\u0026#34;` CreatedAt time.Time `json:\u0026#34;created_at\u0026#34;` UpdatedAt time.Time `json:\u0026#34;updated_at\u0026#34;` } func NewUser(name, email string) (*User, error) { if name == \u0026#34;\u0026#34; { return nil, errors.New(\u0026#34;名前は必須です\u0026#34;) } if email == \u0026#34;\u0026#34; { return nil, errors.New(\u0026#34;メールアドレスは必須です\u0026#34;) } now := time.Now() return \u0026amp;User{ Name: name, Email: email, CreatedAt: now, UpdatedAt: now, }, nil } func (u *User) UpdateInfo(name, email string) error { if name == \u0026#34;\u0026#34; { return errors.New(\u0026#34;名前は必須です\u0026#34;) } if email == \u0026#34;\u0026#34; { return nil, errors.New(\u0026#34;メールアドレスは必須です\u0026#34;) } u.Name = name u.Email = email u.UpdatedAt = time.Now() return nil } この User エンティティは、クリーンアーキテクチャの最も内側に位置する部分です。ここで重要なのは、このエンティティが外部の何にも依存していないことです。データベースがMySQLなのかPostgreSQLなのか、WebフレームワークがGinなのかEchoなのか、そういった技術的な詳細は一切知らないのが特徴になっています。\nNewUser 関数では、ユーザーを作成する際の基本的なビジネスルール（名前とメールアドレスは必須）を実装しています。これらのルールは、技術的な実装に関係なく、ビジネスとして絶対に守らなければならない制約です。また、UpdateInfo メソッドでは、ユーザー情報を更新する際の整合性を保っています。このようにエンティティ層では純粋なビジネスロジックのみを扱うことで、システムの核となる部分を技術的な変更から守ることができます。\n2. Interface レイヤ - 抽象化による疎結合 次に、データアクセスのインターフェースを定義します。これが依存関係逆転の原則の鍵となります。\n// interface/repository/user_repository.go package repository import \u0026#34;go-clean-architecture-playground/entity\u0026#34; type UserRepository interface { Create(user *entity.User) error GetByID(id int) (*entity.User, error) GetByEmail(email string) (*entity.User, error) Update(user *entity.User) error Delete(id int) error List() ([]*entity.User, error) } この UserRepository インターフェースは、クリーンアーキテクチャにおける「Contract」の役割を果たしています。このインターフェースを見ると、ユーザーデータに対してどのような操作ができるのかが定義されています。重要なのは、これが「何をするか」だけを定義してて「どうやってするか」については一切触れていないということです。\n従来のアーキテクチャでは、ビジネスロジックがデータベースの実装に直接依存していました。クリーンアーキテクチャでは、このインターフェースを間に挟むことで依存関係を逆転させてます。つまり、ビジネスロジック（UseCase）はこのインターフェースに依存し、具体的な実装（MemoryUserRepository）がこのインターフェースに合わせて作られます。これにより、データベースをMySQLからPostgreSQLに変更したり、テスト時にモックオブジェクトに置き換えたりすることが、他の層に影響を与えることなく可能になる、ということです。\n3. Use Case レイヤ - ビジネスロジックの実装 Use Case 層では、Entity と repository を組み合わせてアプリケーション固有のビジネスロジックを実装します。\n// usecase/user_usecase.go package usecase import ( \u0026#34;errors\u0026#34; \u0026#34;go-clean-architecture-playground/entity\u0026#34; \u0026#34;go-clean-architecture-playground/interface/repository\u0026#34; ) type UserUsecase struct { userRepo repository.UserRepository } func NewUserUsecase(userRepo repository.UserRepository) *UserUsecase { return \u0026amp;UserUsecase{ userRepo: userRepo, } } func (u *UserUsecase) CreateUser(name, email string) (*entity.User, error) { // ビジネスルール：重複チェック existingUser, _ := u.userRepo.GetByEmail(email) if existingUser != nil { return nil, errors.New(\u0026#34;このメールアドレスは既に使用されています\u0026#34;) } // エンティティ作成（バリデーション付き） user, err := entity.NewUser(name, email) if err != nil { return nil, err } // データ永続化 err = u.userRepo.Create(user) if err != nil { return nil, err } return user, nil } func (u *UserUsecase) GetUser(id int) (*entity.User, error) { if id \u0026lt;= 0 { return nil, errors.New(\u0026#34;無効なユーザーIDです\u0026#34;) } user, err := u.userRepo.GetByID(id) if err != nil { return nil, err } if user == nil { return nil, errors.New(\u0026#34;ユーザーが見つかりません\u0026#34;) } return user, nil } Use Case 層は、クリーンアーキテクチャのオーケストレータのような役割を果たします。この UserUsecase では、Entity が持つ基本的なビジネスルールと、 リポジトリが提供するデータアクセス機能を組み合わせて、より複雑なビジネス要件を実現しています。\n例えば CreateUser メソッドでは同じメールアドレスのユーザーが既に存在するかをリポジトリに問い合わせます。これはアプリケーション固有のビジネスルール（重複禁止）です。次に、Entity の NewUser 関数を使って新しいユーザーを作成します。ここでは Entity が持つ基本的なバリデーション（必須項目チェック）が実行されます。最後に、Repository を使って実際にデータを保存します。\nこの流れで重要なのは、Use Case 層が Entity とリポジトリインターフェースにのみ依存していることです。具体的なデータベース実装やHTTPハンドラーについては何も知りません。これにより、ビジネスロジックを外部の技術的な変更から完全に分離できています。\n4. Infrastructure レイヤ - 外部システムとの連携実装 実際のデータ保存を担当する層です。今回はシンプルにメモリ内保存を実装しています。\n// infrastructure/persistence/memory_user_repository.go package persistence import ( \u0026#34;errors\u0026#34; \u0026#34;go-clean-architecture-playground/entity\u0026#34; \u0026#34;go-clean-architecture-playground/interface/repository\u0026#34; \u0026#34;sync\u0026#34; ) type MemoryUserRepository struct { users map[int]*entity.User lastID int mutex sync.RWMutex } func NewMemoryUserRepository() repository.UserRepository { return \u0026amp;MemoryUserRepository{ users: make(map[int]*entity.User), lastID: 0, } } func (r *MemoryUserRepository) Create(user *entity.User) error { r.mutex.Lock() defer r.mutex.Unlock() r.lastID++ user.ID = r.lastID r.users[user.ID] = user return nil } func (r *MemoryUserRepository) GetByEmail(email string) (*entity.User, error) { r.mutex.RLock() defer r.mutex.RUnlock() for _, user := range r.users { if user.Email == email { return user, nil } } return nil, nil // 見つからない場合はnil } インフラストラクチャ層は、クリーンアーキテクチャにおける実行者とも言えると思います。この MemoryUserRepository は、先ほど定義した UserRepository インターフェースを実際に実装したもので、この実装が先に定義されたインターフェースに合わせて作られていることです。\n従来のアーキテクチャでは、まずデータベース実装があり、それに合わせてビジネスロジックが書かれていました。クリーンアーキテクチャでは、ビジネス要件から生まれたインターフェースに対して、技術的な実装が合わせられます。これが「依存関係逆転の原則」の実践です。\nこの実装では、シンプルにメモリ内のマップを使ってデータを保存していますが、同じインターフェースを実装することで、PostgreSQL や MySQL などのデータベース実装に置き換えることができます。その際、Use Case 層や Entity 層のコードは一切変更する必要がありません。\n5. Controller レイヤ - HTTP APIの提供 外部からのHTTPリクエストを処理し、ユースケースに処理を委譲します。\n// interface/controller/user_controller.go package controller import ( \u0026#34;encoding/json\u0026#34; \u0026#34;go-clean-architecture-playground/usecase\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;strconv\u0026#34; \u0026#34;strings\u0026#34; ) type UserController struct { userUsecase *usecase.UserUsecase } func NewUserController(userUsecase *usecase.UserUsecase) *UserController { return \u0026amp;UserController{ userUsecase: userUsecase, } } type CreateUserRequest struct { Name string `json:\u0026#34;name\u0026#34;` Email string `json:\u0026#34;email\u0026#34;` } func (c *UserController) CreateUser(w http.ResponseWriter, r *http.Request) { if r.Method != http.MethodPost { http.Error(w, \u0026#34;Method not allowed\u0026#34;, http.StatusMethodNotAllowed) return } var req CreateUserRequest if err := json.NewDecoder(r.Body).Decode(\u0026amp;req); err != nil { http.Error(w, \u0026#34;Invalid JSON\u0026#34;, http.StatusBadRequest) return } user, err := c.userUsecase.CreateUser(req.Name, req.Email) if err != nil { http.Error(w, err.Error(), http.StatusBadRequest) return } w.Header().Set(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) json.NewEncoder(w).Encode(user) } コントローラー層は、外部とアプリケーションを繋ぐのモノの役割を果たします。この UserController は、HTTP という外部のプロトコルから送られてくるリクエストをアプリケーション内部で理解できる Go 言語の関数呼び出しに変換しています。\nCreateUser メソッドを見てみるとまず HTTP リクエストから JSON データを読み取り、Go 言語の構造体 CreateUserRequest に変換しています。次に、その構造体から名前とメールアドレスを取り出して、Use Case 層の CreateUser メソッドを呼び出します。最後に、Use Case から返された結果を JSON に変換して HTTP レスポンスとして返します。\n重要なのは、このコントローラーが HTTP リクエストの読み取り、ステータスコードの設定、レスポンスの書き込みのみを担当し、ビジネスロジックには一切関与していないことです。実際のユーザー作成処理は全て Use Case 層に委譲しています。これにより、将来的に HTTP から gRPC や GraphQL に変更する必要が出てきてもビジネスロジックは何も変更する必要がなくなります。\n依存関係を結合 cmd/main.go では、各層のインスタンスを作成し、依存関係を記しています。\n// cmd/main.go func main() { // 依存関係の構築（外側から内側へ） userRepo := persistence.NewMemoryUserRepository() // Infrastructure userUsecase := usecase.NewUserUsecase(userRepo) // Use Case userController := controller.NewUserController(userUsecase) // Interface // HTTPハンドラーの設定 http.HandleFunc(\u0026#34;/users\u0026#34;, func(w http.ResponseWriter, r *http.Request) { if r.Method == http.MethodPost { userController.CreateUser(w, r) } else if r.Method == http.MethodGet { userController.ListUsers(w, r) } }) fmt.Println(\u0026#34;サーバーを :8080 で起動しています...\u0026#34;) log.Fatal(http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil)) } ここで最も重要なのが依存性の部分です。cmd/main.go は、クリーンアーキテクチャの各要素の役割の関係を表現しています。\n依存関係の構築順序を見るとまず最も外側のインフラストラクチャ層（MemoryUserRepository）のインスタンスを作成して、それを Use Case 層（UserUsecase）に渡しています。次に Use Case を Controller 層（UserController）に渡します。\nこの設計は、例えばデータベースをメモリから PostgreSQL に変更したい場合、cmd/main.go の一行を変更するだけで済みます。ビジネスロジックや Controller のコードは変更する必要がありません。\n動作確認 1. アプリケーションの起動 go run cmd/main.go 2. APIのテスト # ユーザー作成 curl -X POST http://localhost:8080/users \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;name\u0026#34;: \u0026#34;田中太郎\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;tanaka@example.com\u0026#34;}\u0026#39; # レスポンス例 { \u0026#34;id\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;田中太郎\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;tanaka@example.com\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2025-08-01T19:52:11Z\u0026#34;, \u0026#34;updated_at\u0026#34;: \u0026#34;2025-08-01T19:52:11Z\u0026#34; } # ユーザー取得 curl http://localhost:8080/users/1 { \u0026#34;id\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;田中太郎\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;tanaka@example.com\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2025-08-01T19:59:11.510652+09:00\u0026#34;, \u0026#34;updated_at\u0026#34;: \u0026#34;2025-08-01T19:59:11.510652+09:00\u0026#34; } その他クリーンアーキテクチャのメリット テスタビリティの向上 各層が独立しているため、ユニットテストが容易になるそうです。\n// usecase のテスト例 func TestUserUsecase_CreateUser(t *testing.T) { // モックリポジトリを注入 mockRepo := \u0026amp;MockUserRepository{} usecase := NewUserUsecase(mockRepo) // テスト実行 user, err := usecase.CreateUser(\u0026#34;テスト太郎\u0026#34;, \u0026#34;test@example.com\u0026#34;) // 検証 assert.NoError(t, err) assert.Equal(t, \u0026#34;テスト太郎\u0026#34;, user.Name) } 拡張性の確保 前述しましたが、新しい要件に対して対応が簡単になります。\n// PostgreSQL実装への切り替え func main() { // メモリ実装から... // userRepo := persistence.NewMemoryUserRepository() // データベース実装へ userRepo := persistence.NewPostgreSQLUserRepository(db) // 他の部分は変更不要！ userUsecase := usecase.NewUserUsecase(userRepo) // ... } まとめ クリーンアーキテクチャを実践することで、依存関係が従来と逆になったことが理解でき、これによってビジネス要件の不意の変更にも柔軟に対応出来そうということがわかりました。ただ小さいアプリケーションを書く際にはオーバーエンジニアリングなのでは？と感じたのも正直なところです。アプリケーションが複雑化するビジネス要件では真価を発揮できそうです。テストを書きやすいという点も個人的に納得。フロントエンジニアは Controller、バックエンドエンジニアは Use Case, インフラエンジニアは Repository と関心事を分離出来ている点も個人的にメリットと感じ取りました。\n","permalink":"https://jedipunkz.github.io/post/go-clean-architecture/","summary":"\u003cp\u003e自分は Go の初学者なのですがクリーンアーキテクチャを学ぶために幾つかの書籍を読んでみたもののなかなかしっくりと理解が出来ていない状態でした。 そこで AI に極力シンプルなコードを書かせて理解するという事をやってみたのですが、なかなかいい感じに理解が進んだのでここで記事にしたいと思っています。\u003c/p\u003e\n\u003ch2 id=\"クリーンアーキテクチャとは\"\u003eクリーンアーキテクチャとは？\u003c/h2\u003e\n\u003cp\u003eクリーンアーキテクチャは、Robert C. Martin によって提唱されたソフトウェア設計原則で最も重要な特徴は依存関係の方向性にあります。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"依存関係図\" loading=\"lazy\" src=\"../../pix/go-clean-architecture-2.png\"\u003e\u003c/p\u003e\n\u003cp\u003e従来のアーキテクチャとは異なり、内側の層は外側の層を知らないという原則に基づいています。これにより以下のメリットが得られます。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eテスタビリティ: ビジネスロジックを単体でテスト可能\u003c/li\u003e\n\u003cli\u003e柔軟性: データベースや Web フレームワークの変更が容易\u003c/li\u003e\n\u003cli\u003e保守性: 関心の分離により変更の影響範囲を限定\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"実装するシステムの全体像\"\u003e実装するシステムの全体像\u003c/h2\u003e\n\u003cp\u003e今回 AI に実装させたのは RESTful API を提供するユーザー管理システムです。\u003c/p\u003e\n\u003cp\u003eコードは下記のレポジトリにあります。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/jedipunkz/go-clean-architecture-playground\"\u003ehttps://github.com/jedipunkz/go-clean-architecture-playground\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"システム構成図\" loading=\"lazy\" src=\"../../pix/go-clean-architecture-1.png\"\u003e\u003c/p\u003e\n\u003cp\u003eシステムは以下の4つの層で構成されています：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eEntity レイヤ - ビジネスの核となるルール\u003c/li\u003e\n\u003cli\u003eInterface レイヤ - 抽象化による疎結合\u003c/li\u003e\n\u003cli\u003eUse Case レイヤ   - ビジネスロジックの実装\u003c/li\u003e\n\u003cli\u003eInfrastructure レイヤ - 外部システムとの連携実装\u003c/li\u003e\n\u003cli\u003eController レイヤ - HTTP APIの提供\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"各層の詳細実装\"\u003e各層の詳細実装\u003c/h2\u003e\n\u003ch3 id=\"1-entity-レイヤ---ビジネスの核となるルール\"\u003e1. Entity レイヤ - ビジネスの核となるルール\u003c/h3\u003e\n\u003cp\u003e最も内側の層から実装を始めます。エンティティ層は他のどの層にも依存しない純粋なビジネスロジックです。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e// entity/user.go\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003epackage\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eentity\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e (\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;errors\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;time\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003etype\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eUser\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003estruct\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#a6e22e\"\u003eID\u003c/span\u003e        \u003cspan style=\"color:#66d9ef\"\u003eint\u003c/span\u003e       \u003cspan style=\"color:#e6db74\"\u003e`json:\u0026#34;id\u0026#34;`\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#a6e22e\"\u003eName\u003c/span\u003e      \u003cspan style=\"color:#66d9ef\"\u003estring\u003c/span\u003e    \u003cspan style=\"color:#e6db74\"\u003e`json:\u0026#34;name\u0026#34;`\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#a6e22e\"\u003eEmail\u003c/span\u003e     \u003cspan style=\"color:#66d9ef\"\u003estring\u003c/span\u003e    \u003cspan style=\"color:#e6db74\"\u003e`json:\u0026#34;email\u0026#34;`\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#a6e22e\"\u003eCreatedAt\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003etime\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eTime\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e`json:\u0026#34;created_at\u0026#34;`\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#a6e22e\"\u003eUpdatedAt\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003etime\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eTime\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e`json:\u0026#34;updated_at\u0026#34;`\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efunc\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eNewUser\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003ename\u003c/span\u003e, \u003cspan style=\"color:#a6e22e\"\u003eemail\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003estring\u003c/span\u003e) (\u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003eUser\u003c/span\u003e, \u003cspan style=\"color:#66d9ef\"\u003eerror\u003c/span\u003e) {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003ename\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u0026#34;\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t\u003cspan style=\"color:#66d9ef\"\u003ereturn\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003enil\u003c/span\u003e, \u003cspan style=\"color:#a6e22e\"\u003eerrors\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eNew\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;名前は必須です\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eemail\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u0026#34;\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t\u003cspan style=\"color:#66d9ef\"\u003ereturn\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003enil\u003c/span\u003e, \u003cspan style=\"color:#a6e22e\"\u003eerrors\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eNew\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;メールアドレスは必須です\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#a6e22e\"\u003enow\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e:=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003etime\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eNow\u003c/span\u003e()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#66d9ef\"\u003ereturn\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e\u0026amp;\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003eUser\u003c/span\u003e{\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t\u003cspan style=\"color:#a6e22e\"\u003eName\u003c/span\u003e:      \u003cspan style=\"color:#a6e22e\"\u003ename\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t\u003cspan style=\"color:#a6e22e\"\u003eEmail\u003c/span\u003e:     \u003cspan style=\"color:#a6e22e\"\u003eemail\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t\u003cspan style=\"color:#a6e22e\"\u003eCreatedAt\u003c/span\u003e: \u003cspan style=\"color:#a6e22e\"\u003enow\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t\u003cspan style=\"color:#a6e22e\"\u003eUpdatedAt\u003c/span\u003e: \u003cspan style=\"color:#a6e22e\"\u003enow\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t}, \u003cspan style=\"color:#66d9ef\"\u003enil\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efunc\u003c/span\u003e (\u003cspan style=\"color:#a6e22e\"\u003eu\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003eUser\u003c/span\u003e) \u003cspan style=\"color:#a6e22e\"\u003eUpdateInfo\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003ename\u003c/span\u003e, \u003cspan style=\"color:#a6e22e\"\u003eemail\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003estring\u003c/span\u003e) \u003cspan style=\"color:#66d9ef\"\u003eerror\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003ename\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u0026#34;\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t\u003cspan style=\"color:#66d9ef\"\u003ereturn\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eerrors\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eNew\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;名前は必須です\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eemail\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u0026#34;\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t\u003cspan style=\"color:#66d9ef\"\u003ereturn\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003enil\u003c/span\u003e, \u003cspan style=\"color:#a6e22e\"\u003eerrors\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eNew\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;メールアドレスは必須です\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#a6e22e\"\u003eu\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eName\u003c/span\u003e = \u003cspan style=\"color:#a6e22e\"\u003ename\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#a6e22e\"\u003eu\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eEmail\u003c/span\u003e = \u003cspan style=\"color:#a6e22e\"\u003eemail\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#a6e22e\"\u003eu\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eUpdatedAt\u003c/span\u003e = \u003cspan style=\"color:#a6e22e\"\u003etime\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eNow\u003c/span\u003e()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#66d9ef\"\u003ereturn\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003enil\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eこの \u003ccode\u003eUser\u003c/code\u003e エンティティは、クリーンアーキテクチャの最も内側に位置する部分です。ここで重要なのは、このエンティティが外部の何にも依存していないことです。データベースがMySQLなのかPostgreSQLなのか、WebフレームワークがGinなのかEchoなのか、そういった技術的な詳細は一切知らないのが特徴になっています。\u003c/p\u003e","title":"Go 初学者が学ぶクリーンアーキテクチャ"},{"content":"自分は Platform Enabler/SRE として従事しています。また AI 関連のアップデートは2025年に入っても属に更新されています。2025年初頭においては自分たちの分野でも AI 関連の利用に関して様々な模索がある状況だと思われますが、Ahthoropic 社が提唱した MCP (Model Context Protocol) がもたらすインパクトはアプリケーションに限定されずインフラ領域のソフトウェアにも大きなメリットをもたらすと思って観測しています。\nこの記事では、MCP の概要とどう実装するのかの学習、またどう我々のような Platform Enabler/SRE にとっての活用例があるかを考察していきたいと思っています。\nMCP の概要 MCP (Model Context Protocol) は、AI モデルと外部システム間のやり取りを効率化するプロトコルで JSON-RPC でやりとりします。ユーザーの自然言語入力を基に、AI アシスタントが MCP サーバーを通じてファイル操作やデータ処理を実行します。\n最近 OpenAI 社もこの Anthropic 社の MCP をサポートするというニュースが流れ、途端に注目を集める状況になってきました。\n処理の流れ ここはあくまでの一例です。Assistant の実装でいかようにも出来ると思います。\n+-------------+ +-----------+ +------------+ +--------+ | User Prompt | \u0026lt;-----\u0026gt; | Assistant | ---\u0026gt; | MCP Server | | AI API | +-------------+ (1),(5) +-----------+ (3) +------------+ +--------+ | ^ | (2),(4) | +----------------------------------+ (1) ユーザからの入力を Assistant が受け取る (2) Assistant はユーザからの自然言語を LLM に問い合わせ。その際に LLM に外部機能を定義 (JSON-RPC(MCP サーバが受け取る)) (3) Aasistant は MCP Server に JSON-RPC でクエリ送信しレスポンスを得る (4) Assistant は MCP Server から得たレスポンスを再び LLM に送信し自然言語としてユーザに返す内容を生成してもらう (5) Assistant はユーザに自然言語で結果を応答する 前提 MCP 学習を目的にしているので、ここでは話を簡潔にするため Linux Filesystem を操作する MCP Server を書き、理解していきます。\nMCP Server の実装 言語は問いませんが自分は Go で書きました。前述どおり、まず学習目的でローカルファイルシステムを操作する MCP Server を実装しました。\npackage main import ( \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;io\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;os\u0026#34; \u0026#34;path/filepath\u0026#34; \u0026#34;sort\u0026#34; ) // MCPRequest はクライアントからのリクエストの構造体 type MCPRequest struct { Action string `json:\u0026#34;action\u0026#34;` Path string `json:\u0026#34;path\u0026#34;` Content string `json:\u0026#34;content,omitempty\u0026#34;` Recursive bool `json:\u0026#34;recursive,omitempty\u0026#34;` Data json.RawMessage `json:\u0026#34;data,omitempty\u0026#34;` } // MCPResponse はクライアントへのレスポンスの構造体 type MCPResponse struct { Status string `json:\u0026#34;status\u0026#34;` Message string `json:\u0026#34;message,omitempty\u0026#34;` Data interface{} `json:\u0026#34;data,omitempty\u0026#34;` } // FileInfo はファイル情報の構造体 type FileInfo struct { Name string `json:\u0026#34;name\u0026#34;` Size int64 `json:\u0026#34;size\u0026#34;` Mode string `json:\u0026#34;mode\u0026#34;` ModTime string `json:\u0026#34;modTime\u0026#34;` IsDir bool `json:\u0026#34;isDir\u0026#34;` Path string `json:\u0026#34;path\u0026#34;` } func main() { http.HandleFunc(\u0026#34;/mcp\u0026#34;, handleMCP) port := \u0026#34;8080\u0026#34; fmt.Printf(\u0026#34;Starting MCP server on port %s...\\n\u0026#34;, port) log.Fatal(http.ListenAndServe(\u0026#34;:\u0026#34;+port, nil)) } func handleMCP(w http.ResponseWriter, r *http.Request) { w.Header().Set(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) w.Header().Set(\u0026#34;Access-Control-Allow-Origin\u0026#34;, \u0026#34;*\u0026#34;) w.Header().Set(\u0026#34;Access-Control-Allow-Methods\u0026#34;, \u0026#34;POST, OPTIONS\u0026#34;) w.Header().Set(\u0026#34;Access-Control-Allow-Headers\u0026#34;, \u0026#34;Content-Type\u0026#34;) if r.Method == \u0026#34;OPTIONS\u0026#34; { w.WriteHeader(http.StatusOK) return } if r.Method != \u0026#34;POST\u0026#34; { sendError(w, \u0026#34;Method not allowed\u0026#34;, http.StatusMethodNotAllowed) return } body, err := io.ReadAll(r.Body) if err != nil { sendError(w, \u0026#34;Failed to read request body\u0026#34;, http.StatusBadRequest) return } defer r.Body.Close() var req MCPRequest if err := json.Unmarshal(body, \u0026amp;req); err != nil { sendError(w, \u0026#34;Invalid JSON\u0026#34;, http.StatusBadRequest) return } switch req.Action { case \u0026#34;list\u0026#34;: handleList(w, req) case \u0026#34;read\u0026#34;: handleRead(w, req) case \u0026#34;write\u0026#34;: handleWrite(w, req) case \u0026#34;delete\u0026#34;: handleDelete(w, req) case \u0026#34;mkdir\u0026#34;: handleMkdir(w, req) default: sendError(w, \u0026#34;Unknown action\u0026#34;, http.StatusBadRequest) } } func handleList(w http.ResponseWriter, req MCPRequest) { path := sanitizePath(req.Path) entries, err := os.ReadDir(path) if err != nil { sendError(w, fmt.Sprintf(\u0026#34;Failed to read directory: %v\u0026#34;, err), http.StatusInternalServerError) return } var fileInfos []FileInfo for _, entry := range entries { info, err := entry.Info() if err != nil { continue } fileInfos = append(fileInfos, FileInfo{ Name: entry.Name(), Size: info.Size(), Mode: info.Mode().String(), ModTime: info.ModTime().Format(\u0026#34;2006-01-02 15:04:05\u0026#34;), IsDir: entry.IsDir(), Path: filepath.Join(path, entry.Name()), }) } sort.Slice(fileInfos, func(i, j int) bool { if fileInfos[i].IsDir \u0026amp;\u0026amp; !fileInfos[j].IsDir { return true } if !fileInfos[i].IsDir \u0026amp;\u0026amp; fileInfos[j].IsDir { return false } return fileInfos[i].Name \u0026lt; fileInfos[j].Name }) sendSuccess(w, \u0026#34;Directory listed successfully\u0026#34;, fileInfos) } func handleRead(w http.ResponseWriter, req MCPRequest) { path := sanitizePath(req.Path) content, err := os.ReadFile(path) if err != nil { sendError(w, fmt.Sprintf(\u0026#34;Failed to read file: %v\u0026#34;, err), http.StatusInternalServerError) return } sendSuccess(w, \u0026#34;File read successfully\u0026#34;, string(content)) } func handleWrite(w http.ResponseWriter, req MCPRequest) { path := sanitizePath(req.Path) dir := filepath.Dir(path) if err := os.MkdirAll(dir, 0755); err != nil { sendError(w, fmt.Sprintf(\u0026#34;Failed to create directory: %v\u0026#34;, err), http.StatusInternalServerError) return } err := os.WriteFile(path, []byte(req.Content), 0644) if err != nil { sendError(w, fmt.Sprintf(\u0026#34;Failed to write file: %v\u0026#34;, err), http.StatusInternalServerError) return } sendSuccess(w, \u0026#34;File written successfully\u0026#34;, nil) } func handleDelete(w http.ResponseWriter, req MCPRequest) { path := sanitizePath(req.Path) var err error if req.Recursive { err = os.RemoveAll(path) } else { err = os.Remove(path) } if err != nil { sendError(w, fmt.Sprintf(\u0026#34;Failed to delete: %v\u0026#34;, err), http.StatusInternalServerError) return } sendSuccess(w, \u0026#34;Deleted successfully\u0026#34;, nil) } func handleMkdir(w http.ResponseWriter, req MCPRequest) { path := sanitizePath(req.Path) var err error if req.Recursive { err = os.MkdirAll(path, 0755) } else { err = os.Mkdir(path, 0755) } if err != nil { sendError(w, fmt.Sprintf(\u0026#34;Failed to create directory: %v\u0026#34;, err), http.StatusInternalServerError) return } sendSuccess(w, \u0026#34;Directory created successfully\u0026#34;, nil) } func sanitizePath(path string) string { // セキュリティ対策の実装はこちらに書くそうです // - 特定のパスは応答させない // など return path } func sendError(w http.ResponseWriter, message string, statusCode int) { w.WriteHeader(statusCode) resp := MCPResponse{ Status: \u0026#34;error\u0026#34;, Message: message, } json.NewEncoder(w).Encode(resp) } func sendSuccess(w http.ResponseWriter, message string, data interface{}) { resp := MCPResponse{ Status: \u0026#34;success\u0026#34;, Message: message, Data: data, } json.NewEncoder(w).Encode(resp) } コードの解説 構造体定義: MCPRequest: クライアントからのリクエストデータを表す構造体。 MCPResponse: サーバーからのレスポンスデータを表す構造体。 FileInfo: ファイルやディレクトリの情報を表す構造体。 エントリーポイント: /mcp エンドポイントをハンドリングする handleMCP を登録。 ポート 8080 でHTTPサーバーを起動。 リクエスト処理: CORS対応とHTTPメソッドの検証（OPTIONSとPOSTのみ許可）。 リクエストボディを読み取り、MCPRequest にパース。 action フィールドに基づいて適切な処理関数を呼び出す。 アクションごとの処理: handleList: 指定されたディレクトリの内容をリスト化。 handleRead: 指定されたファイルの内容を読み取る。 handleWrite: 指定されたパスにファイルを書き込む。 handleDelete: ファイルまたはディレクトリを削除（再帰的削除対応）。 handleMkdir: ディレクトリを作成（再帰的作成対応）。 補助関数: sanitizePath: パスのサニタイズ（セキュリティ対策）。 sendError: エラーレスポンスを送信。 sendSuccess: 成功レスポンスを送信。 MCP Server の動作確認 動作確認を行います。JSON-RPC で下記の要素を含めて POST します。\naction: ファイル操作のアクション path: ファイル・ディレクトリの指定 content: ファイルを作成する場合のファイル内容 またレスポンスには下記の要素が返されます。\nstatus: ステータス message: ファイル操作の結果 MCP Server の起動 先ほど書いた main.go を起動します。これにより localhost の 8080 番ポートでリスンします。\ngo run ./main.go ファイルの書き込み /tmp/file.txt というファイル名を指定しファイル内容 \u0026lsquo;Hello World\u0026rsquo; として保存してみます。\ncurl -X POST http://localhost:8080/mcp -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{\u0026#34;action\u0026#34;:\u0026#34;write\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/tmp/file.txt\u0026#34;,\u0026#34;content\u0026#34;:\u0026#34;Hello World\u0026#34;}\u0026#39; {\u0026#34;status\u0026#34;:\u0026#34;success\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;File written successfully\u0026#34;} status: success が表示されました。\nファイルの一覧表示 /tmp/foo ディレクトリ配下のファイル一覧を表示しています。\ncurl -X POST http://localhost:8080/mcp -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{\u0026#34;action\u0026#34;:\u0026#34;list\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/tmp/foo\u0026#34;}\u0026#39; {\u0026#34;status\u0026#34;:\u0026#34;success\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;Directory listed successfully\u0026#34;,\u0026#34;data\u0026#34;:[{\u0026#34;name\u0026#34;:\u0026#34;bar\u0026#34;,\u0026#34;size\u0026#34;:0,\u0026#34;mode\u0026#34;:\u0026#34;-rw-r--r--\u0026#34;,\u0026#34;modTime\u0026#34;:\u0026#34;2025-04-06 11:13:42\u0026#34;,\u0026#34;isDir\u0026#34;:false,\u0026#34;path\u0026#34;:\u0026#34;/tmp/foo/bar\u0026#34;}]} /tmp/foo ディレクトリ配下に bar というファイルだけがあることを確認できました。\nファイルの読み込み /tmp/foo/bar というファイルの内容を読み込んで出力しています。\ncurl -X POST http://localhost:8080/mcp -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{\u0026#34;action\u0026#34;:\u0026#34;read\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/tmp/foo/bar\u0026#34;}\u0026#39; {\u0026#34;status\u0026#34;:\u0026#34;success\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;File read successfully\u0026#34;,\u0026#34;data\u0026#34;:\u0026#34;bar\\n\u0026#34;} ファイル内容 bar を確認できました。\nAssistant の実装 ここも言語は問いませんが自分は Python で記しました。上記構成図の処理 (1)-(5) 全てをこのコードで行います。\nimport requests import json import os import argparse import sys from openai import OpenAI MCP_SERVER_URL = \u0026#34;http://localhost:8080/mcp\u0026#34; class MCPAssistant: def __init__(self, model: str = \u0026#34;gpt-4\u0026#34;, api_key: str = None): \u0026#34;\u0026#34;\u0026#34; Initialize AI assistant Args: model: Model name to use api_key: API key (uses environment variable if not specified) \u0026#34;\u0026#34;\u0026#34; self.model = model self.api_key = api_key or os.environ.get(\u0026#34;OPENAI_API_KEY\u0026#34;) if not self.api_key: raise ValueError(\u0026#34;OPENAI_API_KEY environment variable must be set\u0026#34;) self.client = OpenAI(api_key=self.api_key) self.conversation_history = [] self.tools = [ { \u0026#34;type\u0026#34;: \u0026#34;function\u0026#34;, \u0026#34;function\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;file_system_operation\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Operate on Linux filesystem\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;action\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;enum\u0026#34;: [\u0026#34;list\u0026#34;, \u0026#34;read\u0026#34;, \u0026#34;write\u0026#34;, \u0026#34;delete\u0026#34;, \u0026#34;mkdir\u0026#34;], \u0026#34;description\u0026#34;: \u0026#34;Type of operation to perform\u0026#34; }, \u0026#34;path\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Path to file or directory\u0026#34; }, \u0026#34;content\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Content to write (for write action)\u0026#34; }, \u0026#34;recursive\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;boolean\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Whether to perform operation recursively (for delete/mkdir)\u0026#34; } }, \u0026#34;required\u0026#34;: [\u0026#34;action\u0026#34;, \u0026#34;path\u0026#34;] } } } ] def call_llm(self, user_input: str): \u0026#34;\u0026#34;\u0026#34;Call LLM and get response\u0026#34;\u0026#34;\u0026#34; system_message = \u0026#34;\u0026#34;\u0026#34;You are an AI assistant that can operate on a Linux filesystem. Understand the user\u0026#39;s natural language requests and translate them to appropriate filesystem operations. You can use the file_system_operation function to access the filesystem. Available operations: list, read, write, delete, mkdir \u0026#34;\u0026#34;\u0026#34; self.conversation_history.append({\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: user_input}) messages = [{\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: system_message}] + self.conversation_history return self.client.chat.completions.create( model=self.model, messages=messages, tools=self.tools, tool_choice=\u0026#34;auto\u0026#34; ) def call_mcp_server(self, params): \u0026#34;\u0026#34;\u0026#34;Call MCP server\u0026#34;\u0026#34;\u0026#34; try: response = requests.post( MCP_SERVER_URL, json=params, headers={\u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;} ) if response.status_code == 200: return response.json() else: return { \u0026#34;status\u0026#34;: \u0026#34;error\u0026#34;, \u0026#34;message\u0026#34;: f\u0026#34;MCP server error: status code {response.status_code}\u0026#34;, \u0026#34;data\u0026#34;: None } except Exception as e: return { \u0026#34;status\u0026#34;: \u0026#34;error\u0026#34;, \u0026#34;message\u0026#34;: f\u0026#34;MCP server connection error: {str(e)}\u0026#34;, \u0026#34;data\u0026#34;: None } def process_response(self, llm_response): \u0026#34;\u0026#34;\u0026#34;Process LLM response and execute tool calls if any\u0026#34;\u0026#34;\u0026#34; assistant_message = llm_response.choices[0].message if hasattr(assistant_message, \u0026#39;tool_calls\u0026#39;) and assistant_message.tool_calls: self.conversation_history.append({ \u0026#34;role\u0026#34;: \u0026#34;assistant\u0026#34;, \u0026#34;content\u0026#34;: assistant_message.content or \u0026#34;\u0026#34;, \u0026#34;tool_calls\u0026#34;: [ { \u0026#34;id\u0026#34;: tool_call.id, \u0026#34;type\u0026#34;: tool_call.type, \u0026#34;function\u0026#34;: { \u0026#34;name\u0026#34;: tool_call.function.name, \u0026#34;arguments\u0026#34;: tool_call.function.arguments } } for tool_call in assistant_message.tool_calls ] }) for tool_call in assistant_message.tool_calls: if tool_call.function.name == \u0026#34;file_system_operation\u0026#34;: # Parse parameters params = json.loads(tool_call.function.arguments) print(f\u0026#34;Executing command: {params[\u0026#39;action\u0026#39;]} {params.get(\u0026#39;path\u0026#39;, \u0026#39;\u0026#39;)}\u0026#34;) mcp_response = self.call_mcp_server(params) self.conversation_history.append({ \u0026#34;role\u0026#34;: \u0026#34;tool\u0026#34;, \u0026#34;tool_call_id\u0026#34;: tool_call.id, \u0026#34;name\u0026#34;: tool_call.function.name, \u0026#34;content\u0026#34;: json.dumps(mcp_response) }) final_response = self.client.chat.completions.create( model=self.model, messages=[{\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Explain the MCP server results in a human-readable way.\u0026#34;}] + self.conversation_history ) final_message = final_response.choices[0].message.content self.conversation_history.append({ \u0026#34;role\u0026#34;: \u0026#34;assistant\u0026#34;, \u0026#34;content\u0026#34;: final_message }) return final_message else: content = assistant_message.content or \u0026#34;\u0026#34; self.conversation_history.append({ \u0026#34;role\u0026#34;: \u0026#34;assistant\u0026#34;, \u0026#34;content\u0026#34;: content }) return content def main(): parser = argparse.ArgumentParser(description=\u0026#34;MCP AI Assistant\u0026#34;) parser.add_argument(\u0026#34;--model\u0026#34;, default=\u0026#34;gpt-4\u0026#34;, help=\u0026#34;Model to use\u0026#34;) parser.add_argument(\u0026#34;--api-key\u0026#34;, help=\u0026#34;API key (uses environment variable if not specified)\u0026#34;) args = parser.parse_args() try: assistant = MCPAssistant( model=args.model, api_key=args.api_key ) print(\u0026#34;MCP AI Assistant\u0026#34;) print(\u0026#34;===============================\u0026#34;) print(\u0026#34;Operate Linux filesystem using natural language\u0026#34;) print(\u0026#34;Type \u0026#39;exit\u0026#39; or \u0026#39;quit\u0026#39; to end\u0026#34;) print(\u0026#34;Example: \u0026#39;Show files in my home directory\u0026#39;\u0026#34;) print(\u0026#34;===============================\u0026#34;) while True: try: user_input = input(\u0026#34;\\n\u0026gt; \u0026#34;) if user_input.lower() in [\u0026#34;exit\u0026#34;, \u0026#34;quit\u0026#34;]: print(\u0026#34;Exiting.\u0026#34;) break if not user_input.strip(): continue llm_response = assistant.call_llm(user_input) final_response = assistant.process_response(llm_response) print(\u0026#34;\\n▶ Assistant:\u0026#34;) print(final_response) except KeyboardInterrupt: print(\u0026#34;\\nExiting.\u0026#34;) break except Exception as e: print(f\u0026#34;\\nError occurred: {str(e)}\u0026#34;) except Exception as e: print(f\u0026#34;Initialization error: {str(e)}\u0026#34;) sys.exit(1) if __name__ == \u0026#34;__main__\u0026#34;: main() コードの解説 コードを解説します。\nユーザからの入力を受け、LLM に \u0026ldquo;MCP サーバと通信するための JSON\u0026rdquo; を生成してもらい、それを MCP Server へリクエスト。結果を得たら再び LLM に解釈してもらい自然言語の応答を得ます。その結果をユーザに出力して処理は完了です。\n定数定義: MCP_SERVER_URL: MCPサーバーのURLを定義。 MCPAssistantクラス: init: モデル名とAPIキーを初期化。 ファイルシステム操作用のツール（file_system_operation）を定義。 call_llm: ユーザー入力をLLM（GPT-4）に送信し、応答を取得。 ツール（file_system_operation）を利用可能に設定。 call_mcp_server: MCPサーバーにリクエストを送信し、レスポンスを取得。 process_response: LLMの応答を処理。 必要に応じてツール呼び出しを実行し、MCPサーバーと連携。 最終的な応答を生成。 動作確認 では Assistant の動作確認を行っていきます。下記の通り、ユーザの入力はプロンプト越しに自然言語で行います。また最終的な LLM からの応答も自然言語です。\n下記は自然言語で /tmp/foo というディレクトリ配下のファイル一覧を得ています。\n$ python assistant.py =============================== Operate Linux filesystem using natural language Type \u0026#39;exit\u0026#39; or \u0026#39;quit\u0026#39; to end Example: \u0026#39;Show files in my home directory\u0026#39; =============================== \u0026gt; /tmp/foo の下のファイル一覧を出力して Executing command: list /tmp/foo ▶ Assistant: The MCP server has successfully obtained the list of files under `/tmp/foo`. There is one file, named `bar`, with a size of 4 bytes. Its permissions are set to `-rw-r--r--`, which means that the owner can read and write the file, while others can only read it. The last modification date and time is April 6th, 2025 at 11:14:42. This is not a directory. 下記は /tmp/foo/buzz というファイルを内容 \u0026lsquo;buzz\u0026rsquo; として保存するように指示しています。\n\u0026gt; 内容\u0026#39;buzz\u0026#39;として/tmp/foo/buzzというファイルを生成して Executing command: write /tmp/foo/buzz ▶ Assistant: The MCP server was instructed to create a file named \u0026#34;buzz\u0026#34; located in the \u0026#34;/tmp/foo\u0026#34; directory with the content \u0026#34;buzz\u0026#34;. The operation was successful and the file was written without any issues. 下記は上記で保存・生成した /tmp/foo/buzz というファイルを読み込んで内容を確認しています。\n\u0026gt; /tmp/foo/buzzというファイルを読み込んで Executing command: read /tmp/foo/buzz ▶ Assistant: The MCP server was asked to read the file named \u0026#34;buzz\u0026#34; located in the \u0026#34;/tmp/foo\u0026#34; directory. The operation was successful and the content of the file is \u0026#34;buzz\u0026#34;. 考察 Platform エンジニアにおける応用 今回は学習目的だったのでローカルファイルシステムでコードを書き仕組みを理解してきました。\nこれは自分のような Platform 系のエンジニアにも十分応用できる技術だと思っていて、普段使っているプロダクトやサービスを扱う MCP サーバを実装すれば、LLM を介す事で自然言語でそれらの機能を操作することが出来る様になります。\n例えば例を挙げると、\nAWS Athena の MCP Server 実装 S3 上のログを自然言語で分析出来るようになる Datadog Metrics, Logs の MCP Server 実装 Datadog Metrics や Logs の傾向を自然言語で分析出来るようになる これらを通じて\n可用性や信頼性に関わる情報の分析 セキュリティ異常などの分析・検知 などの応用が効くとも思っています。\nたとえば、\nSlack Bot に今回の構成の Assitant の実装をする Lambda で MCP Server を起動する という構成を取れば、自然言語で Slack Bot に「昨日一日間の ALB/WAF ログを確認して、セキュリティとアクセス傾向をレポートして」といった指示も出来るようになると思っています。これは Platform 系のエンジニアにとってもとてもインパクトのある機能だと思っています。\n今後の MCP サーバの開発 当初は自前で MCP サーバを実装するエンジニアが増えると思います。ただこれは MCP サーバの操作先プロダクト・サービス毎に汎用的にも開発出来ると思うので、各プロダクトが公式に MCP Server の実装をして公開するという流れも想定されると思います。実際 2025/04 には AWS が公式で MCP Server を公開したという情報を得ました。 https://awslabs.github.io/mcp/\n","permalink":"https://jedipunkz.github.io/post/2025/04/06/local-filesystem-mcp-server/","summary":"\u003cp\u003e自分は Platform Enabler/SRE として従事しています。また AI 関連のアップデートは2025年に入っても属に更新されています。2025年初頭においては自分たちの分野でも AI 関連の利用に関して様々な模索がある状況だと思われますが、Ahthoropic 社が提唱した MCP (Model Context Protocol) がもたらすインパクトはアプリケーションに限定されずインフラ領域のソフトウェアにも大きなメリットをもたらすと思って観測しています。\u003c/p\u003e\n\u003cp\u003eこの記事では、MCP の概要とどう実装するのかの学習、またどう我々のような Platform Enabler/SRE にとっての活用例があるかを考察していきたいと思っています。\u003c/p\u003e\n\u003ch2 id=\"mcp-の概要\"\u003eMCP の概要\u003c/h2\u003e\n\u003cp\u003eMCP (Model Context Protocol) は、AI モデルと外部システム間のやり取りを効率化するプロトコルで JSON-RPC でやりとりします。ユーザーの自然言語入力を基に、AI アシスタントが MCP サーバーを通じてファイル操作やデータ処理を実行します。\u003c/p\u003e\n\u003cp\u003e最近 OpenAI 社もこの Anthropic 社の MCP をサポートするというニュースが流れ、途端に注目を集める状況になってきました。\u003c/p\u003e\n\u003ch3 id=\"処理の流れ\"\u003e処理の流れ\u003c/h3\u003e\n\u003cp\u003eここはあくまでの一例です。Assistant の実装でいかようにも出来ると思います。\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e+-------------+          +-----------+      +------------+   +--------+\n| User Prompt | \u0026lt;-----\u0026gt;  | Assistant | ---\u0026gt; | MCP Server |   | AI API |\n+-------------+ (1),(5)  +-----------+  (3) +------------+   +--------+\n                               |                                  ^\n                               |             (2),(4)              |\n                               +----------------------------------+\n\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\n\u003cli\u003e(1) ユーザからの入力を Assistant が受け取る\u003c/li\u003e\n\u003cli\u003e(2) Assistant はユーザからの自然言語を LLM に問い合わせ。その際に LLM に外部機能を定義 (JSON-RPC(MCP サーバが受け取る))\u003c/li\u003e\n\u003cli\u003e(3) Aasistant は MCP Server に JSON-RPC でクエリ送信しレスポンスを得る\u003c/li\u003e\n\u003cli\u003e(4) Assistant は MCP Server から得たレスポンスを再び LLM に送信し自然言語としてユーザに返す内容を生成してもらう\u003c/li\u003e\n\u003cli\u003e(5) Assistant はユーザに自然言語で結果を応答する\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"前提\"\u003e前提\u003c/h2\u003e\n\u003cp\u003eMCP 学習を目的にしているので、ここでは話を簡潔にするため Linux Filesystem を操作する MCP Server を書き、理解していきます。\u003c/p\u003e","title":"MCP の理解: Platform Enabler/SRE での活用"},{"content":"jedipunkz です。VPC Lattice が ECS に対応したという情報が https://aws.amazon.com/jp/about-aws/whats-new/2024/11/amazon-vpc-lattice-elastic-container-service/ にあがりました。この対応を Terraform を使って構成して検証してみるのが今回の目的になります。\n今回検証で用いたコード 検証コードは下記に置いておきました。 https://github.com/jedipunkz/vpclattice-ecs-playground\n概要 構成の概要としては下記です。(Mermaid 記表でうまく描けていませんが)\nVPC1, VPC2 に跨る形で VPC Lattice Service Network が配置 VPC2 上の何者か (例で EC2) が VPC1 上の ECS に接続可能 その際は VPC Service Network を介して VPC Lattice Service がエンドポイントとして受ける (うまく描けてない) という事は今まで複数の VPC 間で ECS のエンドポイントを共有しようとすると\nVPC1, VPC2 とで VPC Peering を張る VPC1 上の Private Subnets 上で ALB を構築して ECS Service に接続する という構成が必要でしたが、VPC Lattice を使えばそれらが不要になる、という事です。\n今回検証した構成 今回使った Terraform コードで構築した構成は下記です。各 AWS リソースの関係図になっています。 特徴としては\nECS 構成は Cluster, Service ,Task の通常の構成 ECS Service に ALB ではなく VPC Lattice Target Group が紐づいている VPC Lattice Listner が VPC Lattice Service と VPC Lattice Target Group を紐づけている VPC Lattice Listner はリクエストを VPC Lattice Target Group に転送 なおかつ VPC Lattice Listener にはヘルスチェック機能がある Terraform コードを見て把握する 構成を理解するために各リソースの紐づけの指定を観点にコードを見ていきます。\nECS と VPC Lattice Target Group の紐づけ ECS Service のパラメータです。通常は ALB を接続するパラメータを書くと思うのですが、VPC Lattice の場合は vpc_lattice_configrations となります。\nvpc_lattice_configurations { role_arn = aws_iam_role.ecs_infrastructure.arn target_group_arn = aws_vpclattice_target_group.main.arn port_name = \u0026#34;web-80-tcp\u0026#34; } VPC Lattice Listener の VPC Lattice Service, VPC Lattice Target Group の紐づけ VPC Lattice Listener が VPC Lattice Service と VPC Lattice Target Group を紐づけている記述が下記になります。 つまりエンドポイントである VPC Lattice Service が Listener, VPC Lattice Target Group を介して ECS Service に接続している、という事です。\nresource \u0026#34;aws_vpclattice_listener\u0026#34; \u0026#34;main\u0026#34; { name = \u0026#34;http-80\u0026#34; port = 80 protocol = \u0026#34;HTTP\u0026#34; service_arn = aws_vpclattice_service.main.arn service_identifier = aws_vpclattice_service.main.id default_action { forward { target_groups { target_group_identifier = aws_vpclattice_target_group.main.id weight = 1 } } } } VPC Lattice Service Network と VPC Lattice Service の紐づけ 下記のような記述で VPC Lattice Service Network と VPC Latice Serivce が紐づいています。\nresource \u0026#34;aws_vpclattice_service_network_service_association\u0026#34; \u0026#34;service_association\u0026#34; { service_network_identifier = aws_vpclattice_service_network.main.id service_identifier = aws_vpclattice_service.main.id } VPC と VPC Lattice Service Network の紐づけ VPC と VPC Lattice Service Network が紐づいています。またセキュリティグループが設定されていて、VPC Lattice Service Network への通信が制御できるようになっています。\nresource \u0026#34;aws_vpclattice_service_network_vpc_association\u0026#34; \u0026#34;vpc_association\u0026#34; { security_group_ids = [aws_security_group.lattice_service_network.id] service_network_identifier = aws_vpclattice_service_network.main.id vpc_identifier = module.vpc.vpc_id } 考察 導入メリットの観点 複数の VPC 間で ECS Serivce のエンドポイントを接続出来るのはメリットだと思います。またロードバランサが不要になったことも大きいです。ただご存知のように ALB は内部ノードが拡張することでスケールアウトするのですが VPC Lattice Service も拡張性を備えているのかは気になります。\nパブリックにエンドポイントを出せるのかの観点 また今回は Private Subnet 上で構成しました。確かに VPC 上に EC2 等を起動して VPC Lattice Service のエンドポイントに curl を使ってアクセス出来ましたが、この構成が Public Subnet 上に展開出来るのかはまだ自分は分かっていません。軽く Public Subnet を指定して構築してみましたが外部から curl で VPC Lattice Service に接続出来ませんでした。DNS 的な問題が現れました。また ECS Service は Subnet の指定をしていますが Lattice 関連のリソースはサブネットの指定箇所が無いことに気が付きます。(もしかすると Terraform Document 等呼んでいくと指定箇所があるのかもしれないですが)。ですが VPC に跨る形で配置される Lattice の特徴を考えると出来ない気もします。このあたりは引き続き調査します。\n備考 aws コンソールの問題 ちょうど先週発表された機能なので仕方ないのですが現時点では aws コンソールの機能に不十分な点が見受けられました (2024/11時点)\nECS Service の VPC Lattice の設定画面で VPC を選択する事ができるがセキュリティグループを選択できないので default セキュリティグループになってしまう ECS Service の VPC Lattice の設定画面で一度設定した Lattice Target Group を削除して保存。その後、同じ画面で既存の Lattice Target Group を選択出来ないので新たに作らなくてはいけない Terraform 検証にあたり注意点 手順を間違うと eni が削除出来ない問題に遭遇すると思います。terraform destroy すると下記のリソースが主に残ります。おそらく Terraform AWS Provider の不具合な気はしていますが確認したところ Issue には問題がありませんでした。新しい不具合な可能性もあります。この時点で aws コンソールで順にリソースを削除しようと考えると思うのですが ECS Cluster を削除しても ECS Service の Attacment? リソースが残り、結果 eni が消せません。eni が消せないので IGW, Subnet, VPC も消せない、という状況になります。\nVPC Subnet IGW ECS Cluster ECS Service ","permalink":"https://jedipunkz.github.io/post/vpclattice-ecs/","summary":"\u003cp\u003ejedipunkz です。VPC Lattice が ECS に対応したという情報が \u003ca href=\"https://aws.amazon.com/jp/about-aws/whats-new/2024/11/amazon-vpc-lattice-elastic-container-service/\"\u003ehttps://aws.amazon.com/jp/about-aws/whats-new/2024/11/amazon-vpc-lattice-elastic-container-service/\u003c/a\u003e にあがりました。この対応を Terraform を使って構成して検証してみるのが今回の目的になります。\u003c/p\u003e\n\u003ch2 id=\"今回検証で用いたコード\"\u003e今回検証で用いたコード\u003c/h2\u003e\n\u003cp\u003e検証コードは下記に置いておきました。\n\u003ca href=\"https://github.com/jedipunkz/vpclattice-ecs-playground\"\u003ehttps://github.com/jedipunkz/vpclattice-ecs-playground\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"概要\"\u003e概要\u003c/h2\u003e\n\u003cp\u003e構成の概要としては下記です。(Mermaid 記表でうまく描けていませんが)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eVPC1, VPC2 に跨る形で VPC Lattice Service Network が配置\u003c/li\u003e\n\u003cli\u003eVPC2 上の何者か (例で EC2) が VPC1 上の ECS に接続可能\u003c/li\u003e\n\u003cli\u003eその際は VPC Service Network を介して VPC Lattice Service がエンドポイントとして受ける (うまく描けてない)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eという事は今まで複数の VPC 間で ECS のエンドポイントを共有しようとすると\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eVPC1, VPC2 とで VPC Peering を張る\u003c/li\u003e\n\u003cli\u003eVPC1 上の Private Subnets 上で ALB を構築して ECS Service に接続する\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eという構成が必要でしたが、VPC Lattice を使えばそれらが不要になる、という事です。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"VPC Lattice Overview\" loading=\"lazy\" src=\"../../pix/vpclattice-overview.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"今回検証した構成\"\u003e今回検証した構成\u003c/h2\u003e\n\u003cp\u003e今回使った Terraform コードで構築した構成は下記です。各 AWS リソースの関係図になっています。\n特徴としては\u003c/p\u003e","title":"VPC Lattice + ECS 構成を Terraform を通して理解"},{"content":"OpenTelemetry を使って AWS (X-Ray, Cloudwatch Logs) にトレースとログを計装する事に興味があったので調べた内容を記そうと思います。\n構成 今回検証してみた構成は下記の様な構成です。AWS を用いた場合 ECS や EKS, Lambda で Go アプリを起動する事が通常ですが、今回は docker-compose で検証しました。ただ ECS, EKS に置き換えるのは比較的簡単だと思います。\ntrace post PutTelemetryRecords +--------+ +----------------+ +-----------------+ | Go App | -+-\u0026gt; | Otel Collector | ---\u0026gt; | AWS X-Ray | +--------+ | +----------------+ +-----------------+ | +----------------+ +-----------------+ +-\u0026gt; | Fluent-Bit | ---\u0026gt; | Cloudwatch Logs | +----------------+ +-----------------+ PutLogEvents ログとトレースの紐づけ ログは Cloudwatch Logs へ、トレース情報は AWS X-Ray へ転送しますが、このログとトレースを紐付けると、運用する上で追跡が容易になります。この紐づけは AWS の場合は簡単で下記の要件を満たせば紐づけがされます。\nトレース ID は Otel 形式から X-Ray 形式に変換して (下記 IdOtel2Xray() 関数) トレース送信 (参考: https://zenn.dev/k6s4i53rx/articles/69ef65b84dd799) トレース ID をログに埋め込む (下記 log.Printf() ) OpenTelemetry Exporter が初期化の際に毎回用いられる OpenTelemetry リソース作成の際にリソース属性として Cloudwatch Log Group を指定 Go アプリ 早速ですがログとトレース情報を紐づけしつつ AWS に送信する Go アプリのコードの例を記します。このアプリはサイドカーで起動している otel-collector の 4317 ポートに送信し、その otel-collector は AWS X-Ray にトレースを送信。更にログは追加でサイドカー起動している fluent-bit を使って AWS Cloudwatch に送信する想定でコードを書いています。\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;os\u0026#34; \u0026#34;go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp\u0026#34; \u0026#34;go.opentelemetry.io/otel\u0026#34; \u0026#34;go.opentelemetry.io/otel/exporters/otlp/otlptrace\u0026#34; \u0026#34;go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracegrpc\u0026#34; \u0026#34;go.opentelemetry.io/otel/propagation\u0026#34; \u0026#34;go.opentelemetry.io/otel/sdk/resource\u0026#34; \u0026#34;go.opentelemetry.io/otel/sdk/trace\u0026#34; semconv \u0026#34;go.opentelemetry.io/otel/semconv/v1.10.0\u0026#34; ) func initTracer() { ctx := context.Background() collectorAddress := os.Getenv(\u0026#34;OTEL_COLLECTOR_ADDRESS\u0026#34;) if collectorAddress == \u0026#34;\u0026#34; { collectorAddress = \u0026#34;otel-collector:4317\u0026#34; } // gRPCを使用してOpenTelemetry Collectorと通信するクライアントを作成します traceClient := otlptracegrpc.NewClient( otlptracegrpc.WithEndpoint(collectorAddress), otlptracegrpc.WithInsecure(), ) // 作成したクライアントを使用して // トレースデータをエクスポートするエクスポーターを作成します traceExporter, err := otlptrace.New(ctx, traceClient) if err != nil { log.Fatalf(\u0026#34;Failed to create trace exporter: %v\u0026#34;, err) } // 新しいTracerProviderを作成します // これはトレースデータを生成するTracerを作成するためのものです tp := trace.NewTracerProvider( trace.WithBatcher(traceExporter), trace.WithResource(newResource()), ) // 作成したTracerProviderをグローバルなTracerProviderとして設定します otel.SetTracerProvider(tp) // TraceContextを使用したPropagatorを設定します。 // これはトレースコンテキストを伝播させるためのものです。 otel.SetTextMapPropagator(propagation.TraceContext{}) } func main() { initTracer() // http ハンドラ http.Handle(\u0026#34;/\u0026#34;, otelhttp.NewHandler(http.HandlerFunc(home), \u0026#34;Home\u0026#34;)) log.Println(\u0026#34;Server is running on port 8080...\u0026#34;) log.Fatal(http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil)) } func home(w http.ResponseWriter, r *http.Request) { // Span の設定 ctx := r.Context() tr := otel.GetTracerProvider().Tracer(\u0026#34;example\u0026#34;) _, span := tr.Start(ctx, \u0026#34;about\u0026#34;) defer span.End() // Otel 形式の ID を X-Ray 形式に変換 traceID := IdOtel2Xray(span.SpanContext().TraceID().String()) log.Printf(\u0026#34;TraceID: %s, About us page\u0026#34;, traceID) w.Write([]byte(\u0026#34;Welcome to the Home Page!\u0026#34;)) } // OpenTelemetry Exporter が初期化の際に毎回用いられる OpenTelemetry リソース作成 // の際にリソース属性として Cloudwatch Log Group を指定 func newResource() *resource.Resource { LogGroupNames := []string{\u0026#34;otel-test\u0026#34;} return resource.NewWithAttributes( semconv.SchemaURL, semconv.AWSLogGroupNamesKey.StringSlice(LogGroupNames), ) } func IdOtel2Xray(OtelId string) string { xrayId := \u0026#34;1-\u0026#34; + OtelId[0:8] + \u0026#34;-\u0026#34; + OtelId[8:] return xrayId } Docker 関連ファイル 下記が docker-compose.yaml です。前述した通り otel-collector と fluent-bit をサイドカーとして起動しています。また AWS に情報を送信するためその両者では AWS 認証を行っています。\nversion: \u0026#39;3.8\u0026#39; services: go-app: build: . ports: - \u0026#34;8080:8080\u0026#34; environment: - OTEL_COLLECTOR_ADDRESS=otel-collector:4317 depends_on: - otel-collector otel-collector: image: public.ecr.aws/aws-observability/aws-otel-collector:latest command: [\u0026#34;--config=/etc/otel-agent-config.yaml\u0026#34;] volumes: - ./otel-agent-config.yaml:/etc/otel-agent-config.yaml environment: - AWS_ACCESS_KEY_ID=**** - AWS_SECRET_ACCESS_KEY=***** - AWS_REGION=ap-northeast-1 ports: - \u0026#34;4317:4317\u0026#34; fluent-bit: image: fluent/fluent-bit:latest volumes: - /var/lib/docker/containers:/var/lib/docker/containers - /var/run/docker.sock:/var/run/docker.sock - ./fluent-bit.conf:/fluent-bit/etc/fluent-bit.conf environment: - AWS_ACCESS_KEY_ID=**** - AWS_SECRET_ACCESS_KEY=**** - AWS_REGION=ap-northeast-1 下記は Dockerfile です。\nFROM golang:1.21 AS builder WORKDIR /app COPY go.mod go.sum ./ RUN go mod download COPY . . RUN CGO_ENABLED=0 go build -o myapp . FROM alpine:latest WORKDIR /root/ COPY --from=builder /app/myapp . CMD [\u0026#34;./myapp\u0026#34;] fluent-bit ファイル 下記は fluent-bit.conf です。ここで otel-test という Cloudwatch Log Group を指定していますが、これは予め AWS Console で作成しておきます。\n[SERVICE] Flush 1 Daemon Off Log_Level info Parsers_File parsers.conf [INPUT] Name tail Tag docker.* Path /var/lib/docker/containers/*/*.log Parser docker DB /var/log/flb_docker.db Mem_Buf_Limit 5MB Skip_Long_Lines On Refresh_Interval 10 [OUTPUT] Name cloudwatch_logs Match * region ap-northeast-1 log_group_name otel-test log_stream_prefix go-app- auto_create_group On OpenTelemetry ファイル 下記は OpenTelemetry が参照する otel-agent-config.yaml ファイルです。\nReciever として otlp を指定、Exporter として X-Ray を指定。つまり Go アプリからのトレース情報を otel-collector が受け取り AWS X-Ray へ転送する役割を担っています。\nreceivers: otlp: protocols: grpc: http: processors: batch: exporters: awsxray: region: \u0026#39;ap-northeast-1\u0026#39; service: telemetry: logs: level: \u0026#34;DEBUG\u0026#34; pipelines: traces: receivers: - otlp exporters: - awsxray 動作確認 ブラウザで http://localhost:8080/ へアクセスする。結果として AWS Console の X-Ray の画面を確認すると下記のようにトレース情報が出力されそこからログが追跡出来る事が分かります。\nまとめ 今回の Span 設定は http.HandlerFun(home) の home() 関数の中で行いました。よってトレース情報は単純な直線情報となっています。\nfunc home(w http.ResponseWriter, r *http.Request) { ctx := r.Context() tr := otel.GetTracerProvider().Tracer(\u0026#34;example\u0026#34;) _, span := tr.Start(ctx, \u0026#34;about\u0026#34;) defer span.End() traceID := IdOtel2Xray(span.SpanContext().TraceID().String()) log.Printf(\u0026#34;TraceID: %s, About us page\u0026#34;, traceID) w.Write([]byte(\u0026#34;Welcome to the Home Page!\u0026#34;)) } 通常は http リクエストの処理、DB 書き込み、メモリサーバ読み込み等の処理ごとに設定すべきです。これらの処理ごとに Span を記すことで、各処理ごとに要している時間やログを追跡することが可能になります。\n","permalink":"https://jedipunkz.github.io/post/opentelemetry-aws/","summary":"\u003cp\u003eOpenTelemetry を使って AWS (X-Ray, Cloudwatch Logs) にトレースとログを計装する事に興味があったので調べた内容を記そうと思います。\u003c/p\u003e\n\u003ch2 id=\"構成\"\u003e構成\u003c/h2\u003e\n\u003cp\u003e今回検証してみた構成は下記の様な構成です。AWS を用いた場合 ECS や EKS, Lambda で Go アプリを起動する事が通常ですが、今回は docker-compose で検証しました。ただ ECS, EKS に置き換えるのは比較的簡単だと思います。\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e        trace post          PutTelemetryRecords\n+--------+      +----------------+      +-----------------+\n| Go App | -+-\u0026gt; | Otel Collector | ---\u0026gt; |    AWS X-Ray    |\n+--------+  |   +----------------+      +-----------------+\n            |   +----------------+      +-----------------+\n            +-\u0026gt; |   Fluent-Bit   | ---\u0026gt; | Cloudwatch Logs |\n                +----------------+      +-----------------+\n                                PutLogEvents\n\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"ログとトレースの紐づけ\"\u003eログとトレースの紐づけ\u003c/h2\u003e\n\u003cp\u003eログは Cloudwatch Logs へ、トレース情報は AWS X-Ray へ転送しますが、このログとトレースを紐付けると、運用する上で追跡が容易になります。この紐づけは AWS の場合は簡単で下記の要件を満たせば紐づけがされます。\u003c/p\u003e","title":"Go, OpenTelemetry で AWS にログ・トレースを計装してみる"},{"content":"自宅のルータについても可観測性を向上して普段の運用に役立てています。例えば長期スパンでのネットワーク通信料の推移や CPU, Mem 使用率、あとハードウェアの温度の推移などを観測しています。\n今までは Prometheus の Node Exporter を使ってホストの情報を Prometheus Server に提供していたのですが、自分で Go で Prometheus Exporter を書いて運用するにようになったので、それについてまとめます。\nGrafana の可視化情報 下記が可視化された情報です。CPU, Mem やネットワーク送信量、またハードウェアの温度を可視化して運用しています。\nソース置き場 結論になりますが下記にソースを置いています。\nhttps://github.com/jedipunkz/linux-tiny-exporter\nネットワーク送信・受信メトリクスを説明 実際にはこのコードでは CPU 使用率, Memory 使用率, Disk IO, Network トラヒック, ハードウェア温度を取得・提供しているのですが、ここでは例としてネットワークトラヒックに関するメトリクスを Prometheus Server に提供するコードを説明しようと思います。\nパッケージのインポート Prometheus のクライアントライブラリから2つのパッケージをインポートしています。\n\u0026ldquo;github.com/prometheus/client_golang/prometheus\u0026rdquo;: これは Prometheus の基本的なクライアントライブラリで、メトリクスを定義、収集、エクスポートするための機能を提供します。\n\u0026ldquo;github.com/prometheus/client_golang/prometheus/promhttp\u0026rdquo;: これは Prometheus の HTTP サーバーとクライアントのためのライブラリで、HTTP 経由でメトリクスを公開するためのハンドラを提供します。\n\u0026#34;github.com/prometheus/client_golang/prometheus\u0026#34; \u0026#34;github.com/prometheus/client_golang/prometheus/promhttp\u0026#34; ネットワークトラヒックに関する構造体定義 ここからは Internal Packege のコード解説です。\nNetCollector という構造体が定義しています。この構造体は、ネットワークインターフェースごとの受信バイト数、送信バイト数、受信パケット数、送信パケット数の差分を保持します。~Diff はそれぞれの値の差分を保持します。前回のスクレイプ（データ収集）からの変化を表します。\ntype NetCollector struct { receivedBytesDiff *prometheus.Desc transmitBytesDiff *prometheus.Desc receivedPacketsDiff *prometheus.Desc transmitPacketsDiff *prometheus.Desc lastReceivedBytes map[string]float64 lastTransmitBytes map[string]float64 lastReceivedPackets map[string]float64 lastTransmitPackets map[string]float64 } コンストラクタ NewNetCollector 関数は、新しい NetCollector インスタンスを作成します。この関数では、各メトリクスの差分を表す prometheus.Desc オブジェクトを作成し、それらを NetCollector 構造体の対応するフィールドに設定します。また、前回のスクレイプ時の各メトリクスの値を保持するマップも作成します。\nfunc NewNetCollector() *NetCollector { return \u0026amp;NetCollector{ receivedBytesDiff: prometheus.NewDesc(\u0026#34;net_received_bytes_diff\u0026#34;, \u0026#34;Received bytes by network interface since the last scrape.\u0026#34;, []string{\u0026#34;interface\u0026#34;}, nil, ), transmitBytesDiff: prometheus.NewDesc(\u0026#34;net_transmit_bytes_diff\u0026#34;, \u0026#34;Transmitted bytes by network interface since the last scrape.\u0026#34;, []string{\u0026#34;interface\u0026#34;}, nil, ), receivedPacketsDiff: prometheus.NewDesc(\u0026#34;net_received_packets_diff\u0026#34;, \u0026#34;Received packets by network interface since the last scrape.\u0026#34;, []string{\u0026#34;interface\u0026#34;}, nil, ), transmitPacketsDiff: prometheus.NewDesc(\u0026#34;net_transmit_packets_diff\u0026#34;, \u0026#34;Transmitted packets by network interface since the last scrape.\u0026#34;, []string{\u0026#34;interface\u0026#34;}, nil, ), lastReceivedBytes: make(map[string]float64), lastTransmitBytes: make(map[string]float64), lastReceivedPackets: make(map[string]float64), lastTransmitPackets: make(map[string]float64), } } メトリクスを Prometheus に通知する関数 Describe メソッドは、エクスポーターが提供するメトリクスのメタデータを Prometheus に通知するためのものです。このメソッドは、メトリクスの Desc を受け取るチャネルを引数に取ります。エクスポーターは、このチャネルに対して自身が提供するすべてのメトリクスの Desc を送信します。\nfunc (collector *NetCollector) Describe(ch chan\u0026lt;- *prometheus.Desc) { ch \u0026lt;- collector.receivedBytesDiff ch \u0026lt;- collector.transmitBytesDiff ch \u0026lt;- collector.receivedPacketsDiff ch \u0026lt;- collector.transmitPacketsDiff } メトリクス収集関数 Collect メソッドは、Prometheus がメトリクスを収集する際に呼び出されます。このメソッドは、メトリクスの値を受け取るチャネルを引数に取ります。エクスポーターは、このチャネルに対して自身が提供するすべてのメトリクスの値を送信します。\nこの Collect メソッドでは、次の処理が行われています：\ngetNetDevice 関数を呼び出してネットワークデバイスの情報を取得します。 取得したデータを行ごとに分割し各行をループ 各フィールドからネットワークインターフェースの名前と、受信バイト数、送信バイト数、受信パケット数、送信パケット数を取得します。 前回のスクレイプ時の各メトリクスの値と比較して差分を計算し、それを Prometheus のメトリクスとしてチャネルに送信します。 最後に、今回のスクレイプ時の各メトリクスの値を保存します。これは次回のスクレイプ時に前回の値として使用されます。 func (collector *NetCollector) Collect(ch chan\u0026lt;- prometheus.Metric) { data, err := getNetDevice() if err != nil { return } lines := strings.Split(data, \u0026#34;\\n\u0026#34;) for i, line := range lines { if i \u0026lt; 2 { continue } fields := strings.Fields(line) if len(fields) \u0026lt; 10 { continue } interfaceName := strings.Trim(fields[0], \u0026#34;:\u0026#34;) receivedBytes, _ := strconv.ParseFloat(fields[1], 64) transmitBytes, _ := strconv.ParseFloat(fields[9], 64) receivedPackets, _ := strconv.ParseFloat(fields[2], 64) transmitPackets, _ := strconv.ParseFloat(fields[10], 64) if lastReceivedBytes, ok := collector.lastReceivedBytes[interfaceName]; ok { receivedBytesDiff := receivedBytes - lastReceivedBytes ch \u0026lt;- prometheus.MustNewConstMetric(collector.receivedBytesDiff, prometheus. GaugeValue, receivedBytesDiff, interfaceName) } if lastTransmitBytes, ok := collector.lastTransmitBytes[interfaceName]; ok { transmitBytesDiff := transmitBytes - lastTransmitBytes ch \u0026lt;- prometheus.MustNewConstMetric(collector.transmitBytesDiff, prometheus. GaugeValue, transmitBytesDiff, interfaceName) } if lastReceivedPackets, ok := collector.lastReceivedPackets[interfaceName]; ok { receivedPacketsDiff := receivedPackets - lastReceivedPackets ch \u0026lt;- prometheus.MustNewConstMetric(collector.receivedPacketsDiff, prometheu s.GaugeValue, receivedPacketsDiff, interfaceName) } if lastTransmitPackets, ok := collector.lastTransmitPackets[interfaceName]; ok { transmitPacketsDiff := transmitPackets - lastTransmitPackets ch \u0026lt;- prometheus.MustNewConstMetric(collector.transmitPacketsDiff, prometheu s.GaugeValue, transmitPacketsDiff, interfaceName) } collector.lastReceivedBytes[interfaceName] = receivedBytes collector.lastTransmitBytes[interfaceName] = transmitBytes collector.lastReceivedPackets[interfaceName] = receivedPackets collector.lastTransmitPackets[interfaceName] = transmitPackets } } デバイス情報取得関数 最後は getNetDevice 関数は Collect 関数で利用されている Linuxシステムの/proc/net/devファイルを読み込み、その内容を文字列として返すものです。\n/proc/net/dev ファイルは、システムのネットワークインターフェースに関する情報を提供します。各行は、一つのネットワークインターフェースに対応し、そのインターフェースの受信バイト数、送信バイト数、受信パケット数、送信パケット数などの情報を含みます。\nfunc getNetDevice() (string, error) { file, err := os.Open(\u0026#34;/proc/net/dev\u0026#34;) if err != nil { return \u0026#34;\u0026#34;, err } defer file.Close() var sb strings.Builder _, err = io.Copy(\u0026amp;sb, file) if err != nil { return \u0026#34;\u0026#34;, err } return sb.String(), nil } まとめ 高機能な Node Exporter を使えばその他の情報も細やかに取得・提供出来るのですが、自前で開発した Exporter を使って運用するところに趣があるように思っています。Go であれば Prometheus Exporter の開発は比較的簡単な事が分かったと思います。また、今回は Node Exporter でも提供される様な情報をエキスポートしましたが、Linux から取得できる情報以外の Prometheus Exporter も開発出来る事も想像出来ると思います。\n","permalink":"https://jedipunkz.github.io/post/linux-tiny-exporter/","summary":"\u003cp\u003e自宅のルータについても可観測性を向上して普段の運用に役立てています。例えば長期スパンでのネットワーク通信料の推移や CPU, Mem 使用率、あとハードウェアの温度の推移などを観測しています。\u003c/p\u003e\n\u003cp\u003e今までは Prometheus の Node Exporter を使ってホストの情報を Prometheus Server に提供していたのですが、自分で Go で Prometheus Exporter を書いて運用するにようになったので、それについてまとめます。\u003c/p\u003e\n\u003ch2 id=\"grafana-の可視化情報\"\u003eGrafana の可視化情報\u003c/h2\u003e\n\u003cp\u003e下記が可視化された情報です。CPU, Mem やネットワーク送信量、またハードウェアの温度を可視化して運用しています。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"grafana1\" loading=\"lazy\" src=\"../../pix/linux-tiny-exporter-1.png\"\u003e\n\u003cimg alt=\"grafana2\" loading=\"lazy\" src=\"../../pix/linux-tiny-exporter-2.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"ソース置き場\"\u003eソース置き場\u003c/h2\u003e\n\u003cp\u003e結論になりますが下記にソースを置いています。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/jedipunkz/linux-tiny-exporter\"\u003ehttps://github.com/jedipunkz/linux-tiny-exporter\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"ネットワーク送信受信メトリクスを説明\"\u003eネットワーク送信・受信メトリクスを説明\u003c/h2\u003e\n\u003cp\u003e実際にはこのコードでは CPU 使用率, Memory 使用率, Disk IO, Network トラヒック, ハードウェア温度を取得・提供しているのですが、ここでは例としてネットワークトラヒックに関するメトリクスを Prometheus Server に提供するコードを説明しようと思います。\u003c/p\u003e\n\u003ch3 id=\"パッケージのインポート\"\u003eパッケージのインポート\u003c/h3\u003e\n\u003cp\u003ePrometheus のクライアントライブラリから2つのパッケージをインポートしています。\u003c/p\u003e\n\u003cp\u003e\u0026ldquo;github.com/prometheus/client_golang/prometheus\u0026rdquo;: これは Prometheus の基本的なクライアントライブラリで、メトリクスを定義、収集、エクスポートするための機能を提供します。\u003c/p\u003e\n\u003cp\u003e\u0026ldquo;github.com/prometheus/client_golang/prometheus/promhttp\u0026rdquo;: これは Prometheus の HTTP サーバーとクライアントのためのライブラリで、HTTP 経由でメトリクスを公開するためのハンドラを提供します。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;github.com/prometheus/client_golang/prometheus\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;github.com/prometheus/client_golang/prometheus/promhttp\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"ネットワークトラヒックに関する構造体定義\"\u003eネットワークトラヒックに関する構造体定義\u003c/h3\u003e\n\u003cp\u003eここからは Internal Packege のコード解説です。\u003c/p\u003e\n\u003cp\u003eNetCollector という構造体が定義しています。この構造体は、ネットワークインターフェースごとの受信バイト数、送信バイト数、受信パケット数、送信パケット数の差分を保持します。~Diff はそれぞれの値の差分を保持します。前回のスクレイプ（データ収集）からの変化を表します。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003etype\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eNetCollector\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003estruct\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#a6e22e\"\u003ereceivedBytesDiff\u003c/span\u003e   \u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003eprometheus\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eDesc\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#a6e22e\"\u003etransmitBytesDiff\u003c/span\u003e   \u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003eprometheus\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eDesc\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#a6e22e\"\u003ereceivedPacketsDiff\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003eprometheus\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eDesc\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#a6e22e\"\u003etransmitPacketsDiff\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003eprometheus\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eDesc\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#a6e22e\"\u003elastReceivedBytes\u003c/span\u003e   \u003cspan style=\"color:#66d9ef\"\u003emap\u003c/span\u003e[\u003cspan style=\"color:#66d9ef\"\u003estring\u003c/span\u003e]\u003cspan style=\"color:#66d9ef\"\u003efloat64\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#a6e22e\"\u003elastTransmitBytes\u003c/span\u003e   \u003cspan style=\"color:#66d9ef\"\u003emap\u003c/span\u003e[\u003cspan style=\"color:#66d9ef\"\u003estring\u003c/span\u003e]\u003cspan style=\"color:#66d9ef\"\u003efloat64\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#a6e22e\"\u003elastReceivedPackets\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003emap\u003c/span\u003e[\u003cspan style=\"color:#66d9ef\"\u003estring\u003c/span\u003e]\u003cspan style=\"color:#66d9ef\"\u003efloat64\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#a6e22e\"\u003elastTransmitPackets\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003emap\u003c/span\u003e[\u003cspan style=\"color:#66d9ef\"\u003estring\u003c/span\u003e]\u003cspan style=\"color:#66d9ef\"\u003efloat64\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"コンストラクタ\"\u003eコンストラクタ\u003c/h3\u003e\n\u003cp\u003eNewNetCollector 関数は、新しい NetCollector インスタンスを作成します。この関数では、各メトリクスの差分を表す prometheus.Desc オブジェクトを作成し、それらを NetCollector 構造体の対応するフィールドに設定します。また、前回のスクレイプ時の各メトリクスの値を保持するマップも作成します。\u003c/p\u003e","title":"自前開発した Prometheus Exporter で自宅ルータのメトリクス監視運用している話"},{"content":"@jedipunkz です。今まで自宅では PPPoE 接続をして Global IPv4 アドレスを取得して自宅にマイクラサーバーを外部公開しその接続を使って各端末でオンラインゲームやインターネットをしていました。ただ、IPoE 接続すると混雑時間を回避出来ると聞いていたので (これについては後術します)、普段のゲームや各端末のインターネット接続は IPoE 接続を利用しマイクラサーバーは PPPoE で公開、と出来ないかと考えました。IPoE は IPv4 over IPv6 のトンネリング接続するたため IPoE だけでは IPv4 Global IP アドレスによるサーバー公開は不可能だからです。\nここでは VyOS を使ってその両者を満たす接続の設定方法を記します。\n要件 PPPoE して Global IPv4 を取得しサーバーを外部に提供 (その際に Dynamic DNS を利用 (自宅は Cloudflare)) サーバー以外の端末のトラヒックは IPoE 接続した経路に流す 回線はNTT フレッツ想定 (自分の場合は Asahi-net, IPv6 接続オプションあり, その他でも可) ISO イメージをビルド VyOS は Stable のバージョンは有償バージョンでないとダウンロード出来ませんがビルドすれば ISO イメージが作れます。その方法を記します。\n2023/12 時点で最新の Stable バージョン 1.3.4 を指定 適当な Linux 端末でビルドしましたが Docker を利用できればどこでも良さそう git clone -b equuleus https://github.com/vyos/vyos-build cd vyos-build/ docker run --rm -it --privileged -v $(pwd):/vyos -w /vyos vyos/vyos-build:equuleus bash ./configure --architecture amd64 --build-type release --version 1.3.4 sudo make iso ビルドが完了すると build/ ディレクトリ配下に iso イメージが出来上がっているはずです。\n構成 下記のように VyOS マシンには NIC が2つ以上あれば要件満たせます。 10.0.1.254/32 10.0.1.100/32 +--------------+ IPoE +-----------------------+------+ +---------------+ | the internet | -+---- | eth0(IPv6) \u0026lt;- Tunnel -| | --- | IPv4 server | +--------------+ | +-----------------------| eth1 + +---------------+ +---- | eth0(IPv4) \u0026lt;- NAT - | | -+ PPPoE +-----------------------+------+ | +---------------+ | VyOS Router | +- | Other Clients | +------------------------------+ +---------------+ 10.0.1.0/24 手順 eth1 のネットワークアドレス設定 set interface ethernet eth1 address 10.0.1.254 IPoE 接続 eth0 側で IPv6 の IP アドレスを受け取ります。\nset interface ethernet eth0 address dhcpv6 set interface ethernet eth0 dhcpv6-options parameters-only set interface ethernet eth0 ipv6 address autoconf set interface ethernet eth0 ipv6 disable-forwarding commit この時点で数分待つと IPv6 アドレスが取得できます。\nrun show int Codes: S - State, L - Link, u - Up, D - Down, A - Admin Down Interface IP Address S/L Description --------- ---------- --- ----------- eth0 2405:xx:xx:xx:xx:xx:xx:xxx/64 自宅の場合は Asahi-net というプロバイダです。v6 connect という IPv6 接続オプションなのですが、終端の情報は公開していないとサポートから聞きました。よってここにもアドレスを記すのは問題だと思うので記しません。ただ参考資料だけ掲載しておきます。\nhttps://scrapbox.io/for2ando/Asahi%E3%83%8D%E3%83%83%E3%83%88DS-Lite%E3%81%AEAFTR%E3%82%A2%E3%83%89%E3%83%AC%E3%82%B9%E3%82%92%E8%AA%BF%E3%81%B9%E3%82%8bash https://gist.github.com/stkchp/4daea9158439c32d7a70a255d51e568b また DS-Lite で transix の場合は dig @2404:1a8:7f01:b::3 gw.transix.jp aaaa +short で引けるアドレスがそれになるそうです。\nset interfaces tunnel tun0 encapsulation ipip6 set interfaces tunnel tun0 multicast disable set interfaces tunnel tun0 remote 2001:xxx:x:xxx:xx #接続先終端 set interfaces tunnel tun0 source-address 2405:xx:xx:xx:xx:xx:xx:xxx/64 #上記手順で取得 set protocols static interface-route 0.0.0.0/0 next-hop-interface tun0 commit この時点で IPv6 でも IPv4 (IPv4 over IPv6) でもインターネットに接続できるはずです。\nIPoE 接続を利用した宅内端末のインターネット接続 eth1 側に接続した各端末が IPv4 でインターネットに接続できるよう NAT の設定を行います。\nset nat source rule 100 description \u0026#39;LAN Source NAT\u0026#39; set nat source rule 100 outbound-interface tun0 set nat source rule 100 source address 10.0.1.0/24 set nat source rule 100 translation address masquerade commit 宅内の各端末に IPv4 アドレスを DHCP で配布 show service dhcp-server shared-network-name HOME-DHCP name-server 8.8.8.8 show service dhcp-server shared-network-name HOME-DHCP name-server 8.8.4.4 show service dhcp-server shared-network-name HOME-DHCP subnet 10.0.1.0/24 default-router 10.0.1.254 show service dhcp-server shared-network-name HOME-DHCP subnet 10.0.1.0/24 range dhcp1-subnet start 10.0.1.100 show service dhcp-server shared-network-name HOME-DHCP subnet 10.0.1.0/24 range dhcp1-subnet end 10.0.1.190 この時点で宅内の各端末が IPv4 でインターネットに接続出来ます。\nルータ再起動時の対応 DHCPv6 で取得した IP アドレスは固定ではないので、ルータの再起動時に不整合が置きます。その際の対処方法については下記の記事が参考になりますので記しておきます。(情報提供ありがとうございます)\n参考: https://zenn.dev/chattytak/articles/54f99d218300b8\nPPPoE 接続して IPv4 Server 公開 PPPoE 接続を行います。そして IPv4 Server を外部に公開する手順です。\nまず PPPoE 接続します。プロバイダから提供されている PPPoE 用のユーザ名・パスワードを用います。MTU 調整もしておきます。\nset interface pppoe pppoe0 authentication user xxxxx set interface pppoe pppoe0 authentication password xxxxxxx set inetrface pppoe pppoe0 source-interface eth0 set inetrface pppoe pppoe0 mtu 1454 外部公開する IPv4 Server は PPPoE 接続経路からインターネットに接続できるよう Source NAT を設定します。\nset nat source rule 110 description \u0026#39;Server Source NAT\u0026#39; set nat source rule 110 outbound-interface pppoe0 set nat source rule 110 source address 10.0.1.100/32 set nat source rule 110 translation address masquerade Destination NAT で IPv4 Server を外部公開します。\nset nat destination rule 19132 description \u0026#39;Minecraft\u0026#39; set nat destination rule 19132 destination port 19132 #公開するサービス次第 set nat destination rule 19132 inbound-interface pppoe0 set nat destination rule 19132 protocol udp #公開するサービス次第 set nat destination rule 19132 translation address 10.0.1.100 set nat destination rule 19132 translation port 19132 #公開するサービス次第 同様の目的で Policy Based Routing を書きます。ついでにフレッツ接続用のポリシも追加します。\nset policy route PBR rule 10 description \u0026#39;ipv4 server traffic via pppoe\u0026#39; set policy route PBR rule 10 set table 100 set policy route PBR rule 10 source address 10.0.1.100/32 set policy route PBR rule 20 description \u0026#39;FLETS\u0026#39; set policy route PBR rule 20 destination address 0.0.0.0/0 set policy route PBR rule 20 protocol tcp set policy route PBR rule 20 set tcp-mss 1414 set policy route PBR rule 20 tcp flags SYN,!ACK,!FIN,!RST 作成した Policy Based Route を interface にアタッチします。\nset interface ethernet eth1 policy PBR Dynamic DNS して IPv4 Server の DNS 名付与 ここはおまけですが、自分の場合 Cloudflare でドメイン名を取得したのでそのドメインを利用して IPv4 Server に FQDN を付与しています。もちろん固定 IPv4 オプションを契約していればこの手順は不要で、単にホスト名を DNS 解決すれば OK です。\nここでは下記を前提として手順を記します。\n取得したドメイン名は foo.com IPv4 Server のホスト名は bar set service dns dyamic interface pppoe0 service cloudflare host-name bar.foo.com set service dns dyamic interface pppoe0 service cloudflare login \u0026lt;email address\u0026gt; set service dns dyamic interface pppoe0 service cloudflare password \u0026lt;global api key\u0026gt; set service dns dyamic interface pppoe0 service cloudflare protocol cloudflare set service dns dyamic interface pppoe0 service cloudflare zone foo.com commit この状態で下記の操作で状態を確認します。\nrun show dns dynamic status ip address : 14.x.x.x host-name : bar.foo.com last update : 2023-12-DD 07:16:59 update-status: good 名前解決を確認します。\ndig bar.foo.com. a +short x.x.x.x Firewall 設定 set firewall name LAN_OUT description \u0026#39;LAN to the Internet\u0026#39; set firewall name LAN_OUT default-action accept set firewall name WAN_IN description \u0026#39;WAN to Internal\u0026#39; set firewall name WAN_IN default-action drop set firewall name WAN_IN rule 10 action accept set firewall name WAN_IN rule 10 state established enable set firewall name WAN_IN rule 10 state related enable set firewall name WAN_IN rule 20 action drop set firewall name WAN_IN rule 20 state invalid enable set firewall name WAN_IN rule 19132 description \u0026#39;Minecraft\u0026#39; set firewall name WAN_IN rule 19132 action accept set firewall name WAN_IN rule 19132 destination address 10.0.1.100 set firewall name WAN_LOCAL default-action drop set firewall name WAN_LOCAL rule 10 action accept set firewall name WAN_LOCAL rule 10 state established enable set firewall name WAN_LOCAL rule 10 state related enable set firewall name WAN_LOCAL rule 20 action drop set firewall name WAN_LOCAL rule 20 state invalid enable 作成した firewall を interface にアタッチします。\nset interface pppoe pppoe0 firewall in name WAN_IN set interface pppoe pppoe0 firewall local name WAN_LOCAL set interface pppoe pppoe0 firewall out name LAN_OUT commit save まとめ 前述した要件すべてが満たせました。ただ下記の課題もあります。\nIPv6 の Firewall 設定がまだ (後に追記しようと思います) IPoE が結構遅い 前者はただ設定すればいいだけなので解決出来るのですが、後者の課題は想定とは異なっていました。時間帯によりますが帯域が PPPoE の数分の1程度に下がることもありレイテンシも若干劣ります。そもそも自分の場合は Asani-net の v6connect を使っていて、本来終端の情報等を自動設定できる対応ルータのみサポートしているそうなのですが、サポート外の方法で接続している時点で不安はあります\u0026hellip;。\nよって PPPoE 接続オンリーのホットスタンバイ機を今回設定したルータの横に隣接して並べています。この点はゲーミングマシン等接続を安定させたいマシンに対しての Policy Based Routing を書ける準備をすればいいのかもしれません。\nまた、NIC を3枚以上積んだルーターであれば Source NAT, Policy Based Routing の記述を変えて、万が一接続性が安定しない場合に直ちに端末の接続するサブネットを切り替えるなんて運用も出来るかもしれません。\nとタイトルと結果が乖離しているようですが、とは言え VyOS で IPoE 接続し NAT し各端末から IPv4 接続でインターネットに接続し、外部へサーバーも公開するという要件の情報源がほぼ無かったので記事にしました。\n","permalink":"https://jedipunkz.github.io/post/vyos-ipoe-pppoe/","summary":"\u003cp\u003e\u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e です。今まで自宅では PPPoE 接続をして Global IPv4 アドレスを取得して自宅にマイクラサーバーを外部公開しその接続を使って各端末でオンラインゲームやインターネットをしていました。ただ、IPoE 接続すると混雑時間を回避出来ると聞いていたので (これについては後術します)、普段のゲームや各端末のインターネット接続は IPoE 接続を利用しマイクラサーバーは PPPoE で公開、と出来ないかと考えました。IPoE は IPv4 over IPv6 のトンネリング接続するたため IPoE だけでは IPv4 Global IP アドレスによるサーバー公開は不可能だからです。\u003c/p\u003e\n\u003cp\u003eここでは VyOS を使ってその両者を満たす接続の設定方法を記します。\u003c/p\u003e\n\u003ch2 id=\"要件\"\u003e要件\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003ePPPoE して Global IPv4 を取得しサーバーを外部に提供\u003c/li\u003e\n\u003cli\u003e(その際に Dynamic DNS を利用 (自宅は Cloudflare))\u003c/li\u003e\n\u003cli\u003eサーバー以外の端末のトラヒックは IPoE 接続した経路に流す\u003c/li\u003e\n\u003cli\u003e回線はNTT フレッツ想定 (自分の場合は Asahi-net, IPv6 接続オプションあり, その他でも可)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"iso-イメージをビルド\"\u003eISO イメージをビルド\u003c/h2\u003e\n\u003cp\u003eVyOS は Stable のバージョンは有償バージョンでないとダウンロード出来ませんがビルドすれば ISO イメージが作れます。その方法を記します。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e2023/12 時点で最新の Stable バージョン 1.3.4 を指定\u003c/li\u003e\n\u003cli\u003e適当な Linux 端末でビルドしましたが Docker を利用できればどこでも良さそう\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003egit clone -b equuleus https://github.com/vyos/vyos-build\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ecd vyos-build/\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edocker run --rm -it --privileged -v \u003cspan style=\"color:#66d9ef\"\u003e$(\u003c/span\u003epwd\u003cspan style=\"color:#66d9ef\"\u003e)\u003c/span\u003e:/vyos -w /vyos vyos/vyos-build:equuleus bash\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e./configure --architecture amd64 --build-type release --version 1.3.4\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003esudo make iso\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eビルドが完了すると \u003ccode\u003ebuild/\u003c/code\u003e ディレクトリ配下に iso イメージが出来上がっているはずです。\u003c/p\u003e","title":"VyOS で IPoE/PPPoE を併用して安定した接続\u0026サーバー公開"},{"content":"こんにちは。jedipunkz です。\n今回は、kubectl プラグインを開発したことがなかったので、Go の学習と合わせてためしに1つ作ってみたのでその内容を記したいと思います。\n開発した kubectl plugin: kubectl-fuzzy-login 下記が今回開発した kubectl プラグインです。\nhttps://github.com/jedipunkz/kubectl-fuzzy-login\n何が出来るか 下記のキャプチャをご覧頂くと一目瞭然だと思います。\nKubernetes のポッドとコンテナをインクリメンタルサーチしつつ選択し、最終的にコンテナにログイン出来るプラグインになっています。コンテナがサイドカー構成になっていた場合は、そのうちのどのコンテナにログインするかもインクリメンタルサーチ出来ます。なお、このプラグインは Go で開発しました。\nインストール方法 Krew を利用している場合は下記の操作でインストールできます。Krew が事前にインストールされている必要があります。\ngit clone https://github.com/jedipunkz/kubectl-fuzzy-login.git kubectl krew install --manifest=./kubectl-fuzzy-login/krew/fuzzy-login.yaml マニュアル操作でインストールする場合は下記です。\ngit clone https://github.com/jedipunkz/kubectl-fuzzy-login.git cd kubectl-fuzzy-login go build cp kubectl-fuzzy-login /your/bin/path 使用方法 オプション無しで、全 Namespaces を対象に検索・ログインする オプションを使用しない場合は下記のように実行します。\nkubectl fuzzy login まず Pod を選択します。Pod 名の一部を入力することでインクリメンタル・ファジー検索出来ます。その Pod に複数のコンテナ (サイドカー) がある場合、更にコンテナをインクリメンタルサーチ出来ます。最終的にコンテナを選択し Enter ボタンを押すことでコンテナにログイン出来ます。ただしコンテナイメージにシェルが入っていない場合は入ることが出来ません。\nシェル指定 また下記のように -s オプションでデフォルトのシェルを指定することもできます。\nkubectl fuzzy login -s /bin/bash Namespace 指定 Namespace を -n オプションで指定することもできます。\nkubectl fuzzy login -n default Kubectl Plugin 開発の基本 Kubectl Plugin の開発方法の基本について軽く記します。\nKubectl Plugin は、Kubernetes の CLI ツールである kubectl の拡張機能を提供します。これらのプラグインは、ユーザーが独自のカスタムコマンドを作成して kubectl に追加できるようにするためのものです。開発方法は以下の通りです。\n言語の選択 プラグインは任意の言語で書くことができます。Bash でも書けるそうです。kubectlはプラグインを単に新しいプロセスとして実行します。そのため、プラグインはkubectlと同じマシン上で実行できる任意の言語で書くことができます。\nPlugin 名 kubectl- のプレフィックスで始まる名前を持つ必要があります。例えば kubectl-foo-bar という Plugin 名の場合、下記のように Plugin を使用することが出来ます。\nkubectl foo bar Kubernetes クライアントライブラリの使用 プラグインが Kubernetes API と対話する必要がある場合、適切な Kubernetes クライアントライブラリを使用することが推奨されます。Goの場合は、kubernetes/client-go ライブラリが広く使用されているそうです。\nkubectl-fuzzy-login のコード構成の説明 ディレクトリ構成 cmd/root.go ここには、プラグインのエントリーポイントとなるコードが含まれています。このプラグインでは root.go というファイルがエントリーポイントとして機能しています。\n主に以下の処理を行います。\nKubernetesクラスタとの接続を確立します。 PodGetter インターフェースを使用して、クラスタ内の全Podを取得します。 取得したPodの中からユーザーが選択したPodを特定します。 ユーザーが選択したPod内のコンテナを特定します。 PodExecutor インターフェースを使用して、特定のコンテナにログインします。 internal/kubernetes このディレクトリは、プラグインの内部機能を提供します。このプラグインでは、internal/kubernetes というサブディレクトリがあり、Kubernetesクラスタとのやり取りを担当しています。\nPodGetter: このインターフェースは、Kubernetesクラスタから Pod を取得するメソッドを定義しています。 PodExecutor: このインターフェースは、Kubernetesクラスタ上の特定の Pod でコマンドを実行するメソッドを定義しています。 Krew 対応について Krew は kubectl プラグインを管理するためのプラグインマネージャーでプラグインの検索・インストール・更新が行なえます。Krew の公式レポジトリに PR を送って、誰もが自分の開発したプラグインを利用できるようにする方法もあるのですが、PR がマージされる必要があるので、今回はプライベートレポジトリ上に Manifest を作成してそれを利用する方法を取りました。(このあたりは HomeBrew に似ていますね)\n作成した Manifest は下記になります。\nhttps://github.com/jedipunkz/kubectl-fuzzy-login/blob/main/krew/fuzzy-login.yaml\n上記と重複しますが、プライベートレポジトリ上の Manifest を利用して下記の通りこのプラグインをインストール出来ます。\ngit clone https://github.com/jedipunkz/kubectl-fuzzy-login.git kubectl krew install --manifest=./kubectl-fuzzy-login/krew/fuzzy-login.yaml まとめ Kubectl プラグインの開発自体は\nプラグインの命名によって操作方法が設定できる 任意の言語で開発できる Go を使う場合は client-go を使えば良い ということで、比較的簡単に開発することが出来ました。今回は Go の Viper を使って -n や -s オプションを実装しましたが、このあたりも何でも良い気もします。自由度が高い分、逆にどうするべきなのか迷う点はあります。\n","permalink":"https://jedipunkz.github.io/post/kubectl-plugin/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://twitter.com/jedipunkz\"\u003ejedipunkz\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003e今回は、kubectl プラグインを開発したことがなかったので、Go の学習と合わせてためしに1つ作ってみたのでその内容を記したいと思います。\u003c/p\u003e\n\u003ch2 id=\"開発した-kubectl-plugin-kubectl-fuzzy-login\"\u003e開発した kubectl plugin: kubectl-fuzzy-login\u003c/h2\u003e\n\u003cp\u003e下記が今回開発した kubectl プラグインです。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/jedipunkz/kubectl-fuzzy-login\"\u003ehttps://github.com/jedipunkz/kubectl-fuzzy-login\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"何が出来るか\"\u003e何が出来るか\u003c/h3\u003e\n\u003cp\u003e下記のキャプチャをご覧頂くと一目瞭然だと思います。\u003c/p\u003e\n\u003cp\u003eKubernetes のポッドとコンテナをインクリメンタルサーチしつつ選択し、最終的にコンテナにログイン出来るプラグインになっています。コンテナがサイドカー構成になっていた場合は、そのうちのどのコンテナにログインするかもインクリメンタルサーチ出来ます。なお、このプラグインは Go で開発しました。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"kubectl-fuzzy-login\" loading=\"lazy\" src=\"../../pix/kubectl-fuzzy-login.gif\"\u003e\u003c/p\u003e\n\u003ch3 id=\"インストール方法\"\u003eインストール方法\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://krew.sigs.k8s.io/\"\u003eKrew\u003c/a\u003e を利用している場合は下記の操作でインストールできます。Krew が事前にインストールされている必要があります。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003egit clone https://github.com/jedipunkz/kubectl-fuzzy-login.git\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ekubectl krew install --manifest\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e./kubectl-fuzzy-login/krew/fuzzy-login.yaml\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eマニュアル操作でインストールする場合は下記です。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003egit clone https://github.com/jedipunkz/kubectl-fuzzy-login.git\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ecd kubectl-fuzzy-login\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ego build\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ecp kubectl-fuzzy-login /your/bin/path\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"使用方法\"\u003e使用方法\u003c/h3\u003e\n\u003ch4 id=\"オプション無しで全-namespaces-を対象に検索ログインする\"\u003eオプション無しで、全 Namespaces を対象に検索・ログインする\u003c/h4\u003e\n\u003cp\u003eオプションを使用しない場合は下記のように実行します。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ekubectl fuzzy login \n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eまず Pod を選択します。Pod 名の一部を入力することでインクリメンタル・ファジー検索出来ます。その Pod に複数のコンテナ (サイドカー) がある場合、更にコンテナをインクリメンタルサーチ出来ます。最終的にコンテナを選択し Enter ボタンを押すことでコンテナにログイン出来ます。ただしコンテナイメージにシェルが入っていない場合は入ることが出来ません。\u003c/p\u003e\n\u003ch4 id=\"シェル指定\"\u003eシェル指定\u003c/h4\u003e\n\u003cp\u003eまた下記のように \u003ccode\u003e-s\u003c/code\u003e オプションでデフォルトのシェルを指定することもできます。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ekubectl fuzzy login -s /bin/bash\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch4 id=\"namespace-指定\"\u003eNamespace 指定\u003c/h4\u003e\n\u003cp\u003eNamespace を \u003ccode\u003e-n\u003c/code\u003e オプションで指定することもできます。\u003c/p\u003e","title":"k8s コンテナをインクリメンタルサーチ\u0026ログインする kubectl プラグインの開発"},{"content":"こんにちは。jedipunkz🚀 です。\n以前こちらの PipeCD 検証の記事 で Progressive Deliver について調査したのですが、Kubernetes でこの Progressive Delivery を実現する方法を調べておきたいなと思って手元の Macbook 上で検証してみたのでその際の手順を記そうかと思います。\nProgressive Delivery の概要 ここで概要だけ記しておきます。Canary リリースは新しいデプロイメントをある程度の割合だけリリースし、徐々にリリースを進行させるデプロイ方式ということはご存知だと思いますが、Progressive Delivery はその過程で\n新しいデプロイメントの統計情報を得る 予め定義したデプロイ成功定義に対して条件満たしているかを過程毎にチェックする チェック OK であれば次の過程にデプロイを進める 予め定義した幾つかのデプロイ過程を全て終えるとデプロイ完了となる というステップを経ます。\n用いるソフトウェア Kubernetes で Progressive Delivery を実現するには下記のソフトウェアを用いる事が可能です。 また今回の手順は MacOS を前提に記します。\nArgo Rollouts Prometheus Istio Kubernetes (今回は Minikube を使いました) 事前の準備 Istio Istio をダウンロードします。\ncurl -L https://istio.io/downloadIstio | ISTIO_VERSION=17.2 sh - Istio を Minikube にデプロイします。\ncd istio-17.2 istioctl install --set profile=demo -y Kubernetes Namespace default で起動した Pod が自動的に Envoy サイドカーを取得するように設定します。\nkubectl label namespace default istio-injection=enabled Prometheus 下記の Istio のディレクトリにある Manifest を用いる事で、Istio のメトリクスが自動的に Prometheus に収集されます。\nkubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.17/samples/addons/prometheus.yaml 下記のコマンドを実施して Prometheus UI にアクセスします。コマンド実行と共に自動的にブラウザで UI ページへ遷移します。\nminikube service -n istio-system prometheus Nginx コンテナで動作確認 Nginx のコンテナイメージを用いて動作確認を実施しようと思います。\nDeployment, Service のデプロイ 下記の内容を nginx.yaml というファイル名で保存します。\napiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 1 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.23.4 # tag はあえて古くしています ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: nginx spec: selector: app: nginx ports: - protocol: TCP port: 80 targetPort: 80 下記の通りデプロイします。\nkubectl apply -f nginx.yaml Istio Gateway のデプロイ 下記の内容を nginx-istio.yaml というファイル名で保存します。\napiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: nginx-gateway spec: selector: istio: ingressgateway servers: - port: number: 80 name: http protocol: HTTP hosts: - \u0026#34;*\u0026#34; --- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: nginx spec: hosts: - \u0026#34;*\u0026#34; gateways: - nginx-gateway http: - match: - uri: exact: / route: - destination: host: nginx.default.svc.cluster.local port: number: 80 Istio Gateway をデプロイします。\nkubectl apply -f nginx-istio.yaml 下記のコマンドを実行すると、Istio Gateway 経由で Nginx のコンテンツにアクセス出来ます。\nminikube service istio-ingressgateway -n istio-system |--------------|----------------------|-------------------|---------------------------| | NAMESPACE | NAME | TARGET PORT | URL | |--------------|----------------------|-------------------|---------------------------| | istio-system | istio-ingressgateway | status-port/15021 | http://192.168.49.2:31483 | | | | http2/80 | http://192.168.49.2:30891 | | | | https/443 | http://192.168.49.2:31959 | | | | tcp/31400 | http://192.168.49.2:31196 | | | | tls/15443 | http://192.168.49.2:32100 | |--------------|----------------------|-------------------|---------------------------| 🏃 Starting tunnel for service istio-ingressgateway. |--------------|----------------------|-------------|------------------------| | NAMESPACE | NAME | TARGET PORT | URL | |--------------|----------------------|-------------|------------------------| | istio-system | istio-ingressgateway | | http://127.0.0.1:52115 | | | | | http://127.0.0.1:52116 | | | | | http://127.0.0.1:52117 | | | | | http://127.0.0.1:52118 | | | | | http://127.0.0.1:52119 | |--------------|----------------------|-------------|------------------------| 実際には http2/80 のポートにアクセスすることになるので上記の場合は http://127.0.0.1:52116 ブラウザでアクセスすると nginx のコンテンツにアクセス出来ます。上記の minikube コマンドを実行した状態でターミナルで loop しつつ curl でアクセスしておきます。これは Progressive Delivery の Analysis の定義で Istio メトリクスからステータスコードによる統計結果を記し利用するためです。\nwhile true; do curl http://127.0.0.1:52116; sleep 1; done Argo Rollouts, Analysis の定義 Argo Rollouts の Rollout, Analysis を定義します。\nAnalysis を定義するため下記の内容を nginx-analysis.yaml というファイル名で保存します。\napiVersion: argoproj.io/v1alpha1 kind: AnalysisTemplate metadata: name: http-req-check spec: args: - name: service-name value: nginx metrics: - name: request-success-rate interval: 1m successCondition: result[0] \u0026gt;= 0.95 failureLimit: 1 provider: prometheus: address: http://prometheus.istio-system:9090 query: | sum(irate( istio_requests_total{reporter=\u0026#34;source\u0026#34;,destination_service=~\u0026#34;nginx.default.svc.cluster.local\u0026#34;,response_code!~\u0026#34;5.*\u0026#34;}[5m] )) / sum(irate( istio_requests_total{reporter=\u0026#34;source\u0026#34;,destination_service=~\u0026#34;nginx.default.svc.cluster.local\u0026#34;}[5m] )) このファイル中で Analysis が定義されているのですが、Prometheus へのクエリの定義も行っています。先程開いた Prometheus UI から上記のクエリが実際に実行できるか確認しておくと良いでしょう。上記のクエリを 🔍 マークに入力して Execute ボタンを押すだけです。\nAnalysis をデプロイします。\nkubectl apply -f nginx-analysis.yaml 次に Rollout を定義するため下記の内容を nginx-rollout.yaml というファイル名で保存します。\napiVersion: argoproj.io/v1alpha1 kind: Rollout metadata: name: nginx spec: replicas: 2 selector: matchLabels: app: nginx strategy: canary: analysis: templates: - templateName: http-req-check maxUnavailable: 0 maxSurge: 1 steps: - setWeight: 30 - pause: duration: \u0026#34;30s\u0026#34; - setWeight: 60 - pause: duration: \u0026#34;30s\u0026#34; - setWeight: 100 - pause: duration: \u0026#34;10s\u0026#34; template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.23.4 # 使用するnginxのバージョンを指定 ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: nginx labels: app: nginx spec: ports: - port: 80 targetPort: 80 selector: app: nginx Rollout をデプロイします。\nkubectl apply -f nginx-rollout.yaml Progressive Delivery の進行状況を確認する では実際に Progressive Delivery のデプロイ進行状況を確認していきます。\n下記のコマンドで Argo Rollouts のデプロイ進行状況をウォッチし続ける事が可能です。新しいターミナルを起動して実行しておきます。\nkubectl argo rollouts get rollout nginx --watch まず nginx:1.23.4 というイメージでデプロイされている様子が下記のように確認出来るはずです。Status: Healty となっています。\nName: nginx Namespace: default Status: ✔ Healthy Strategy: Canary Step: 6/6 SetWeight: 100 ActualWeight: 100 Images: nginx:1.23.4 (stable) Replicas: Desired: 2 Current: 2 Updated: 2 Ready: 2 Available: 2 NAME KIND STATUS AGE INFO ⟳ nginx Rollout ✔ Healthy 24s └──# revision:1 └──⧉ nginx-8595d69c7f ReplicaSet ✔ Healthy 24s stable ├──□ nginx-8595d69c7f-p5cxl Pod ✔ Running 24s ready:2/2 └──□ nginx-8595d69c7f-tvqx9 Pod ✔ Running 24s ready:2/2 新しいデプロイメントを行うためコンテナイメージのタグを更新してみます。下記の操作で行えます。\nkubectl argo rollouts set image nginx nginx=nginx:1.24.0 Progressive Delivery の各過程が全て通過した状況を下記に記します。Revision: 2 の状態で Status: Healty となっている事が確認出来ます。\nName: nginx Namespace: default Status: ✔ Healthy Strategy: Canary Step: 6/6 SetWeight: 100 ActualWeight: 100 Images: nginx:1.24.0 (stable) Replicas: Desired: 2 Current: 2 Updated: 2 Ready: 2 Available: 2 NAME KIND STATUS AGE INFO ⟳ nginx Rollout ✔ Healthy 2m35s ├──# revision:2 │ ├──⧉ nginx-67d8cf46 ReplicaSet ✔ Healthy 81s stable │ │ ├──□ nginx-67d8cf46-jbjch Pod ✔ Running 81s ready:2/2 │ │ └──□ nginx-67d8cf46-8rksc Pod ✔ Running 48s ready:2/2 │ └──α nginx-67d8cf46-2 AnalysisRun ✔ Successful 81s ✔ 2 └──# revision:1 └──⧉ nginx-8595d69c7f ReplicaSet • ScaledDown 2m35s ここまで到達するために nginx-rollout.yaml で下記の通り定義した内容の各ステップを経ている事になります。実際に過程は 30%, 60%, 100% の割合で新しいデプロイメントをデプロイし、pause で指定した期間、Analysis でメトリクスを Prometheus にクエリを実行することで異常がないか計測し、問題なければその過程を終え、最終的に新しいデプロイメントの割合が 100% となります。\nsteps: - setWeight: 30 - pause: duration: \u0026#34;30s\u0026#34; - setWeight: 60 - pause: duration: \u0026#34;30s\u0026#34; - setWeight: 100 - pause: duration: \u0026#34;10s\u0026#34; まとめ 比較的簡単に Progressive Delivery が実践出来ました。実際に運用する際にはもちろん Istio, Prometheus の設定は精査したほうがいいと思います。また Analysis の定義や Prometheus へのクエリも、環境やサービスの性質に合わせて調整する必要がありますが、今回紹介した構成が基本となると思っています。\n","permalink":"https://jedipunkz.github.io/post/argo-rollout-progressive-delivery/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://twitter.com/jedipunkz\"\u003ejedipunkz🚀\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003e以前\u003ca href=\"https://jedipunkz.github.io/post/ecs-pipecd/\"\u003eこちらの PipeCD 検証の記事\u003c/a\u003e で Progressive Deliver について調査したのですが、Kubernetes でこの Progressive Delivery を実現する方法を調べておきたいなと思って手元の Macbook 上で検証してみたのでその際の手順を記そうかと思います。\u003c/p\u003e\n\u003ch2 id=\"progressive-delivery-の概要\"\u003eProgressive Delivery の概要\u003c/h2\u003e\n\u003cp\u003eここで概要だけ記しておきます。Canary リリースは新しいデプロイメントをある程度の割合だけリリースし、徐々にリリースを進行させるデプロイ方式ということはご存知だと思いますが、Progressive Delivery はその過程で\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e新しいデプロイメントの統計情報を得る\u003c/li\u003e\n\u003cli\u003e予め定義したデプロイ成功定義に対して条件満たしているかを過程毎にチェックする\u003c/li\u003e\n\u003cli\u003eチェック OK であれば次の過程にデプロイを進める\u003c/li\u003e\n\u003cli\u003e予め定義した幾つかのデプロイ過程を全て終えるとデプロイ完了となる\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eというステップを経ます。\u003c/p\u003e\n\u003ch2 id=\"用いるソフトウェア\"\u003e用いるソフトウェア\u003c/h2\u003e\n\u003cp\u003eKubernetes で Progressive Delivery を実現するには下記のソフトウェアを用いる事が可能です。\nまた今回の手順は MacOS を前提に記します。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://argo-rollouts.readthedocs.io/en/stable/b\"\u003eArgo Rollouts\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://prometheus.io/\"\u003ePrometheus\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://istio.io/\"\u003eIstio\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eKubernetes (今回は Minikube を使いました)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"事前の準備\"\u003e事前の準備\u003c/h2\u003e\n\u003ch3 id=\"istio\"\u003eIstio\u003c/h3\u003e\n\u003cp\u003eIstio をダウンロードします。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ecurl -L https://istio.io/downloadIstio | ISTIO_VERSION\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e17.2 sh -\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eIstio を Minikube にデプロイします。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ecd istio-17.2\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eistioctl install --set profile\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003edemo -y\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eKubernetes Namespace \u003ccode\u003edefault\u003c/code\u003e で起動した Pod が自動的に Envoy サイドカーを取得するように設定します。\u003c/p\u003e","title":"手軽にローカルで Argo Rollouts, Istio, Prometheus で Progressive Delivery を試す"},{"content":"こんにちは @jedipunkz 🚀 です。\n普段仕事で AWS ECS を使っていて Autoscallng Group によってアプリケーションを据えケールさせて運用していますが、運用している中でより高速にオートスケール出来ないものだろうか？と思うシチュエーションが何回か発生し、対応方法について模索していました。\n実際に発生したシチュエーション 下記はコンテナ毎の CPU 使用率です。1分未満の間に急激にアクセスが増えコンテナの CPU 使用率が 100% に達し (実際には vCPU に基づいて 200% となっている)、ECS Service のヘルスチェックに Fail して、コンテナが落ち、新しいコンテナは起動するものの、アクセス不可に耐えられず、コンテナ停止と起動を繰り返すといった状況でした。\nAutoscaling Policy, Cloudwatch Metrics Alarm の調整 まず最初に考えたのが下記の値の調整です。\naws_app_autoscaling_policy の cooldown 値 aws_cloudwatch_metric_alarm の period 値 具体的には 60sec となっていた値を 10sec などに変更しました。これによって 60sec のインターバルでしきい値計算してスケールさせていたところを 10sec にインターバルを縮めつつスケールさせる。つまりより迅速にスケールさせることで上記のシチュエーションに耐えられるのではと考えました。\nですが、結果は NG でした。\n下記は Cloudwatch Metrics の様子です。データはプロットされているものの、データ不足 という状態に陥っている事がわかります。\n実際に ECS はこの設定をした Metrics Alarm ではスケールしてくれませんでした。\n高解像度メトリクスの利用について であれば高解像度メトリクス を利用すれば良いのではと考えました。\n歴史的に\n2009年当初、5分間隔だった Cloudwatch Metrics 2010年に1分間隔に変更 という背景があるようなのですが、それに対して高解像度メトリクスは1秒の間隔でメトリクスを発行することが可能になるとのこと。\nただ ECS が Cloudwatch Alarm にメトリクス発行するのですが、AWS サポートに聞いたところ、ECS は高解像度メトリクスに対応していない、と回答いただきました。\nサイドカーコンテナで動かす自前ツール ESP の利用で解決 ということで、自前のツールを Go で開発しました。これによって問題が解決すると考えています。(ただ、現時点ではまだ運用に乗せていません)\nhttps://github.com/jedipunkz/esp\n前提としては下記を考慮する必要があります\n自前ツール ESP は対象となるアプリコンテナのサイドカーとして起動 ツール内で行っていることは下記です。\n(1) ESP は ECS メタデータエンドポイントにアクセスしてアプリコンテナの CPU 使用率を計算する上で必要な値を取得 (2) ESP はアプリコンテナの CPU 使用率を計算 (3) ESP は CPU 使用率を Cloudwatch Metrics へ高解像度メトリクスとして Put 結果、Terraform 等で下記のように設定すれば秒単位でスケールする ECS が構築できます。\nresource \u0026#34;aws_cloudwatch_metric_alarm\u0026#34; \u0026#34;example_high\u0026#34; { alarm_name = \u0026#34;example-CPU-Utilization-High-30\u0026#34; comparison_operator = \u0026#34;GreaterThanOrEqualToThreshold\u0026#34; evaluation_periods = \u0026#34;1\u0026#34; metric_name = \u0026#34;CPUUtilization\u0026#34; namespace = \u0026#34;\u0026lt;ESP 起動時に設定するネームスペース名\u0026gt;\u0026#34; period = \u0026#34;10\u0026#34; # ここは秒単位で設定出来る statistic = \u0026#34;Average\u0026#34; threshold = \u0026#34;15\u0026#34; dimensions = { ClusterName = aws_ecs_cluster.example.name ServiceName = aws_ecs_service.example.name } alarm_actions = [aws_appautoscaling_policy.scale_out.arn] } CPU 使用率の計算について ECS メタデータエンドポイントにアクセスすると下記の様なデータが取得できます。\n\u0026#34;cpu_stats\u0026#34;: { \u0026#34;cpu_usage\u0026#34;: { \u0026#34;total_usage\u0026#34;: 1137691504, \u0026#34;percpu_usage\u0026#34;: [ 696479228, 441212276, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ], \u0026#34;usage_in_kernelmode\u0026#34;: 80000000, \u0026#34;usage_in_usermode\u0026#34;: 810000000 }, \u0026#34;system_cpu_usage\u0026#34;: 9393210000000, \u0026#34;online_cpus\u0026#34;: 2, \u0026#34;throttling_data\u0026#34;: { \u0026#34;periods\u0026#34;: 0, \u0026#34;throttled_periods\u0026#34;: 0, \u0026#34;throttled_time\u0026#34;: 0 } }, Classi さんのブログ記事 (https://tech.classi.jp/entry/2022/05/24/120000) を参考にさせていただきました。この記事では Datadog の AWS Integration のコードを参考にされたそうです。Datadog では下記のようなコードになっています。\nhttps://github.com/DataDog/integrations-core/blob/4cf0f7dc759683b454cf46b9ff5e2de561a58339/ecs_fargate/datadog_checks/ecs_fargate/ecs_fargate.py#L220\n結果的に ESP では、下記のように計算しました。\nhttps://github.com/jedipunkz/esp/blob/main/esp.go#L40-L42\ncpuUsage := ((float64(s.CPUStats.CPUUsage.TotalUsage) - float64(s.PreCPUStats.CPUUsage.TotalUsage)) / (float64(s.CPUStats.SystemCPUUsage) - float64(s.PreCPUStats.SystemCPUUsage))) * float64(s.CPUStats.OnlineCPUs) * 100 まとめ ESP を使って ECS が秒単位でもスケールさせることが出来ると思われます。ただ運用に乗せるまでは至っていないので、それまでに負荷試験等を実施して効果を測る必要がありそうです。\nまた、高速にすると記しましたが、より具体的に表現するとスケールのサイクルを速く回す、となります。よって Fargate の弱点であるコンテナ起動までの時間が比較的掛かる問題やコンテナイメージの大きさから発生するコンテナ起動時間の長時間化に対しては、また別のアプローチがあると思うので、それらも合わせて対応取ると良いかもしれません。\n","permalink":"https://jedipunkz.github.io/post/esp/","summary":"\u003cp\u003eこんにちは \u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e 🚀 です。\u003c/p\u003e\n\u003cp\u003e普段仕事で AWS ECS を使っていて Autoscallng Group によってアプリケーションを据えケールさせて運用していますが、運用している中でより高速にオートスケール出来ないものだろうか？と思うシチュエーションが何回か発生し、対応方法について模索していました。\u003c/p\u003e\n\u003ch2 id=\"実際に発生したシチュエーション\"\u003e実際に発生したシチュエーション\u003c/h2\u003e\n\u003cp\u003e下記はコンテナ毎の CPU 使用率です。1分未満の間に急激にアクセスが増えコンテナの CPU 使用率が 100% に達し (実際には vCPU に基づいて 200% となっている)、ECS Service のヘルスチェックに Fail して、コンテナが落ち、新しいコンテナは起動するものの、アクセス不可に耐えられず、コンテナ停止と起動を繰り返すといった状況でした。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"CPU Usage\" loading=\"lazy\" src=\"../../pix/esp_cpu_usage.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"autoscaling-policy-cloudwatch-metrics-alarm-の調整\"\u003eAutoscaling Policy, Cloudwatch Metrics Alarm の調整\u003c/h2\u003e\n\u003cp\u003eまず最初に考えたのが下記の値の調整です。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eaws_app_autoscaling_policy の cooldown 値\u003c/li\u003e\n\u003cli\u003eaws_cloudwatch_metric_alarm の period 値\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e具体的には \u003ccode\u003e60sec\u003c/code\u003e となっていた値を \u003ccode\u003e10sec\u003c/code\u003e などに変更しました。これによって 60sec のインターバルでしきい値計算してスケールさせていたところを 10sec にインターバルを縮めつつスケールさせる。つまりより迅速にスケールさせることで上記のシチュエーションに耐えられるのではと考えました。\u003c/p\u003e\n\u003cp\u003eですが、結果は NG でした。\u003c/p\u003e\n\u003cp\u003e下記は Cloudwatch Metrics の様子です。データはプロットされているものの、\u003ccode\u003eデータ不足\u003c/code\u003e という状態に陥っている事がわかります。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Cloudwatch Metrics\" loading=\"lazy\" src=\"../../pix/esp_cloudwatch_metrics.png\"\u003e\u003c/p\u003e\n\u003cp\u003e実際に ECS はこの設定をした Metrics Alarm ではスケールしてくれませんでした。\u003c/p\u003e\n\u003ch2 id=\"高解像度メトリクスの利用について\"\u003e高解像度メトリクスの利用について\u003c/h2\u003e\n\u003cp\u003eであれば\u003ca href=\"https://aws.amazon.com/jp/blogs/news/new-high-resolution-custom-metrics-and-alarms-for-amazon-cloudwatch/\"\u003e高解像度メトリクス\u003c/a\u003e を利用すれば良いのではと考えました。\u003c/p\u003e","title":"自前ツールと Cloudwatch 高解像度メトリクスを使ったより高速な ECS オートスケールの実現"},{"content":"こんにちは @jedipunkz 🚀 です。\nECS 構成をもう少しセキュアに保てる構成はないものだろうかと模索しているなかで Sysdig を見つけました。まだ導入できる目処は立っていないのですがある程度ノウハウ蓄積出来てきたのでここで検証内容等を記事にしようかと思っています。\nSysdig は幾つかのサービスが存在するのですが今回検証したのは Sysdig Serverless Security と呼ばれるモノで ECS Fargate 上のコンテナランタイムセキュリティを実践することができるサービスです。\nSysdig とは AWS のサービスにも脅威検知を行うことができるサービスが揃っているのはご存知と思います\n対象 目的 技術・サービス AWS リソース 驚異検知 AWS GuardDuty また予防の観点で脆弱性診断が出来るサービスもありあす\n対象 目的 技術・サービス AWS リソース セキュリティ診断 AWS Trusted Advisor ECS コンテナ 脆弱性診断 ECR Image Scan EC2 上のソフトウェア 脆弱性診断 AWS Inspector ここで気がつくと思うのですがコンテナ上の驚異検知を行うサービスが AWS には無いと思っています。 (2022/09 時点)\nSysdig Serverless Security は ECS Fargate コンテナ上の脅威検知を行うサービスです。ECS Fargate 利用時にコンテナ上の脅威検知を行うサービスは他にも幾つかありますが、Sysdig はシステムコールを利用したコンテナランタイムセキュリティを実践して脅威検知・通知が行えるものになります。自分も詳しくないのですがこれを CWPP (Cloud Workload Protection Platform) と言うらしいです。ワークロードというのはクラウド上の仮想マシン・稼働中のソフトウェアを指して、CWPP はマルウェア保護、脆弱性スキャン、アクセス制御、異常検知の機能を使用してそれぞれのワークロードを保護する、ということらしいです。\nSysdig 以外の CWPP はどういうものがあるのか? Sysdig 以外ですと代表するものが下記になるそうです。\nDatadog Aqua CWWP Palo Alto Prisma Sysdig をもう少し具体的に説明 Sysdig は CNCF の Falco をベースとして脅威検出を行うための SaaS を含めたシステムとなります。コンテナ上の脅威と共にクラウドアカウントを紐付けることでクラウド自体のセキュリティチェックを行うことも出来ます。たとえば問題のある S3 バケット公開設定や、IAM の権限に関するものなど。今回自分は前者のコンテナ上の脅威検知について調べましたので、この記事でもそれについて記そうと思います。\nパフォーマンス ベースとなっている Falco では Linux カーネルの ptrace システムコールを用いて脅威検知 (ファイルの読み書きやディスクへのアクセス・ネットワーク確立、ネットワークデータ送受信などについて) します。ですが ptrace はコンテナの本来のパフォーマンスに結構な影響があるそうです。それに比較して Sysdig はワークロードセキュリティを実装していないコンテナとほぼ同等のパフォーマンスが出ているように見えます。\n参考資料: Sysdig Blog\nSysdig + ECS Fargate の構成 ここから具体的に検証した内容を記していきたいと思います。\n自分は ECS Fargate を用いた Sysdig 利用を前提にして構成を組みました。\n構成の特徴としては下記になります。\n2エージェント構成 Sysdig サーバレス・オーケストレーター・エージェント Sysdig サーバーレス・ワークロード・エージェント 2つとも ECS Fargate 上に起動 タスク定義毎にワークロードエージェントが起動しプロキシ的に動作するオーケストレーター・エージェントと通信 オーケストレーター・エージェントはワークロード・エージェントからのデータを収集して Sysdig バックエンドへ転送する役割 下記が構成図になります。\n参考: https://docs.sysdig.com/en/docs/installation/serverless-agents/aws-fargate-serverless-agents/ 構築 Sysdig Region 情報 下記のドキュメントにある通り Sysdig には利用者が意識すべき Region という概念があります。どの Region を用いるか決定し、エンドポイント情報を構築の事前に収集する必要があります。\n参考: https://docs.sysdig.com/en/docs/administration/saas-regions-and-ip-ranges Sysdig サーバレス・オーケストレーター・エージェントの構築 下記の Sysdig 公式の Terraform Module を用いる事で構築することが可能です。\n参考: https://github.com/sysdiglabs/terraform-aws-fargate-orchestrator-agent 下記が検証目的で作成した Terraform コードです。\nmodule \u0026#34;sysdig_orchestrator_agent\u0026#34; { source = \u0026#34;sysdiglabs/fargate-orchestrator-agent/aws\u0026#34; version = \u0026#34;0.2.0\u0026#34; name = \u0026#34;example-sysdigorchestrator\u0026#34; vpc_id = var.vpc_id subnets = [ aws_subnet.example1.id, aws_subnet.example2.id, ] access_key = var.my_sysdig_access_key collector_host = \u0026#34;ingest-us2.app.sysdig.com\u0026#34; # 定めたリージョンのコレクタホストアドレス collector_port = \u0026#34;6443\u0026#34; agent_image = \u0026#34;quay.io/sysdig/orchestrator-agent:latest\u0026#34; assign_public_ip = false # Internet Gateway を用いる場合は true を指定 } Sysdig サーバレス・ワークロード・エージェントを盛り込んだアプリケーションコンテナの構築 検証では Sysdig 公式の Terraform Provider を用いて ECS タスク定義をレンダリングした。下記がその際のコードとなります。\ndata \u0026#34;sysdig_fargate_workload_agent\u0026#34; \u0026#34;sysdig\u0026#34; { container_definitions = file(\u0026#34;container_definitions/sysdigapp.json\u0026#34;) sysdig_access_key = var.my_sysdig_access_key workload_agent_image = \u0026#34;quay.io/sysdig/workload-agent:latest\u0026#34; orchestrator_host = module.sysdig_orchestrator_agent.orchestrator_host orchestrator_port = module.sysdig_orchestrator_agent.orchestrator_port } resource \u0026#34;aws_ecs_task_definition\u0026#34; \u0026#34;sysdig\u0026#34; { family = \u0026#34;example-sysdigapp-taskdef\u0026#34; cpu = \u0026#34;2048\u0026#34; memory = \u0026#34;8192\u0026#34; network_mode = \u0026#34;awsvpc\u0026#34; requires_compatibilities = [\u0026#34;FARGATE\u0026#34;] execution_role_arn = aws_iam_role.task_exec.arn task_role_arn = aws_iam_role.task.arn container_definitions = data.sysdig_fargate_workload_agent.sysdig.output_container_definitions } 尚、Sysdig Provider を用いてタスク定義をレンダリングしていますが、手動でもタスク定義を記すことが可能だそうです。その際には下記の処理をタスク定義に施すことで要件を満たせるそうです。\nSysdig に関する環境変数 Volume マウント指定 capabilities 指定 Sysdig コンテナ指定 構築後の Sysdig UI へのアクセス 構築後、(US-West を利用した場合は) 下記の URL で Sysdig Secure UI にアクセス出来ます。\nhttps://us2.app.sysdig.com/secure/#/policies\nイベント検知 ポリシーの種別 イベント検知のためのポリシーが用意されていて幾つかあるのですが ECS Fargate という条件であれば下記の2つの利用が可能だそうです。\nWorkload Policy (Powered By Falco) List Maching Policy 下記は Kubernetes 前提だそうです。\nContainer Drift Policy Machine Learning Policy 下記はコンテナセキュリティとは別枠 Policy で前述したクラウドアカウントを Sysdig に紐付けた際に利用できるポリシーです。\nKubernetes Audit Policy AWS CloudTrail Policy GCP Audit Log Policy Azure Platform Log Policy List Maching Policy における 特定ポートのリスン検知 下記の Rule を作成して Policy にアタッチすることでイベント検知出来ます。 また Policy アクションとして kill, stop, pause, nothing(notify only) が選べます。脅威を検知した際にプロセスの停止などが行えるということになります。\n設定パラメータ名 値 allInbounds Deny allOutbounds Deny udpListenPortsMatchItems true tcpListenPortsMatchItems true udpListenPorts 8080 tcpListenPorts 8080 特定ファイルの読み込み検知 下記の Rule を作成して Policy にアタッチすることでイベント検知出来ます。 また同様に Policy アクションとして kill, stop, pause, nothing(notify only) が選べます。\n設定パラメータ名 値 readPathsMatchItems true readPaths \u0026lt;ファイルのパス\u0026gt; readWritePathsMatchItems false Workload Policy 特定プロセスの起動 下記の Rule を作成して Policy にアタッチすることで特定のプロセスが特定のオプションを用いて起動されているかどうかを検知出来ます。\n設定パラメータ名 値 condition user.name = \u0026ldquo;nginx\u0026rdquo; and proc.name = \u0026ldquo;nginx\u0026rdquo; output 任意の情報を出力 動作確認した際の system call の情報は下記になるのでこれらを condition に記せば検知されることになります。\n(user.name=nginx user.loginuid=-1 proc.cmdline=nginx container.id=1e2a2f03e1934a719d98c45276ea57eb-265927825 container_name=web evt.type=accept evt.res= proc.pid=76 proc.cwd=/ proc.ppid=39 proc.pcmdline=nginx -g daemon off; proc.sid=15 proc.exepath=/usr/sbin/nginx user.uid=101 user.loginname= group.gid=101 group.name=nginx container.name=web image=nginx:latest)\nベストプラクティス的な Policy 設定 上記のように個別に Policy, Rule を定義しても、漏れが発生・想定外が発生するので、Sysdig が Managed Rules, Policies を用意してくれています。少なくとも下記を有効化することで大抵の脅威検知は可能だそうです。\nSysdig Runtime Threat Detection (Workload Policy)\nThis policy contains rules which Sysdig considers High Confidence of a security incident. They are tightly coupled to common attacker TTP\u0026rsquo;s. They have been designed to minimize false positives but may still result in some depending on your environment.\nSysdig Runtime Threat Intelligence (Workload Policy)\nThis policy contains rules using Indicators of Compromise curated by the Sysdig Threat Intelligence Team from premium, OSINT, and custom threat intelligence feeds. Any events from the rules in this policy should be considered serious and investigated as soon as possible.\nSysdig Runtime Notable Events (Workload Policy)\nThis Notable Events policy contains rules which may indicate undesired behavior including security threats. The rules are more generalized than Threat Detection policies and may result in more noise. Tuning will likely be required for the events generated from this policy.\nコスト AWS Marketplae に Sysdig Secure DevOps Platform があり、Pricing が記載ありました。\nプラン名 1ヶ月コスト 1年間コスト 説明 Secure Cloud Security $500 $5,000 CloudTrail 連携など Secure Enterprise CaaS $12/task $120/task Container Runtime Security まとめ Terraform Sysdig Provider が用意されているので、自分たちの環境に取っ付き易い構築方法を取ることが出来ました。オーケストレーターエージェントは Module を使えば簡単に構築可能。外部と通信させるのですが VPC 内部に置くことも可能。ワークロードエージェントも Terraform で容易に構築可能。タスク定義のレンダリングを Terraform 以外の技術を使って行っている場合でも4つほどの要件を予め満たしておけば比較的簡単に Sysdig を適用したコンテナが起動可能です。今回はパフォーマンスは測定出来ていませんが、公式ブログの情報を信じると ptrace に比べてパフォーマンス低下は気にする必要なくなりそうです。ここは有償版だなという印象。\nまた、構築したアプリケーションコンテナ (例で Nginx) の中を除いてみると下記のようなプロセスが稼働していました。\n# ps auxf (一分情報を抜粋) /dev/init -- /opt/draios/bin/instrument /docker-entrypoint.sh nginx -g daemon off; /opt/draios/bin/instrument /docker-entrypoint.sh nginx -g daemon off; \\_ /opt/draios/bin/agentino --name=ip-10-0-65-79.ap-northeast-1.compute.internal --aws-account-id=395127550274 --aws-region=ap-north east-1 --aws-az=ap-northeast-1a --aws-fargate-cluster-arn=arn \\_ /opt/draios/bin/pdig -C -t -1 -l /opt/draios/bin/libudigembed.so /docker-entrypoint.sh nginx -g daemon off; \\_ nginx: master process nginx -g daemon off; \\_ nginx: worker process \\_ nginx: worker process \u0026lt;snip\u0026gt; コンテナプロセスが /opt/draios/bin/instrument というコマンドの引数に渡されていました。更にサイドカーで起動している Instrument コンテナのボリューム領域をアプリケーションコンテナがマウントしているのが判りました。\nまた、注意点があります。ECS Exec を用いてコンテナにログインしてもイベント検知の動作確認としての操作を行っても検知してくれませんでした。Policy を独自に作成した際に不便なのですが、動作確認のためにはコンテナ上で sshd を起動するか、タスク定義のエントリポイント指定を書き換えて行う必要があるそうです。ただ、Sysdig 社でも ECS Exec でログインした際の操作に関しても検知範囲にするための開発を予定しているそうです。ECS Exec は IAM の権限があればログイン出来てしまうもので、比較的セキュリティには気を使う機能だと思っているので Sysdig でログイン後の操作を検知出来るようになるのは助かりますし、期待したいところです。\n","permalink":"https://jedipunkz.github.io/post/sysdig-ecs-fargate/","summary":"\u003cp\u003eこんにちは \u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e 🚀 です。\u003c/p\u003e\n\u003cp\u003eECS 構成をもう少しセキュアに保てる構成はないものだろうかと模索しているなかで Sysdig を見つけました。まだ導入できる目処は立っていないのですがある程度ノウハウ蓄積出来てきたのでここで検証内容等を記事にしようかと思っています。\u003c/p\u003e\n\u003cp\u003eSysdig は幾つかのサービスが存在するのですが今回検証したのは Sysdig Serverless Security と呼ばれるモノで ECS Fargate 上のコンテナランタイムセキュリティを実践することができるサービスです。\u003c/p\u003e\n\u003ch2 id=\"sysdig-とは\"\u003eSysdig とは\u003c/h2\u003e\n\u003cp\u003eAWS のサービスにも脅威検知を行うことができるサービスが揃っているのはご存知と思います\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003e対象\u003c/th\u003e\n          \u003cth\u003e目的\u003c/th\u003e\n          \u003cth\u003e技術・サービス\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eAWS リソース\u003c/td\u003e\n          \u003ctd\u003e驚異検知\u003c/td\u003e\n          \u003ctd\u003eAWS GuardDuty\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eまた予防の観点で脆弱性診断が出来るサービスもありあす\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003e対象\u003c/th\u003e\n          \u003cth\u003e目的\u003c/th\u003e\n          \u003cth\u003e技術・サービス\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eAWS リソース\u003c/td\u003e\n          \u003ctd\u003eセキュリティ診断\u003c/td\u003e\n          \u003ctd\u003eAWS Trusted Advisor\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eECS コンテナ\u003c/td\u003e\n          \u003ctd\u003e脆弱性診断\u003c/td\u003e\n          \u003ctd\u003eECR Image Scan\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eEC2 上のソフトウェア\u003c/td\u003e\n          \u003ctd\u003e脆弱性診断\u003c/td\u003e\n          \u003ctd\u003eAWS Inspector\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eここで気がつくと思うのですがコンテナ上の驚異検知を行うサービスが AWS には無いと思っています。 (2022/09 時点)\u003c/p\u003e\n\u003cp\u003eSysdig Serverless Security は ECS Fargate コンテナ上の脅威検知を行うサービスです。ECS Fargate 利用時にコンテナ上の脅威検知を行うサービスは他にも幾つかありますが、Sysdig はシステムコールを利用したコンテナランタイムセキュリティを実践して脅威検知・通知が行えるものになります。自分も詳しくないのですがこれを CWPP (Cloud Workload Protection Platform) と言うらしいです。ワークロードというのはクラウド上の仮想マシン・稼働中のソフトウェアを指して、CWPP はマルウェア保護、脆弱性スキャン、アクセス制御、異常検知の機能を使用してそれぞれのワークロードを保護する、ということらしいです。\u003c/p\u003e","title":"Sysdig+ECS Fargate でコンテナランタイムセキュリティ実践"},{"content":"こんにちは @jedipunkz 🚀 です。\n今回は CNCF にジョインした PipeCD と Datadog を用いて ECS 環境にてプログレッシブデリバリーを実現する方法について調査したので、その内容を記したいと思います。\nそもそもプログレッシブデリバリーとは アプリケーションのデリバリー方法はカナリーリリースやブルーグリーンデプロイメント等がよく知られていると思います。プログレッシブデリバリーはその一歩先を行くデリバリー方式で、Prometheus や Datadog 等のメトリクスを用いて SLO (SRE の SLO と言うよりはデプロイのための指標という意味での) を元にカナリーリリースしたアプリケーションが期待した動作をしているかを確認し (プログレッシブデリバリー的にはこのフェーズを ANALYSIS という様です)、その上でカナリーリリースを完了するというフローになります。\n構成 Pipecd, Piped 共に Kubernetes (EKS) クラスタ上に起動する構成 この検証ではこちらの構成を選択しました。この構成の特徴は\npiped は pipecd の API エンドポイントを指し示す pipecd は UI を提供 pipecd は Filestore (S3, GCS, Minio など), Datastore (MySQL, Firestore など) を利用可 (今回は Minio, MySQL を選択) piped は Target Group, ECS タスク定義等の操作を行うため ECS API へのアクセス権限が必要 piped の pipeline 上のステージで ANALYSIS という Datadog 等のメトリクスを解析する機能を有している アプリケーションレポジトリには app.pipecd.yaml を配置しターゲットグループ・タスク定義・ECS サービスを指し示す piped は GitHub レポジトリを参照 となっています。\nまたこちらの公式ドキュメント には piped のプロセスを ECS/Fargate に起動する構成も紹介されていましたが、その際に\npiped -\u0026gt; pipecd のアクセスが同一 namespace 内で完結しないので Ingress が必要 という制約が出てきます。また piped はシングルバイナリでどこで稼働していても同じなので、今回は下記の構成を選択しました。\n検証環境の構築手順 前提の環境 前提として下記を事前に構築・準備する必要があります。今回は情報量が多くなってしまうのでここの手順は割愛します。\nローカルマシンに helm をインストール EKS クラスタを構築 pipecd という名前の namespace をアサインしている Fargate Profile を用意 blue, green という ALB ターゲットグループ・リスナーを用意 アプリケーションレポジトリの用意 下記のディレクトリ構成でレポジトリを作成していきます。実際には IAM, Subnet, Security Group 等、構築した環境に合わせる必要があります。\n. ├── app.pipecd.yaml ├── servicedef.yaml └── taskdef.yaml タスク定義ファイル taskdef.yaml として保存します。\nfamily: pipecd-nginx-sample executionRoleArn: arn:aws:iam::********:role/ecs-taskexecution-iamrole containerDefinitions: - command: null cpu: 100 image: public.ecr.aws/nginx/nginx:1.23-alpine memory: 100 mountPoints: [] name: web portMappings: - containerPort: 80 compatibilities: - FARGATE requiresCompatibilities: - FARGATE networkMode: awsvpc memory: 512 cpu: 256 pidMode: \u0026#34;\u0026#34; volumes: [] ECS サービスファイル servicedef.yaml として保存します。\ncluster: arn:aws:ecs:ap-northeast-1:********:cluster/ecs-cluster serviceName: pipecd-nginx-sample desiredCount: 2 deploymentConfiguration: maximumPercent: 200 minimumHealthyPercent: 0 schedulingStrategy: REPLICA deploymentController: type: EXTERNAL enableECSManagedTags: true propagateTags: SERVICE launchType: FARGATE networkConfiguration: awsvpcConfiguration: assignPublicIp: ENABLED securityGroups: - sg-******** subnets: - subnet-******** - subnet-******** Piped が参照するコンフィギュレーションファイル このファイルについて説明すると\nkind: ECSApp として pipecd.dev/vbeta1 API にアクセス ECS サービスファイル・タスク定義ファイルの指定を行う primary, cannary として先程作成した blue, green のターゲットグループを指定する pipeline 設定で各パイプラインのステージを指定する ECS_CANARY_ROLLOUT で green ターゲットグループの ECS タスクをローリングデプロイ ECS_TRAFFIC_ROUTING で green ターゲットグループに対して 20% のトラヒックを寄せる ANALYSIS で Datadog Metrics にクエリを投げ、閾値超過の際は FAIL するように設定 下記の例では全体のリクエスト数に対しての 5xx 系エラーの率が 10% を超えない事を期待しています ECS_PRIMARY_ROLLOUT で blue ターゲットグループの ECS タスクのローリングデプロイを実施 ECS_TRAFFIC_ROUTING で blue ターゲットグループに対して 100% のトラヒックを寄せる ECS_CANARY_CLEAN で green ターゲットグループの ECS タスクをクリーンアップ ※ ここでは THRESHOLD (閾値超過) の strategy を選択していますが、その他のものについては考察で述べます。\napp.pipecd.yaml として保存します。\napiVersion: pipecd.dev/v1beta1 kind: ECSApp spec: name: canary labels: env: example team: xyz input: serviceDefinitionFile: servicedef.yaml taskDefinitionFile: taskdef.yaml targetGroups: primary: targetGroupArn: arn:aws:elasticloadbalancing:ap-northeast-1:********:targetgroup/blue/******** containerName: web containerPort: 80 canary: targetGroupArn: arn:aws:elasticloadbalancing:ap-northeast-1:********:targetgroup/green/******** containerName: web containerPort: 80 pipeline: stages: - name: ECS_CANARY_ROLLOUT with: scale: 30 - name: ECS_TRAFFIC_ROUTING with: canary: 20 - name: ANALYSIS with: duration: 10m metrics: - strategy: THRESHOLD provider: datadog-provider interval: 1m expected: max: 10 query: | sum:aws.applicationelb.httpcode_elb_5xx{env:prd,hostname:sample-lb-********.ap-northeast-1.elb.amazonaws.com}.as_count() / sum:aws.applicationelb.request_count{env:prd,hostname:sample-lb-********.ap-northeast-1.elb.amazonaws.com}.as_count() - name: ECS_PRIMARY_ROLLOUT - name: ECS_TRAFFIC_ROUTING with: primary: 100 - name: ECS_CANARY_CLEAN Pipecd 構築 Pipecd のコンフィギュレーション作成 Pipecd (Control Plane) のコンフィギュレーション control-plane-values.yaml を下記の通り用意します。 運用を想定すると quickstart.enabled: false として S3 や RDS 等を用いる構成が望ましいと思いますが、今回の目的ではないのでここでは quickstart.enabled: true として Pipecd を構築します。\nquickstart: enabled: true config: data: | apiVersion: \u0026#34;pipecd.dev/v1beta1\u0026#34; kind: ControlPlane spec: datastore: type: MYSQL config: url: root:test@tcp(pipecd-mysql:3306) database: quickstart filestore: type: MINIO config: endpoint: http://pipecd-minio:9000 bucket: quickstart accessKeyFile: /etc/pipecd-secret/minio-access-key secretKeyFile: /etc/pipecd-secret/minio-secret-key autoCreateBucket: true projects: - id: quickstart staticAdmin: username: hello-pipecd passwordHash: \u0026#34;$2a$10$ye96mUqUqTnjUqgwQJbJzel/LJibRhUnmzyypACkvrTSnQpVFZ7qK\u0026#34; # bcrypt value of \u0026#34;hello-pipecd\u0026#34; secret: encryptionKey: data: encryption-key-just-used-for-quickstart minioAccessKey: data: quickstart-access-key minioSecretKey: data: quickstart-secret-key mysql: rootPassword: \u0026#34;test\u0026#34; database: \u0026#34;quickstart\u0026#34; Pipecd のデプロイ 下記のように helm を使って EKS 上に Pipecd をデプロイします。\nhelm install pipecd oci://ghcr.io/pipe-cd/chart/pipecd --version v0.34.0 \\ --namespace pipecd --create-namespace \\ --values ./control-plane-values.yaml Piped 構築 Pipecd UI にログインし piped の id, key を取得 kubernetes service に作業端末から port forwarding します。\nkubectl -n pipecd port-forward svc/pipecd 8080 ブラウザで http://localhost:8080 にログインします。\nproject name: quickstart username: hello-pipecd password: hello-pipecd トップページ -\u0026gt; プロフィールアイコン -\u0026gt; Settings に遷移して Piped タブを選択し +ADD ボタンを押下。適当な名前・説明を入力し Piped ID, Key を生成したらメモする\nPiped のコンフィギュレーション piped-key-file に上記で得た Piped Key を記します。\necho \u0026#39;\u0026lt;PIPED_KEY\u0026gt;\u0026#39; \u0026gt; piped-key-file コンフィギュレーションには下記のような情報を記します。\n上記で得た情報等を記します。\nPipeCD UI で得た Piped ID PipeCD UI で得た Piped Key ファイルの指定 上記の手順で作成した Git レポジトリ指定 プライベート Git レポジトリにアクセスするための SSH 鍵 AWS リージョン情報 AWS 機密情報のファイル指定 (後にローカルのファイルパスを指定) AWS 機密情報ファイル内のプロファイル名 Datadog API, APP Key 指定 apiVersion: pipecd.dev/v1beta1 kind: Piped spec: projectID: quickstart pipedID: \u0026lt;上記で得た PipedID を記す\u0026gt; pipedKeyFile: /etc/piped-secret/piped-key apiAddress: pipecd:8080 git: sshKeyFile: /etc/piped-secret/ssh-key repositories: - repoId: \u0026lt;Git レポジトリ名\u0026gt; remote: git@github.com:jedipunkz/\u0026lt;レポジトリ名\u0026gt;.git branch: main syncInterval: 1m cloudProviders: - name: sample-ecs type: ECS config: region: ap-northeast-1 credentialsFile: /etc/piped-secret/credentials-key profile: \u0026lt;AWS Profile 名\u0026gt; analysisProviders: - name: rf-sandbox-datadog type: DATADOG config: apiKeyFile: /etc/piped-secret/datadog-api-key applicationKeyFile: /etc/piped-secret/datadog-application-key Piped の起動 事前に Datadog API, APP Key の内容をファイルに保存します。\necho \u0026#39;\u0026lt;Datadog API Key\u0026gt;\u0026#39; \u0026gt; datadog-api-key echo \u0026#39;\u0026lt;Datadog APP Key\u0026gt;\u0026#39; \u0026gt; datadog-application-key 下記の情報を加えて Piped を起動する。\n上記で作成したコンフィギュレーションファイル名 piped-config-k8s-canary.yaml 上で作成した Piped Key の内容をしるした piped-key-file プライベート Git レポジトリにアクセスするための SSH 秘密鍵 AWS 機密情報を記したファイル ~/.aws/credentials Datadog API, APP Key の内容を記したファイル指定 helm upgrade -i piped oci://ghcr.io/pipe-cd/chart/piped --version=v0.34.0 --namespace=pipecd \\ --set-file config.data=./piped-config-k8s-canary.yaml \\ --set-file secret.data.piped-key=./piped-key-file \\ --set-file secret.data.ssh-key=/Users/foo/.ssh/pipecd \\ --set-file secret.data.credentials-key=/Users/foo/.aws/credentials \\ --set args.insecure=true \\ --set-file secret.data.datadog-api-key=./pipecd/datadog-api-key \\ --set-file secret.data.datadog-application-key=./datadog-application-key 事前のタスク定義のレジスト 事前に利用するタスク定義をレジストする必要がある。 下記の内容で taskdef-nginx.json というファイルに保存します。\n{ \u0026#34;family\u0026#34;: \u0026#34;pipecd-nginx-sample\u0026#34;, \u0026#34;executionRoleArn\u0026#34;: \u0026#34;arn:aws:iam::********:role/ecs-taskexecution-iamrole\u0026#34;, \u0026#34;containerDefinitions\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;web\u0026#34;, \u0026#34;essential\u0026#34;: true, \u0026#34;image\u0026#34;: \u0026#34;public.ecr.aws/nginx/nginx:1.23-alpine\u0026#34;, \u0026#34;mountPoints\u0026#34;: [], \u0026#34;portMappings\u0026#34;: [ { \u0026#34;containerPort\u0026#34;: 80, \u0026#34;hostPort\u0026#34;: 80, \u0026#34;protocol\u0026#34;: \u0026#34;tcp\u0026#34; } ] } ], \u0026#34;requiresCompatibilities\u0026#34;: [ \u0026#34;FARGATE\u0026#34; ], \u0026#34;networkMode\u0026#34;: \u0026#34;awsvpc\u0026#34;, \u0026#34;memory\u0026#34;: \u0026#34;512\u0026#34;, \u0026#34;cpu\u0026#34;: \u0026#34;256\u0026#34; } 下記のように awscli を用いてレジストします。\naws ecs register-task-definition --cli-input-json file://taskdef-nginx.json aws ecs list-task-definitions | grep nginx #\u0026lt;-- 確認 動作確認 PipeCD UI の Application の画面において下記の内容で PipeCD UI 上の Application を +ADD します。\nName に任意の名前を入力 kind で ECS を選択 Piped で上記の手順で登録した Piped を選択 Cloud Provider で sample-ecs を選択 Repository で Git レポジトリ名を選択 Config Filename で app.pipecd.yaml を選択 Sync ボタンを押してデプロイ開始。結果 Deployment 画面を確認すると下記の状態になっている。\nデプロイが進み、Primary (上記の green) への ROLLOUT (デプロイ) が完了する\nTarget Group green のターゲットにタスクが一つ起動し始める\nまた同じタイミングで ALB リスナーを確認すると 20% のトラヒックが green に寄せられていることを確認できる\nその後 Deployment が SUCCESS で完了する\nその結果 green のターゲットが draining になる\nALB Listener も blue: 100% と Canary リリースの Primary の Target Group へトラヒックが 100% 寄せられている事を確認出来る。これでデプロイ完了となります。\nFail するパターンの動作 常時 0.3 以上のパーセンテージを示しているクエリ(当方の環境)に対して下記のように expected.max: 0.01 と指定してみる。\n- name: ANALYSIS with: duration: 30m metrics: - strategy: THRESHOLD provider: sample-datadog interval: 10m expected: max: 0.01 query: | query: | sum:aws.applicationelb.httpcode_elb_5xx{env:prd,hostname:sample-lb-********.ap-northeast-1.elb.amazonaws.com}.as_count() / sum:aws.applicationelb.request_count{env:prd,hostname:sample-lb-********.ap-northeast-1.elb.amazonaws.com}.as_count() 結果としてはパイプラインの ANALYSIS ステージで想定した通り Fail し、結果 ROLLBACK された。\n考察 検証環境では Canary リリース・Datadog Analysis・THRESHOLDS 戦略を用いて動作確認し、\n(1) デプロイ開始 (2) Canary 環境へデプロイ (3) 部分的にトラヒックを Canary に寄せる (3) Datadog Metrics を解析しつつ問題なければ Primary 環境へデプロイ (4) Primary に100%のトラヒックを寄せる (5) Canary 環境をクリーンアップ という流れで、カナリーリリースとプログレッシブデリバリーが実践出来ました。\nDatadog Analysis Provider 利用時の注意点 通常であればアプリケーションをデプロイし、ALB のエラー率等を計測しそれを Analysis Provider で指定することになります。その場合 AWS Intergration の機能で Cloudwatch Metrics -\u0026gt; Datadog Metrics とメトリクス情報を送信する必要がありますが、\n遅延が数分ある ALB メトリクスのプロットのインターバルが1分である という問題が浮上します。\nこのことは秒単位で exporter からのデータを Scrape する Prometheus では問題になりません。実際 PipeCD は Prometheus を一番のターゲットにして開発されています。(コンフィギュレーションの scrape_interval (default: 1s) に相当する) よって、Datadog を Analysis Provider に利用する際には下記の幾つかの方法を検討する必要があります。\nその他の Analysis のパイプライン戦略 検証では THRESHOLDS という閾値設定型の戦略を取りましたが、他にも幾つかの戦略が PipeCD には存在します。下記がそれらです。\n(1) PREVIOUS : メトリックを最後に成功したデプロイメントと比較する方法 下記は例で前回 (最後) のデプロイメントのメトリクスクエリ計測結果と比較して、前回よりも偏差が高い場合、Fail する、というものになっています。\napiVersion: pipecd.dev/v1beta1 kind: KubernetesApp spec: pipeline: stages: - name: ANALYSIS with: duration: 30m metrics: - strategy: PREVIOUS provider: my-prometheus deviation: HIGH interval: 5m query: | sum (rate(http_requests_total{status=~\u0026#34;5.*\u0026#34;}[5m])) / sum (rate(http_requests_total[5m])) (2) CANARY_BASELINE : Canary バリアントと Baseline バリアントの間でメトリックを比較する方法 下記は例で Canary, Baseline とでメトリクスを比較しつつ、最終的に deviantion: HIGH という条件で Fail します。\napiVersion: pipecd.dev/v1beta1 kind: KubernetesApp spec: pipeline: stages: - name: ANALYSIS with: duration: 30m metrics: - strategy: CANARY_BASELINE provider: my-prometheus deviation: HIGH interval: 5m query: | sum (rate(http_requests_total{job=\u0026#34;foo-{{ .Variant.Name }}\u0026#34;, status=~\u0026#34;5.*\u0026#34;}[5m])) / sum (rate(http_requests_total{job=\u0026#34;foo-{{ .Variant.Name }}\u0026#34;}[5m])) (3) CANARY_PRIMARY (非推奨) : Canary バリアントとPrimary バリアントの間でメトリックを比較する方法 非推奨の戦略。何らかの理由でベースラインバリアントを提供できない場合は、Canary と Primary を比較することができる。\n所感 一応 PipeCD と ECS, Datadog を用いることでプログレッシブデリバリーを実践出来ました。運用を想定すると RDS, S3 等を用いる構成を組んだほうが良いと思います。またユーザからのリクエストを受けている ECS とは切り離して EKS 上の PipeCD, Piped を運用出来るので、万が一何かあった際、またバージョンアップ等の移行を想定したとしても比較的運用しやすいように思います。あとは考察のところに記した Datadog を用いる際に出てくる難しさをどうクリアするかはいくつかの検討を重ねる必要がありそうです。そのあたりの詳細については自分が努めている企業のテックブログの方に記したいと思います！\n","permalink":"https://jedipunkz.github.io/post/ecs-pipecd/","summary":"\u003cp\u003eこんにちは \u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e 🚀 です。\u003c/p\u003e\n\u003cp\u003e今回は CNCF にジョインした \u003ca href=\"https://pipecd.dev/\"\u003ePipeCD\u003c/a\u003e と Datadog を用いて ECS 環境にてプログレッシブデリバリーを実現する方法について調査したので、その内容を記したいと思います。\u003c/p\u003e\n\u003ch2 id=\"そもそもプログレッシブデリバリーとは\"\u003eそもそもプログレッシブデリバリーとは\u003c/h2\u003e\n\u003cp\u003eアプリケーションのデリバリー方法はカナリーリリースやブルーグリーンデプロイメント等がよく知られていると思います。プログレッシブデリバリーはその一歩先を行くデリバリー方式で、Prometheus や Datadog 等のメトリクスを用いて SLO (SRE の SLO と言うよりはデプロイのための指標という意味での) を元にカナリーリリースしたアプリケーションが期待した動作をしているかを確認し (プログレッシブデリバリー的にはこのフェーズを ANALYSIS という様です)、その上でカナリーリリースを完了するというフローになります。\u003c/p\u003e\n\u003ch2 id=\"構成-pipecd-piped-共に-kubernetes-eks-クラスタ上に起動する構成\"\u003e構成 Pipecd, Piped 共に Kubernetes (EKS) クラスタ上に起動する構成\u003c/h2\u003e\n\u003cp\u003eこの検証ではこちらの構成を選択しました。この構成の特徴は\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003epiped は pipecd の API エンドポイントを指し示す\u003c/li\u003e\n\u003cli\u003epipecd は UI を提供\u003c/li\u003e\n\u003cli\u003epipecd は Filestore (S3, GCS, Minio など), Datastore (MySQL, Firestore など) を利用可 (今回は Minio, MySQL を選択)\u003c/li\u003e\n\u003cli\u003epiped は Target Group, ECS タスク定義等の操作を行うため ECS API へのアクセス権限が必要\u003c/li\u003e\n\u003cli\u003epiped の pipeline 上のステージで ANALYSIS という Datadog 等のメトリクスを解析する機能を有している\u003c/li\u003e\n\u003cli\u003eアプリケーションレポジトリには app.pipecd.yaml を配置しターゲットグループ・タスク定義・ECS サービスを指し示す\u003c/li\u003e\n\u003cli\u003epiped は GitHub レポジトリを参照\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eとなっています。\u003c/p\u003e","title":"ECS + PipeCD + Datadog でプログレッシブデリバリーを実現"},{"content":"こんにちは。jedipunkz🚀 です。\n引き続き Go を学習しています。前回の記事 ECS コンテナにログインする CLI を Go 言語で作った話 のまとめにも記したのですが Go のコードを書くアイデアとして下記をぼんやり考えていました。\nECR 脆弱性スキャンのパッケージを開発 そのパッケージを利用して Datadog のカスタムメトリクスとして送信 同様にそのパッケージを利用して ECR スキャンの CLI を作成 その紹介を軽くしたいと思います。\n開発した ECR 脆弱性スキャンの Go パッケージ 開発したパッケージは https://github.com/jedipunkz/ecrscan になります。\n下記のように Ecr 構造体を初期化します。\ne := myecr.Ecr{} e.Repositories = [][]string{ {\u0026#34;image-to-scan\u0026#34;, \u0026#34;latest\u0026#34;}, } e.Resion = \u0026#34;ap-northeast-1\u0026#34; finding, vulFindings, _ := e.ListFindings() その後 ListFindings() メソッドでスキャンします。結果、finding.FindingSeverityCounts には下記の深刻度毎のイメージに含まれている脆弱性の数が入ります。\nINFORMATIONAL LOW MEDIUM HIGH CRITICAL UNDEFINED また、vulFindings には含まれている脆弱性の\nCVE 名 深刻度レベル (INFORMATIONAL, LOW, MEDIUM, HIGH, CRITICAL, UNDEFINED) CVE URI 説明 が入ります。\nCLI の開発 このパッケージを利用して開発したのが https://github.com/jedipunkz/evs という CLI です。利用方法は下記のように --image でイメージとタグを指定、--region でリージョンを指定して実行するだけです。\n結果、下記のように含まれている脆弱性の深刻度レベル毎のカウント数が一覧表示されます。\n$ evs scan --image image-to-scan:latest --region ap-northeast-1 +----------------+--------+ | SEVERITY LEVEL | COUNTS | +----------------+--------+ | MEDIUM | 2 | | LOW | 12 | | INFORMATIONAL | 4 | +----------------+--------+ また list サブコマンドを利用すると、下記のように CVE 名や深刻度、URI、説明の一覧が出力されます。\n$ evs list --image testimage:latest --region ap-northeast-1 +------------------+---------------+----------------------------------------------------------------+---------------------------------+ | CVE NAME | SEVERITY | URI | DESCRIPTION | +------------------+---------------+----------------------------------------------------------------+---------------------------------+ | CVE-2021-20305 | MEDIUM | http://people.ubuntu.com/~ubuntu-security/cve/CVE-2021-20305 | A flaw was found in Nettle | | | | | in versions before 3.7.2, | | | | | where several Nettle signature | | | | | verification functions | | | | | (GOST DSA, EDDSA \u0026amp; ECDSA) | \u0026lt;snip\u0026gt; | CVE-2020-14155 | INFORMATIONAL | http://people.ubuntu.com/~ubuntu-security/cve/CVE-2020-14155 | libpcre in PCRE before 8.44 | | | | | allows an integer overflow | | | | | via a large number after a (?C | | | | | substring. | | CVE-2017-11164 | INFORMATIONAL | http://people.ubuntu.com/~ubuntu-security/cve/CVE-2017-11164 | In PCRE 8.41, the OP_KETRMAX | | | | | feature in the match function | | | | | in pcre_exec.c allows stack | | | | | exhaustion (uncontrolled | | | | | recursion) when processing a | | | | | crafted regular expression. | +------------------+---------------+----------------------------------------------------------------+---------------------------------+ 単純な CLI なのでもうひと工夫欲しいところですが、今のところアイデアが無いです\u0026hellip;。\nECR スキャン結果を Datadog カスタムメトリクスにサブミット 同様に紹介した Go パッケージを利用して複数のイメージの脆弱性スキャン結果を Datadog に送ることを検討していました。日々、開発者はアプリケーションのコンテナイメージを CI/CD の中でビルドしているのですがそのイメージのセキュリティ意識を持つという点において課題があると感じていたからです。元々、Slack に週次でイメージに含まれている脆弱性の一覧を出力していたのですが、ただ流れているだけで開発者がそれらについて深堀りして追う作業をする傾向はほとんどありませんでした。この課題に対して、脆弱性のカウント数の推移が Datadog ダッシュボードとして可視化されれば何かしらアクションを起こして下さるのでは？という仮説の元に開発してみた次第です。(結果はこれから分かると思います \u0026hellip;)\nこちらは業務で開発したものなのでコードをお見せすることが出来ませんが、AWS Lambda 関数を用いて1時間に一度、複数のイメージの脆弱性スキャンを先程の Go パッケージで行い、Datadog のカスタムメトリクスにサブミットする事で、Datadog ダッシュボードをプロットしています。\n※ イメージ名はマスクしています。\n見ての通り、長いスパンで見ないと変化が見られません😷。 この Lambda で用いた Datadog API は下記に公式の情報があります。 https://docs.datadoghq.com/ja/api/latest/metrics/\nまとめ CLI の方は前述したとおり、追加でサブコマンドを開発したいところですが、何かいいアイデアあるかなぁ\u0026hellip;。思い浮かばない。また、Datadog ダッシュボードの方は誰か見てくれているかな\u0026hellip;。不安。\n仕事では今、GitHub の Dependabot Alerts, Security Updates, ersion Updates 等導入したり、GitHub CodeQL 検証したり、その他のセキュリティ対策ソフトウェアを検証したりと、色々頑張っています。また運用も回ってきて、良き。\nまた Go の方も引き続きアイデアが乏しいながらも何かしら積極的に開発していきたいところです。\n","permalink":"https://jedipunkz.github.io/post/ecr-scan-datadog-go/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://twitter.com/jedipunkz\"\u003ejedipunkz🚀\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003e引き続き Go を学習しています。前回の記事 \u003ca href=\"http://localhost:1313/post/ecs-login-cli/\"\u003eECS コンテナにログインする CLI を Go 言語で作った話\u003c/a\u003e のまとめにも記したのですが Go のコードを書くアイデアとして下記をぼんやり考えていました。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eECR 脆弱性スキャンのパッケージを開発\n\u003cul\u003e\n\u003cli\u003eそのパッケージを利用して Datadog のカスタムメトリクスとして送信\u003c/li\u003e\n\u003cli\u003e同様にそのパッケージを利用して ECR スキャンの CLI を作成\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eその紹介を軽くしたいと思います。\u003c/p\u003e\n\u003ch3 id=\"開発した-ecr-脆弱性スキャンの-go-パッケージ\"\u003e開発した ECR 脆弱性スキャンの Go パッケージ\u003c/h3\u003e\n\u003cp\u003e開発したパッケージは \u003ca href=\"https://github.com/jedipunkz/ecrscan\"\u003ehttps://github.com/jedipunkz/ecrscan\u003c/a\u003e になります。\u003c/p\u003e\n\u003cp\u003e下記のように Ecr 構造体を初期化します。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#a6e22e\"\u003ee\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e:=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003emyecr\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eEcr\u003c/span\u003e{}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#a6e22e\"\u003ee\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eRepositories\u003c/span\u003e = [][]\u003cspan style=\"color:#66d9ef\"\u003estring\u003c/span\u003e{\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t{\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;image-to-scan\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;latest\u0026#34;\u003c/span\u003e},\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#a6e22e\"\u003ee\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eResion\u003c/span\u003e = \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;ap-northeast-1\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#a6e22e\"\u003efinding\u003c/span\u003e, \u003cspan style=\"color:#a6e22e\"\u003evulFindings\u003c/span\u003e, \u003cspan style=\"color:#a6e22e\"\u003e_\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e:=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003ee\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eListFindings\u003c/span\u003e()\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eその後 \u003ccode\u003eListFindings()\u003c/code\u003e メソッドでスキャンします。結果、\u003ccode\u003efinding.FindingSeverityCounts\u003c/code\u003e には下記の深刻度毎のイメージに含まれている脆弱性の数が入ります。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eINFORMATIONAL\u003c/li\u003e\n\u003cli\u003eLOW\u003c/li\u003e\n\u003cli\u003eMEDIUM\u003c/li\u003e\n\u003cli\u003eHIGH\u003c/li\u003e\n\u003cli\u003eCRITICAL\u003c/li\u003e\n\u003cli\u003eUNDEFINED\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eまた、\u003ccode\u003evulFindings\u003c/code\u003e には含まれている脆弱性の\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCVE 名\u003c/li\u003e\n\u003cli\u003e深刻度レベル (INFORMATIONAL, LOW, MEDIUM, HIGH, CRITICAL, UNDEFINED)\u003c/li\u003e\n\u003cli\u003eCVE URI\u003c/li\u003e\n\u003cli\u003e説明\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eが入ります。\u003c/p\u003e","title":"ECR 脆弱性スキャン結果表示 CLI の開発と Datadog プロット"},{"content":"こんにちは @jedipunkz 🚀 です。\n今回は Go 言語で ECS コンテナにログインする CLI を作った話を書きます。\n開発の経緯 自分はまだ Go 言語の初学者で学習のために開発するアイデアを探していた状態でした。そこで自分の勤めている会社で ECS Execute 機能を使ったコンテナログインの機能を開発者に提供していた事を思い出し色々調べていて「もう少し手間が省けないか？」と思い立った、という経緯で開発をはじめました。\nawscli を使った ECS Execute 機能によるコンテナログイン 手間が多いと書きましたが実際に awscli を使う場合どの程度の手間があるのか簡単に記します。まず下記のコマンドを実行して\n$ aws ecs list-tasks --cluster \u0026lt;クラスタ名\u0026gt; --service \u0026lt;サービス名\u0026gt; taskArn が得られるので Arn から task ID を拾って、その task ID を使って\n$ aws ecs execute --cluster \u0026lt;クラスタ名\u0026gt; \\ --task \u0026lt;task ID\u0026gt; \\ -- container \u0026lt;コンテナ名\u0026gt; \\ --interfactive \\ --command \u0026#34;sh\u0026#34; とコマンドを実行することでコンテナにログイン出来ます。が手間が少し多いのと task ID を拾い出す作業も辛いので改善したい\u0026hellip;。\n操作画面 ということで miniecs という CLI を作ったのですが、 まずは操作している様子を貼り付けます。😷 Fuzzy Finder なインクリメンタルサーチが出来る CLI になっていて、ECS クラスタ名・ECS サービス名・コンテナ名を一部入力するとログインしたい環境が選択出来るツールになっています。\n使ったライブラリ 下記のライブラリを使って開発しました。CLI のフレームワークは spf13/conbra。コンフィギュレーションローダの spf13/viper。ログ出力は sirupsen/logrus。Fuzzy Finder は ktr0731/go-fuzzyfinder を使った感じです。\ngithub.com/aws/aws-sdk-go github.com/ktr0731/go-fuzzyfinder github.com/sirupsen/logrus github.com/spf13/cobra github.com/spf13/viper コード 下記のレポジトリで Apache 2.0 ライセンスで公開しています。miniecs という名前です。\nhttps://github.com/jedipunkz/miniecs.git\naws-sdk-go の利用と AWS サポートに教えてもらった内容 aws-sdk-go v1 を今回利用しました。下記の関数 ExecuteCommand を用います。\nhttps://docs.aws.amazon.com/sdk-for-go/api/service/ecs/#ECS.ExecuteCommand\nインプットとしては ExecuteCommandInput で下記のような情報を入れる形です。\ntype ExecuteCommandInput struct { Cluster *string `locationName:\u0026#34;cluster\u0026#34; type:\u0026#34;string\u0026#34;` Command *string `locationName:\u0026#34;command\u0026#34; type:\u0026#34;string\u0026#34; required:\u0026#34;true\u0026#34;` Container *string `locationName:\u0026#34;container\u0026#34; type:\u0026#34;string\u0026#34;` Interactive *bool `locationName:\u0026#34;interactive\u0026#34; type:\u0026#34;boolean\u0026#34; required:\u0026#34;true\u0026#34;` Task *string `locationName:\u0026#34;task\u0026#34; type:\u0026#34;string\u0026#34; required:\u0026#34;true\u0026#34;` } アウトプットは ExecuteCommandOutput で下記のようになります。\ntype ExecuteCommandOutput struct { ClusterArn *string `locationName:\u0026#34;clusterArn\u0026#34; type:\u0026#34;string\u0026#34;` ContainerArn *string `locationName:\u0026#34;containerArn\u0026#34; type:\u0026#34;string\u0026#34;` ContainerName *string `locationName:\u0026#34;containerName\u0026#34; type:\u0026#34;string\u0026#34;` Interactive *bool `locationName:\u0026#34;interactive\u0026#34; type:\u0026#34;boolean\u0026#34;` Session *Session `locationName:\u0026#34;session\u0026#34; type:\u0026#34;structure\u0026#34;` TaskArn *string `locationName:\u0026#34;taskArn\u0026#34; type:\u0026#34;string\u0026#34;` } ここで疑問点が浮上。関数 ExecuteCommand を実行すると何も考えずにコンテナにログインが可能になるのかと思いきや出来ない模様。 そこで AWS サポートに問い合わせ下記のような回答を頂きました。\naws-sdk-go の ExecuteCommand を使ってコンテナにログインするには戻り値 ExecuteCommandOutput の情報を利用して session-manager-plugin を利用する必要があります。\nふむ。ただ session-manager-plugin は OS にインストールするバイナリ形式のファイルで特にオプションを渡して実行できるような CLI 形式のコマンドではない模様でした。更に AWS の回答を読むと\nぜひ AWS で開発して Apache 2.0 で公開している copilot-cli を参考にして下さい。\nとあって、実際にコードを見てみました。下記が実際にコンテナへのログインをしているセッションを張っている関数のようです。 https://github.com/aws/copilot-cli/blob/mainline/internal/pkg/exec/ssm_plugin.go\n// StartSession starts a session using the ssm plugin. func (s SSMPluginCommand) StartSession(ssmSess *ecs.Session) error { response, err := json.Marshal(ssmSess) if err != nil { return fmt.Errorf(\u0026#34;marshal session response: %w\u0026#34;, err) } if err := s.runner.InteractiveRun(ssmPluginBinaryName, []string{string(response), aws.StringValue(s.sess.Config.Region), startSessionAction}); err != nil { return fmt.Errorf(\u0026#34;start session: %w\u0026#34;, err) } return nil } Apache 2.0 ライセンスということで今回開発した miniecs でもこのあたりのコードは利用・参考にしています。\nまとめ まだテストを書いていないのとリファクタの進んでいないですしサブコマンドの数も list (環境一覧表示), exec (コマンド実行), login (コンテナログイン) だけという状況なので、もう少し機能を増やしたいなと思いつつあまりアイデアがないです。Go 初学者にとっては良い開発テーマだったとは思っています。\n次に何を開発するか \u0026hellip; aws-sdk-go を使った開発は幾つかしてきて、そろそろ方向性を変えたいなとも思いつつ何を作るべきか\u0026hellip; 🤔 。Prometheus の Exporter や、Datadog の Go SDK を使った開発なども良さそうかなぁ。😷 ちょうど昨日思いついたアイデアとしては ECR のイメージスキャン (また aws-sdk-go 使うけど) を実行する Lambda を開発して Datadog か Prometheus にデータを入れて、可視化・分析するなどどうかなぁと思いました。\nGitHub Actions でアプリの Docker ビルド \u0026amp; ECR プッシュと同時に API Gateway 経由でイメージスキャンして結果を Datadog カスタムメトリクスとして送信 一定期間毎に ECR イメージスキャンする Prometheus Exporter を開発 どちらも面白そう🤤\n","permalink":"https://jedipunkz.github.io/post/ecs-login-cli/","summary":"\u003cp\u003eこんにちは \u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e 🚀 です。\u003c/p\u003e\n\u003cp\u003e今回は Go 言語で ECS コンテナにログインする CLI を作った話を書きます。\u003c/p\u003e\n\u003ch3 id=\"開発の経緯\"\u003e開発の経緯\u003c/h3\u003e\n\u003cp\u003e自分はまだ Go 言語の初学者で学習のために開発するアイデアを探していた状態でした。そこで自分の勤めている会社で ECS Execute 機能を使ったコンテナログインの機能を開発者に提供していた事を思い出し色々調べていて「もう少し手間が省けないか？」と思い立った、という経緯で開発をはじめました。\u003c/p\u003e\n\u003ch3 id=\"awscli-を使った-ecs-execute-機能によるコンテナログイン\"\u003eawscli を使った ECS Execute 機能によるコンテナログイン\u003c/h3\u003e\n\u003cp\u003e手間が多いと書きましたが実際に awscli を使う場合どの程度の手間があるのか簡単に記します。まず下記のコマンドを実行して\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e$ aws ecs list-tasks --cluster \u0026lt;クラスタ名\u0026gt; --service \u0026lt;サービス名\u0026gt;\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003etaskArn が得られるので Arn から task ID を拾って、その task ID を使って\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e$ aws ecs execute --cluster \u0026lt;クラスタ名\u0026gt; \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  --task \u0026lt;task ID\u0026gt; \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  -- container \u0026lt;コンテナ名\u0026gt; \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  --interfactive \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  --command \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;sh\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eとコマンドを実行することでコンテナにログイン出来ます。が手間が少し多いのと task ID を拾い出す作業も辛いので改善したい\u0026hellip;。\u003c/p\u003e\n\u003ch3 id=\"操作画面\"\u003e操作画面\u003c/h3\u003e\n\u003cp\u003eということで miniecs という CLI を作ったのですが、 まずは操作している様子を貼り付けます。😷 Fuzzy Finder なインクリメンタルサーチが出来る CLI になっていて、ECS クラスタ名・ECS サービス名・コンテナ名を一部入力するとログインしたい環境が選択出来るツールになっています。\u003c/p\u003e","title":"ECS コンテナにログインする CLI を Go 言語で作った話"},{"content":"こんにちは。jedipunkz🚀 です。\n今回も READYFOR Advent Calendar 2021 の記事として執筆します。\n今回のテーマ 前回の記事 では ECS 移行後の構成について検討する内容を記しました。Progressive Delivery を実践する上でもその一歩手前の構成と言っていいカナリーリリース構成について、今回は記していきたいと思います。\nデグレしてしまっていたカナリーリリース READYFOR では AWS ECS 移行を行い ECS + CodeDeploy による Blue/Green デプロイメントを導入しました。逆に移行前までに出来ていたカナリーリリースが実施できなくなりました。とは言ってもそれまで開発者がカナリーリリースに対して求めていた主な機能はロールバックだったため、ひとまずは Blue/Green デプロイメントで事足りている状況なのですが、今後大きな機能をリリースする際にはトラヒックを徐々に寄せ影響を把握した上でリリース進めるという作業は必要になってくる可能性があります。\nよって、\nProgressive Delivery の一歩手前の構成を実践する 大きなリリースのための環境整備 という意味でも、一回カナリーリリース構成について検討しておこうと考えました。\n環境構築 今回用意した Terraform コード 検証で作成した AWS 環境をデプロイするための Terraform コードを下記の場所に置いてあります。参考にしてください。\nhttps://github.com/jedipunkz/tf-appmesh-ecs-canary-release\n(今回検証で作成したコードは業務上作成したものですが、READYFOR の OSS ポリシーに則り著作権譲渡をうけており、自らのGitHubリポジトリで公開しています。)\nApp Mesh ECS NLB Service Discovery Envoy X-Ray といった技術要素で構成されています。\nTerraform コードによるデプロイ実施 上記に記した Terraform コードを使った構成のデプロイ手順を記します。\n前提として Terraform バージョン 1.0.x 系以上をローカルにインストールする必要があります。\n$ # AWS クレデンシャル情報を設定 $ git clone https://github.com/jedipunkz/tf-appmesh-ecs-canary-release $ cd tf-appmesh-ecs-canary-release $ terraform plan $ terraform apply 構成 検証で構築した構成(上記の Terraform コードで構築できる) は下記になります。\n構成の特徴とリクエスト処理の流れ NLB で受け付けた TCP 80 番ポートのリクエストを ECS Task 上で起動している Envoy (Gateway) にリクエスト分散 Envoy (Gateway) はトラヒックを AppMesh Virtual Gateway のルーティング先に指定している VirtualService へ分散 VirtualService には VirtualRouter が設定されておりルーティング情報として VirtualNode x 2 台を設定している 結果 VirtualNode x 2 \u0026ldquo;example1\u0026rdquo;, \u0026ldquo;example2\u0026rdquo; へ荷重ルーティングによってトラヒックがルーティングされる 各 VirtualNode は ECS Task (example1, example2) に紐付いている ※example1, example2 は上記 Terraform コード内で設定している AWS リソース名です。\nサービスディスカバリの構成 AWS ServiceDiscovery を用いてサービスディスカバリの機能を用いています。名前の関係については下記になります。\nNamespace として example.internal を作成 (CloudMap として生成される) Namespace example.internal 配下に exmaplegw.example.internal をサービスディスカバリサービスとして生成 Namespace example.internal 配下に exmaple1.example.internal をサービスディスカバリサービスとして生成 Namespace example.internal 配下に exmaple2.example.internal をサービスディスカバリサービスとして生成 また、各サービスディスカバリサービスはヘルスチェックの機能を有していて、実体である ECS Task が無いと Route53 レコードは生成されません。\n動作確認 構築した App Mesh + ECS 構成の動作確認をしてみます。\n下記の App Mesh Route の記述 にある通り、各 VirtualNode (exmaple1, example2) への荷重ルーティングとして 95:5 という比率を設定しています。\nspec { http_route { match { prefix = \u0026#34;/\u0026#34; } action { weighted_target { virtual_node = aws_appmesh_virtual_node.example1.name weight = 95 } weighted_target { virtual_node = aws_appmesh_virtual_node.example2.name weight = 5 } } } } この設定した比率 95:5 が機能しているかを確認するため、下記のような簡単なスクリプトを用意して実行してみます。\n#!/bin/sh i=0 while [ $i -ne 100 ] do i=$(($i+1)) echo \u0026#34;$i\u0026#34; curl http://\u0026lt;NLB の DNS 名/ \u0026gt;\u0026gt; /tmp/example.log sleep 1 done 結果として下記のに 97:3 という比率でそれぞれの VirtualNode (ECS Task) から応答があり、App Mesh Route の設定値 95:5 とほぼ同等である事が判りました。\n$ grep example1 /tmp/example.log | wc -l 97 $ grep example2 /tmp/example.log | wc -l 3 aws-sdk-go を使った荷重ルーティング操作について READYFOR ではインフラのコードを Go 言語を使って開発する機会が多いのですが、ここでは aws-sdk-go を使った荷重ルーティング操作について調べたので記していきます。\nなぜ aws-sdk-go なのか？ 荷重ルーティングは App Mesh の VirtualRoute というルーティング設定に設定を施すのですが、それ自体が AWS リソースです。よって Terraform コードでリソース作成したのですが、SRE やインフラメンバと異なり、開発者自身にカナリーリリースを実践してもらう 事を想定すると Terraform コードによる操作は不向きと考えました。よって他の方法を考えなくてはいけません。\nSRE の持っている権限と機能を開発者に提供するという意味では Slack 等のコミュニケーションツールの入力によるボット操作が非常に融和性が高いと考えています。よって、このボットを開発する上でも aws-sdk-go を使って荷重ルーティング操作が出来てしまえば、あとは開発するだけとなります。\n参考資料 aws-sdk-go の荷重ルーティング操作については下記の関数を利用します。\nhttps://docs.aws.amazon.com/sdk-for-go/api/service/appmesh/#AppMesh.UpdateRoute\n荷重ルーティング操作コード 下記に動作確認まで行った Go のコードを記します。 UpdateRoute メソッドを実行するだけで操作が行えました。UpdateRouteInput に環境情報を記しつつ、Spec に荷重設定値を入力する事で荷重ルーティングを操作できることが分かります。\npackage main import ( \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;github.com/aws/aws-sdk-go/aws\u0026#34; \u0026#34;github.com/aws/aws-sdk-go/aws/session\u0026#34; \u0026#34;github.com/aws/aws-sdk-go/service/appmesh\u0026#34; ) func main() { sess := session.New() svc := appmesh.New(sess) jsonBlob := []byte(` { \u0026#34;HttpRoute\u0026#34;: { \u0026#34;Action\u0026#34;: { \u0026#34;WeightedTargets\u0026#34;: [{ \u0026#34;VirtualNode\u0026#34;: \u0026#34;example1\u0026#34;, \u0026#34;Weight\u0026#34;: 90 },{ \u0026#34;VirtualNode\u0026#34;: \u0026#34;example2\u0026#34;, \u0026#34;Weight\u0026#34;: 10 }] }, \u0026#34;Match\u0026#34;: { \u0026#34;Prefix\u0026#34;: \u0026#34;/\u0026#34; } } }`) var spec *appmesh.RouteSpec err := json.Unmarshal(jsonBlob, \u0026amp;spec) if err != nil { log.Fatal(err) } input := \u0026amp;appmesh.UpdateRouteInput{ // ClientToken: aws.String(\u0026#34;foo\u0026#34;), MeshName: aws.String(\u0026#34;example\u0026#34;), MeshOwner: aws.String(\u0026#34;\u0026lt;AWS アカウント ID\u0026gt;\u0026#34;), RouteName: aws.String(\u0026#34;example\u0026#34;), Spec: spec, VirtualRouterName: aws.String(\u0026#34;example\u0026#34;), } result, err := svc.UpdateRoute(input) if err != nil { log.Println(err) } fmt.Println(result) } まとめと考察 結果として下記のことが分かりました。\nApp Mesh + ECS の最小構成が組め、カナリーリリース機能を開発者へ提供できる 操作自体も開発者自身に行ってもらえる ですが、幾つかか考えなくてはいけない事があります。\n問題点 一方の VirualNode A (仮の名前として用います) からもう一方の VirtualNode B へカナリーリリースを実施すると通常時にリクエストを受ける環境は VirtualNode B とります。その次のリリースタイミングでは逆に VirtualNode B -\u0026gt; VirtualNode A と切り替えなくてはいけないのか？またアプリケーションのデプロイワークフローの対象リソースが A なのか B なのか、という問題が浮上してきます。\nこれらが解決できないと、運用負担増やトラブルシュートの難易度高といった受け入れがたい具体的な問題に繋がると考えています。\n問題点の解消: 構成案 #1 そこで上記の問題を回避しつつどう構成するかを考えてみました。\n前提の環境 下記の前提で構成を考えてみます。\nVirtualNode A を通常時にサービスを受ける ECS Service として用意 VirtualNode B をカナリーリリース時のサービスを受ける ECS Service として用意 ブランチと ECS Service の対応 レポジトリのブランチ main , canary に対して下記のような対応付けで ECS Service をデプロイする戦略です。\nブランチ名 デプロイ先の ECS Serivice main ECS Service (VIrtualNode A) canary ECS Service (VirtualNode B) リリースの流れ リリースの流れとしては下記が考えれます。\n(1) canary ブランチにマージ \u0026amp; VirtualNode B ECS へデプロイ (2) 荷重ルーティングにより A -\u0026gt; B へ徐々にトラヒックを流し最終的に 100% に (3) main ブランチにマージして VirtualNode A ECS へデプロイ (4) 荷重ルーティング A:B = 100:0 として VirtualNode A ECS へ 100% 流す これにより、VirtualNode A は通常時用 ECS 環境という前提を守ることが出来ます。また、A -\u0026gt; B, B -\u0026gt; A とカナリーリリースの流れの向きを切り替える問題も解消されます。\nただ、これは一案であって、他にも良い構成が考えられるかもしれません。\n今回紹介した構成以外の構成について 今回は検証しなかったのですが、VirtualNode の backend 設定が可能なようです。詳細はこちらの Terraform ドキュメント にあります。これによって下記のような流れが組めると想定しています。\nNLB -\u0026gt; VirtualGateway (ECS) -\u0026gt; VirtualSerivce α -\u0026gt; VirtualRoute α -\u0026gt; VirtualNod α (ECS) x n -\u0026gt; VirtualService β -\u0026gt; VirtualRouter β -\u0026gt; VirtualNode β (ECS) x n ロードバランサなのかサービスディスカバリなのかの考察 ロードバランサとサービスディスカバリが提供できる機能は\n冗長性 保守性 拡張性 という意味ではほぼ同等の機能を有していると考えています。なのでより枯れた技術であるロードバランサを使って構成を考えられないか検討してみました。\n結果、VirtualNode -\u0026gt; SerivceDiscovery -\u0026gt; ECS Service と連携する際に ECS Service は ServiceDiscovery 自体をレジスト出来るので上記の構成図の様な構成が組めるのですが、ロードバランサにすると、ECS Service に当てはめられるのは LB となります。荷重ルーティング・カナリーリリースをする際に ECS Service が複数必要なわけですが、そうすると ECS Service の数分の LB が必要になります。もちろんそれを構成する TargetGroup, Listenr (ListenrRule) も必要になります。\n尚、その際には Virtual Node の Service Discovery の dns パラメータに hostname だけを記して、LB の DNS 名を知るせば良さそうに見受けました。が、この構成については無駄なリソースが発生すると判断したことを受け、検証未実施です。\nservice_discovery { dns { hostname = \u0026#34;nlb-****.example.internal\u0026#34; } } これは同等の機能を提供してくれる ServiceDiscovery を使ったほうが無駄な aws リソースを作らなくて済む、という結果につながります。\nまとめのまとめ(所感) 以上、ECS を使ったカナリーリリース構成について記しました。個人的には導入の前に Envoy や X-ray についてもう少し情報収集して調査の解像度を上げていく必要があると感じています。すべてのリクエストのトラヒックが Envoy コンテナを介す事になり、それらの知識が十分に無いと万が一のトラブルシュートの際に困るだろうなぁと考えています。\n","permalink":"https://jedipunkz.github.io/post/app-mesh-ecs-canary/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://twitter.com/jedipunkz\"\u003ejedipunkz🚀\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003e今回も \u003ca href=\"https://qiita.com/advent-calendar/2021/readyfor\"\u003eREADYFOR Advent Calendar 2021\u003c/a\u003e の記事として執筆します。\u003c/p\u003e\n\u003ch2 id=\"今回のテーマ\"\u003e今回のテーマ\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://jedipunkz.github.io/post/progressive_delivery/\"\u003e前回の記事\u003c/a\u003e では ECS 移行後の構成について検討する内容を記しました。Progressive Delivery を実践する上でもその一歩手前の構成と言っていいカナリーリリース構成について、今回は記していきたいと思います。\u003c/p\u003e\n\u003ch3 id=\"デグレしてしまっていたカナリーリリース\"\u003eデグレしてしまっていたカナリーリリース\u003c/h3\u003e\n\u003cp\u003eREADYFOR では AWS ECS 移行を行い ECS + CodeDeploy による Blue/Green デプロイメントを導入しました。逆に移行前までに出来ていたカナリーリリースが実施できなくなりました。とは言ってもそれまで開発者がカナリーリリースに対して求めていた主な機能はロールバックだったため、ひとまずは Blue/Green デプロイメントで事足りている状況なのですが、今後大きな機能をリリースする際にはトラヒックを徐々に寄せ影響を把握した上でリリース進めるという作業は必要になってくる可能性があります。\u003c/p\u003e\n\u003cp\u003eよって、\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eProgressive Delivery の一歩手前の構成を実践する\u003c/li\u003e\n\u003cli\u003e大きなリリースのための環境整備\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eという意味でも、一回カナリーリリース構成について検討しておこうと考えました。\u003c/p\u003e\n\u003ch2 id=\"環境構築\"\u003e環境構築\u003c/h2\u003e\n\u003ch3 id=\"今回用意した-terraform-コード\"\u003e今回用意した Terraform コード\u003c/h3\u003e\n\u003cp\u003e検証で作成した AWS 環境をデプロイするための Terraform コードを下記の場所に置いてあります。参考にしてください。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/jedipunkz/tf-appmesh-ecs-canary-release\"\u003ehttps://github.com/jedipunkz/tf-appmesh-ecs-canary-release\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e(今回検証で作成したコードは業務上作成したものですが、READYFOR の OSS ポリシーに則り著作権譲渡をうけており、自らのGitHubリポジトリで公開しています。)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eApp Mesh\u003c/li\u003e\n\u003cli\u003eECS\u003c/li\u003e\n\u003cli\u003eNLB\u003c/li\u003e\n\u003cli\u003eService Discovery\u003c/li\u003e\n\u003cli\u003eEnvoy\u003c/li\u003e\n\u003cli\u003eX-Ray\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eといった技術要素で構成されています。\u003c/p\u003e\n\u003ch3 id=\"terraform-コードによるデプロイ実施\"\u003eTerraform コードによるデプロイ実施\u003c/h3\u003e\n\u003cp\u003e上記に記した Terraform コードを使った構成のデプロイ手順を記します。\u003c/p\u003e\n\u003cp\u003e前提として Terraform バージョン 1.0.x 系以上をローカルにインストールする必要があります。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e$ \u003cspan style=\"color:#75715e\"\u003e# AWS クレデンシャル情報を設定\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e$ git clone https://github.com/jedipunkz/tf-appmesh-ecs-canary-release\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e$ cd tf-appmesh-ecs-canary-release\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e$ terraform plan\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e$ terraform apply\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"構成\"\u003e構成\u003c/h3\u003e\n\u003cp\u003e検証で構築した構成(上記の Terraform コードで構築できる) は下記になります。\u003c/p\u003e","title":"App Mesh と ECS によるカナリーリリース構成を検証してみた"},{"content":"こんにちは。jedipunkz です。\n今回は READYFOR Advent Calendar 2021 の記事として執筆します。\nREADYFOR では 2021 年の夏に AWS ECS へプラットフォーム移行をしました。ECS は自分達の要件を全て満たしてくれ運用コストも極小化できて更に周辺の技術も AWS 公式のものが揃っているので、とても満足している状況です。\n移行を終えたばかりなので「では次のアーキテクチャは？」という話にはまだなっていないのですが、今は準備期間として余裕を持ってスケジューリングできる状態にして頂いているので、SRE チームとしては色々な技術をリサーチしている段階になります。\n今現在は ECS + CodeDeploy を使って Blue/Green デプロイメントを実現しているのですが、よりモダンなデプロイ方式 Progressive Delivery について去年あたりから興味を持っていました。ただ、今までは実際に技術を触るまでには至っていなかったのでこの機会に色々と触ってみたという次第です。\n今までも Blue/Green デプロイメント, Canary リリースとデプロイ方式が複数ありましたが、これらを含む継続的デリバリの次のステップと言われているのが Progressive Delivery です。2020年に Hashicorp 社の Mitchell Hashimoto 氏 が来日した際に「今一番気になっているワード」としてあげていましたのが印象的でした。\nECS を使った Canary リリース Progressive Delivery の話をする前に ECS を使った Canary リリースについて少し触れておきます。 (具体的な話についても、どこかのタイミングで記事にできればと思っています)\nAWS App Mesh と ECS, X-Ray を使って下記のような構成を作りました。この構成中の App Mesh の Virtual Router のルーティング情報を修正する形で Canary リリースのトラヒック操作が行えます。ECS 以前は Canary リリースを実現できていて ECS 導入によってそれがデグレした状態だったので、この構成の検証は一つの成果だったと思っていますし、今回話をする Progressive Delivery のひとつ前のステップとも考えています。\n+----------------+ +----------------+ +---------------+ App Mesh | VirtualGateway | -\u0026gt; | VirtualService | -\u0026gt; | VirtualRouter | +----------------+ +----------------+ +---------------+ | | +--------+ +-----+ +-----------+-----------+ +---------------+ +---------------------+ | client | -\u0026gt; | NLB | -\u0026gt; | Envoy Gateway | X-Ray | | VirtualNode | -\u0026gt; | App | Envoy | X-ray | +--------+ +-----+ +-----------------------+ +---------------+ +---------------------+ | ECS | | | ECS | +-----------------------+ | +---------------------+ | +---------------+ +---------------------+ | VirtualNode | -\u0026gt; | App | Envoy | X-ray | +---------------+ +---------------------+ | ECS | +---------------------+ Progressive Delivery の基本的な考え方 継続的デリバリは下記の様な流れでデプロイを実施していましたが、\n+---------+ +-------+ +------+ +--------+ | Develop | -\u0026gt; | Build | -\u0026gt; | Test | -\u0026gt; | Deploy | +---------+ +-------+ +------+ +--------+ Progressive Delivery では下記の様になります。 (ref: https://static.sched.com/hosted_files/kccncna19/f2/Progressive%20Delivery%20%26%20Argo%20Rollouts.pdf)\n+---------+ +---------+ +--------+ | Develop | -\u0026gt; | Build | -\u0026gt; | Test | +---------+ +---------+ +--------+ | +---------+ +--------+ | Analyze | \u0026lt;- | Deploy | +---------+ +--------+ | | +---------+ +---------+ +--------+ | Rollback| \u0026lt;- | Success?| -\u0026gt; |Progress| +---------+ +---------+ +--------+ No Yes つまり、Canaly リリースをベースにする場合、\n継続的デリバリと同様にビルド・テスト・デプロイを実施 徐々にトラヒックを Canary 環境へ寄せる Canary 環境に SLO をベースにしたテスト/メトリクスを元にしたクエリ実行を実施 その結果、閾値を下回る場合はロールバックを実施 逆に下回らない場合はトラヒック操作を継続して、デプロイ継続 という流れなります。この Analyze (Analysis) というステップが追加されているのと、その結果自動的にロールバックさせる点が今までのデプロイ方式と異なる点です。\n実際に調査した技術たち 実査に動かしてみたり、またはドキュメントベースで調査した技術は下記の通りです。\nArogCD Rollouts Flagger PipeCD マルチクラウド・プラットフォーム対応 まず ArgoCD Rollouts と Flagger は Kubernetes が前提になります。その点で PipeCD は AWS ECS にも対応しています。がもちろん Kubernetes にも対応しています。ちなみに PipeCD は日本国内のエンジニアの方々が開発に携わっています。国内初 OSS として世界に広まればいいなと個人的に考えています。\nマルチ Analysis プロバイダ対応 Analysis でテスト・メトリクス収集とそれを元にしたクエリを発行する、と上記に記しましたが、その取得方法や利用できるソフトウェア or サービスに関してもそれぞれ複数対応している様です。代表的なものは下記になります。\nPrometheus Datadog AWS Cloudwatch New Relic Graphite Google Stackdriver InfluxDB etc\u0026hellip; 実査に Analysis のコンフィギュレーションを見てみる 実際に Analysis のコンフィギュレーションを見て理解を深めたいと思います。下記は ArogCD Rollouts の Prometheus の Analysis のコンフィギュレーションです。\napiVersion: argoproj.io/v1alpha1 kind: AnalysisTemplate metadata: name: success-rate spec: args: - name: service-name metrics: - name: success-rate interval: 5m successCondition: result[0] \u0026gt;= 0.95 failureLimit: 3 provider: prometheus: address: http://prometheus.example.com:9090 query: | sum(irate( istio_requests_total{reporter=\u0026#34;source\u0026#34;,destination_service=~\u0026#34;{{args.service-name}}\u0026#34;,response_code!~\u0026#34;5.*\u0026#34;}[5m] )) / sum(irate( istio_requests_total{reporter=\u0026#34;source\u0026#34;,destination_service=~\u0026#34;{{args.service-name}}\u0026#34;}[5m] )) Analysis に関わる Kubernetes CRD であると見てわかると思います。下記にこのコンフィギュレーションの意味を記します。\ninterval でクエリを発行する間隔を指定 successCondition でクエリ結果の閾値を指定 failtureLimit で最大失敗許可数を指定 provider.prometheus で Prometheus プロバイダ利用を宣言 address で Prometheus Server の URL を指定 query で Prometheus Server 上で実行するクエリを指定 よって、前述した SLO というのがこのクエリに相当すると考えています。SLO をクエリベースで定義することによって、SLO に即さない場合は自動的に古いアプリケーションへロールバックする、という仕組みです。\nただご存知のように SLO はこのようなデプロイの短い期間での計測に指定することが適しているのか？という話はあると思っていて、あくまでもデプロイを前提とした SLO という意味で定義するべきなのでは、と考えています。\nまとめ ECS 移行を終えて次のステップを検討するにあたって Progressive Delivery を考えるのは着眼点としては良いと個人的には考えています。が ECS を選択したことによって、Progressive Delivery を実践するための技術の選択肢が狭まったのは事実で、ECS 以後のアーキテクチャを検討する際にこの点はネックになりそう、と考えています。2021年12月時点では PipeCD 以外にそれを実現する技術を見つけられていません。\nまた、Canary リリースに対して Analisys とその結果の自動ロールバックという要素さえ追加できれば、Progressive Delivery が実現できるのも事実です。前述したように App Mesh と ECS によって Canary リリースは実現できているので、Analisys と 自動ロールバックを実現する仕組みを自社で Lambda などを使って開発できれば良いのではと考え始めています。ですが、もちろん汎用性がないですし運用・保守のコストをかけてまで実践するのか、という問題はありそうです。\nということで Progressive Delivery をポイントにした次期アーキテクチャについて、今考えていることを記させていただきました。\n","permalink":"https://jedipunkz.github.io/post/progressive_delivery/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://twitter.com/jedipunkz\"\u003ejedipunkz\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003e今回は \u003ca href=\"https://qiita.com/advent-calendar/2021/readyfor\"\u003eREADYFOR Advent Calendar 2021\u003c/a\u003e の記事として執筆します。\u003c/p\u003e\n\u003cp\u003eREADYFOR では 2021 年の夏に AWS ECS へプラットフォーム移行をしました。ECS は自分達の要件を全て満たしてくれ運用コストも極小化できて更に周辺の技術も AWS 公式のものが揃っているので、とても満足している状況です。\u003c/p\u003e\n\u003cp\u003e移行を終えたばかりなので「では次のアーキテクチャは？」という話にはまだなっていないのですが、今は準備期間として余裕を持ってスケジューリングできる状態にして頂いているので、SRE チームとしては色々な技術をリサーチしている段階になります。\u003c/p\u003e\n\u003cp\u003e今現在は ECS + CodeDeploy を使って Blue/Green デプロイメントを実現しているのですが、よりモダンなデプロイ方式 Progressive Delivery について去年あたりから興味を持っていました。ただ、今までは実際に技術を触るまでには至っていなかったのでこの機会に色々と触ってみたという次第です。\u003c/p\u003e\n\u003cp\u003e今までも Blue/Green デプロイメント, Canary リリースとデプロイ方式が複数ありましたが、これらを含む継続的デリバリの次のステップと言われているのが Progressive Delivery です。2020年に Hashicorp 社の \u003ca href=\"https://twitter.com/mitchellh\"\u003eMitchell Hashimoto 氏\u003c/a\u003e が来日した際に「今一番気になっているワード」としてあげていましたのが印象的でした。\u003c/p\u003e\n\u003ch2 id=\"ecs-を使った-canary-リリース\"\u003eECS を使った Canary リリース\u003c/h2\u003e\n\u003cp\u003eProgressive Delivery の話をする前に ECS を使った Canary リリースについて少し触れておきます。\n(具体的な話についても、どこかのタイミングで記事にできればと思っています)\u003c/p\u003e\n\u003cp\u003eAWS App Mesh と ECS, X-Ray を使って下記のような構成を作りました。この構成中の App Mesh の Virtual Router のルーティング情報を修正する形で Canary リリースのトラヒック操作が行えます。ECS 以前は Canary リリースを実現できていて ECS 導入によってそれがデグレした状態だったので、この構成の検証は一つの成果だったと思っていますし、今回話をする Progressive Delivery のひとつ前のステップとも考えています。\u003c/p\u003e","title":"ECS 以後の構成と Progressive Delivery の調査"},{"content":"こんにちは @jedipunkz 🚀 です。\n最近、職場では ECS/Fargate でサービスを運用しています。そこで ECS Task 上でコマンドを実行する必要に迫られて幾つか調べたのですが、複数の方法があり検証をしてみました。これには 2021/03 にリリースされたばかりの ECS 上のコンテナでコマンドを実行する機能も含まれています。\n自分たちは自動化する必要があったので Go 言語 (aws-sdk-go) を中心に検証実施しましたが同時に awscli でも動作検証しましたので、その方法をこの記事に記そうかと思います。\n下記の2つの ECS の機能についてそれぞれ Go 言語, awscli で動作検証実施しました。\n(1) ECS Execute Command (2021/03 リリース) (2) ECS Run Task 用いるツール類 下記のツールを前提に記事を記します。\naws-sdk-go Terraform awscli 共通で必要な taskRoleArn まず Task Definition に対して executeRoleArn とは別に TaskRoleArn の付与が必要になります。\nresource \u0026#34;aws_ecs_task_definition\u0026#34; \u0026#34;sample\u0026#34; { family = \u0026#34;sample\u0026#34; cpu = \u0026#34;256\u0026#34; memory = \u0026#34;512\u0026#34; network_mode = \u0026#34;awsvpc\u0026#34; requires_compatibilities = [\u0026#34;FARGATE\u0026#34;] execution_role_arn = module.ecs_task_execution_role.iam_role_arn task_role_arn = module.ecs_task__role.iam_role_arn container_definitions = data.template_file.container_definition_sample.rendered } taksRoleArn の内容については https://docs.aws.amazon.com/ja_jp/AmazonECS/latest/developerguide/task-iam-roles.html に情報があり、下記が必要になります。\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;ecs-tasks.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } ] } コマンド実行 (1) ECS Exuecute Command (2021/03 リリース) まず最近リリースされた ECS コマンド実行について試します。Terraform やその他の周辺の技術 (aws 公式 GitHub Actions 等も)、この機能にはまだ対応していません。 awscli (v1, v2), session-manager, aws-sdk-go 等が既に対応しています。ここでは awscli, aws-sdk-go を使ってこの機能を試してみます。\n(2021/04/21 追記) 下記の通り Terraform も aws 公式 GitHub Actions も既に対応されていました。\nTerraform Execute Command 対応 aws 公式 ECS Task Definition Render GitHub Actions awscli, session-manager, aws-sdk-go は比較的新しいバージョンを事前にインストールする必要があります。\nまず taskExecute IAM Role に下記の権限が追加で必要です。\nstatement { effect = \u0026#34;Allow\u0026#34; actions = [ \u0026#34;ssmmessages:CreateControlChannel\u0026#34;, \u0026#34;ssmmessages:CreateDataChannel\u0026#34;, \u0026#34;ssmmessages:OpenControlChannel\u0026#34;, \u0026#34;ssmmessages:OpenDataChannel\u0026#34; ] resources = [\u0026#34;*\u0026#34;] } ECS Service に対してコマンド実行の有効化を実施します。\n$ aws ecs update-service \\ --cluster \u0026lt;ClusterArn\u0026gt; \\ --service \u0026lt;ServiceName\u0026gt; \\ --task \u0026lt;TaskName\u0026gt; \\ --enable-execute-command #\u0026lt;--- 有効化 次に --enable-execute-command オプションを付与して Task を起動します。\n$ aws ecs run-task --cluster \u0026lt;ClusterArn\u0026gt; \\ --task-definition \u0026lt;TaskDefinition:Revision\u0026gt; \\ --network-configuration \u0026#34;awsvpcConfiguration={subnets=[\u0026#34;Subnet_ID1\u0026#34;, \u0026#34;Subnet_ID2\u0026#34;],securityGroups=[\u0026#34;SecurityGroupId\u0026#34;],assignPublicIp=DISABLED}\u0026#34; \\ --launch-type FARGATE --enable-execute-command 準備が整ったので、コマンドを実行してみます。\naws ecs execute-command --cluster \u0026lt;ClusterArn\u0026gt; \\ --task \u0026lt;TaskId\u0026gt; --container \u0026lt;ContainerName\u0026gt; \\ --interactive --command \u0026#34;ps ax\u0026#34; The Session Manager plugin was installed successfully. Use the AWS CLI to start a session. Starting session with SessionId: ecs-execute-command-0b2c79e1e775f274c PID USER TIME COMMAND 1 root 0:00 /bin/sh -c nginx -g \u0026#34;daemon off;\u0026#34; 7 root 0:00 nginx: master process nginx -g daemon off; 8 nginx 0:00 nginx: worker process 9 nginx 0:00 nginx: worker process 10 root 0:00 /managed-agents/execute-command/amazon-ssm-agent 23 root 0:00 /managed-agents/execute-command/ssm-agent-worker 72 root 0:00 /managed-agents/execute-command/ssm-session-worker ecs-ex 80 root 0:00 ps ax Exiting session with sessionId: ecs-execute-command-0b2c79e1e775f274c. 上記の通り、コマンド ps ax の結果が得られました。\n次に aws-sdk-go を使ってコマンドを実行してみます。\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;github.com/aws/aws-sdk-go/aws\u0026#34; \u0026#34;github.com/aws/aws-sdk-go/aws/awserr\u0026#34; \u0026#34;github.com/aws/aws-sdk-go/aws/session\u0026#34; \u0026#34;github.com/aws/aws-sdk-go/service/ecs\u0026#34; ) func main() { svc := ecs.New(session.New(aws.NewConfig().WithRegion(\u0026#34;ap-northeast-1\u0026#34;))) input := \u0026amp;ecs.ExecuteCommandInput{ Cluster: aws.String(\u0026#34;\u0026lt;ClusterName\u0026gt;\u0026#34;), Command: aws.String(\u0026#34;ps aux\u0026#34;), Container: aws.String(\u0026#34;\u0026lt;ContainerName\u0026gt;\u0026#34;), Interactive: aws.Bool(true), Task: aws.String(\u0026#34;\u0026lt;TaskId\u0026gt;\u0026#34;), } result, err := svc.ExecuteCommand(input) if err != nil { if aerr, ok := err.(awserr.Error); ok { fmt.Println(aerr.Error()) } fmt.Println(err.Error()) return } fmt.Println(result) } これをビルドし、実行すると下記のようなシンタックスで結果が得られます。(参考: https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_ExecuteCommand.html)\n{ \u0026#34;clusterArn\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;containerArn\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;containerName\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;interactive\u0026#34;: boolean, \u0026#34;session\u0026#34;: { \u0026#34;sessionId\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;streamUrl\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;tokenValue\u0026#34;: \u0026#34;string\u0026#34; }, \u0026#34;taskArn\u0026#34;: \u0026#34;string\u0026#34;: } 残念ながらコマンド結果はレスポンスには入っていないようです。が、コマンドは正常を正常に叩いたかの API レスポンスとしてはエラーも検知出来ます。\n※[2021/04/14] aws の Tori さんから指摘頂きました。API からのレスポンスを利用して session-manager-plugin コマンドを実行するとコンテナに接続できるそうです！\n(2) ECS Run Task によるコマンドのオーバーライド 次に ECS Run Task によるコマンドのオーバーライドについて記します。こちらは以前からある機能なのでほぼすべての周辺ツールが対応している認識です。まず awscli で動作確認してみます。\n下記の json ファイルを作成して、タスク定義に記してあるコンテナ名と、コマンドを記します。\n{ \u0026#34;containerOverrides\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;\u0026lt;ContainerName\u0026gt;\u0026#34;, \u0026#34;command\u0026#34;: [ \u0026#34;\u0026lt;Command\u0026gt;\u0026#34; ] } ] } awscli を用いてコマンドを Overrides しつつ Run Task 実行します。\naws ecs run-task --cluster \u0026lt;ClusterArn\u0026gt; \\ --task-definition \u0026lt;TaskDefinition:Revision\u0026gt; \\ --network-configuration \u0026#34;awsvpcConfiguration={subnets=[\u0026#34;\u0026lt;SubnetId1\u0026gt;\u0026#34;, \u0026#34;\u0026lt;SubnetId2\u0026gt;\u0026#34;],securityGroups=[\u0026#34;SecurityGroupId\u0026#34;],assignPublicIp=DISABLED}\u0026#34; \\ --launch-type FARGATE \\ --overrides file://\u0026lt;作成した json ファイル\u0026gt;.json 結果は API からの応答内容だけでコマンド実行結果は含まれていません。タスク定義で logConfiguration を記していると、そこにコマンド実行結果が出力されます。\nではつぎに aws-sdk-go を使って Run Task してみます。下記のサンプルコードを記します。環境情報は適宜差し替える必要があります。\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;github.com/aws/aws-sdk-go/aws\u0026#34; \u0026#34;github.com/aws/aws-sdk-go/aws/awserr\u0026#34; \u0026#34;github.com/aws/aws-sdk-go/aws/session\u0026#34; \u0026#34;github.com/aws/aws-sdk-go/service/ecs\u0026#34; ) func main() { // svc := ecs.New(session.New(aws.NewConfig().WithRegion(\u0026#34;ap-northeast-1\u0026#34;))) sess := session.Must(session.NewSessionWithOptions(session.Options{ Config: aws.Config{ CredentialsChainVerboseErrors: aws.Bool(true), }, })) svc := ecs.New(sess) input := \u0026amp;ecs.RunTaskInput{ Cluster: aws.String(\u0026#34;\u0026lt;ClusterArn\u0026gt;), TaskDefinition: aws.String(\u0026#34;\u0026lt;TaskDefinition:Revision\u0026gt;), Overrides: \u0026amp;ecs.TaskOverride{ ContainerOverrides: []*ecs.ContainerOverride{ { Name: aws.String(\u0026#34;\u0026lt;ContainerName\u0026gt;\u0026#34;), Command: aws.StringSlice([]string{\u0026#34;\u0026lt;実行したいコマンドを記す\u0026gt;\u0026#34;}), }, }, }, NetworkConfiguration: \u0026amp;ecs.NetworkConfiguration{ AwsvpcConfiguration: \u0026amp;ecs.AwsVpcConfiguration{ Subnets: aws.StringSlice([]string{\u0026#34;\u0026lt;SubnetId1\u0026gt;\u0026#34;, \u0026#34;\u0026lt;SubnetId2\u0026gt;\u0026#34;}), // サブネットID AssignPublicIp: aws.String(\u0026#34;DISABLED\u0026#34;), // 必要に応じて }, }, LaunchType: aws.String(\u0026#34;FARGATE\u0026#34;), } result, err := svc.RunTask(input) if err != nil { if aerr, ok := err.(awserr.Error); ok { switch aerr.Code() { case ecs.ErrCodeServerException: fmt.Println(ecs.ErrCodeServerException, aerr.Error()) case ecs.ErrCodeClientException: fmt.Println(ecs.ErrCodeClientException, aerr.Error()) case ecs.ErrCodeInvalidParameterException: fmt.Println(ecs.ErrCodeInvalidParameterException, aerr.Error()) case ecs.ErrCodeClusterNotFoundException: fmt.Println(ecs.ErrCodeClusterNotFoundException, aerr.Error()) case ecs.ErrCodeUnsupportedFeatureException: fmt.Println(ecs.ErrCodeUnsupportedFeatureException, aerr.Error()) case ecs.ErrCodePlatformUnknownException: fmt.Println(ecs.ErrCodePlatformUnknownException, aerr.Error()) case ecs.ErrCodePlatformTaskDefinitionIncompatibilityException: fmt.Println(ecs.ErrCodePlatformTaskDefinitionIncompatibilityException, aerr.Error()) case ecs.ErrCodeAccessDeniedException: fmt.Println(ecs.ErrCodeAccessDeniedException, aerr.Error()) case ecs.ErrCodeBlockedException: fmt.Println(ecs.ErrCodeBlockedException, aerr.Error()) default: fmt.Println(aerr.Error()) } } else { fmt.Println(err.Error()) } return } fmt.Println(result) } ビルド \u0026amp; 実行するとレスポンスが得られます。がこちらもレスポンスにはコマンド結果が入っていないので、awscli の際と同様にタスク定義内で logConfiguration を指定してログ送信設定を行うと良いでしょう。Cloudwatch Logs などを介して、コマンド実行結果を確認することが可能です。\nまとめ それぞれの良い点・悪い点を下記にまとめてみました。\nCommand Exec の特徴 Terraform, aws 公式 GitHub Actions 等がまだ対応していない awscli ではコマンド結果が得られるが aws-sdk を用いると API のレスポンスにコマンド結果が入ってない 実行する際には TaskId を指定。よって予め Run Task などで Task を起動させるのが前提になる インタラクティブ(対話的) にコマンドを実行出来るそう (未検証) Run Task の特徴 古くからある機能で公式・周辺の技術が対応済み コマンド実行結果は Task 定義に記した logConfiguration に転送することが可能 (Cloudwatch, Datadog 等など) 実行する際には Task Defintion を指定。予め Task が起動している必要はない。 それぞれ特徴がありますが、Execute Command の方はコマンド結果を得る方法が今の所、別途仕組みを用意する必要がありそうです。ログ転送や、Task を別途起動しておく必要が無い点、またコマンド実行終了と共に Task が自動的に終了してくれる点など、自動化する上では Run Task が好ましいなぁと感じています。\n今後 Execute Command がどう変わってくるか期待ですが、awscli を使って単純にデバッグしたい等の要望の時には重宝しそうだなと感じてます。\n","permalink":"https://jedipunkz.github.io/post/ecs-execute-command/","summary":"\u003cp\u003eこんにちは \u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e 🚀 です。\u003c/p\u003e\n\u003cp\u003e最近、職場では ECS/Fargate でサービスを運用しています。そこで ECS Task 上でコマンドを実行する必要に迫られて幾つか調べたのですが、複数の方法があり検証をしてみました。これには 2021/03 にリリースされたばかりの ECS 上のコンテナでコマンドを実行する機能も含まれています。\u003c/p\u003e\n\u003cp\u003e自分たちは自動化する必要があったので Go 言語 (aws-sdk-go) を中心に検証実施しましたが同時に awscli でも動作検証しましたので、その方法をこの記事に記そうかと思います。\u003c/p\u003e\n\u003cp\u003e下記の2つの ECS の機能についてそれぞれ Go 言語, awscli で動作検証実施しました。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://aws.amazon.com/jp/blogs/news/new-using-amazon-ecs-exec-access-your-containers-fargate-ec2/\"\u003e(1) ECS Execute Command (2021/03 リリース)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://docs.aws.amazon.com/cli/latest/reference/ecs/run-task.html\"\u003e(2) ECS Run Task\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"用いるツール類\"\u003e用いるツール類\u003c/h2\u003e\n\u003cp\u003e下記のツールを前提に記事を記します。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://docs.aws.amazon.com/sdk-for-go/api/service/ecs/\"\u003eaws-sdk-go\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.terraform.io/\"\u003eTerraform\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://aws.amazon.com/jp/cli/\"\u003eawscli\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"共通で必要な-taskrolearn\"\u003e共通で必要な taskRoleArn\u003c/h2\u003e\n\u003cp\u003eまず Task Definition に対して executeRoleArn とは別に TaskRoleArn の付与が必要になります。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-hcl\" data-lang=\"hcl\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eresource\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;aws_ecs_task_definition\u0026#34; \u0026#34;sample\u0026#34;\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  family                   \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;sample\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  cpu                      \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;256\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  memory                   \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;512\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  network_mode             \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;awsvpc\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  requires_compatibilities \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e [\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;FARGATE\u0026#34;\u003c/span\u003e]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  execution_role_arn       \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003emodule\u003c/span\u003e.\u003cspan style=\"color:#66d9ef\"\u003eecs_task_execution_role\u003c/span\u003e.\u003cspan style=\"color:#66d9ef\"\u003eiam_role_arn\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  task_role_arn            \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003emodule\u003c/span\u003e.\u003cspan style=\"color:#66d9ef\"\u003eecs_task__role\u003c/span\u003e.\u003cspan style=\"color:#66d9ef\"\u003eiam_role_arn\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  container_definitions    \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003edata\u003c/span\u003e.\u003cspan style=\"color:#66d9ef\"\u003etemplate_file\u003c/span\u003e.\u003cspan style=\"color:#66d9ef\"\u003econtainer_definition_sample\u003c/span\u003e.\u003cspan style=\"color:#66d9ef\"\u003erendered\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003etaksRoleArn の内容については \u003ca href=\"https://docs.aws.amazon.com/ja_jp/AmazonECS/latest/developerguide/task-iam-roles.html\"\u003ehttps://docs.aws.amazon.com/ja_jp/AmazonECS/latest/developerguide/task-iam-roles.html\u003c/a\u003e に情報があり、下記が必要になります。\u003c/p\u003e","title":"Go 言語と awscli を使って ECS/Fargate 上でコマンド実行してみた"},{"content":"クラウド・コンテナ基盤, CNCF AWS GCP OpenStack Kubernetes Prometheus Grafana ArgoCD (Rollouts) Flux PipeCD OpenTelemetry Envoy X-Ray Docker プログラミング言語 Go Rust Python IaC Terraform Ansible AWS CDK CI/CD GitHub Actions テーマ AWS, GCP を使った SRE 活動 プログレッシブデリバリー サービスメッシュ データ基盤 ログ基盤 Datadog Hashicorp の各技術・ソフトウェア セキュリティ ","permalink":"https://jedipunkz.github.io/about/interested/","summary":"\u003ch3 id=\"クラウドコンテナ基盤-cncf\"\u003eクラウド・コンテナ基盤, CNCF\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eAWS\u003c/li\u003e\n\u003cli\u003eGCP\u003c/li\u003e\n\u003cli\u003eOpenStack\u003c/li\u003e\n\u003cli\u003eKubernetes\u003c/li\u003e\n\u003cli\u003ePrometheus\u003c/li\u003e\n\u003cli\u003eGrafana\u003c/li\u003e\n\u003cli\u003eArgoCD (Rollouts)\u003c/li\u003e\n\u003cli\u003eFlux\u003c/li\u003e\n\u003cli\u003ePipeCD\u003c/li\u003e\n\u003cli\u003eOpenTelemetry\u003c/li\u003e\n\u003cli\u003eEnvoy\u003c/li\u003e\n\u003cli\u003eX-Ray\u003c/li\u003e\n\u003cli\u003eDocker\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"プログラミング言語\"\u003eプログラミング言語\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eGo\u003c/li\u003e\n\u003cli\u003eRust\u003c/li\u003e\n\u003cli\u003ePython\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"iac\"\u003eIaC\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eTerraform\u003c/li\u003e\n\u003cli\u003eAnsible\u003c/li\u003e\n\u003cli\u003eAWS CDK\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"cicd\"\u003eCI/CD\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eGitHub Actions\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"テーマ\"\u003eテーマ\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eAWS, GCP を使った SRE 活動\u003c/li\u003e\n\u003cli\u003eプログレッシブデリバリー\u003c/li\u003e\n\u003cli\u003eサービスメッシュ\u003c/li\u003e\n\u003cli\u003eデータ基盤\u003c/li\u003e\n\u003cli\u003eログ基盤\u003c/li\u003e\n\u003cli\u003eDatadog\u003c/li\u003e\n\u003cli\u003eHashicorp の各技術・ソフトウェア\u003c/li\u003e\n\u003cli\u003eセキュリティ\u003c/li\u003e\n\u003c/ul\u003e","title":"興味のある技術/テーマたち"},{"content":"こんにちは。jedipunkz です。\n仕事ではこれから会社のサービス環境として AWS ECS の導入を始めていくところなのですが、最近の SRE/インフラ界隈のトレンドを自分たちが追うために自分たち SRE が管理しているボット環境を EKS を使って GitOps 化しようということになり色々と準備を進めています。導入までもう一歩のところまで来たので、構成や技術要素についてここに記したいと思います。\nどんなボットが動いているの？ まずボット開発に用いてる言語は Go 言語です。主に aws-sdk-go を使ってボットを開発しています。私達はコミュニケーションツールとして Slack を使っているので Slack ボット化するためには slack-go を使っています。 ただまだボットの数が少なく主に利用されてるのは Ansible を実行するモノです。開発環境へアプリをデプロイするのに Ansible を使っています。もうすぐ ECS 化するので役割はそろそろ終えるのですが\u0026hellip; 利点は開発者だけでなく非エンジニアの方が GitHub のブランチ上のアプリの動作をしたい際に Slack を使って簡単にアプリの動作ができるところです。今後は自動化目的でもっと沢山のボットを開発していきたいです。\nEKS/Fargate vs EKS/EC2 EKS の利用を検討する際に Fargate タイプと EC2 タイプがあります。2020年の今年頭に評価した際には ALB Ingress Controller と HPA のための Metrics Server が正常に動作しない状態だったので、まだ EC2 タイプを選択すべきなのかな\u0026hellip;と考えたのですが AWS 的にも Fargate を推してる気もするし再度評価実施しました。結果ドキュメントもソフトウェアもだいぶ更新されていて ALB Ingress Controller も Metrics Server もあっけなく動作し、今回のボット環境も EKS/Fargte を選択しました。\n(ちょっと余談) そもそもサービス環境はなぜ EKS でなく ECS？ AWS ECS の導入を始めようとしていると前述しましたが、なぜサービス環境には EKS ではなく ECS を選択したかというと、この両者で下記の観点で評価実施しました。※ () 内はまだ未実施。\nコード化 オートスケール ロードバランサ 機密情報の格納展開 CI/CD, GitOps ロギング メトリクス収集 モニタリング カナリーリリース等のリリース方式 (バッチ) (サービスメッシュ) 結果的にインフラをデプロイする主な機能 (特に ALB Ingress Controller, Metric Server) を時前で Pod として運用するのは運用コストが高い、という判断をしました。Pod の調子が悪いからロードバランサがデプロイできない！というのは辛いです。自分の務めている会社は少人数で運用コストを減らすという視点で技術を選択しているので、今の時点では ECS を選択するのは必然でした。ECS であれば Terraform -\u0026gt; AWS API で確実に必要なインフラがデプロイ出来ますし。\nボット環境を GitOps 化する全体構成 構成は下記のとおりです。\n(3 get manifest +----------------------------------------+ | | +----------------+ (2 +----------------+ | | GitHub Actions |---------\u0026gt; | ECR Repository | | +----------------+ +----------------+ | + + (4 pull image | | (1 merge to master | | | +----------------+ | | | Bots | | | +----------------+ | | | ArgoCD |--+ +----------------+ +----------------+ +---------------+ | Engineer | | EKS/Fargate |---\u0026gt;| EFS Volumes | +----------------+ +----------------+ +---------------+ 処理の流れは\u0026hellip;\n(1 Engineer がアプリ用 Git レポジトリの master ブランチへ Push (2 GitHub Actions により Docker コンテナビルド, ECR への Push \u0026amp; Kustomize によりコンテナイメージタグが更新 (3 ArgoCD が GitHub 上の Manifest ファイルの更新をトリガに EKS へローリングデプロイ開始 (4 ArgoCD によって新たに指定されたイメージを ECR から Pull しデプロイ完了 なぜ EFS を使っているか ? EFS は 永続的なストレージとして利用しています。今後は不要になる可能性が高いのですが前述した Ansible を使ったボットは Playbook を収めた Git レポジトリにアクセスしてデプロイを実行します。コンテナに Git レポジトリを収めるのはコンテナイメージサイズが肥大化しますし良くないと判断しました。結果、EKS から EFS を Volume マウントして Ansible を実行します。また Ansible 以外のボットも環境ごとの情報を格納した yaml ファイルを扱うのですが、その yaml ファイルを EFS 上に Kubernetes Secrets として展開しています。\nArgoCD を選択した理由 ArgoCD が CNCF にジョインしたというニュースが今年の夏頃？ありました。また同時に検討していた Flux がちょうどメジャーバージョンアップをして、だいぶ利用方法やドキュメントが刷新されました。また一世代前のバージョンはメンテナンスモードに突入。新しいバージョンは利用方法が少し複雑化していましたし CLI は劇的に利用方法が変化していました。ということで GitOps をしたいという要望だけ満たせれば良いので CNDF の ArgoCD を利用することにしました。\nArgoCD はボットと同じ EKS クラスタ上にデプロイしました。UI は普段利用しないので Ingress は設定せず EKS Fargate Profile の稼働しているプライベートサブネット上で起動させました。ArgoCD アプリは下記の手順で作成します。\n$ argocd app create hello --repo https://github.com/jedipunkz/no-exsist-bot-repo.git --path bots/hello/manifests/envs/dev --dest-server https://kubernetes.default.svc --dest-namespace infra-bot $ argocd app set hello --sync-policy automated $ arogcd app set hello --auto-prune この操作で ArgoCD が GitHub 上の Manifests の変化に応じてボットのコンテナイメージを ECR から取得して GitOps を実現してくれます。\nKustomize で環境 Manifest の環境を分離し、コンテナイメージのタグを編集するために Kustomize を使っています。イメージの更新するためには Kustomize は必須な技術要素な気がしています。\nbots └── ansible ├── Dockerfile ├── README.md ├── bot.go ├── go.mod ├── go.sum ├── main.go ├── manifests │ ├── base │ │ ├── bot-deployment.yaml │ │ ├── bot-svc.yaml │ │ └── kustomization.yaml │ ├── envs │ │ ├── dev │ │ │ ├── deployment.yaml │ │ │ └── kustomization.yaml │ │ └── prd │ │ ├── deployment.yaml │ │ └── kustomization.yaml │ └── manual │ └── secrets.yaml └── requirements.txt GitHub Actions を用いた Docker ビルド, ECR イメージプッシュ, Kustomize Edit GitHub Actions を使ってボットの追加・編集 PR をマージしたタイミングで\nDocker ビルド ECR へのプッシュ Kustomize Edit をして Manifest に記しているコンテナイメージのタグを更新 をしています。GitHub Actions の処理としてはこれだけで、あとは ArgoCD が GitHub 上の Manifests の更新に反応して自動で GitOps してくれます。\nまとめ 構成する技術要素としては以上です。先に述べましたが kubernetes を SRE として運用することは昨今の SRE/インフラ界隈のトレンドを押さえるという意味で意味があることですし、今回数ヶ月ぶりに EKS を評価・検証して、技術として速いスピードで成熟しているのを実感しました。今後 EKS を本番サービスでも利用することもあるかもしれないです。ALB Ingress Controller, Metrics Server だけではなく ArgoCD も今年の2月に評価した際よりだいぶ完成度が増している気がします。\nEKS 導入前は Docker Compose でボットをデプロイしていて半自動化状態でした。ボットを作る足かせにもなっていたので、EKS の導入で今後のボット開発の敷居が下がり、自動化がますます推進されればいいなと思っています。\n","permalink":"https://jedipunkz.github.io/post/eks-fargate-bot-env/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://twitter.com/jedipunkz\"\u003ejedipunkz\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003e仕事ではこれから会社のサービス環境として AWS ECS の導入を始めていくところなのですが、最近の SRE/インフラ界隈のトレンドを自分たちが追うために自分たち SRE が管理しているボット環境を EKS を使って GitOps 化しようということになり色々と準備を進めています。導入までもう一歩のところまで来たので、構成や技術要素についてここに記したいと思います。\u003c/p\u003e\n\u003ch2 id=\"どんなボットが動いているの\"\u003eどんなボットが動いているの？\u003c/h2\u003e\n\u003cp\u003eまずボット開発に用いてる言語は Go 言語です。主に aws-sdk-go を使ってボットを開発しています。私達はコミュニケーションツールとして Slack を使っているので Slack ボット化するためには \u003ca href=\"https://github.com/slack-go/slack\"\u003eslack-go\u003c/a\u003e を使っています。 ただまだボットの数が少なく主に利用されてるのは Ansible を実行するモノです。開発環境へアプリをデプロイするのに Ansible を使っています。もうすぐ ECS 化するので役割はそろそろ終えるのですが\u0026hellip; 利点は開発者だけでなく非エンジニアの方が GitHub のブランチ上のアプリの動作をしたい際に Slack を使って簡単にアプリの動作ができるところです。今後は自動化目的でもっと沢山のボットを開発していきたいです。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"sss\" loading=\"lazy\" src=\"../../pix/ansible-bot.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"eksfargate-vs-eksec2\"\u003eEKS/Fargate vs EKS/EC2\u003c/h2\u003e\n\u003cp\u003eEKS の利用を検討する際に Fargate タイプと EC2 タイプがあります。2020年の今年頭に評価した際には ALB Ingress Controller と HPA のための Metrics Server が正常に動作しない状態だったので、まだ EC2 タイプを選択すべきなのかな\u0026hellip;と考えたのですが AWS 的にも Fargate を推してる気もするし再度評価実施しました。結果ドキュメントもソフトウェアもだいぶ更新されていて ALB Ingress Controller も Metrics Server もあっけなく動作し、今回のボット環境も EKS/Fargte を選択しました。\u003c/p\u003e","title":"EKS/Fargate + ArgoCD でボット環境 GitOps 化"},{"content":"こんにちは。@jedipunkz です。\n今日は Pulumi (https://www.pulumi.com/) について紹介したいと思います。\n最近ではすっかり Terraform がマルチクラウドな IaC ツールとして定着しましたが、巷では AWS CDK を使う現場も増えてきている印象です。AWS CDK は Typescript, Python などのプログラミング言語の中でインフラを定義・操作することができる AWS が提供しているツールです。ただ AWS CDK は名前の通り AWS にのみ対応していて内部的には Cloudformation Template がエキスポートされています。AWS オンリーという点と Cloudformation という点、また 2019 年時点で進化が激しく後方互換性を全く失っているので AWS CDK のアップデートに追従するのに苦労する点、色々ありまだ利用するには早いのかなぁという印象を個人的には持っています。\nそこで今回紹介する Pulumi なのですが CDK 同様にプログラミング言語の中でインフラを定義できて尚且つマルチクラウド対応しています。どちらかというと旧来の SDK の分類だと思うのですが、Terraform 同様にマルチクラウドという点で個人的には以前よりウォッチしていました。\n今回は公式の Getting Started 的なドキュメントに従って作業を進め Kubernetes の上に Pod を起動、その後コードを修正して再デプロイを実施して理解を深めてみたいと思います。\n作業に必要なソフトウェア 下記のソフトウェア・ツールが事前にインストールされていることを前提としたいと思います。また macOS を前提に手順を記します。\nPython3, Pip Minikube 参考 https://www.pulumi.com/docs/get-started/kubernetes/ 事前準備 まず macOS を使っている場合 Pulumi は下記の通り brew を使ってインストールできます。\n$ brew install pulumi また minikube を使って kubernetes を起動します。\n$ minikube start --memory=4096 --cpus=2 Pulumi を使った Nginx Pod の起動 早速 Nginx Pod を Kubernetes 情に起動する作業を行ってみます。適当なディレクトリを作成しその中で pulumi new を実行します。\n$ mkdir quickstart \u0026amp;\u0026amp; cd quickstart $ pulumi new kubernetes-python 下記の通り出力され入力を促されます。今回はデフォルト値をそのまま利用するのでエンターを数回入力します。\nproject name: (quickstart) project description: (A minimal Kubernetes Python Pulumi program) Created project \u0026#39;quickstart\u0026#39; Please enter your desired stack name. To create a stack in an organization, use the format \u0026lt;org-name\u0026gt;/\u0026lt;stack-name\u0026gt; (e.g. `acmecorp/dev`). stack name: (dev) Created stack \u0026#39;dev\u0026#39; すると __main__.py というファイルが自動生成されています。Kubernetes 上に nginx pod を起動するための Python コードです。中身を見てみましょう。nginx の Deployment が定義されているのが理解できると思います。\nimport pulumi from pulumi_kubernetes.apps.v1 import Deployment app_labels = { \u0026#34;app\u0026#34;: \u0026#34;nginx\u0026#34; } deployment = Deployment( \u0026#34;nginx\u0026#34;, spec={ \u0026#34;selector\u0026#34;: { \u0026#34;match_labels\u0026#34;: app_labels }, \u0026#34;replicas\u0026#34;: 1, \u0026#34;template\u0026#34;: { \u0026#34;metadata\u0026#34;: { \u0026#34;labels\u0026#34;: app_labels }, \u0026#34;spec\u0026#34;: { \u0026#34;containers\u0026#34;: [{ \u0026#34;name\u0026#34;: \u0026#34;nginx\u0026#34;, \u0026#34;image\u0026#34;: \u0026#34;nginx\u0026#34; }] } } }) pulumi.export(\u0026#34;name\u0026#34;, deployment.metadata[\u0026#34;name\u0026#34;]) ここで今回は Python を使っているので pip を使って Pulumi Module をインストールする必要があります。同じディレクトリ内に自動生成された requirements.txt がありますのでそれに従って pip install します。\n$ pip install -r requirements.txt それでは下記の通り pulumi up を実行し Nginx Pod を起動していましょう。\n$ pulumi up Previewing update (dev): Type Name Plan + pulumi:pulumi:Stack quickstart-dev create + └─ kubernetes:apps:Deployment nginx create Resources: + 2 to create Do you want to perform this update? \u0026gt; yes no details 結果として下記のように出力されステータス \u0026lsquo;created\u0026rsquo; となりました。\nType Name Status + pulumi:pulumi:Stack quickstart-dev created + └─ kubernetes:apps:Deployment nginx created Outputs: name: \u0026#34;nginx-td4rq3xr\u0026#34; Resources: + 2 created Duration: 14s Permalink: https://app.pulumi.com/jedipunkz/quickstart/dev/updates/1 kubectl コマンドでも確認していましょう。下記のように nginx-**** pod が起動していることが分かります。\n$ kubectl get pod NAME READY STATUS RESTARTS AGE nginx-td4rq3xr-86c57db685-nfblb 1/1 Running 0 46m __main__.py コードを修正して外部から Nginx にアクセス 次に __main__.py を下記の通り修正して Nginx Pod に curl を使ってアクセスできるようにしてみます。 コードの中で minikube か否かについての Config pulumi.Config() に関する記述と Deployment に IP を確保する記述があります。\nimport pulumi from pulumi_kubernetes.apps.v1 import Deployment from pulumi_kubernetes.core.v1 import Service # Minikube does not implement services of type `LoadBalancer`; require the user to specify if we\u0026#39;re # running on minikube, and if so, create only services of type ClusterIP. config = pulumi.Config() is_minikube = config.require_bool(\u0026#34;isMinikube\u0026#34;) app_name = \u0026#34;nginx\u0026#34; app_labels = { \u0026#34;app\u0026#34;: app_name } deployment = Deployment( app_name, spec={ \u0026#34;selector\u0026#34;: { \u0026#34;match_labels\u0026#34;: app_labels }, \u0026#34;replicas\u0026#34;: 1, \u0026#34;template\u0026#34;: { \u0026#34;metadata\u0026#34;: { \u0026#34;labels\u0026#34;: app_labels }, \u0026#34;spec\u0026#34;: { \u0026#34;containers\u0026#34;: [{ \u0026#34;name\u0026#34;: app_name, \u0026#34;image\u0026#34;: \u0026#34;nginx\u0026#34; }] } } }) # Allocate an IP to the Deployment. frontend = Service( app_name, metadata={ \u0026#34;labels\u0026#34;: deployment.spec[\u0026#34;template\u0026#34;][\u0026#34;metadata\u0026#34;][\u0026#34;labels\u0026#34;], }, spec={ \u0026#34;type\u0026#34;: \u0026#34;ClusterIP\u0026#34; if is_minikube else \u0026#34;LoadBalancer\u0026#34;, \u0026#34;ports\u0026#34;: [{ \u0026#34;port\u0026#34;: 80, \u0026#34;target_port\u0026#34;: 80, \u0026#34;protocol\u0026#34;: \u0026#34;TCP\u0026#34; }], \u0026#34;selector\u0026#34;: app_labels, }) # When \u0026#34;done\u0026#34;, this will print the public IP. if is_minikube: pulumi.export(\u0026#34;ip\u0026#34;, frontend.spec.apply(lambda v: v[\u0026#34;cluster_ip\u0026#34;] if \u0026#34;cluster_ip\u0026#34; in v else None)) else: pulumi.export(\u0026#34;ip\u0026#34;, frontend.status.apply(lambda v: v[\u0026#34;load_balancer\u0026#34;][\u0026#34;ingress\u0026#34;][0][\u0026#34;ip\u0026#34;] if \u0026#34;load_balancer\u0026#34; in v else None)) 修正が終わったら下記の通り変数 isMinikube に true を設定して、先ほどと同様に pulumi up を実行します。\n$ pulumi config set isMinikube true $ pulumi up 次に下記のコマンドを実行し Pods に付与された IP Addr を調べます。\n$ pulumi stack output ip 最後に minikube のノードにログインし curl を使って先ほど調べた IP Addr 宛に curl でアクセスします。\n$ minikube ssh $ curl 10.108.14.27 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;For online documentation and support please refer to \u0026lt;a href=\u0026#34;http://nginx.org/\u0026#34;\u0026gt;nginx.org\u0026lt;/a\u0026gt;.\u0026lt;br/\u0026gt; Commercial support is available at \u0026lt;a href=\u0026#34;http://nginx.com/\u0026#34;\u0026gt;nginx.com\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;em\u0026gt;Thank you for using nginx.\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; nginx のデフォルトのコンテンツが応答あったことを確認出来ると思います。\nまとめ Python コードの中でインフラを定義することが出来ました。今回は Kubernetes で試しましたが AWS や GCP にも対応しています。同様のことを AWS SDK でも自分は今まで行ってきましたが前述した通りマルチクラウドという点が優位性あるのかなぁという印象です。ただ、クラウド側も常に進化していて、それらに Pulumi が追従し続けられるのか不安な点も感じます。その点ではやはり AWS を利用しているエンジニアにとっては AWS SDK がベストな選択と今時点では言わざるを得ません。また Pulumi という企業自体が安定しているのか、買収されたりしないのかという不安も付き纏います。\nSDK の利用は DevOps・SRE 的に業務を行うのであれば自動化を推進するにあたり必須とも言える技術であると言えます。個人的にはこれはインフラを構築・管理するツールとしての Terraform とは立ち位置が若干異なるという認識でいます。それに対して CDK はどういう立ち位置になるのか。今回紹介した Pulumi が SDK としてのデファクトにのしあがるのかいなか。DevOps・SRE 界隈エンジニアが用いる技術もこれから更に変化していきそうな気がしますし、その過程の中で Pulumi を知って頂くのは良いことだと思います。\n","permalink":"https://jedipunkz.github.io/post/pulumi/","summary":"\u003cp\u003eこんにちは。@jedipunkz です。\u003c/p\u003e\n\u003cp\u003e今日は Pulumi (\u003ca href=\"https://www.pulumi.com/\"\u003ehttps://www.pulumi.com/\u003c/a\u003e) について紹介したいと思います。\u003c/p\u003e\n\u003cp\u003e最近ではすっかり Terraform がマルチクラウドな IaC ツールとして定着しましたが、巷では AWS CDK を使う現場も増えてきている印象です。AWS CDK は Typescript, Python などのプログラミング言語の中でインフラを定義・操作することができる AWS が提供しているツールです。ただ AWS CDK は名前の通り AWS にのみ対応していて内部的には Cloudformation Template がエキスポートされています。AWS オンリーという点と Cloudformation という点、また 2019 年時点で進化が激しく後方互換性を全く失っているので AWS CDK のアップデートに追従するのに苦労する点、色々ありまだ利用するには早いのかなぁという印象を個人的には持っています。\u003c/p\u003e\n\u003cp\u003eそこで今回紹介する Pulumi なのですが CDK 同様にプログラミング言語の中でインフラを定義できて尚且つマルチクラウド対応しています。どちらかというと旧来の SDK の分類だと思うのですが、Terraform 同様にマルチクラウドという点で個人的には以前よりウォッチしていました。\u003c/p\u003e\n\u003cp\u003e今回は公式の Getting Started 的なドキュメントに従って作業を進め Kubernetes の上に Pod を起動、その後コードを修正して再デプロイを実施して理解を深めてみたいと思います。\u003c/p\u003e\n\u003ch2 id=\"作業に必要なソフトウェア\"\u003e作業に必要なソフトウェア\u003c/h2\u003e\n\u003cp\u003e下記のソフトウェア・ツールが事前にインストールされていることを前提としたいと思います。また macOS を前提に手順を記します。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePython3, Pip\u003c/li\u003e\n\u003cli\u003eMinikube\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"参考\"\u003e参考\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://www.pulumi.com/docs/get-started/kubernetes/\"\u003ehttps://www.pulumi.com/docs/get-started/kubernetes/\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"事前準備\"\u003e事前準備\u003c/h2\u003e\n\u003cp\u003eまず macOS を使っている場合 Pulumi は下記の通り brew を使ってインストールできます。\u003c/p\u003e","title":"マルチクラウド対応 SDK の Pulumi を使って Kubernetes を操作"},{"content":"こんにちは。@jedipunkz です。\n今回は AWS ECS についてです。直近の仕事で ECS の Terraform コード開発をしていたのですがコードの構造化について考えていました。一枚岩のコードを書いても運用に耐えられるとは考えられません。また ECS を構成するにあたって ECS のネットワークモードとコンテナのロギングについて考えているうちに、どの構成が一番適しているのか？について時間を掛けて考えました。ここではそれらについてまとめたいと思います。\nTerraform コードの構造化 運用の精神的な負担を軽減するという観点で Terraform のコード開発をする上で一番重要なのはコードの構造化だと思います。前回のブログ記事に書いたのですがコードの構造化をする上で下記に留意して考えると良いと思います。\n影響範囲 ステートレスかステートフルか 安定度 ライフサイクル 結果、具体的に Terraform のコードはどのような構造になるでしょうか。自分は下記のようにコンポーネント化して Terraform の実行単位を別けました。ここは人それぞれだと思いますが、ECS 本体と ECS の周辺 AWS サービスの一般的な物を考慮しつつ、いかにシンプルに構造化するかを考えると自然と下記の区分けになる気がします。\nコンポーネント 具体的なリソース ネットワーク vpc, route table, igw, endpoint, subnet ECS 本体 ecs, alb, autoscaling, cloudwatch, iam CI/CD codebuild, codepipeline, ecr, iam パラメータ ssm, kms データストア s3, rds, elasticache \u0026hellip; vpc や subnet に関して頻繁に更新を掛ける人は少ないのではないでしょうか。よってネットワークは \u0026ldquo;影響範囲\u0026rdquo; を考慮しつつコンポーネントを別けました。また、同じ理由でパラメータ・CI/CD も ECS 本体とは実行単位を別けた方が好ましいと思います。また \u0026ldquo;ステートフルかステートレスか\u0026rdquo; という観点でデータベースやストレージは頻繁に更新する ECS 本体とは別けるべきでしょう。\nコンポーネント化する際に Terraform Remote States 機能を用いる 構造化が上記の通りできましたが、具体的にどの様に Terraform の構造化を行うのかについて記したいと思います。\n一枚岩のコードの場合、各 Resources 間で Resource 作成時に得られた id, name, arn の様な Attribute を再利用するケースが多いと思います。ですが Terraform の構造化を行うと Terraform コマンドの実行単位が異なるわけですからそういった処置が行えません。だからと言って構造単位で Variables に値を格納していては問題があるので、ここの対処法として Terraform の Remote State 機能を用います。下記に例を挙げて説明します。\n上記構造の \u0026ldquo;ネットワーク\u0026rdquo; 内の provider.tf で下記のように記して states を s3 へ格納します。\nterraform { required_version = \u0026#34;\u0026gt;= 0.12.0\u0026#34; backend \u0026#34;s3\u0026#34; { bucket = \u0026#34;example-bucket\u0026#34; key = \u0026#34;network/terraform.tfstate\u0026#34; region = \u0026#34;ap-northeast-1\u0026#34; encrypt = true } } \u0026ldquo;ネットワーク\u0026rdquo; 内 vpc.tf で下記のように vpc を作成します。\nresource \u0026#34;aws_vpc\u0026#34; \u0026#34;example\u0026#34; { cidr_block = \u0026#34;10.0.0.0/16\u0026#34; } \u0026ldquo;ネットワーク\u0026rdquo; 内 output.tf で下記のように Terraform Output を指定します。これにより別の構造で値を再利用できます。\noutput \u0026#34;example_vpc_id\u0026#34; { value = aws_vpc.example.id } すると \u0026ldquo;ネットワーク\u0026rdquo; 以外の構造 (例 : ECS 本体等) で下記のように terraform_remote_state を記すと\ndata \u0026#34;terraform_remote_state\u0026#34; \u0026#34;network\u0026#34; { backend = \u0026#34;s3\u0026#34; config = { bucket = \u0026#34;example-bucket\u0026#34; key = \u0026#34;network/terraform.tfstate\u0026#34; region = \u0026#34;ap-northeast-1\u0026#34; encrypt = true } } 下記のように他の構造(この場合ネットワーク VPC) を作成した際の Terraform Attribute が呼び出せます。\nvpc_id = data.terraform_remote_state.network.outputs.example_vpc_id 実際の動きとしては vpc を作成した際の terraform tfstate が s3 に格納されていてその s3 内の tfstate を他のコンポーネント作成時に呼び出す、といったことをしています。この機能によりコンポーネント化を容易に実現することが可能です。\nECS 構成 次に ECS 自体の構成がどの様なものが最適なのか検討していきます。\n自分は ECS の構成を考える上で重要な要素として下記があると考えています。\nnetworkMode logDriver これら二つについて記していきます。\nnetworkMode ECS にある networkMode のどれを用いるのがベストなのか検討する必要があります。後述するロギングの情報と合わせて考えるので、ここでは一般的な情報のみを記しておきます。\nbridge ECS インスタンスの任意のポートをコンテナのポートにマッピング ECS インスタンスの ENI を複数のタスクが共有で利用 host コンテナで expose したポートをインスタンスでも利用 よって基本、同じポートを用いるコンテナを複数起動できない awsvpc ENI がタスク毎にアタッチされる タスク間でのポートマッピングの考慮不要 ネットワークパフォーマンスが優れている ENI 毎に SecurityGroup を紐づけ ALB と NLB に IP ターゲットとして登録が可能 結果的には awsvpc がパフォーマンス的にも優れていて最適解です。また logDriver fluentd の通信のことを考えると bridge も検討するべきだと解りますが詳しくは後術します。\nlogDriver ECS の場合のロギングの選択肢としては Cloudwatch Logs か Fluentd か、となると思います。ログの格納先は何が考えられるでしょうか。s3, cloudwatch logs, elasticsearch を通常考えると思いますが、格納した後のログの再利用を考えるとどうなるでしょう。自分は Elasticsearch にログを格納して Kibana で可視化するケースが多いのですが、ここで注意が必要です。Cloudwatch Logs から Elasticsearch Service へログのストリームをする際に圧縮されたデータを展開しつつストリームする必要があります。これを Terraform でコード化するのは結構厄介です。具体的には Lambda Function を作成して Python or Nodejs でコードを書き Firehose を使って Elasticsearch Service へ繋ぎ込みます。その点では Fluentd -\u0026gt; Elasticsearch Service と直接ストリーム出来れば都合がいいです。ということは Fluentd 一択？と考えたいところなのですが\u0026hellip; Fluentd を ECS で扱うことを考えると結構構成に悩みます。\n考慮点を踏まえた結果の ECS 構成 結果、networkMode, logDriver の検討を行うと下記のような構成パターンが選択肢としてあげられます。\nECS/Fargate 構成の場合 networkMode : awsvpc logDriver : awslogs -\u0026gt; cloudwatch logs ECS/EC2 構成の場合 構成(2)\nnetworkMode : awsvpc logDriver : awslogs -\u0026gt; cloudwatch logs 構成(3)\nnetworkMode : awsvpc logDriver : fluentd 通信 : socket 起動タイプ : daemon type -\u0026gt; s3, cloudwatch logs, elasticsearch service 構成(4)\nnetworkMode : bridge logDriver : fluentd 通信 : tcp 起動タイプ : daemon type -\u0026gt; s3, cloudwatch logs, elasticsearch service 構成(5)\nnetworkMode : awsvpc logDriver : fluentd 通信 : socket 起動タイプ : sidecar -\u0026gt; s3, cloudwatch logs, elasticsearch service 構成(6)\nnetworkMode : bridge logDriver : fluentd 通信 : tcp 起動タイプ : sidecar -\u0026gt; s3, cloudwatch logs, elasticsearch service 上記構成案の説明と結果 結果として ECS/Fargate を用いる場合は logDriver fluentd がまだサポートされていないので (2019/10時点)、awslogs となり cloudwatch logs をログ格納先として選択せざるを得ないです。\nまた ECS/EC2 構成の場合は結論を言うと構成 (5), (6) は sidecar 構成で fluentd コンテナを起動して隣接したコンテナのログを転送するのですが、この構成をとる場合下記が原則になります。\n「1タスク / 1インスタンス」\nこれは fluentd で用いる通信が socket, tcp に関わらずインスタンス内の一つのリソースを複数タスクで共有することができないからです。1タスク/1インスタンスでもサービスは提供できますし考えることは少なくなるのですがそうするとデプロイ時にどのような影響を与えるかは考慮しておいた方がいいです。デプロイ発生時にクラスタインスタンスのオートスケールが発生する事はエンジニアによっては良いとは考えないかもしれません。構成(3), (4) なら「nタスク/1インスタンス」の構成が可能で autoscaling policy configuration をうまく設計すればでデプロイ時もオートスケールが発動せずに済みます。\nこの sidecar 構成のタスクとインスタンスの関係についてもう少し記すと\u0026hellip;、\n下記は tcp の場合のコンテナ定義内 logConfiguration です。この場合、インスタンスの24224ポートを指定しています。また fluentd コンテナは portMappings オプションで public (インスタンス) の 24224 ポートを binding しているため、1インスタンスで1 fluentd コンテナしか起動出来ない。つまり1タスク/1インスタンス。\n\u0026#34;logConfiguration\u0026#34;: { \u0026#34;logDriver\u0026#34;: \u0026#34;fluentd\u0026#34;, \u0026#34;options\u0026#34;: { \u0026#34;fluentd-address\u0026#34;: \u0026#34;localhost:24224\u0026#34;, そしてこちらは socket の場合コンテナ定義内 logConfiguration。この場合はインスタンスの volume を コンテナ上でマウントするため1インスタンス上に1 fluentd コンテナしか起動しない。つまり1タスク/1インスタンス\n\u0026#34;logConfiguration\u0026#34;: { \u0026#34;logDriver\u0026#34;: \u0026#34;fluentd\u0026#34;, \u0026#34;options\u0026#34;: { \u0026#34;fluentd-address\u0026#34;: \u0026#34;unix:///var/run/fluent/fluent.sock\u0026#34;, また(3), (4) の daemon type について簡単に述べると、ECS/EC2 のクラスタインスタンス1つに対して必ず1つの fluentd コンテナを起動させる起動モードのことを言います。(https://docs.aws.amazon.com/ja_jp/AmazonECS/latest/developerguide/ecs_services.html)\nよって、Fargate を用いる場合は構成(1)で EC2 構成の場合は構成(2), (3), (4) となるでしょうか。\nアーキテクチャ 図を描くとこんな感じになると思います。大体構成が決まってくるのではないでしょうか。\n構成(1), (2), (3), (4)の場合 +---------------------+ + | instance / fargate | ... | +---------------------+ | +------+ +------+ | ap-northeast-1a | task | | task | ... | +------+ +------+ | | + +---------------------+ | alb | +---------------------+ | + +------+ +------+ | | task | | task | ... | +------+ +------+ | ap-northeast-1c +---------------------+ | | instance / fargate | ... | +---------------------+ + 構成 (5), (6) の場合 +----------+ + | instance | ... | +----------+ | +----------+ | ap-northeast-1a | task | ... | +----------+ | | + +---------------------+ | alb | +---------------------+ | + +----------+ | | task | ... | +----------+ | ap-northeast-1c +----------+ | | instance | ... | +----------+ + バッチについて検討 バッチ処理も ECS を使って行うことができます。この場合の構成を考えると下記が選択肢として残りました。\nFargate networkMode : awsvpc logDriver : awslogs 構成イメージは下記の様になります。\n+---------+ +------------------+ | task | \u0026lt;--- | cloudwatch event | +---------+ +------------------+ +---------+ | Fargate | +---------+ この構成になったのは ECS/EC2 構成の場合に構成上の問題があることが発覚したからです。EC2 の場合 ECS Service を起動しないとバッチのための Task が起動しないので daemon として何かしらのタスクをインスタンス上に常時稼働させる必要があります。そして aws_cloudwatch_event_target で下記の通りコンテナ定義を上書きすることでバッチ処理を実行できます。\n{ \u0026#34;containerOverrides\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;nginx\u0026#34;, \u0026#34;command\u0026#34;: [\u0026#34;バッチ処理のコマンド\u0026#34;] } ] } よって常時稼働させるインスタンスや ECS Service のリソースが無駄に使用されることになります。そういった点で ECS/Fargate の場合は無駄なインスタンスを起動しなくていい、というメリットがあります。\nCI/CD CI/CD について考えます。下記のようなアーキテクチャと処理の流れを想定します。ここでは CircleCI ではなく CodeBuild, CodePipeline を想定します。\n+--------+ +-----------+ +--------------+ +---------------------+ | Github | - webhook -\u0026gt; | Codebuild | -\u0026gt; | CodePipeline | -\u0026gt; | ECS Cluster/Service | +--------+ +-----------+ +--------------+ +---------------------+ 処理の流れ\n(1) Github の指定ブランチにプッシュされたのをトリガーに CodeBuild でコンテナがビルドされる。ビルド情報は buildspec.yml に指定 (2) ビルドされた結果 imagedefinitions.json が生成される (3) imagedefinitions.json を元に CodePipeline を経由して ECS Clusnter/Service にデプロイ実施 (4) rolling-update が実施される 下記は buildspec.yml。\nversion: 0.2 phases: pre_build: commands: - $(aws ecr get-login --region $AWS_DEFAULT_REGION --no-include-email) - WEB_REPO=$(aws ecr describe-repositories --repository-names web --output text --query \u0026#34;repositories[0].repositoryUri\u0026#34;) - WEB_IMAGE=$WEB_REPO:latest - APP_REPO=$(aws ecr describe-repositories --repository-names app --output text --query \u0026#34;repositories[0].repositoryUri\u0026#34;) - APP_IMAGE=$APP_REPO:latest build: commands: - docker build -t $WEB_IMAGE ./web - docker push $WEB_IMAGE - docker build -t $APP_IMAGE ./app - docker push $APP_IMAGE post_build: commands: - printf \u0026#39;[{\u0026#34;name\u0026#34;:\u0026#34;web\u0026#34;,\u0026#34;imageUri\u0026#34;:\u0026#34;%s\u0026#34;}, {\u0026#34;name\u0026#34;:\u0026#34;app\u0026#34;,\u0026#34;imageUri\u0026#34;:\u0026#34;%s\u0026#34;}]\u0026#39; $WEB_IMAGE $APP_IMAGE \u0026gt; imagedefinitions.json artifacts: files: - imagedefinitions.json まとめ networkMode, logDriver を考える中で構成が決まりました。また Terraform でコード化する際の構造化についての考えをまとめ、付随する CI/CD, バッチについてもまとめました。完成するコードは必然的にシンプルなものになると思います。また ECS/Fargate 構成での logDriver の fluentd 対応については今進んでいる最中とのことで、期待しています。そうすると2019年初めのコストダウンと相まってより Fargate を選択しやすい状況になるのではないでしょうか。\n","permalink":"https://jedipunkz.github.io/post/ecs/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003e今回は AWS ECS についてです。直近の仕事で ECS の Terraform コード開発をしていたのですがコードの構造化について考えていました。一枚岩のコードを書いても運用に耐えられるとは考えられません。また ECS を構成するにあたって ECS のネットワークモードとコンテナのロギングについて考えているうちに、どの構成が一番適しているのか？について時間を掛けて考えました。ここではそれらについてまとめたいと思います。\u003c/p\u003e\n\u003ch2 id=\"terraform-コードの構造化\"\u003eTerraform コードの構造化\u003c/h2\u003e\n\u003cp\u003e運用の精神的な負担を軽減するという観点で Terraform のコード開発をする上で一番重要なのはコードの構造化だと思います。前回のブログ記事に書いたのですがコードの構造化をする上で下記に留意して考えると良いと思います。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e影響範囲\u003c/li\u003e\n\u003cli\u003eステートレスかステートフルか\u003c/li\u003e\n\u003cli\u003e安定度\u003c/li\u003e\n\u003cli\u003eライフサイクル\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e結果、具体的に Terraform のコードはどのような構造になるでしょうか。自分は下記のようにコンポーネント化して Terraform の実行単位を別けました。ここは人それぞれだと思いますが、ECS 本体と ECS の周辺 AWS サービスの一般的な物を考慮しつつ、いかにシンプルに構造化するかを考えると自然と下記の区分けになる気がします。\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eコンポーネント\u003c/th\u003e\n          \u003cth\u003e具体的なリソース\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eネットワーク\u003c/td\u003e\n          \u003ctd\u003evpc, route table, igw, endpoint, subnet\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eECS 本体\u003c/td\u003e\n          \u003ctd\u003eecs, alb, autoscaling, cloudwatch, iam\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eCI/CD\u003c/td\u003e\n          \u003ctd\u003ecodebuild, codepipeline, ecr, iam\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eパラメータ\u003c/td\u003e\n          \u003ctd\u003essm, kms\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eデータストア\u003c/td\u003e\n          \u003ctd\u003es3, rds, elasticache \u0026hellip;\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003evpc や subnet に関して頻繁に更新を掛ける人は少ないのではないでしょうか。よってネットワークは \u0026ldquo;影響範囲\u0026rdquo; を考慮しつつコンポーネントを別けました。また、同じ理由でパラメータ・CI/CD も ECS 本体とは実行単位を別けた方が好ましいと思います。また \u0026ldquo;ステートフルかステートレスか\u0026rdquo; という観点でデータベースやストレージは頻繁に更新する ECS 本体とは別けるべきでしょう。\u003c/p\u003e","title":"ECS の構成と Terraform コード化する際の構造化について"},{"content":"こんにちは。@jedipunkz です。\n今回は電子書籍 \u0026lsquo;Pragmatic Terraform on AWS\u0026rsquo; を読んでとても内容が素晴らしかったので紹介させて頂きます。書籍の購入は下記の URL から可能です。\nhttps://booth.pm/ja/items/1318735\nブログ記事では書籍の細かい内容については述べません。これから購入される方々が買う意欲を無くすようなブログ記事にしたくないからです。なのでこのブログでは自分が Terraform 運用について感じていた問題点とこの電子書籍がどう解決してくれたのかについて記したいと思います。\n自分が Terraform 運用で感じていた問題点 Terraform を使ったインフラコード化と構築は自分の結構経験があるのですが、その構築に使ったコードで構成を運用する難しさはいつも感じていました。Terraform を使った継続的なインフラの運用についてです。具体的には下記のような疑問と言いますか問題点です。\n(1) どのような実行単位で .tf コードを書くか (2) 削除系・修正系の操作も Terraform で行うのか (3) ステートフルなインフラとステートレスなインフラの管理方法 (1) は terraform plan/apply を実行するディレクトリの構造についてです。Terraform は同じディレクトリ上にある .tf ファイル全てを読み込んでくれますし一斉にインフラをデプロイすることも可能です。ですが、何かインフラを修正・削除したい場合、削除してはいけないリソースも同ディレクトリ上の .tf ファイルで管理しているわけですから何かしらのミスで大事なインフラに影響を与えてしまう事になります。影響範囲が大きすぎるのです。\n(2) は、\u0026lsquo;初期の構成のみを Terraform で構築自動化する\u0026rsquo; のかどうか、ということになります。構築に使ったコードで継続的に削除系・修正系の操作も行うのか。これも (1) と同様に管理するインフラの規模が大きくなると影響範囲が大きくなり、運用者の精神的負担が増します。\n(3) は RDS, S3 等のステートフルなインフラと、それ以外のステートレスなインフラを同じ .tf コードで管理していいのか、という疑問です。修正・削除が多発するインフラは .tf コードで継続的に運用出来たとしても、RDS, S3 の様な状態が重要になるインフラは滅多に削除・修正操作は通常行いません。これら二種類のインフラを同じように管理してしまっていいのか？という疑問です。\nこれらの疑問や思っていた問題点について、この \u0026lsquo;Pragmatic Terraform on AWS\u0026rsquo; は全て解決してくれました。\nPragmatic Terraform on AWS の構成 章ごとの説明は詳細には書きませんが、大体の流れは下記のようになっています。\n(1) 基本的な利用方法 (2) Tips 集 (3) ECS Fargate を中心にした実践的な構成のコード化 (4) 構造化 (5) ベストプラクティス (6) モジュール設計 (2) Tips 集について 席に述べたように自分は Terraform の経験が結構ある方だと思うのですが、それでも知らない Terraform の機能が複数ありました。特に Terraform 利用者にとって、要点だけをおさえた説明文と共に次々に多くの Tips を説明してくれるこの章はとても意味があるものではないでしょうか。素晴らしいです。\n(3) ECS Fargate を中心にした実践的な構成のコード化 賛否両論あるかもしれませんが2019年現在でサービスを構成しようとすると GKE か ECS か EKS かの3択なのではないでしょうか。特にサービス・プロバイダにとってマネージドサービスは主流になると思います。その中でも国内では AWS の利用率が高いわけですから ECS を理解しておくことはインフラ系のエンジニアにとって非常に重要になります。\nこの章では ECS Fargate を中心として下記のような特徴を持ったシステムをコード化するという実践的なものになっています。\nECS とその裏側にある ALB, IAM, VPC 等の基本的な構成構築 ECS による Web サービス・ECS によるバッチサービス構成構築 鍵管理 設定管理 デプロイメントパイプライン ロギング 特に CodePipeline, Github, ECR を使ったコンテナの継続的デプロイに関する説明と、Cloudwatch Logs, S3, Athena, Kinesis Data Firehose を使ったトレーサビリティを上げるという意味でのログ管理の説明は、「ここまで実践的な説明してくれるのか」という印象を持ちました。2019年時点でインフラエンジニアをしている方は駆らず理解しておくべき内容だと思います。\n(4) 構造化 ここが自分的には一番知りたかった内容でした。公式ドキュメントに断片的な情報はあっても、やはり Terraform ユーザの生の声を聞きたいなと感じていたので、この章は非常に自分にとって役立つものでした。\n誰でも思いつく .tf コードの分離・別ディレクトリ管理・別レポジトリ管理から、何を意識してコンポーネント化するかの考え方がソフトウェア設計を元に非常に説得力ある説明と共に解説されています。\nこの章は概略を書いてしまうと内容が把握出来てしまうので、興味ある方は書籍を購入して読んでみてください。きっと損はしないです。\n(5), (6) ベストプラクティスとモジュール設計 運用するなかでおさえておくべきポイントが掲載されています。モジュール化を行う上での注意点等、運用に入ってから苦労しないように色んな技が掲載されています。\nまとめ 以上、自分の読書感想でしたが、素晴らしさは伝わりましたでしょうか。Terraform を使っているエンジニアであれば必ず疑問に感じる点を見事に解決に導いている良書だと思いますし、説明がとても簡潔で良いテンポで理解できる内容でした。また情報量や文字数等、自分にとって最適でしたし、内容的には \u0026ldquo;Terraform を使い始めた人\u0026rdquo; から \u0026ldquo;Terraform を使っているが苦労している人\u0026rdquo; までをカバーしたものになっています。\nネット上には断片的には情報はあっても、ここまでまとまった内容のサイトは無いと思いますし、実際に Terraform を使っているエンジニアだからこその \u0026ldquo;運用を行う上での解決策\u0026rdquo; が満載な書籍でした。\n","permalink":"https://jedipunkz.github.io/post/2019/07/27/pragmatic-terraform/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003e今回は電子書籍 \u0026lsquo;Pragmatic Terraform on AWS\u0026rsquo; を読んでとても内容が素晴らしかったので紹介させて頂きます。書籍の購入は下記の URL から可能です。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://booth.pm/ja/items/1318735\"\u003ehttps://booth.pm/ja/items/1318735\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eブログ記事では書籍の細かい内容については述べません。これから購入される方々が買う意欲を無くすようなブログ記事にしたくないからです。なのでこのブログでは自分が Terraform 運用について感じていた問題点とこの電子書籍がどう解決してくれたのかについて記したいと思います。\u003c/p\u003e\n\u003ch2 id=\"自分が-terraform-運用で感じていた問題点\"\u003e自分が Terraform 運用で感じていた問題点\u003c/h2\u003e\n\u003cp\u003eTerraform を使ったインフラコード化と構築は自分の結構経験があるのですが、その構築に使ったコードで構成を運用する難しさはいつも感じていました。Terraform を使った継続的なインフラの運用についてです。具体的には下記のような疑問と言いますか問題点です。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e(1) どのような実行単位で .tf コードを書くか\u003c/li\u003e\n\u003cli\u003e(2) 削除系・修正系の操作も Terraform で行うのか\u003c/li\u003e\n\u003cli\u003e(3) ステートフルなインフラとステートレスなインフラの管理方法\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e(1) は \u003ccode\u003eterraform plan/apply\u003c/code\u003e を実行するディレクトリの構造についてです。Terraform は同じディレクトリ上にある .tf ファイル全てを読み込んでくれますし一斉にインフラをデプロイすることも可能です。ですが、何かインフラを修正・削除したい場合、削除してはいけないリソースも同ディレクトリ上の .tf ファイルで管理しているわけですから何かしらのミスで大事なインフラに影響を与えてしまう事になります。影響範囲が大きすぎるのです。\u003c/p\u003e\n\u003cp\u003e(2) は、\u0026lsquo;初期の構成のみを Terraform で構築自動化する\u0026rsquo; のかどうか、ということになります。構築に使ったコードで継続的に削除系・修正系の操作も行うのか。これも (1) と同様に管理するインフラの規模が大きくなると影響範囲が大きくなり、運用者の精神的負担が増します。\u003c/p\u003e\n\u003cp\u003e(3) は RDS, S3 等のステートフルなインフラと、それ以外のステートレスなインフラを同じ .tf コードで管理していいのか、という疑問です。修正・削除が多発するインフラは .tf コードで継続的に運用出来たとしても、RDS, S3 の様な状態が重要になるインフラは滅多に削除・修正操作は通常行いません。これら二種類のインフラを同じように管理してしまっていいのか？という疑問です。\u003c/p\u003e\n\u003cp\u003eこれらの疑問や思っていた問題点について、この \u0026lsquo;Pragmatic Terraform on AWS\u0026rsquo; は全て解決してくれました。\u003c/p\u003e\n\u003ch2 id=\"pragmatic-terraform-on-aws-の構成\"\u003ePragmatic Terraform on AWS の構成\u003c/h2\u003e\n\u003cp\u003e章ごとの説明は詳細には書きませんが、大体の流れは下記のようになっています。\u003c/p\u003e","title":"Pragmatic Terraform on AWS の内容が素晴らしかったので感想を述べる"},{"content":"こんにちは。@jedipunkz です。\n少し前の話なのですが Google Cloud Platform が Terraformer というツールを出しました。正確には数年前に Google が買収した Waze というサービスのエンジニア達が開発したようです。このツールは GCP, AWS, OpenStack, Kubernetes 等、各クラウド・プラットフォームに対応したリバース Terraform と言えるツールになっていて、構築されたクラウド上の状態を元に terraform の .tf コードと .tfstate ファイルをインポートしてくれます。terraform import は tfstate のインポートのみに対応してたのでこれは夢のツールじゃないか！ということで当初期待して使い始めたのですが、使ってみる中で幾つかの問題点も見えてきました。今回はその気になった問題点を中心に Terraformer の基本的な動作を説明していきたいと思います。\n公式サイト 下記の Github アカウントで公開されています。\nhttps://github.com/GoogleCloudPlatform/terraformer\nRequrements Terraformer を動作させるには下記のソフトウェアが必要です。今回は macos を想定して情報を記していますが Linux でも動作します。適宜読み替えてください。インストール方法と設定方法はここでは割愛させて頂きます。\nmacos homebrew terraform awscli 今回の前提のクラウドプラットフォーム 自分がいつも使っているプラットフォームということで今回は aws を前提に記事を書いていきます。ここが結構肝心なところで、Google Cloud Platform が開発したこともあり GCP 向けの機能が一番 Feature されているように読み取れます。つまり aws を対象とした Terraformer の機能が一部追いついていない点も後に述べたいと思います。\n動作させるまでの準備 Terraform と同様に Terraformer でも動作せせるディレクトリが必要になります。\nmkdir working_dir cd working_dir Terraformer を動作させるために Terraform の plugin が必要です。先に述べたようにここでは \u0026lsquo;aws\u0026rsquo; Plugin をダウンロードします。そのために init.tf を下記の通り作成します。\necho \u0026#39;provider \u0026#34;aws\u0026#34; {}\u0026#39; \u0026gt; init.tf terraform init コマンドを実行して Terraform \u0026lsquo;aws\u0026rsquo; plugin をダウンロードします。\nterraform init awscli の profile を用意します。インポートしたい環境を管理している aws アカウントに IAM ユーザを作成して、その secret key, secret access key とリージョン情報を記して下さい。\naws configure 基本的動作の確認 インスタンス・サブネット・セキュリティグループ情報をインポート ここではインスタンスを構成する EC2 インスタンス・サブネット・セキュリティグループのリソースタイプを指定してインポート操作を行ってみます。インポートは下記のコマンド1つ実行するのみです。\nterraformer import aws --resources=ec2_instance,sg,subnet --connect=true --regions=ap-northeast-1 結果、下記のようなディレクトリ構成でファイルがインポートされます。\n. ├ ─ ─ generated │ └ ─ ─ aws │ ├ ─ ─ ec2_instance │ │ └ ─ ─ ap-northeast-1 │ │ ├ ─ ─ instance.tf │ │ ├ ─ ─ outputs.tf │ │ ├ ─ ─ provider.tf │ │ └ ─ ─ terraform.tfstate │ ├ ─ ─ sg │ │ └ ─ ─ ap-northeast-1 │ │ ├ ─ ─ outputs.tf │ │ ├ ─ ─ provider.tf │ │ ├ ─ ─ security_group.tf │ │ └ ─ ─ terraform.tfstate │ └ ─ ─ subnet │ └ ─ ─ ap-northeast-1 │ ├ ─ ─ outputs.tf │ ├ ─ ─ provider.tf │ ├ ─ ─ subnet.tf │ ├ ─ ─ terraform.tfstate │ └ ─ ─ variables.tf └ ─ ─ init.tf 早速インスタンスを構成する generated/aws/ec2_instance/ap-northeast-1/instance.tf を確認してみます。\nresource \u0026#34;aws_instance\u0026#34; \u0026#34;i-************_test-instance\u0026#34; { ami = \u0026#34;ami-***********\u0026#34; associate_public_ip_address = true availability_zone = \u0026#34;ap-northeast-1a\u0026#34; cpu_core_count = \u0026#34;1\u0026#34; cpu_threads_per_core = \u0026#34;1\u0026#34; snip ... subnet_id = \u0026#34;subnet-****************\u0026#34; // サブネット snip... vpc_security_group_ids = [\u0026#34;sg-****************\u0026#34;] // セキュリティグループ このディレクトリ構成と instance.tf の内容から下記のことが言えると思います。\naws インフラリソース毎に terraform を実行する想定のディレクトリが別れている insntace.tf を見るとリソース ID が直打ちになっていて依存関係が解決されていないコードになっている filter オプションで特定のリソースのコードだけインポート 下記のように filter オプションを用いて特定の ID のリソースだけ .tf, .tfstate ファイルをインポートすることも可能です。\nterraformer import aws --resources=vpc --filter=aws_vpc=vpc-********* --regions=ap-northeast-1 ここまでで幾つか問題点に気がつくと思います。\n自分が気になった利用する上での問題点その1 Terraform は様々な単位でコードを束ねる事は勿論なのですが、何かしらのサービスを形成するシステム一式という単位でコードを束ねる事はしてくれない、ということになります。例えば instance, subnet 両方に影響を与える修正をする場合 terraform コマンドを2つのディレクトリで実行しなければいけないことになります。なんだかスッキリしません\u0026hellip;。\n自分が気になった利用する上での問題点その2 通常 Terraform のコードを書く際に Resource 間の依存関係保ちます。例えば \u0026lsquo;aws_instance\u0026rsquo; 内の \u0026lsquo;subnet_id\u0026rsquo; には通常下記のように依存関係を記します。\nsubnet_id = \u0026#34;${aws_subnet.foo.id}\u0026#34; ですが生成されたコードを確認すると id がベタ書きされています。Resource 間の依存関係が解決されていません。\n自分が気になった利用する上での問題点その3 Terraform の Module 機能が用いられていないコードが生成されています。コード全体を簡潔に尚且可読性良くするために Module 機能を用いて Terraform コードを書くのが一般的になっているので、全くのベタ書きなコードが生成されているところに若干問題を感じます。\n今現在 2019/07 で対応している aws インフラリソース一覧 2019/07 現在で対応している aws のインフラリソース一覧です。Terraform の Resources 名で記してあります。\naws_elb aws_lb aws_lb_listener aws_lb_listener_rule aws_lb_listener_certificate aws_lb_target_group aws_lb_target_group_attachment aws_autoscaling_group aws_launch_configuration aws_launch_template aws_db_instance aws_db_parameter_group aws_db_subnet_group aws_db_option_group aws_db_event_subscription aws_iam_role aws_iam_role_policy aws_iam_user aws_iam_user_group_membership aws_iam_user_policy aws_iam_policy_attachment aws_iam_policy aws_iam_group aws_iam_group_membership aws_iam_group_policy aws_internet_gateway aws_network_acl aws_s3_bucket aws_s3_bucket_policy aws_security_group aws_subnet aws_vpc aws_vpn_connection aws_vpn_gateway aws_route53_zone aws_route53_record aws_acm_certificate aws_elasticache_cluster aws_elasticache_parameter_group aws_elasticache_subnet_group aws_elasticache_replication_group aws_cloudfront_distribution aws_instance 自分が気になった利用する上での問題点その3 Google Cloud Platform が開発していることもあり Google Cloud の対応状況は良いと思うのですが(ごめんなさい、あまり確認出来ていません)ですが aws に関して対応されているリソースは下記で全てです。例えば基本的なインフラリソース aws_iam_instance_profile もないのでリソース ID を直打ちする必要がありますし、cloudwatch 等もありません。対応状況はだいぶ良くないのかなぁという印象です。\n考察 以上、簡単にですが基本的な利用方法と自分が気になった箇所を紹介させてもらいました。\nTerraform 自体にもインポート機能がありますが tfstate のみ対応していて .tf をインポートすることが出来ません。なのでこのツールへの期待はとても大きかったのですが、いま現時点では既存環境のコードを書く時にインポートして参考にする、という程度の使い方しか出来ないかもしれません。問題点を中心に Terraformer がインポートするコードの特徴を下記にあげます。\nterraform plan/apply 実行する単位が Resource 単位に別れてしまっている Resource 間の依存関係が解決されていない Module が用いられていない .tf コード 対応している aws Resource がまだまだ追いついていない ただ、まだ発表されて間もないツールなので今後に期待です。完成度が上がれば最高のツールになるでしょう。対応している aws resource がまだ追い付いていないもの今後開発が進めば時間だけの問題な気もします。また今回私は動作確認出来ていないのですが、GCP であれば対応している Resource 状況も良さそうです。\n","permalink":"https://jedipunkz.github.io/post/2019/07/26/terraformer/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003e少し前の話なのですが Google Cloud Platform が Terraformer というツールを出しました。正確には数年前に Google が買収した Waze というサービスのエンジニア達が開発したようです。このツールは GCP, AWS, OpenStack, Kubernetes 等、各クラウド・プラットフォームに対応したリバース Terraform と言えるツールになっていて、構築されたクラウド上の状態を元に terraform の .tf コードと .tfstate ファイルをインポートしてくれます。\u003ccode\u003eterraform import\u003c/code\u003e は tfstate のインポートのみに対応してたのでこれは夢のツールじゃないか！ということで当初期待して使い始めたのですが、使ってみる中で幾つかの問題点も見えてきました。今回はその気になった問題点を中心に Terraformer の基本的な動作を説明していきたいと思います。\u003c/p\u003e\n\u003ch2 id=\"公式サイト\"\u003e公式サイト\u003c/h2\u003e\n\u003cp\u003e下記の Github アカウントで公開されています。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/GoogleCloudPlatform/terraformer\"\u003ehttps://github.com/GoogleCloudPlatform/terraformer\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"requrements\"\u003eRequrements\u003c/h2\u003e\n\u003cp\u003eTerraformer を動作させるには下記のソフトウェアが必要です。今回は macos を想定して情報を記していますが Linux でも動作します。適宜読み替えてください。インストール方法と設定方法はここでは割愛させて頂きます。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003emacos\u003c/li\u003e\n\u003cli\u003ehomebrew\u003c/li\u003e\n\u003cli\u003eterraform\u003c/li\u003e\n\u003cli\u003eawscli\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"今回の前提のクラウドプラットフォーム\"\u003e今回の前提のクラウドプラットフォーム\u003c/h2\u003e\n\u003cp\u003e自分がいつも使っているプラットフォームということで今回は aws を前提に記事を書いていきます。ここが結構肝心なところで、Google Cloud Platform が開発したこともあり GCP 向けの機能が一番 Feature されているように読み取れます。つまり aws を対象とした Terraformer の機能が一部追いついていない点も後に述べたいと思います。\u003c/p\u003e\n\u003ch2 id=\"動作させるまでの準備\"\u003e動作させるまでの準備\u003c/h2\u003e\n\u003cp\u003eTerraform と同様に Terraformer でも動作せせるディレクトリが必要になります。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003emkdir working_dir\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ecd working_dir\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTerraformer を動作させるために Terraform の plugin が必要です。先に述べたようにここでは \u0026lsquo;aws\u0026rsquo; Plugin をダウンロードします。そのために init.tf を下記の通り作成します。\u003c/p\u003e","title":"期待のツール Terrafomer の基本動作方法と問題点"},{"content":"こんにちは。@jedipunkz です。\n今回は Hashicorp の Consul クラスタを Kubernetes 上で稼働させる方法について調べてみました。\nHashicorp Consul はサービスディスカバリが行えるソフトウェアで、私も以前居た職場で利用していました。アプリケーション間で互いに接続先を確認し合う事が出来ます。以前構築した Consul クラスタはインスタンス上に直に起動していたのですが最近だとどうやってデプロイするのか興味を持ち Kubernetes 上にデプロイする方法を調べた所 Helm を使って簡単にデプロイ出来る事が分かりました。\nまた今回は minikube を使って複数のレプリカを起動するようにしていますが、Helm Chart を用いると Kubernetes のノード毎に Consul Pod が1つずつ起動するようになっていて、ノードの障害を考慮した可用性という点でも優れているなぁと感じました。また Kubernetes の Pod ですのでプロセスが落ちた際に即座に再起動が行われるという点でも優れています。勿論 Consul クラスタの Leader が落ちた場合には Leader Election (リーダ昇格のための選挙) が行われ、直ちに隣接した Kubernetes ノード上の Consul Pod がリーダーに昇格します。といった意味でも Kubernetes 上に Consul をデプロイするという考えは優れているのではないでしょうか。\nRequirements 下記のソフトウェアが事前に必要です。この手順では予めこれらがインストールされていることとして記していきます。\nminikube kubectl helm Consul クラスタ起動までの手順 早速ですが手順を記していきます。\nHashicorp の Github にて Consul の Helm Chart が公開されています。helm search しても出てきますが、今回は Github のものを用いました。\ngit clone https://github.com/hashicorp/consul-helm.git cd consul-helm git checkout v0.7.0 次にコンフィギュレーションを記した yaml ファイルを生成 (修正) します。本来、Kubernetes のノード毎に1つずつの Pod が起動するようになっていて、逆に言うと1ノードに複数の Consul Pod は起動しません。今回は手元も端末の minikube でお手軽に試せるようレポジトリ上のファイル value.yml を下記のように修正加えました。この手順は Github の Issue https://github.com/hashicorp/consul-helm/issues/13 で記されています。\nもちろん、minikube ではなく Kubernetes 環境を利用できる方はこの手順は飛ばして構いません。\n# 下記はコメントアウト # affinity: | # podAntiAffinity: # requiredDuringSchedulingIgnoredDuringExecution: # - labelSelector: # matchLabels: # app: {{ template \u0026#34;consul.name\u0026#34; . }} # release: \u0026#34;{{ .Release.Name }}\u0026#34; # component: server # topologyKey: kubernetes.io/hostname # minikube 用に下記を有効にする affinity: | podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 podAffinityTerm: topologyKey: kubernetes.io/hostname labelSelector: matchExpressions: - key: component operator: In values: - \u0026#34;{{ .Release.Name }}-{{ .Values.Component }}\u0026#34; kubectl, helm コマンドが minikube を向くように下記のようにコマンドを実行します。\nkubectl config use-context minikube helm init Consul クラスタを helm を用いてデプロイします。下記のコマンドでデプロイが一気に完了します。\nhelm install --name consul . 暫くすると下記の通り Pods, Services の状態が確認出来ると思います。\n$ kubectl get pods NAME READY STATUS RESTARTS AGE consul-q5s62 1/1 Running 0 21m consul-server-0 1/1 Running 0 21m consul-server-1 1/1 Running 0 21m consul-server-2 1/1 Running 0 21m $ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE consul-dns ClusterIP 10.108.171.87 \u0026lt;none\u0026gt; 53/TCP,53/UDP 22m consul-server ClusterIP None \u0026lt;none\u0026gt; 8500/TCP,8301/TCP,8301/UDP,8302/TCP,8302/UDP,8300/TCP,8600/TCP,8600/UDP 22m consul-ui ClusterIP 10.107.40.197 \u0026lt;none\u0026gt; 80/TCP 22m kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 48m 起動した Consul クラスタの状態を確認してみます。consul members コマンドを各 Pod 上で実行すると、クラスタに Join しているノード (この場合 Consul Pod) の状態を一覧表示出来ます。minikube ノードにも consul-agent が起動していることが確認出来ます。\n$ for i in {0..2}; do kubectl exec consul-server-$i -- sh -c \u0026#39;consul members\u0026#39;; done Node Address Status Type Build Protocol DC Segment consul-server-0 172.17.0.6:8301 alive server 1.4.4 2 dc1 \u0026lt;all\u0026gt; consul-server-1 172.17.0.7:8301 alive server 1.4.4 2 dc1 \u0026lt;all\u0026gt; consul-server-2 172.17.0.8:8301 alive server 1.4.4 2 dc1 \u0026lt;all\u0026gt; minikube 172.17.0.5:8301 alive client 1.4.4 2 dc1 \u0026lt;default\u0026gt; Node Address Status Type Build Protocol DC Segment consul-server-0 172.17.0.6:8301 alive server 1.4.4 2 dc1 \u0026lt;all\u0026gt; consul-server-1 172.17.0.7:8301 alive server 1.4.4 2 dc1 \u0026lt;all\u0026gt; consul-server-2 172.17.0.8:8301 alive server 1.4.4 2 dc1 \u0026lt;all\u0026gt; minikube 172.17.0.5:8301 alive client 1.4.4 2 dc1 \u0026lt;default\u0026gt; Node Address Status Type Build Protocol DC Segment consul-server-0 172.17.0.6:8301 alive server 1.4.4 2 dc1 \u0026lt;all\u0026gt; consul-server-1 172.17.0.7:8301 alive server 1.4.4 2 dc1 \u0026lt;all\u0026gt; consul-server-2 172.17.0.8:8301 alive server 1.4.4 2 dc1 \u0026lt;all\u0026gt; minikube 172.17.0.5:8301 alive client 1.4.4 2 dc1 \u0026lt;default\u0026gt; 次に Consul のリーダーがどの Pod なのかを確認してみます。下記の結果から consule-server-2 という Pod がリーダーだと分かります。\n$ for i in {0..2}; do kubectl exec consul-server-$i -- sh -c \u0026#39;consul info | grep leader\u0026#39;; done leader = false leader_addr = 172.17.0.8:8300 leader = false leader_addr = 172.17.0.8:8300 leader = true leader_addr = 172.17.0.8:8300 この状態で consule-server-2 Pod を削除してみます。削除しても Kubernetes Pod なので即座に再作成・起動がされるのですが、Consul 的には「リーダーが落ちた」という判断が下り、リーダー選出のための選挙が consule-server-0, consul-server-1 の 2 Pods で行われます。\n$ kubectl delete pod consul-server-2 下記が consul-server-0 で確認した Consul プロセスのログになります。\u0026ldquo;consul: New leader elected: consul-server-1\u0026rdquo; と表示され新たに consul-server-1 Pod がリーダーに選出されたことが確認出来ます。\n$ kubectl logs consul-server-0 \u0026lt;snip\u0026gt; 2019/04/26 07:31:22 [INFO] serf: EventMemberLeave: consul-server-2.dc1 172.17.0.8 2019/04/26 07:31:22 [INFO] consul: Handled member-leave event for server \u0026#34;consul-server-2.dc1\u0026#34; in area \u0026#34;wan\u0026#34; 2019/04/26 07:31:26 [INFO] serf: EventMemberLeave: consul-server-2 172.17.0.8 2019/04/26 07:31:26 [INFO] consul: Removing LAN server consul-server-2 (Addr: tcp/172.17.0.8:8300) (DC: dc1) 2019/04/26 07:31:29 [WARN] raft: Rejecting vote request from 172.17.0.7:8300 since we have a leader: 172.17.0.8:8300 2019/04/26 07:31:33 [WARN] raft: Heartbeat timeout from \u0026#34;172.17.0.8:8300\u0026#34; reached, starting election 2019/04/26 07:31:33 [INFO] raft: Node at 172.17.0.6:8300 [Candidate] entering Candidate state in term 3 2019/04/26 07:31:36 [ERR] agent: Coordinate update error: No cluster leader 2019/04/26 07:31:36 [INFO] serf: EventMemberJoin: consul-server-2 172.17.0.8 2019/04/26 07:31:36 [INFO] consul: Adding LAN server consul-server-2 (Addr: tcp/172.17.0.8:8300) (DC: dc1) 2019/04/26 07:31:37 [INFO] serf: EventMemberJoin: consul-server-2.dc1 172.17.0.8 2019/04/26 07:31:37 [INFO] consul: Handled member-join event for server \u0026#34;consul-server-2.dc1\u0026#34; in area \u0026#34;wan\u0026#34; 2019/04/26 07:31:37 [INFO] raft: Node at 172.17.0.6:8300 [Follower] entering Follower state (Leader: \u0026#34;\u0026#34;) 2019/04/26 07:31:37 [INFO] consul: New leader elected: consul-server-1 \u0026lt;snip\u0026gt; 先程と同様にリーダーの確認を下記の通り行います。\n$ for i in {0..2}; do kubectl exec consul-server-$i -- sh -c \u0026#39;consul info | grep leader\u0026#39;; done leader = false leader_addr = 172.17.0.7:8300 leader = true leader_addr = 172.17.0.7:8300 leader = false leader_addr = 172.17.0.7:8300 コンフィギュレーションの例 今回レポジトリ上にある value.yaml を一部修正しただけでしたが、yaml 内にある各設定値を変更することで構成を変更することが可能です。各設定値については下記のサイトに詳細が記されています。公式のドキュメントになります。\nhttps://www.consul.io/docs/platform/k8s/helm.html\n設定値の例をあげると\u0026hellip;\nserver - replicas replicas はレプリカ数。Consul クラスタの Pod 数になります。但し冒頭で述べたとおり Consul Helm Chat は各 Kubernetes ノードに対して 1 Pod の原則で起動しますので、その点認識しておく必要があります。\nserver - bootstrapExpect 何台の Consul クラスタ Pod が起動していればリーダー選出を行うか？の設定値です。\nserver - storageClass デフォルトはローカルディスクです。Ceph 等を選択することも可能です。\nclient - grpc agent が gRPC リスナを持つかどうかです。true に設定すると gRPC リスナが 8502 番ポートで起動します。\nまとめ Kubernetes 上で Helm を使って簡単に Consul クラスタをデプロイ出来る事が分かりました。また運用を考慮された設計になっていることも確認出来ました。ロードバランサを用いずとも、アプリケーション間、API 間で互いに正常に可動している先をディスカバリし接続し合えるという点で Consul はとても有用なので Kubernetes を用いたアーキテクチャにも適しているのではないでしょうか。\n","permalink":"https://jedipunkz.github.io/post/consul-helm-chart/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003e今回は Hashicorp の Consul クラスタを Kubernetes 上で稼働させる方法について調べてみました。\u003c/p\u003e\n\u003cp\u003eHashicorp Consul はサービスディスカバリが行えるソフトウェアで、私も以前居た職場で利用していました。アプリケーション間で互いに接続先を確認し合う事が出来ます。以前構築した Consul クラスタはインスタンス上に直に起動していたのですが最近だとどうやってデプロイするのか興味を持ち Kubernetes 上にデプロイする方法を調べた所 Helm を使って簡単にデプロイ出来る事が分かりました。\u003c/p\u003e\n\u003cp\u003eまた今回は minikube を使って複数のレプリカを起動するようにしていますが、Helm Chart を用いると Kubernetes のノード毎に Consul Pod が1つずつ起動するようになっていて、ノードの障害を考慮した可用性という点でも優れているなぁと感じました。また Kubernetes の Pod ですのでプロセスが落ちた際に即座に再起動が行われるという点でも優れています。勿論 Consul クラスタの Leader が落ちた場合には Leader Election (リーダ昇格のための選挙) が行われ、直ちに隣接した Kubernetes ノード上の Consul Pod がリーダーに昇格します。といった意味でも Kubernetes 上に Consul をデプロイするという考えは優れているのではないでしょうか。\u003c/p\u003e\n\u003ch3 id=\"requirements\"\u003eRequirements\u003c/h3\u003e\n\u003cp\u003e下記のソフトウェアが事前に必要です。この手順では予めこれらがインストールされていることとして記していきます。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eminikube\u003c/li\u003e\n\u003cli\u003ekubectl\u003c/li\u003e\n\u003cli\u003ehelm\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"consul-クラスタ起動までの手順\"\u003eConsul クラスタ起動までの手順\u003c/h3\u003e\n\u003cp\u003e早速ですが手順を記していきます。\u003c/p\u003e\n\u003cp\u003eHashicorp の Github にて Consul の Helm Chart が公開されています。\u003ccode\u003ehelm search\u003c/code\u003e しても出てきますが、今回は Github のものを用いました。\u003c/p\u003e","title":"Consul Helm Chart で Kubernetes 上に Consul をデプロイ"},{"content":"こんにちは。@jedipunkz です。\n前回の記事 「Istio, Helm を使って Getting Started 的なアプリをデプロイ」で kubernetes 上で istio をインストールし sidecar injection を有効化しサンプルアプリケーションを起動しました。その結果、sidecar 的に envoy コンテナが起動するところまで確認しました。今回はもう少し単純な pod を用いて \u0026lsquo;sidecar injection\u0026rsquo; の中身をもう少しだけ深掘りして見ていきたいと思います。\nRquirements 記事と同等の動きを確認するために下記のソフトウェアが必要になります。 それぞれのソフトウェアは事前にインストールされた前提で記事を記していきます。\nmacos or linux os kubectl istioctl minikube 参考 URL 下記の istio 公式ドキュメントを参考に動作確認しました。\nhttps://istio.io/docs/setup/kubernetes/additional-setup/sidecar-injection/ https://istio.io/docs/setup/kubernetes/additional-setup/sidecar-injection/ minikube で kubenetes をデプロイ 前回同様に minikube 上で動作を確認していきます。パラメータは適宜、自分の環境に読み替えてください。\nminikube start --memory=8192 --cpus=4 --kubernetes-version=v1.10.0 \\ --extra-config=controller-manager.cluster-signing-cert-file=\u0026#34;/var/lib/minikube/certs/ca.crt\u0026#34; \\ --extra-config=controller-manager.cluster-signing-key-file=\u0026#34;/var/lib/minikube/certs/ca.key\u0026#34; \\ --vm-driver=virtualbox istio を稼働させる 下記のコマンドを用いてカレントディレクトリに istio のサンプル yaml が入ったフォルダを展開します。\ncurl -L https://git.io/getLatestIstio | sh - 次に下記のコマンドで kubernetes 上に istio をインストールします。 istio コンテナ間の通信をプレインテキスト or TLS で行うよう istio-demo.yml を apply しています。\ncd istio-1.1.3/ kubectl apply -f install/kubernetes/helm/istio-init/files/crd-10.yaml kubectl apply -f install/kubernetes/helm/istio-init/files/crd-11.yaml kubectl apply -f install/kubernetes/helm/istio-init/files/crd-certmanager-10.yaml kubectl apply -f install/kubernetes/helm/istio-init/files/crd-certmanager-11.yaml kubectl apply -f install/kubernetes/istio-demo.yaml 稼働状況を確認します。まず Service の状態です。\nkubectl get svc -n istio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE grafana ClusterIP 10.98.111.63 \u0026lt;none\u0026gt; 3000/TCP 9s istio-citadel ClusterIP 10.97.197.128 \u0026lt;none\u0026gt; 8060/TCP,15014/TCP 8s istio-egressgateway ClusterIP 10.96.35.77 \u0026lt;none\u0026gt; 80/TCP,443/TCP,15443/TCP 9s istio-galley ClusterIP 10.100.143.114 \u0026lt;none\u0026gt; 443/TCP,15014/TCP,9901/TCP 9s istio-ingressgateway LoadBalancer 10.105.202.136 \u0026lt;pending\u0026gt; 15020:30773/TCP,80:31380/TCP,443:31390/TCP,31400:31400/TCP,15029:31398/TCP,15030:32184/TCP,15031:31724/TCP,15032:30064/TCP,15443:30160/TCP 9s istio-pilot ClusterIP 10.102.53.62 \u0026lt;none\u0026gt; 15010/TCP,15011/TCP,8080/TCP,15014/TCP 8s istio-policy ClusterIP 10.105.107.53 \u0026lt;none\u0026gt; 9091/TCP,15004/TCP,15014/TCP 8s istio-sidecar-injector ClusterIP 10.104.82.138 \u0026lt;none\u0026gt; 443/TCP 8s istio-telemetry ClusterIP 10.97.117.166 \u0026lt;none\u0026gt; 9091/TCP,15004/TCP,15014/TCP,42422/TCP 8s jaeger-agent ClusterIP None \u0026lt;none\u0026gt; 5775/UDP,6831/UDP,6832/UDP 7s jaeger-collector ClusterIP 10.107.35.224 \u0026lt;none\u0026gt; 14267/TCP,14268/TCP 7s jaeger-query ClusterIP 10.108.172.46 \u0026lt;none\u0026gt; 16686/TCP 7s kiali ClusterIP 10.107.129.129 \u0026lt;none\u0026gt; 20001/TCP 8s prometheus ClusterIP 10.109.114.141 \u0026lt;none\u0026gt; 9090/TCP 8s tracing ClusterIP 10.108.154.22 \u0026lt;none\u0026gt; 80/TCP 7s zipkin ClusterIP 10.96.151.43 \u0026lt;none\u0026gt; 9411/TCP 7s Pod の状態です。\nkubectl get pods -n istio-system NAME READY STATUS RESTARTS AGE grafana-688b8999cd-d4clx 1/1 Running 0 115m istio-citadel-5749f4b6dd-jd6qm 1/1 Running 0 115m istio-cleanup-secrets-1.1.3-mv8lq 0/1 Completed 0 115m istio-egressgateway-666b76dbf7-mjjx6 1/1 Running 0 115m istio-galley-d68bdc684-nwtzz 1/1 Running 0 115m istio-grafana-post-install-1.1.3-gkn4s 0/1 Completed 0 115m istio-ingressgateway-d67598f4-pwddm 1/1 Running 0 115m istio-pilot-865f6997cd-7jmq4 2/2 Running 0 115m istio-policy-56957d4666-vljk9 2/2 Running 5 115m istio-security-post-install-1.1.3-f894p 0/1 Completed 0 115m istio-sidecar-injector-5cf67ccc65-9p69k 1/1 Running 0 115m istio-telemetry-786796559d-dqwr2 2/2 Running 5 115m istio-tracing-5d8f57c8ff-xps9b 1/1 Running 0 115m kiali-95fcf457f-kfdhp 1/1 Running 0 115m prometheus-5554746896-ccs5x 1/1 Running 0 115m istio-injection な状態でサンプル pod コンテナ \u0026lsquo;sleep\u0026rsquo; を起動 ここで sleep コマンドが起動するだけの pod コンテナを istio-injection=enabled な状態でデプロイします。まず先程ダウンロードしたディレクトリ上の sleep.yaml を見てみましょう。\ncat sample/sleep/sleep.yaml apiVersion: v1 kind: ServiceAccount metadata: name: sleep --- apiVersion: v1 kind: Service metadata: name: sleep labels: app: sleep spec: ports: - port: 80 name: http selector: app: sleep --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: sleep spec: replicas: 1 template: metadata: labels: app: sleep spec: serviceAccountName: sleep containers: - name: sleep image: pstauffer/curl command: [\u0026#34;/bin/sleep\u0026#34;, \u0026#34;3650d\u0026#34;] imagePullPolicy: IfNotPresent --- この yaml ファイルを用いるのですが、istioctl コマンドのサブコマンド kube-inject を用いることでこの元となる yaml ファイルを istio-injection=enabled な状態の yaml ファイルに変換することが出来ます。よって kubectl コマンドで apply する手順は下記になります。\nkubectl apply -f \u0026lt;(istioctl kube-inject -f samples/sleep/sleep.yaml) デプロイされた状態を確認し sidecar コンテナを知る デプロイされた Deployment を見てみましょう。\nkubectl get deployments sleep -o yaml apiVersion: extensions/v1beta1 kind: Deployment metadata: annotations: deployment.kubernetes.io/revision: \u0026#34;1\u0026#34; \u0026lt;snip\u0026gt; spec: containers: - command: - /bin/sleep - 3650d image: pstauffer/curl imagePullPolicy: IfNotPresent name: sleep \u0026lt;snip\u0026gt; image: docker.io/istio/proxyv2:1.1.3 imagePullPolicy: IfNotPresent name: istio-proxy ports: - containerPort: 15090 \u0026lt;snip\u0026gt; image: docker.io/istio/proxy_init:1.1.3 imagePullPolicy: IfNotPresent name: istio-init \u0026lt;snip\u0026gt; pstauffer/curl イメージ内で \u0026lsquo;/bin/sleep\u0026rsquo; コマンドが起動しているコンテナと隣接して istio-proxy と istio-init というコンテナが起動していることが確認出来ると思います。それぞれの役割は下記のとおりです。\nistio-proxy : 他の istio からの通信をサービスコンテナに中継するためのコンテナ istio-init : istio-proxy コンテナを介す通信を iptables で制御するためのコンテナ これらが istio を用いることで起動した sidecar コンテナとなります。\n下記の通り pods を describe することでもこれらのコンテナが稼働していることが分かります。\nkubectl describe pod sleep-759d5cb4ff-btqvl \u0026lt;snip\u0026gt; Init Containers: istio-init: Container ID: docker://ded035851fa141997fdca47a0c317203cda650a0f9dce2f8d46ab264aa0e168b Image: docker.io/istio/proxy_init:1.1.3 Image ID: docker-pullable://istio/proxy_init@sha256:000d022d27c198faa6cc9b03d806482d08071e146423d6e9f81aa135499c4ed3 Port: \u0026lt;none\u0026gt; Host Port: \u0026lt;none\u0026gt; \u0026lt;snip\u0026gt; Containers: sleep: Container ID: docker://b6121c749a2eb394b81728063046eb0eb48ea1b48c464debe395e4b58768513c Image: pstauffer/curl Image ID: docker-pullable://pstauffer/curl@sha256:2663156457abb72d269eb19fe53c2d49e2e4a9fdcb9fa8f082d0282d82eb8e42 Port: \u0026lt;none\u0026gt; Host Port: \u0026lt;none\u0026gt; \u0026lt;snip\u0026gt; istio-proxy: Container ID: docker://0baee18b7d6ecec985b61fd10eff3409131245d390fad8f274d420f0807bc941 Image: docker.io/istio/proxyv2:1.1.3 Image ID: docker-pullable://istio/proxyv2@sha256:b682918f2f8fcca14b3a61bbd58f4118311eebc20799f24b72ceddc5cd749306 Port: 15090/TCP Host Port: 0/TCP sidecar のテンプレートとなる configmap を確認する 今回 istio-injection=enabled な状態で \u0026lsquo;sleep\u0026rsquo; コンテナをデプロイし本体のコンテナとは別に sidecar なコンテナ2つが稼働することが確認できました。次に説明するのが configmap です。どの様な状態でどの様なコンテナを sidecar 的に稼働させるかのルールを記したものが istio-sidecar-injector という configmap になります。その configmap の内容を確認してみましょう。\nkubectl -n istio-system get configmap istio-sidecar-injector -o=jsonpath=\u0026#39;{.data.config}\u0026#39; policy: enabled template: |- rewriteAppHTTPProbe: false initContainers: [[ if ne (annotation .ObjectMeta `sidecar.istio.io/interceptionMode` .ProxyConfig.InterceptionMode) \u0026#34;NONE\u0026#34; ]] - name: istio-init image: \u0026#34;docker.io/istio/proxy_init:1.1.3\u0026#34; args: - \u0026#34;-p\u0026#34; - [[ .MeshConfig.ProxyListenPort ]] - \u0026#34;-u\u0026#34; - 1337 - \u0026#34;-m\u0026#34; - [[ annotation .ObjectMeta `sidecar.istio.io/interceptionMode` .ProxyConfig.InterceptionMode ]] - \u0026#34;-i\u0026#34; - \u0026#34;[[ annotation .ObjectMeta `traffic.sidecar.istio.io/includeOutboundIPRanges` \u0026#34;*\u0026#34; ]]\u0026#34; - \u0026#34;-x\u0026#34; - \u0026#34;[[ annotation .ObjectMeta `traffic.sidecar.istio.io/excludeOutboundIPRanges` \u0026#34;\u0026#34; ]]\u0026#34; - \u0026#34;-b\u0026#34; - \u0026#34;[[ annotation .ObjectMeta `traffic.sidecar.istio.io/includeInboundPorts` (includeInboundPorts .Spec.Containers) ]]\u0026#34; - \u0026#34;-d\u0026#34; - \u0026#34;[[ excludeInboundPort (annotation .ObjectMeta `status.sidecar.istio.io/port` 15020 ) (annotation .ObjectMeta `traffic.sidecar.istio.io/excludeInboundPorts` \u0026#34;\u0026#34; ) ]]\u0026#34; [[ if (isset .ObjectMeta.Annotations `traffic.sidecar.istio.io/kubevirtInterfaces`) -]] - \u0026#34;-k\u0026#34; - \u0026#34;[[ index .ObjectMeta.Annotations `traffic.sidecar.istio.io/kubevirtInterfaces` ]]\u0026#34; [[ end -]] imagePullPolicy: IfNotPresent resources: requests: cpu: 10m memory: 10Mi limits: cpu: 100m memory: 50Mi securityContext: runAsUser: 0 runAsNonRoot: false capabilities: add: - NET_ADMIN restartPolicy: Always [[ end -]] containers: - name: istio-proxy image: [[ annotation .ObjectMeta `sidecar.istio.io/proxyImage` \u0026#34;docker.io/istio/proxyv2:1.1.3\u0026#34; ]] ports: - containerPort: 15090 protocol: TCP name: http-envoy-prom \u0026lt;snip\u0026gt; この configmap から下記のパラメータが記されていることが分かります。\nコンテナイメージ CPU 割当 メモリ割り当て 修正するために必要な権限 ポート指定 プロトコル etc.. pod の iptables を確認してトラヒックを理解する 次に minikube のホストにログインしサービス pod (今回は \u0026lsquo;sleep\u0026rsquo;) に割り当てられた iptables の内容を確認してみましょう。\nminikube ssh docker ps | grep sleep b6121c749a2e pstauffer/curl \u0026#34;/bin/sleep 3650d\u0026#34; 3 hours ago Up 3 hours k8s_sleep_sleep-759d5cb4ff-btqvl_default_6f3f6854-6651-11e9-970b-080027d5d6a7_0 この docker id `b6121c749a2e\u0026rsquo; と nsenter コマンドを用いて pod (sleep) に適用されている iptables の内容を確認します。\ndocker inspect b6121c749a2e --format \u0026#39;{{ .State.Pid }}\u0026#39; 20066 sudo nsenter -t 20066 -n iptables -t nat -S -P PREROUTING ACCEPT -P INPUT ACCEPT -P OUTPUT ACCEPT -P POSTROUTING ACCEPT -N ISTIO_IN_REDIRECT -N ISTIO_OUTPUT -N ISTIO_REDIRECT -A OUTPUT -p tcp -j ISTIO_OUTPUT -A ISTIO_IN_REDIRECT -p tcp -j REDIRECT --to-ports 15001 -A ISTIO_OUTPUT ! -d 127.0.0.1/32 -o lo -j ISTIO_REDIRECT -A ISTIO_OUTPUT -m owner --uid-owner 1337 -j RETURN -A ISTIO_OUTPUT -m owner --gid-owner 1337 -j RETURN -A ISTIO_OUTPUT -d 127.0.0.1/32 -j RETURN -A ISTIO_OUTPUT -j ISTIO_REDIRECT -A ISTIO_REDIRECT -p tcp -j REDIRECT --to-ports 15001 結果から、この pod は INBOUND 通信のリダイレクト先ポートとして 15001 番ポートが指定されているのが分かります。このポートは istio-proxy が待ち受けているポートになります。また OUTBOUND 通信に関しても同様に 15001 番ポートにリダイレクトされているのが分かります。よって \u0026lsquo;sleep\u0026rsquo; pod コンテナの全ての通信が istio-proxy を介すようになっています。また今回は単純に sleep するだけのコンテナを起動しましたが、http サーバ等を起動する pod を立ち上げた場合 -A ISTIO_INBOUND -p tcp -m tcp --dport 80 -j ISTIO_IN_REDIRECT といった制御も確認出来ると思います。\nまとめ kubernetes 上に istio をインストールすることで、テストで起動した pod コンテナに隣接する形で sidecar コンテナ istio-proxy, istio-init コンテナが起動することが確認出来ました。またそれらのコンテナを起動するテンプレートとなる configmap の内容を確認することができました。この configmap は修正することが可能な様です。そしてこの pod コンテナのインバウンド・アウトバウンドの通信は全て istio-proxy コンテナにリダイレクトされていることも分かりました。また今回は configmap の内容を確認するに留まりましたが、istio の機能としては routing, service discovery 等も有しているため、次回は routing あたりを調べようかと思っています。この routing を操作することで今回確認した iptables の内容も変わってくるのではないでしょうか。\n","permalink":"https://jedipunkz.github.io/post/istio-sidecar-injection/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003e前回の記事 \u003ca href=\"https://jedipunkz.github.io/blog/2018/12/31/istio/\"\u003e「Istio, Helm を使って Getting Started 的なアプリをデプロイ」\u003c/a\u003eで kubernetes 上で istio をインストールし sidecar injection を有効化しサンプルアプリケーションを起動しました。その結果、sidecar 的に envoy コンテナが起動するところまで確認しました。今回はもう少し単純な pod を用いて \u0026lsquo;sidecar injection\u0026rsquo; の中身をもう少しだけ深掘りして見ていきたいと思います。\u003c/p\u003e\n\u003ch3 id=\"rquirements\"\u003eRquirements\u003c/h3\u003e\n\u003cp\u003e記事と同等の動きを確認するために下記のソフトウェアが必要になります。\nそれぞれのソフトウェアは事前にインストールされた前提で記事を記していきます。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003emacos or linux os\u003c/li\u003e\n\u003cli\u003ekubectl\u003c/li\u003e\n\u003cli\u003eistioctl\u003c/li\u003e\n\u003cli\u003eminikube\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"参考-url\"\u003e参考 URL\u003c/h3\u003e\n\u003cp\u003e下記の istio 公式ドキュメントを参考に動作確認しました。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://istio.io/docs/setup/kubernetes/additional-setup/sidecar-injection/\"\u003ehttps://istio.io/docs/setup/kubernetes/additional-setup/sidecar-injection/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://istio.io/docs/setup/kubernetes/additional-setup/sidecar-injection/\"\u003ehttps://istio.io/docs/setup/kubernetes/additional-setup/sidecar-injection/\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"minikube-で-kubenetes-をデプロイ\"\u003eminikube で kubenetes をデプロイ\u003c/h3\u003e\n\u003cp\u003e前回同様に minikube 上で動作を確認していきます。パラメータは適宜、自分の環境に読み替えてください。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eminikube start --memory\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e8192\u003c/span\u003e --cpus\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e4\u003c/span\u003e --kubernetes-version\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003ev1.10.0 \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  --extra-config\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003econtroller-manager.cluster-signing-cert-file\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;/var/lib/minikube/certs/ca.crt\u0026#34;\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  --extra-config\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003econtroller-manager.cluster-signing-key-file\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;/var/lib/minikube/certs/ca.key\u0026#34;\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  --vm-driver\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003evirtualbox\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"istio-を稼働させる\"\u003eistio を稼働させる\u003c/h3\u003e\n\u003cp\u003e下記のコマンドを用いてカレントディレクトリに istio のサンプル yaml が入ったフォルダを展開します。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ecurl -L https://git.io/getLatestIstio | sh -\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e次に下記のコマンドで kubernetes 上に istio をインストールします。\nistio コンテナ間の通信をプレインテキスト or TLS で行うよう istio-demo.yml を apply しています。\u003c/p\u003e","title":"Istio Sidecar Injection を理解する"},{"content":"こんにちは。@jedipunkz です。\n最近は kubernetes を触ってなかったのですが Istio や Envoy 等 CNCF 関連のソフトウェアの記事をよく見かけるようになって、少し理解しておいたほうがいいかなと思い Istio と Minikube を使って Getting Started 的な事をやってみました。Istio をダウンロードすると中にサンプルアプリケーションが入っているのでそれを利用してアプリのデプロイまでを行ってみます。\nIstio をダウンロードするとお手軽に Istio 環境を作るための yaml ファイルがあり、それを kubectl apply することで Istio 環境を整えられるのですが、ドキュメントにプロダクション環境を想定した場合は Helm Template を用いた方がいいだろう、と記載あったので今回は Helm Template を用いて Istio 環境を作ります。\n前提の環境 下記の環境でテストを行いました。\nmacos Mojave minikube v0.32.0 kubectl v1.10.3 helm v2.12.1 virtualbox 準備 kubectl と helm のインストール kubctl と helm をインストールします。両者共に homebrew でインストールします。\nbrew install kubernetes-cli brew install kubernetes-helm minikube のインストールと起動 minikube をインストールして起動します。\ncurl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.32.0/minikube-darwin-amd64 \u0026amp;\u0026amp; chmod +x minikube \u0026amp;\u0026amp; sudo cp minikube /usr/local/bin/ \u0026amp;\u0026amp; rm minikube minikube start --memory 2048 Istio のダウンロードとインストール Istio のダウンロードとインストールを行います。後術しますがこのディレクトリの中に Istio 環境を構築するためのファイルやサンプルアプリケーションが入っています。\ncurl -L https://git.io/getLatestIstio | sh - cd istio-1.0.5 sudo cp bin/istioctl /usr/local/bin/istioctl 構築作業 Istio の Custom Resource Definitions をインストール Istio の Custom Resource Definitions (以下 CRDs) をインストールします。Kubernetes の CRDs は独自のカスタムリソースを定義し追加するものです。Kubernets API Server を介して作成することで作成したリソースの CRUD の API が Kubernetes API に追加されます。\n先程ダウンロードした Istio のディレクトリに crds.yaml があるのでそれを適用します。\nkubectl apply -f install/kubernetes/helm/istio/templates/crds.yaml Helm Template を使って Istio の Core Components をインストール Helm Template の仕組みをつかって Istio の Core Components をインストールします。まず Helm Template を出力し istio-system というネームスペースを作成、その後生成した Template を用いて kubectl コマンドで適用します。\nhelm template install/kubernetes/helm/istio --name istio --namespace istio-system \u0026gt; ./istio.yaml kubectl create namespace istio-system kubectl apply -f ./istio.yaml 状態の確認 この状態で service と pods の状態を確認してみます。\nkubectl get svc -n istio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istio-citadel ClusterIP 10.100.143.194 \u0026lt;none\u0026gt; 8060/TCP,9093/TCP 55s istio-egressgateway ClusterIP 10.108.243.97 \u0026lt;none\u0026gt; 80/TCP,443/TCP 55s istio-galley ClusterIP 10.98.99.11 \u0026lt;none\u0026gt; 443/TCP,9093/TCP 55s istio-ingressgateway LoadBalancer 10.97.17.220 \u0026lt;pending\u0026gt; 80:31380/TCP,443:31390/TCP,31400:31400/TCP,15011:30080/TCP,8060:31309/TCP,853:31151/TCP,15030:30455/TCP,15031:30836/TCP 55s istio-pilot ClusterIP 10.102.75.110 \u0026lt;none\u0026gt; 15010/TCP,15011/TCP,8080/TCP,9093/TCP 55s istio-policy ClusterIP 10.101.145.62 \u0026lt;none\u0026gt; 9091/TCP,15004/TCP,9093/TCP 55s istio-sidecar-injector ClusterIP 10.107.131.48 \u0026lt;none\u0026gt; 443/TCP 55s istio-telemetry ClusterIP 10.96.248.64 \u0026lt;none\u0026gt; 9091/TCP,15004/TCP,9093/TCP,42422/TCP 55s prometheus ClusterIP 10.98.228.190 \u0026lt;none\u0026gt; 9090/TCP 55s kubectl get pods -n istio-system NAME READY STATUS RESTARTS AGE istio-citadel-55cdfdd57c-64tnn 0/1 ContainerCreating 0 1m istio-cleanup-secrets-x6dmj 0/1 Completed 0 1m istio-egressgateway-7798845f5d-qfx2k 1/1 Running 0 1m istio-galley-76bbb946c8-5l626 0/1 ContainerCreating 0 1m istio-ingressgateway-78c6d8b8d7-68r2z 1/1 Running 0 1m istio-pilot-5fcb895bff-pmg8z 0/2 Pending 0 1m istio-policy-7b6cc95d7b-w7ndg 2/2 Running 0 1m istio-security-post-install-jcwg5 0/1 Completed 0 1m istio-sidecar-injector-9c6698858-8lt92 0/1 ContainerCreating 0 1m istio-telemetry-bfc9ff784-2mzzj 2/2 Running 0 1m prometheus-65d6f6b6c-nttwz 0/1 ContainerCreating 0 1m サンプルアプリケーションのデプロイ 先程ダウンロードした Istio のディレクトリにサンプルアプリケーション \u0026lsquo;bookinfo\u0026rsquo; がありますのでそれをデプロイしてみます。 デプロイ方法は2パターンあります。\n方法1. istioctl を使ってアプリの yaml に対して sidecar を記すよう変換しそれを kubctl apply します。\nkubectl apply -f \u0026lt;(istioctl kube-inject -f samples/bookinfo/platform/kube/bookinfo.yaml) 方法2. Sidecar Injection をデフォルトの動きとして設定し kubectl apply します\nkubectl label namespace default istio-injection=enabled kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yam 最後にアプリケーションの状態を確認します\nkubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE details ClusterIP 10.111.252.7 \u0026lt;none\u0026gt; 9080/TCP 17s kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 6m productpage ClusterIP 10.108.205.216 \u0026lt;none\u0026gt; 9080/TCP 10s ratings ClusterIP 10.102.161.24 \u0026lt;none\u0026gt; 9080/TCP 14s reviews ClusterIP 10.106.18.105 \u0026lt;none\u0026gt; 9080/TCP 12s kubectl get pods NAME READY STATUS RESTARTS AGE details-v1-5458f64c65-svrr7 0/2 PodInitializing 0 48s productpage-v1-577c9594b7-4f49s 0/2 Init:0/1 0 43s ratings-v1-79467df9b5-vrx4w 0/2 PodInitializing 0 47s reviews-v1-5d46b744bd-686s9 0/2 Init:0/1 0 46s reviews-v2-7f7d7f99f7-xsvm8 0/2 Init:0/1 0 46s reviews-v3-7bc67f66-cmt4x 0/2 Init:0/1 0 45s まとめ プロダクション環境を想定した Istio 構築と言っても Helm Template を用いて簡単に操作できることが分かりました。\nまたここで重要なのはサンプルアプリケーション \u0026lsquo;bookinfo\u0026rsquo; の Kubenetes Pods に Envoy プロキシを Sidecar 的に配置するための変換コマンドとして\nkubectl apply -f \u0026lt;(istioctl kube-inject -f samples/bookinfo/platform/kube/bookinfo.yaml) が用いられていることです。下記の操作を実行してみるとよくわかるでしょう。\nistioctl kube-inject -f samples/bookinfo/platform/kube/bookinfo.yaml \u0026gt; ./bookinfo.sidecar.yaml diff -u bookinfo.yaml bookinfo.sidecar.yaml 差分が長くなるためここでは省略しますが元の bookinfo.yaml には記されていなかった sidecar の文字が読み取れると思います。アプリに隣接した pods に Envoy が起動している pods が Sidecar 的に配置され下記のような構成になりました。\ningress \u0026#34;python\u0026#34; \u0026#34;java\u0026#34; \u0026#34;nodejs\u0026#34; +-----------+ +-------------+ +-------------+ +-------------+ | +-------+ | | +-------+ | | +-------+ | | +-------+ | request -\u0026gt; | | envoy | | -\u0026gt; | | envoy | | -\u0026gt; | | envoy | | -\u0026gt; | | envoy | | | +-------+ | | +-------+ | | +-------+ | | +-------+ | +-----------+ | productpage | | review-v1 | | ratings | +-------------+ +-------------+ +-------------+ +-------------+ | +-------+ | | | envoy | | | +-------+ | | review-v2 | +-------------+ +-------------+ | +-------+ | | | envoy | | | +-------+ | | review-v3 | +-------------+ +-------------+ | +-------+ | | | envoy | | | +-------+ | | details | +-------------+ ruby 参考 URL https://istio.io/docs/setup/kubernetes/quick-start/ https://istio.io/docs/examples/bookinfo/ https://istio.io/docs/setup/kubernetes/helm-install/ ","permalink":"https://jedipunkz.github.io/post/2018/12/31/istio/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003e最近は kubernetes を触ってなかったのですが Istio や Envoy 等 CNCF 関連のソフトウェアの記事をよく見かけるようになって、少し理解しておいたほうがいいかなと思い Istio と Minikube を使って Getting Started 的な事をやってみました。Istio をダウンロードすると中にサンプルアプリケーションが入っているのでそれを利用してアプリのデプロイまでを行ってみます。\u003c/p\u003e\n\u003cp\u003eIstio をダウンロードするとお手軽に Istio 環境を作るための yaml ファイルがあり、それを kubectl apply することで Istio 環境を整えられるのですが、ドキュメントにプロダクション環境を想定した場合は Helm Template を用いた方がいいだろう、と記載あったので今回は Helm Template を用いて Istio 環境を作ります。\u003c/p\u003e\n\u003ch2 id=\"前提の環境\"\u003e前提の環境\u003c/h2\u003e\n\u003cp\u003e下記の環境でテストを行いました。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003emacos Mojave\u003c/li\u003e\n\u003cli\u003eminikube v0.32.0\u003c/li\u003e\n\u003cli\u003ekubectl v1.10.3\u003c/li\u003e\n\u003cli\u003ehelm v2.12.1\u003c/li\u003e\n\u003cli\u003evirtualbox\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"準備\"\u003e準備\u003c/h2\u003e\n\u003ch3 id=\"kubectl-と-helm-のインストール\"\u003ekubectl と helm のインストール\u003c/h3\u003e\n\u003cp\u003ekubctl と helm をインストールします。両者共に homebrew でインストールします。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ebrew install kubernetes-cli\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ebrew install kubernetes-helm\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"minikube-のインストールと起動\"\u003eminikube のインストールと起動\u003c/h3\u003e\n\u003cp\u003eminikube をインストールして起動します。\u003c/p\u003e","title":"Istio, Helm を使って Getting Started 的なアプリをデプロイ"},{"content":"こんにちは。@jedipunkz です。\n以前、\u0026ldquo;Test-Kitchen, Docker で Ansible Role 開発サイクル高速化！\u0026rdquo; ってタイトルで Ansible Role の開発を test-kitchen を使って行う方法について記事にしたのですが、やっぱりローカルで Docker コンテナ立ち上げてデプロしてテストして.. ってすごく楽というか速くて今の現場でも便利につかっています。前の記事の URL は下記です。\nhttps://jedipunkz.github.io/blog/2016/07/14/test-kitchen-with-ansible/\n最近？は ansible container って技術もあるけど、僕らが Docker 使う目的はコンテナでデプロイするのではなくて単に Ansible を実行するローカル環境が欲しいってこととか、Serverspec をローカル・実機に実行する環境が欲しいってことなので、今でも test-kitchen 使っています。\nで、最近になって複数ノードの構成の Ansible Role を test-kitchen, Docker を使って開発できることに気がついたので記事にしようと思います。これができるとローカルで Redis Master + Redis Slave(s) + Sentinel って環境も容易にできると思います。\n使うソフトウェア 前提は macOS ですが Linux マシンでも問題なく動作するはずです。\nほぼ前回と同じです。\nAnsible Docker test-kitchen kitchen-docker (test-kitchen ドライバ) kitchen-ansible (test-kitchen ドライバ) Serverspec インストール ソフトウェアののインストール方法については前回の記事を見てもらうこととして割愛します。\ntest-kitchen の環境を作る test-kitchen の環境を作ります。\u0026lsquo;kitchen init\u0026rsquo; を実行して基本的には生成された .kitchen.yml を弄るんじゃなくて .kitchen.local.yml を修正していきます。こちらの記述が必ず上書きされて優先されます。\n.kitchen.local.yml の例を下記に記します。\n--- driver: name: docker binary: /usr/local/bin/docker socker: unix:///var/run/docker.sock use_sudo: false provisioner: name: ansible_playbook roles_path: ../../roles group_vars_path: ../../group_vars/local/ hosts: kitchen-deploy require_ansible_omnibus: false ansible_platform: centos require_chef_for_busser: true platforms: - name: centos driver_config: image: centos:7.2.1511 # (例) platform: centos require_chef_omnibus: false privileged: true # systemd 使うときの例 run_command: /sbin/init; sleep 3 # 同上 suites: - name: master provisioner: name: ansible_playbook playbook: ./site_master.yml driver_config: run_options: --net=kitchen --ip=172.18.0.11 - name: slave provisioner: name: ansible_playbook playbook: ./site_slave.yml driver_config: run_options: --net=kitchen --ip=172.18.0.12 各パラーメタの詳細については kitchen-ansibke, kitchen-docker のドキュメントを見ていただくとして\u0026hellip;\nhttps://github.com/neillturner/kitchen-ansible/blob/master/provisioner_options.md https://github.com/test-kitchen/kitchen-docker 特徴としては suites: の項目で mater, slave として Docker コンテナを2つ扱うことを宣言している部分です。\nname: master で site_master.yml という Playbook を実行することを宣言 name: master で 172.18.0.11 という IP アドレスを使うことを宣言 name: slave で site_slave.yml という Playbook を実行することを宣言 name: slave で 172.18.0.12 という IP アドレスを使うことを宣言 勿論、同様の記述で 3, 4.. 個目のコンテナ環境を作ることもできます。\nこんな感じです。ここで \u0026ldquo;固定 IP アドレス\u0026rdquo; を使うよう宣言したことはとても重要で、例えば NSD のクラスタ構成を作りたい時、お互いのコンテナ同士が相手のコンテナの IP アドレスを知り合う必要があります。\nNSD Master は Slave へ Xfer するため Slave コンテナの IP アドレスを知る必要あり NSD Slave は Master からのみ Xfer 受信する設定が必要なので Master IP を知る必要あり って感じです。RabbitMQ や Redis, Consul などのクラスタの際にも同様に互い・もしくは一方の IP アドレスを知る必要が出てくる場合があります。\nで、固定 IP アドレスなのですが Docker では動的 IP が基本なので、固定 IP アドレスを用いるために macOS + Docker 環境の中に一つネットワークを作る必要があります。下記を実行するだけで作成できます。\ndocker network create --subnet 172.18.0.0/24 kitchen Serverspec のディレクトリ構成 前回の記事でも書いたように、Ansible でコンテナに対してデプロイした結果に対して Servrspec テストが流せます。で今回は、クラスタ構成で複数のコンテナを扱うのでそれぞれのコンテナに対して流すテストが必要になります。下記のようなディレクトリ構成があればそれを実現できます。\n. ├── README.md ├── chefignore ├── site_master.yml ├── site_slave.yml └── test └── integration ├── master │ └── serverspec │ └── nsd_master_spec.rb └── slave └── serverspec └── nsd_slave_spec.rb そして 上記の nsd_master_spec,rb, nsd_slave_spec.rb の記述の仕方ですが下記のようになります。\nrequire \u0026#39;serverspec\u0026#39; # Required by serverspec set :backend, :exec describe package(\u0026#39;nsd\u0026#39;) do it { should be_installed } end describe process(\u0026#39;nsd\u0026#39;) do it { should be_running } end ...\u0026lt;省略\u0026gt;... コンテナデプロイ・テスト実行方法 ここまでで環境が整ったと思うので (実際には site_master.yml 等の Playbook もしくは Ansible Role の指定が必要) 、コンテナに対して Ansible デプロイする方法・Serverspec テスト実行する方法を書いていきます。\nkitchen create # 2 コンテナ作られます kitchen converge # 2 コンテナに対して Ansible デプロイされます kitchen verify # 2 コンテナに対して Serverspec テストされます kitchen test # destroy, create, converge, veriy, destroy が一斉実行されます 各コンテナ毎に操作する場合\nkitchen create master-centos # or slave-centos kitchen converge master-centos # or slave-centos kitchen verify master-centos # or slave-centos kitchen destroy master-centos # or slave-centos まとめ ローカルの Docker コンテナに対してデプロイテストできるのでとても手軽に、尚且つ高速に Ansible の Playbook や Role が開発出来ることは前回の記事でわかったと思うのですが、クラスタ構成についてもローカルで開発出来ることがわかりました。\nちなみに私達の環境だと .kitchen.local.yml に下記の記述をしているので\u0026hellip;\nroles_path: ../../roles site_master(slave).yml の中の記述としては下記のようにしています。\u0026rsquo;nsd\u0026rsquo; っていう Role を作る想定で Role 指定がしてあった、尚且つローカルでの Ansible Variable を上書き(一番優先される) することで、Role のローカルデプロイを実現しています。(variable 名は Role の作り方によります)\n--- - hosts: kitchen-deploy sudo: yes roles: - { role: nsd, tags: nsd } vars: nsd_type: \u0026#39;master\u0026#39; nsd_master_addr: \u0026#39;172.18.0.11\u0026#39; nsd_slave_addr: \u0026#39;172.18.0.12\u0026#39; test-kitchen 自体はリリースされてだいぶ時間が経過している分、技術的に枯れてきていて未だに私達は Ansible の開発に役立てています。元々は Chef 向けの技術ではあるのだけどそこは今回紹介した .kitchen.local.yml の書き方で回避できるし、Serverspec のテストも流せるし..。\nまたローカル(ホストの macOS) へのポートマッピング等もコンテナ毎に記せますのでその辺りはドキュメントを読んでみてください。\n最終的にはここで書いた Serverspec テストを実機へ流せれば文句なしだと思います。方法はあると思います。また気がついたら紹介しようと思います。\n","permalink":"https://jedipunkz.github.io/post/test-kitchen-cluster/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003e以前、\u0026ldquo;Test-Kitchen, Docker で Ansible Role 開発サイクル高速化！\u0026rdquo; ってタイトルで Ansible Role の開発を test-kitchen を使って行う方法について記事にしたのですが、やっぱりローカルで Docker コンテナ立ち上げてデプロしてテストして.. ってすごく楽というか速くて今の現場でも便利につかっています。前の記事の URL は下記です。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://jedipunkz.github.io/blog/2016/07/14/test-kitchen-with-ansible/\"\u003ehttps://jedipunkz.github.io/blog/2016/07/14/test-kitchen-with-ansible/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e最近？は ansible container って技術もあるけど、僕らが Docker 使う目的はコンテナでデプロイするのではなくて単に Ansible を実行するローカル環境が欲しいってこととか、Serverspec をローカル・実機に実行する環境が欲しいってことなので、今でも test-kitchen 使っています。\u003c/p\u003e\n\u003cp\u003eで、最近になって複数ノードの構成の Ansible Role を test-kitchen, Docker を使って開発できることに気がついたので記事にしようと思います。これができるとローカルで Redis Master + Redis Slave(s) + Sentinel って環境も容易にできると思います。\u003c/p\u003e\n\u003ch2 id=\"使うソフトウェア\"\u003e使うソフトウェア\u003c/h2\u003e\n\u003cp\u003e前提は macOS ですが Linux マシンでも問題なく動作するはずです。\u003c/p\u003e\n\u003cp\u003eほぼ前回と同じです。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAnsible\u003c/li\u003e\n\u003cli\u003eDocker\u003c/li\u003e\n\u003cli\u003etest-kitchen\u003c/li\u003e\n\u003cli\u003ekitchen-docker (test-kitchen ドライバ)\u003c/li\u003e\n\u003cli\u003ekitchen-ansible (test-kitchen ドライバ)\u003c/li\u003e\n\u003cli\u003eServerspec\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"インストール\"\u003eインストール\u003c/h2\u003e\n\u003cp\u003eソフトウェアののインストール方法については前回の記事を見てもらうこととして割愛します。\u003c/p\u003e\n\u003ch2 id=\"test-kitchen-の環境を作る\"\u003etest-kitchen の環境を作る\u003c/h2\u003e\n\u003cp\u003etest-kitchen の環境を作ります。\u0026lsquo;kitchen init\u0026rsquo; を実行して基本的には生成された .kitchen.yml を弄るんじゃなくて .kitchen.local.yml を修正していきます。こちらの記述が必ず上書きされて優先されます。\u003c/p\u003e","title":"Docker,Test-Kitchen,Ansible でクラスタを構成する"},{"content":"こんにちは。@jedipunkz です。\n少し前まで Google Cloud Platform (GCP) を使っていたのですが、今回はその時に得たノウハウを記事にしようと思います。\nGoogle Container Engine (GKE) とロードバランサを組み合わせてサービスを立ち上げていました。手動でブラウザ上からポチポチして構築すると人的ミスや情報共有という観点でマズイと思ったので Terraform を使って GCP の各リソースを構築するよう仕掛けたのですが、まだまだ Terraform を使って GCP インフラを構築されている方が少ないため情報が無く情報収集や検証に時間が掛かりました。よって今回はまだネット上に情報が少ない GKE とロードバランサの構築を Terraform を使って行う方法を共有します。\n構築のシナリオ 構築するにあたって2パターンの構築の流れがあると思います。\nGKE クラスタとロードバランサを同時に構築する GKE クラスタを予め構築しそのクラスタ向けにロードバランサを構築する 両方の方法を記していきます。\nGKE クラスタとロードバランサを同時に構築する GKE クラスタとロードバランサを同時に作る方法です。\n早速ですが下記に terraform ファイルを記しました。それぞれのシンタックスの意味については Terraform の公式ドキュメントをご覧になってください。\n公式ドキュメント : https://www.terraform.io/docs/providers/google/\nここで特筆すべき点としては下記が挙げられます。\nロードバランサに紐付けるバックエンドの ID 取得のため \u0026ldquo;${replace(element\u0026hellip;\u0026rdquo; 的なことをしている ロードバランサのバックエンドサービスに対して GKE クラスタを作成した際に自動で作成されるインスタンスグループの URI を紐付ける必要があります。ユーザとしては URI ではなく \u0026ldquo;インスタンスグループ名\u0026rdquo; であると扱いやすいのですが、URI が必要になります。この情報は下記のサイトを参考にさせていただきました。\n参考サイト : GKEでkubernetesのnodesをロードバランサーのバックエンドとして使いたいとき with terraform URL : http://qiita.com/techeten/items/b2ec5f11f4a70dd21d70 ロードバランサ一つ作るために 6 個ものインフラリソースを作っている 一つのロードバランサを作るために6つのインフラリソースが必要になるというのも驚きですが公式ドキュメントを読むとなかなかその感覚がつかめませんでした。それぞれの簡単な意味を下記に記しておきます。\ngoogle_compute_http_health_check : ヘルスチェック google_compute_backend_service : バックエンドサービス google_compute_url_map : ロードバランサ名となるリソース google_compute_target_http_proxy : プロキシ google_compute_global_address : グローバル IP アドレス google_compute_global_forwarding_rule : ポートマッピングによるフォワーディングルール それでは実際の Terraform コードです\n# 共通変数 variable \u0026#34;credentials\u0026#34; {default = \u0026#34;/path/to/credentials.json\u0026#34;} variable \u0026#34;project\u0026#34; {default = \u0026#34;test01\u0026#34;} variable \u0026#34;region\u0026#34; {default = \u0026#34;asia-northeast\u0026#34;} variable \u0026#34;zone\u0026#34; {default = \u0026#34;asia-northeast1-b\u0026#34;} # GKE クラスタ用の変数 variable \u0026#34;gke_name\u0026#34; {default = \u0026#34;gke-terraform-test\u0026#34;} variable \u0026#34;machine_type\u0026#34; {default = \u0026#34;n1-standard-1\u0026#34;} variable \u0026#34;disk_size_gb\u0026#34; {default = \u0026#34;50\u0026#34;} variable \u0026#34;node_count\u0026#34; {default = \u0026#34;2\u0026#34;} variable \u0026#34;network\u0026#34; {default = \u0026#34;default\u0026#34;} variable \u0026#34;subnetwork\u0026#34; {default = \u0026#34;default\u0026#34;} variable \u0026#34;cluster_ipv4_cidr\u0026#34; {default = \u0026#34;10.0.10.0/14\u0026#34;} variable \u0026#34;username\u0026#34; {default = \u0026#34;username\u0026#34;} variable \u0026#34;password\u0026#34; {default = \u0026#34;password\u0026#34;} # ロードバランサ用の変数 variable \u0026#34;lb_name\u0026#34; {default = \u0026#34;lb-terraform-test\u0026#34;} variable \u0026#34;healthcheck_name\u0026#34; {default = \u0026#34;healthcheck-terraform-test\u0026#34;} variable \u0026#34;healthcheck_host\u0026#34; {default = \u0026#34;test.example.com\u0026#34;} variable \u0026#34;healthcheck_port\u0026#34; {default = \u0026#34;30300\u0026#34;} variable \u0026#34;backend_name\u0026#34; {default = \u0026#34;backend-terraform-test\u0026#34;} variable \u0026#34;http_proxy_name\u0026#34; {default = \u0026#34;terraform-proxy\u0026#34;} variable \u0026#34;global_address_name\u0026#34; {default = \u0026#34;terraform-global-address\u0026#34;} variable \u0026#34;global_forwarding_rule_name\u0026#34; {default = \u0026#34;terraform-global-forwarding-rule\u0026#34;} variable \u0026#34;global_forwarding_rule_port\u0026#34; {default =\u0026#34;80\u0026#34;} variable \u0026#34;port_name\u0026#34; {default = \u0026#34;port-test\u0026#34;} variable \u0026#34;enable_cdn\u0026#34; {default = false} provider \u0026#34;google\u0026#34; { credentials = \u0026#34;${file(\u0026#34;${var.credentials}\u0026#34;)}\u0026#34; project = \u0026#34;${var.project}\u0026#34; region = \u0026#34;${var.region}\u0026#34; } resource \u0026#34;google_container_cluster\u0026#34; \u0026#34;cluster-terraform\u0026#34; { name = \u0026#34;${var.gke_name}\u0026#34; zone = \u0026#34;${var.zone}\u0026#34; initial_node_count = \u0026#34;${var.node_count}\u0026#34; network = \u0026#34;${var.network}\u0026#34; subnetwork = \u0026#34;${var.subnetwork}\u0026#34; cluster_ipv4_cidr = \u0026#34;${var.cluster_ipv4_cidr}\u0026#34; master_auth { username = \u0026#34;${var.username}\u0026#34; password = \u0026#34;${var.password}\u0026#34; } node_config { machine_type = \u0026#34;${var.machine_type}\u0026#34; disk_size_gb = \u0026#34;${var.disk_size_gb}\u0026#34; oauth_scopes = [ \u0026#34;https://www.googleapis.com/auth/compute\u0026#34;, \u0026#34;https://www.googleapis.com/auth/devstorage.read_only\u0026#34;, \u0026#34;https://www.googleapis.com/auth/logging.write\u0026#34;, \u0026#34;https://www.googleapis.com/auth/monitoring\u0026#34; ] } addons_config { http_load_balancing { disabled = true } horizontal_pod_autoscaling { disabled = true } } } resource \u0026#34;google_compute_http_health_check\u0026#34; \u0026#34;healthcheck-terraform\u0026#34; { name = \u0026#34;${var.healthcheck_name}\u0026#34; project = \u0026#34;${var.project}\u0026#34; request_path = \u0026#34;/\u0026#34; host = \u0026#34;${var.healthcheck_host}\u0026#34; check_interval_sec = 5 timeout_sec = 5 port = \u0026#34;${var.healthcheck_port}\u0026#34; } resource \u0026#34;google_compute_backend_service\u0026#34; \u0026#34;backend-terraform\u0026#34; { name = \u0026#34;${var.backend_name}\u0026#34; port_name = \u0026#34;${var.port_name}\u0026#34; protocol = \u0026#34;HTTP\u0026#34; timeout_sec = 10 enable_cdn = \u0026#34;${var.enable_cdn}\u0026#34; region = \u0026#34;${var.region}\u0026#34; project = \u0026#34;${var.project}\u0026#34; backend { group = \u0026#34;${replace(element(google_container_cluster.cluster-terraform.instance_group_urls, 1), \u0026#34;Manager\u0026#34;,\u0026#34;\u0026#34;)}\u0026#34; } health_checks = [\u0026#34;${google_compute_http_health_check.healthcheck-terraform.self_link}\u0026#34;] } resource \u0026#34;google_compute_url_map\u0026#34; \u0026#34;lb-terraform\u0026#34; { name = \u0026#34;${var.lb_name}\u0026#34; default_service = \u0026#34;${google_compute_backend_service.backend-terraform.self_link}\u0026#34; project = \u0026#34;${var.project}\u0026#34; } resource \u0026#34;google_compute_target_http_proxy\u0026#34; \u0026#34;http_proxy-terraform\u0026#34; { name = \u0026#34;${var.http_proxy_name}\u0026#34; url_map = \u0026#34;${google_compute_url_map.lb-terraform.self_link}\u0026#34; } resource \u0026#34;google_compute_global_address\u0026#34; \u0026#34;ip-terraform\u0026#34; { name = \u0026#34;${var.global_address_name}\u0026#34; project = \u0026#34;${var.project}\u0026#34; } resource \u0026#34;google_compute_global_forwarding_rule\u0026#34; \u0026#34;forwarding_rule-terraform\u0026#34; { name = \u0026#34;${var.global_forwarding_rule_name}\u0026#34; target = \u0026#34;${google_compute_target_http_proxy.http_proxy-terraform.self_link}\u0026#34; ip_address = \u0026#34;${google_compute_global_address.ip-terraform.address}\u0026#34; port_range = \u0026#34;${var.global_forwarding_rule_port}\u0026#34; project = \u0026#34;${var.project}\u0026#34; } GKE クラスタを予め構築しそのクラスタ向けにロードバランサを構築する 次に GKE クラスタとロードバランサを別々のタイミングに構築する方法です。 実際には GKE クラスタの上には多数のコンテナが起動されるのですでにクラスタが存在する状態でロードバランサを別サービスのために作成したいというケースが一般的なように思います。\nGKE クラスタのインスタンスグループ URI を取得し設定 既存 GKE クラスタを用いる場合、インスタンスグループ URI がいずれにせよ必要になります。 インスタンスグループ URI を取得するには gcloud CLI を使って下記のように知ることができます。\n$ gcloud compute instance-groups managed describe | grep -rin URI ここで得たインスタンスグループ URI は下記のように variable \u0026ldquo;backend_group\u0026rdquo; に記します。\nPort マッピングを手動で設定 ここは残念なのですが 2017/04 現在、Port マッピングの設定を WebUI 上から行う必要があります。UI から GKE クラスタのインスタンスグループを選択し、Port マッピングの設定を行い名前を記します。ここでは \u0026ldquo;port-test\u0026rdquo; として作成したとし説明します。\nTerraform のコードを記述 ここでロードバランサ単独で構築する際の Terraform コードを見てみます。\nvariable \u0026#34;credentials\u0026#34; {default = \u0026#34;/path/to/credentials.json\u0026#34;} variable \u0026#34;project\u0026#34; {default = \u0026#34;test01\u0026#34;} variable \u0026#34;region\u0026#34; {default = \u0026#34;asia-northeast1\u0026#34;} variable \u0026#34;lb_name\u0026#34; {default = \u0026#34;lb-terraform-test\u0026#34;} variable \u0026#34;healthcheck_name\u0026#34; {default = \u0026#34;healthcheck-terraform-test\u0026#34;} variable \u0026#34;healthcheck_host\u0026#34; {default = \u0026#34;test.example.com\u0026#34;} variable \u0026#34;healthcheck_port\u0026#34; {default = \u0026#34;30300\u0026#34;} variable \u0026#34;backend_name\u0026#34; {default = \u0026#34;backend-terraform-test\u0026#34;} variable \u0026#34;backend_group\u0026#34; {default = \u0026#34;https://www.googleapis.com/compute/v1/projects/test01/zones/asia-northeast1-b/instanceGroups/gke-gke-terraform-test-default-pool-c001020e-grp\u0026#34;} variable \u0026#34;http_proxy_name\u0026#34; {default = \u0026#34;terraform-proxy\u0026#34;} variable \u0026#34;global_address_name\u0026#34; {default = \u0026#34;terraform-global-address\u0026#34;} variable \u0026#34;global_forwarding_rule_name\u0026#34; {default = \u0026#34;terraform-global-forwarding-rule\u0026#34;} variable \u0026#34;global_forwarding_rule_port\u0026#34; {default =\u0026#34;80\u0026#34;} variable \u0026#34;port_name\u0026#34; {default = \u0026#34;port-test\u0026#34;} variable \u0026#34;enable_cdn\u0026#34; {default = false} provider \u0026#34;google\u0026#34; { credentials = \u0026#34;${file(\u0026#34;${var.credentials}\u0026#34;)}\u0026#34; project = \u0026#34;${var.project}\u0026#34; region = \u0026#34;${var.region}\u0026#34; } resource \u0026#34;google_compute_http_health_check\u0026#34; \u0026#34;healthcheck-terraform-test\u0026#34; { name = \u0026#34;${var.healthcheck_name}\u0026#34; project = \u0026#34;${var.project}\u0026#34; request_path = \u0026#34;/\u0026#34; host = \u0026#34;${var.healthcheck_host}\u0026#34; check_interval_sec = 5 timeout_sec = 5 port = \u0026#34;${var.healthcheck_port}\u0026#34; } resource \u0026#34;google_compute_backend_service\u0026#34; \u0026#34;backend-terraform-test\u0026#34; { name = \u0026#34;${var.backend_name}\u0026#34; port_name = \u0026#34;${var.port_name}\u0026#34; protocol = \u0026#34;HTTP\u0026#34; timeout_sec = 10 enable_cdn = \u0026#34;${var.enable_cdn}\u0026#34; region = \u0026#34;${var.region}\u0026#34; project = \u0026#34;${var.project}\u0026#34; backend { group = \u0026#34;${var.backend_group}\u0026#34; } health_checks = [\u0026#34;${google_compute_http_health_check.healthcheck-terraform-test.self_link}\u0026#34;] } resource \u0026#34;google_compute_url_map\u0026#34; \u0026#34;lb-terraform-test\u0026#34; { name = \u0026#34;${var.lb_name}\u0026#34; default_service = \u0026#34;${google_compute_backend_service.backend-terraform-test.self_link}\u0026#34; project = \u0026#34;${var.project}\u0026#34; } resource \u0026#34;google_compute_target_http_proxy\u0026#34; \u0026#34;http_proxy-terraform-test\u0026#34; { name = \u0026#34;${var.http_proxy_name}\u0026#34; url_map = \u0026#34;${google_compute_url_map.lb-terraform-test.self_link}\u0026#34; } resource \u0026#34;google_compute_global_address\u0026#34; \u0026#34;ip-terraform-test\u0026#34; { name = \u0026#34;${var.global_address_name}\u0026#34; project = \u0026#34;${var.project}\u0026#34; } resource \u0026#34;google_compute_global_forwarding_rule\u0026#34; \u0026#34;forwarding_rule-terraform-test\u0026#34; { name = \u0026#34;${var.global_forwarding_rule_name}\u0026#34; target = \u0026#34;${google_compute_target_http_proxy.http_proxy-terraform-test.self_link}\u0026#34; ip_address = \u0026#34;${google_compute_global_address.ip-terraform-test.address}\u0026#34; port_range = \u0026#34;${var.global_forwarding_rule_port}\u0026#34; project = \u0026#34;${var.project}\u0026#34; まとめ GCP のロードバランサが特徴的な構築方法が必要になることが分かったと思います。将来的に URI で記す箇所を名前で記せれば構築がもっと簡単になると思いますので GCP 側の API バージョンアップを期待します。また今回は記しませんでしたが SSL (HTTPS) 証明書をロードバランサに紐付けることも Terraform を使えば容易に出来ます。試してみてください。一旦 Terraform 化すればパラメータを変更するだけで各ロードバランサが一発で構築できるので自動化は是非しておきたいところです。私は以前、この構築を Terraform + Hubot により ChatOps 化していました。作業の見える化と、メンバ間のコードレビューが可能になるからです。\n","permalink":"https://jedipunkz.github.io/post/gke-lb/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003e少し前まで Google Cloud Platform (GCP) を使っていたのですが、今回はその時に得たノウハウを記事にしようと思います。\u003c/p\u003e\n\u003cp\u003eGoogle Container Engine (GKE) とロードバランサを組み合わせてサービスを立ち上げていました。手動でブラウザ上からポチポチして構築すると人的ミスや情報共有という観点でマズイと思ったので Terraform を使って GCP の各リソースを構築するよう仕掛けたのですが、まだまだ Terraform を使って GCP インフラを構築されている方が少ないため情報が無く情報収集や検証に時間が掛かりました。よって今回はまだネット上に情報が少ない GKE とロードバランサの構築を Terraform を使って行う方法を共有します。\u003c/p\u003e\n\u003ch2 id=\"構築のシナリオ\"\u003e構築のシナリオ\u003c/h2\u003e\n\u003cp\u003e構築するにあたって2パターンの構築の流れがあると思います。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003col\u003e\n\u003cli\u003eGKE クラスタとロードバランサを同時に構築する\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003col start=\"2\"\u003e\n\u003cli\u003eGKE クラスタを予め構築しそのクラスタ向けにロードバランサを構築する\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e両方の方法を記していきます。\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eGKE クラスタとロードバランサを同時に構築する\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003cp\u003eGKE クラスタとロードバランサを同時に作る方法です。\u003c/p\u003e\n\u003cp\u003e早速ですが下記に terraform ファイルを記しました。それぞれのシンタックスの意味については Terraform の公式ドキュメントをご覧になってください。\u003c/p\u003e\n\u003cp\u003e公式ドキュメント : \u003ca href=\"https://www.terraform.io/docs/providers/google/\"\u003ehttps://www.terraform.io/docs/providers/google/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eここで特筆すべき点としては下記が挙げられます。\u003c/p\u003e\n\u003ch3 id=\"ロードバランサに紐付けるバックエンドの-id-取得のため-replaceelement-的なことをしている\"\u003eロードバランサに紐付けるバックエンドの ID 取得のため \u0026ldquo;${replace(element\u0026hellip;\u0026rdquo; 的なことをしている\u003c/h3\u003e\n\u003cp\u003eロードバランサのバックエンドサービスに対して GKE クラスタを作成した際に自動で作成されるインスタンスグループの URI を紐付ける必要があります。ユーザとしては URI ではなく \u0026ldquo;インスタンスグループ名\u0026rdquo; であると扱いやすいのですが、URI が必要になります。この情報は下記のサイトを参考にさせていただきました。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e参考サイト : GKEでkubernetesのnodesをロードバランサーのバックエンドとして使いたいとき with terraform\u003c/li\u003e\n\u003cli\u003eURL : \u003ca href=\"http://qiita.com/techeten/items/b2ec5f11f4a70dd21d70\"\u003ehttp://qiita.com/techeten/items/b2ec5f11f4a70dd21d70\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"ロードバランサ一つ作るために-6-個ものインフラリソースを作っている\"\u003eロードバランサ一つ作るために 6 個ものインフラリソースを作っている\u003c/h3\u003e\n\u003cp\u003e一つのロードバランサを作るために6つのインフラリソースが必要になるというのも驚きですが公式ドキュメントを読むとなかなかその感覚がつかめませんでした。それぞれの簡単な意味を下記に記しておきます。\u003c/p\u003e","title":"GCP ロードバランサと GKE クラスタを Terraform を使って構築する"},{"content":"こんにちは。 @jedipunkz です。\n今日は Kubernetes を使って Serverless を実現するソフトウェア Fission を紹介します。\nAWS の Lambda とよく似た動きをします。Lambda も内部では各言語に特化したコンテナが起動してユーザが開発した Lambda Function を実行してくれるのですが、Fission も各言語がインストールされた Docker コンテナを起動しユーザが開発したコードを実行し応答を返してくれます。\nそれでは早速なのですが、Fission を動かしてみましょう。\n動作させるための環境 macOS か Linux を前提として下記の環境を用意する必要があります。また Kubernetes 環境は minikube が手っ取り早いので用いますが、もちろん minikube 以外の kubernetes 環境でも動作します。\nmacOS or Linux minikube or kubernetes kubectl fission ソフトウェアのインストール方法 簡単にですが、ソフトウェアのインストール方法を書きます。\nOS 私は Linux で動作させましたが筆者の方は macOS を使っている方が多数だと思いますので、この手順では macOS を使った利用方法を書いていきます。\nminikube ここでは簡単な手順で kubernetes 環境を構築できる minikube をインストールします。\ncurl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.16.0/minikube-darwin-amd64 \u0026amp;\u0026amp; chmod +x minikube \u0026amp;\u0026amp; sudo mv minikube /usr/local/bin/fission kubectl 直接必要ではありませんが、kubectl があると minikube で構築した kubernetes 環境を操作できますのでインストールしておきます。\ncurl -Lo kubectl https://storage.googleapis.com/kubernetes-release/release/v1.5.2/bin/linux/amd64/kubectl \u0026amp;\u0026amp; chmod +x kubectl \u0026amp;\u0026amp; sudo mv kubectl /usr/local/bin/ Fission Fission のインストールです。\ncurl http://fission.io/linux/fission \u0026gt; fission \u0026amp;\u0026amp; chmod +x fission \u0026amp;\u0026amp; sudo mv fission /usr/local/bin/fission kubernetes の起動 ソフトウェアのインストールが完了したら minikube を使って kubernetes を起動します。\n$ minikube start $ minikube status minikubeVM: Running localkube: Running $ kubectl get nodes NAME STATUS AGE minikube Ready 1h Fission の起動と環境変数の設定 Fission を起動します。\n$ kubectl create -f http://fission.io/fission.yaml $ kubectl create -f http://fission.io/fission-nodeport.yaml 次に環境変数を設定します。\n$ export FISSION_URL=http://$(minikube ip):31313 $ export FISSION_ROUTER=$(minikube ip):31314 Fission を使って Python のコードを実行する 例として Python の Hello World を用意します。hello.py として保存します。\ndef main(): return \u0026#34;Hello, world!\\n\u0026#34; ではいよいよ、kubernetes と Fission を使って上記の Hello World を実行させます。\nまず Fission が用意してくれている Docker コンテナを扱うように \u0026rsquo;env\u0026rsquo; を作ります。\n$ fission env create --name python-env --image fission/python-env 次に Fission で Function を作ります。その際に上記の env と python コードを指定します。つまり、hello.py を fission/python-env という Docker コンテナで稼働する、という意味です。\n$ fission function create --name python-hello -env python-env --code ./hello.py 次に Router を作ります。クエリの Path に対して Fuction を関連付けることができます。\n$ fission route add --function python-hello --url /python-hello Function を実行する環境ができました。実際に curl を使ってアクセスしてみましょう。\n$ curl http://$FISSION_ROUTER/python-hello Hello, world! hello.py の実行結果が得られました。\nまとめと考察 結果から Fission は \u0026ldquo;各言語の実行環境として Docker コンテナを用いていて、ユーザが開発したコードをそのコンテナ上で起動し実行結果を得られる。また各コード毎に URL パスが指定することができ、それをルータとして関係性を持たせられる\u0026rdquo; ということが分かりました。AWS の Lambda とほぼ同じことが実現出来ていることが分かると思います。\nAWS には Lambda の実行結果を応答するための API Gateway があり、このマネージド HTTP サーバと併用することで API 環境を用意出来るのですが Fission の場合には HTTP サーバも込みで提供されていることも分かります。\nあとは、この Fission を提供している元が \u0026ldquo;Platform9\u0026rdquo; という企業なのですが、この企業は OpenStack や kubernetes を使ったホスティングサービスを提供しているようです。開発元が一企業ということと、完全な OSS 開発体制になっていない可能性があって、万が一この企業に何かあった場合にこの Fission を使い続けられるのか問題がしばらくはありそうです。Fission 同等のソフトウェアを kubernetes が取り込むという話題は\u0026hellip;あるのかなぁ？\nkubernetes の Job Scheduler が同等の機能を提供してくれるかもしれませんが、まだ Job Scheduler は利用するには枯れていない印象があります。Fission と Job Scheduler 、どちらがいち早く完成度を上げられるのでしょうか。\n","permalink":"https://jedipunkz.github.io/post/serverless-fission/","summary":"\u003cp\u003eこんにちは。 @jedipunkz です。\u003c/p\u003e\n\u003cp\u003e今日は Kubernetes を使って Serverless を実現するソフトウェア Fission を紹介します。\u003c/p\u003e\n\u003cp\u003eAWS の Lambda とよく似た動きをします。Lambda も内部では各言語に特化したコンテナが起動してユーザが開発した Lambda Function を実行してくれるのですが、Fission も各言語がインストールされた Docker コンテナを起動しユーザが開発したコードを実行し応答を返してくれます。\u003c/p\u003e\n\u003cp\u003eそれでは早速なのですが、Fission を動かしてみましょう。\u003c/p\u003e\n\u003ch2 id=\"動作させるための環境\"\u003e動作させるための環境\u003c/h2\u003e\n\u003cp\u003emacOS か Linux を前提として下記の環境を用意する必要があります。また Kubernetes 環境は minikube が手っ取り早いので用いますが、もちろん minikube 以外の kubernetes 環境でも動作します。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003emacOS or Linux\u003c/li\u003e\n\u003cli\u003eminikube or kubernetes\u003c/li\u003e\n\u003cli\u003ekubectl\u003c/li\u003e\n\u003cli\u003efission\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"ソフトウェアのインストール方法\"\u003eソフトウェアのインストール方法\u003c/h2\u003e\n\u003cp\u003e簡単にですが、ソフトウェアのインストール方法を書きます。\u003c/p\u003e\n\u003ch4 id=\"os\"\u003eOS\u003c/h4\u003e\n\u003cp\u003e私は Linux で動作させましたが筆者の方は macOS を使っている方が多数だと思いますので、この手順では macOS を使った利用方法を書いていきます。\u003c/p\u003e\n\u003ch4 id=\"minikube\"\u003eminikube\u003c/h4\u003e\n\u003cp\u003eここでは簡単な手順で kubernetes 環境を構築できる minikube をインストールします。\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003ecurl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.16.0/minikube-darwin-amd64 \u0026amp;\u0026amp; chmod +x minikube \u0026amp;\u0026amp; sudo mv minikube /usr/local/bin/fission\n\u003c/code\u003e\u003c/pre\u003e\u003ch4 id=\"kubectl\"\u003ekubectl\u003c/h4\u003e\n\u003cp\u003e直接必要ではありませんが、kubectl があると minikube で構築した kubernetes 環境を操作できますのでインストールしておきます。\u003c/p\u003e","title":"Serverless on Kubernetes : Fission を使ってみた"},{"content":"こんにちは。 @jedipunkz です。\nいま僕らは職場では GKE 上に Replication Controller と Services を使って Pod を起動しているのですが最近の Kubernetes 関連のドキュメントを拝見すると Deployments を使っている記事をよく見掛けます。Kubernetes 1.2 から実装されたようです。今回は Kubernetes の Replication Controller の次世代版と言われている Deployments について調べてみましたので理解したことを書いていこうかと思います。\n参考資料 今回は Kubernetes 公式の下記のドキュメントに記されているコマンドを一通り実行していきます。追加の情報もあります。\nhttps://kubernetes.io/docs/user-guide/deployments/ Deployments を使って nginx Pod を起動 nginx をデプロイするための Yaml ファイルを用意します。\napiVersion: extensions/v1beta1 kind: Deployment metadata: name: nginx-deployment spec: replicas: 2 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 作成した yaml ファイルを指定して Pod を作ります。 下記の通りここで \u0026ldquo;\u0026ndash;record\u0026rdquo; と記しているのは、後に Deployments の履歴を表示する際に \u0026ldquo;何を行ったか\u0026rdquo; を出力するためです。このオプションを指定しないと \u0026ldquo;何を行ったか\u0026rdquo; の出力が \u0026ldquo;NONE\u0026rdquo; となります。\n$ kubectl create -f nginx.yaml --record ここで\ndeployments replica set pod rollout の状態をそれぞれ確認してみます。\n$ kubectl get deployments NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-deployment 2 2 2 2 8s $ kubectl get rs NAME DESIRED CURRENT READY AGE nginx-deployment-4087004473 2 2 2 10s $ kubectl get pods --show-labels NAME READY STATUS RESTARTS AGE LABELS nginx-deployment-4087004473-6csa7 1/1 Running 0 21s app=nginx,pod-template-hash=4087004473 nginx-deployment-4087004473-teyzc 1/1 Running 0 21s app=nginx,pod-template-hash=4087004473 $ kubectl rollout status deployment/nginx-deployment deployment nginx-deployment successfully rolled out 結果から、下記の事が分かります。\nyaml に記した通り \u0026ldquo;nginx-deployment\u0026rdquo; という名前で deployment が生成された \u0026ldquo;nginx-deployment-4087004473\u0026rdquo; という名前の rs (レプリカセット) が生成された yaml に記した通り2つの Pod が起動した \u0026ldquo;nginx-deployment\u0026rdquo; が正常に Rollout された Replication Controller でデプロイした際には作られない replica set, rollout というモノが出てきました。後に Deployments を使うメリットに繋がっていきます。\nnginx イメージの Tag を更新してみる ここで yaml ファイル内で指定していた \u0026ldquo;image: nginx:1.7.9\u0026rdquo; を \u0026ldquo;image: nginx:1.9.1\u0026rdquo; と更新してみます。 Replication Controller で言う Rolling-Update になります。後に述べますが他にも更新方法があります。\n$ kubectl set image deployment/nginx-deployment nginx=nginx:1.9.1 ここで先ほどと同様に状態を確認してみます。\n$ kubectl get deployments NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-deployment 2 2 2 2 2m $ kubectl get rs NAME DESIRED CURRENT READY AGE nginx-deployment-3599678771 2 2 2 39s nginx-deployment-4087004473 0 0 0 2m $ kubectl get pods NAME READY STATUS RESTARTS AGE nginx-deployment-3599678771-0vj9m 1/1 Running 0 53s nginx-deployment-3599678771-t1y62 1/1 Running 0 53s ここで\u0026hellip;\n新しい Replica Set \u0026ldquo;nginx-deployment-3599678771\u0026rdquo; が作成された 古い Replica Set \u0026ldquo;nginx-deployment-4087004473\u0026rdquo; の Pod は 0 個になった Pod 内コンテナが更新された (NAME より判断) となったことが分かります。 Replication Controller と異なり、Deployments では以前の状態が Replica Set として保存されていて状態の履歴が追えるようになっています。\nここで Rollout の履歴を確認してみます。\n$ kubectl rollout history deployment/nginx-deployment deployments \u0026#34;nginx-deployment\u0026#34; REVISION CHANGE-CAUSE 1 kubectl create -f nginx.yaml --record 2 kubectl set image deployment/nginx-deployment nginx=nginx:1.9.1 REVISION という名前で履歴番号が付き、どの様な作業を行ったか CHANGE-CAUSE という項目で記されていることがわかります。作業の履歴がリビジョン管理されています。\n下記のように REVSION 番号を付与して履歴内容を表示することも可能です。\n$ kubectl rollout history deployment/nginx-deployment --revision=2 deployments \u0026#34;nginx-deployment\u0026#34; with revision #2 Labels: app=nginx pod-template-hash=3599678771 Annotations: kubernetes.io/change-cause=kubectl set image deployment/nginx-deployment nginx=nginx:1.9.1 Containers: nginx: Image: nginx:1.9.1 Port: 80/TCP Volume Mounts: \u0026lt;none\u0026gt; Environment Variables: \u0026lt;none\u0026gt; No volumes. 作業を切り戻してみる 先程 nginx の Image Tag を更新しましたが、ここで Deployments の機能を使って作業を切り戻してみます。下記の様に実行します。\n$ kubectl rollout undo deployment/nginx-deployment 状態を確認します。\n$ kubectl get deployments NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-deployment 2 2 2 2 5m $ kubectl rollout history deployment/nginx-deployment deployments \u0026#34;nginx-deployment\u0026#34; REVISION CHANGE-CAUSE 2 kubectl set image deployment/nginx-deployment nginx=nginx:1.9.1 3 kubectl create -f nginx.yaml --record ここでは\u0026hellip;\nコンテナが2個、正常に起動した REVISION 番号 3 として初期構築の状態 (kubectl create ..) が新たに保存 ということが分かります。注意したいのは REVSION 番号 1 が削除され 3 が生成されたことです。1 と 3 は同じ作業ということと推測します。\n念のため \u0026rsquo;nginx\u0026rsquo; コンテナの Image Tag が切り戻っているか確認してみます。\n$ kubectl describe pod nginx-deployment-4087004473-nq35u | grep \u0026#34;Image:\u0026#34; Image: nginx:1.7.9 最初の Yaml ファイルに記した \u0026rsquo;nginx\u0026rsquo; イメージ Tag \u0026ldquo;1.7.9\u0026rdquo; となっていることが確認できました。set image \u0026hellip; でイメージ更新をした作業が正常に切り戻ったことになります。\nレプリカ数を 2-\u0026gt;3 へスケールしてみる 更に replicas の数値を 2 から 3 へスケールしてみます。\n$ kubectl scale deployment nginx-deployment --replicas 3 同様に状態を確認してみます。\nkubectl get pods NAME READY STATUS RESTARTS AGE nginx-deployment-4087004473-esj5l 1/1 Running 0 6s nginx-deployment-4087004473-nq35u 1/1 Running 0 4m nginx-deployment-4087004473-tyibo 1/1 Running 0 4m kubectl get rs NAME DESIRED CURRENT READY AGE nginx-deployment-3599678771 0 0 0 9m nginx-deployment-4087004473 3 3 3 11m kubectl rollout history deployment/nginx-deployment deployments \u0026#34;nginx-deployment\u0026#34; REVISION CHANGE-CAUSE 2 kubectl set image deployment/nginx-deployment nginx=nginx:1.9.1 3 kubectl scale deployment nginx-deployment --replicas 3 ここで気になるのは REVISION 3 が上書きされたことです。REVSION 番号 4 が新たに作成されると思っていたからです。先程 REVISION 番号 3 として保存されていた下記の履歴が消えてしまいました。この点については引き続き検証してみます。今の自分には理解できませんでした。ご存知の方いましたら、コメントお願いします！\n3 kubectl create -f nginx.yaml --record Puase, Resume 機能を使ってみる 次は deployments の機能を使って Image Tag を更に 1.9.1 へ変更し、その処理をポーズしてみます。\nkubectl set image deployment/nginx-deployment nginx=nginx:1.9.1; kubectl 同様に状態を確認してみます。\n$ kubectl get rs NAME DESIRED CURRENT READY AGE nginx-deployment-3599678771 2 2 2 10m nginx-deployment-4087004473 2 2 2 12m $ kubectl rollout status deployment/nginx-deployment Waiting for rollout to finish: 2 out of 3 new replicas have been updated... Ctrl-C #\u0026lt;--- キー入力 rollout status で deployment \u0026ldquo;deployment/nginx-deployment\u0026rdquo; を確認すると \u0026ldquo;waiting for rollout to finish\u0026rdquo; と表示され処理がポーズされていることが確認できました。また古い Deployment \u0026ldquo;nginx-deployment-4087004473\u0026rdquo; 上に未だコンテナが残り、新しい Deployment もコンテナが生成中であることが分かります。\nでは Resume します。\n$ kubectl rollout resume deployment/nginx-deployment deployment \u0026#34;nginx-deployment\u0026#34; resumed この時点の状態を確認しましょう。\n$ kubectl rollout status deployment/nginx-deployment deployment nginx-deployment successfully rolled out $ kubectl get rs NAME DESIRED CURRENT READY AGE nginx-deployment-3599678771 3 3 3 11m nginx-deployment-4087004473 0 0 0 14m ここからは\u0026hellip;\n正常に Deployment \u0026ldquo;nginx-deployment\u0026rdquo; が Rollout されたこと 古い Deployment 上のコンテナ数が 0 に、新しい Deployment 上のコンテナ数が 3 になった ということが分かります。\nRolling-Update 相当の作業を行う方法 前述した通り、Replication Controller 時代にあった Rolling-Update 作業 (イメージタグ・レプリカ数等の更新) ですが、Deployments では下記の方法をとることが出来ます。\nset オプションを付与する場合 set オプションを付与して Key 項目に対して新しい Value を渡します。\n$ kubectl set image deployment/nginx-deployment nginx=nginx:1.9.1 yaml ファイルを修正する場合 yaml ファイルの内容を更新して適用したい場合、下記のように apply オプションを付与します。\n$ kubectl apply -f \u0026lt;新しいYamlファイル\u0026gt; まとめと考察 REVISION の履歴が上書きされる点など、まだ未完成な感が否めませんでしたが(自分の勘違いかもしれません！)、Replication Controller と比べると作業の切り戻しや、履歴が保存され履歴内容も確認できる点など機能が追加されていることが分かりました。公式ドキュメントを読んでいてもコマンド結果等怪しい点があって流石に API バージョンが \u0026ldquo;v1beta1\u0026rdquo; だなぁという感じではありますが、機能が整理されていて利便性が上がっているので Replication Controller を使っているユーザは自然と今後、Deployments に移行していくのではないかと感じました。\n","permalink":"https://jedipunkz.github.io/post/kubernetes-deployments/","summary":"\u003cp\u003eこんにちは。 @jedipunkz です。\u003c/p\u003e\n\u003cp\u003eいま僕らは職場では GKE 上に Replication Controller と Services を使って Pod を起動しているのですが最近の Kubernetes 関連のドキュメントを拝見すると Deployments を使っている記事をよく見掛けます。Kubernetes 1.2 から実装されたようです。今回は Kubernetes の Replication Controller の次世代版と言われている Deployments について調べてみましたので理解したことを書いていこうかと思います。\u003c/p\u003e\n\u003ch2 id=\"参考資料\"\u003e参考資料\u003c/h2\u003e\n\u003cp\u003e今回は Kubernetes 公式の下記のドキュメントに記されているコマンドを一通り実行していきます。追加の情報もあります。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://kubernetes.io/docs/user-guide/deployments/\"\u003ehttps://kubernetes.io/docs/user-guide/deployments/\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"deployments-を使って-nginx-pod-を起動\"\u003eDeployments を使って nginx Pod を起動\u003c/h2\u003e\n\u003cp\u003enginx をデプロイするための Yaml ファイルを用意します。\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eapiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.7.9\n        ports:\n        - containerPort: 80\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e作成した yaml ファイルを指定して Pod を作ります。\n下記の通りここで \u0026ldquo;\u0026ndash;record\u0026rdquo; と記しているのは、後に Deployments の履歴を表示する際に \u0026ldquo;何を行ったか\u0026rdquo; を出力するためです。このオプションを指定しないと \u0026ldquo;何を行ったか\u0026rdquo; の出力が \u0026ldquo;NONE\u0026rdquo; となります。\u003c/p\u003e","title":"Kubernetes Deployments を使ってみた！"},{"content":"こんにちは @jedipunkz です。\n今回は Kubernetes を使った構成で Google-Fluentd をどのコンテナに載せるか？ってことを考えてみたので書きたいと思います。\nKubernetes は Docker を利用したソフトウェアなので Docker と同じく \u0026ldquo;1コンテナ, 1プロセス\u0026rdquo; というポリシがあります。つまり、コンテナ上のプロセスが停止したら Kubernetes がそれを検知してコンテナを起動しなおしてくれます。ですが、複数プロセスを1コンテナに稼働させると、それが出来ません。そうは言っても中には複数のプロセスを稼働させたい場面があります。その場面として考えられる具体的な例として HTTPD サーバのログを Google-Fluentd を使って GCP Cloud Logging に転送したい場合があります。\n今回は上記の例を fluentd-sidecar-gcp と kubernetes volumes を使って解決する方法を記したいと思います。\n構成のシナリオ シナリオとしては下記のとおりです。\nマルチコンテナポッドを扱う 1つの Kubernetes Volumes を複数コンテナで共有する HTTPD ログをその Volume に出力 隣接する Google-Fluentd コンテナでその Volume に出力されたログを読み込みログ転送 fluentd-sidecar-gcp とは 次に説明するのは fluentd-sidecar-gcp の概略です。これは Kubernetes が contrib で扱っているコンテナです。下記の URL にあります。\nhttps://github.com/kubernetes/contrib/tree/master/logging/fluentd-sidecar-gcp\nGoogle-Fluentd を稼働させる Dockerfile が用意されているのですが、下記の記述を確認するとこのコンテナに環境変数 $FILES_TO_COLLECT を渡すと Google Fluentd でログを取得してくれることが分かります。\nhttps://github.com/kubernetes/contrib/blob/master/logging/fluentd-sidecar-gcp/config_generator.sh#L22-L37\nつまり、fluentd-sidecar-gcp コンテナに隣接する HTTPD コンテナのログが出力される Kubernetes Volumes 上のファイルパスを指定すれば HTTPD のログが取得でき、Google Cloud Logging へログが転送できます。\nサンプルの Kubernetes YAML 下記にサンプルとして Kubernetes YAML を記します。\napiVersion: v1 kind: Pod metadata: labels: run: my-nginx name: nginx-fluentd-logging-example spec: containers: - name: nginx-container image: nginx ports: - containerPort: 80 volumeMounts: - name: log-storage mountPath: /var/log/nginx - name: sidecar-log-collector image: gcr.io/google_containers/fluentd-sidecar-gcp:1.4 resources: limits: cpu: 100m memory: 200Mi env: - name: FILES_TO_COLLECT value: \u0026#34;/mnt/log/nginx/access.log /mnt/log/nginx/error.log\u0026#34; volumeMounts: - name: log-storage readOnly: true mountPath: /mnt/log/nginx volumes: - name: log-storage emptyDir: {} --- apiVersion: v1 kind: Service metadata: name: my-nginx labels: run: my-nginx spec: ports: - port: 80 protocol: TCP selector: run: my-nginx 特徴を下記に解説します。\nnginx-container, sidecar-log-collector のマルチコンテナポッドです。 sidecar-log-collector の image: としては gcr.io/google_containers/fluentd-sidecar-gcp:1.4 が指定されています \u0026rsquo;log-storage\u0026rsquo; として nginx-container の /var/log/nginx が sidecar-log-collector の /mnt/log/nginx として共有されています FILES_TO_COLLECT として共有 Volume 上の access.log, error.log が指定されています 結果、Nginx コンテナのログが Kubernetes Volume で Google-Fluentd コンテナに読み込み専用で共有され (readOnly 行) 、この Google-Fluentd は環境変数で渡された /mnt/log/nginx/access.log と /mnt/log/nginx/error.log を読み込み開始し、内容を Google Cloud Logging へ転送します。\nデプロイ方法 デプロイは下記の通り実施します。\n$ kubectl create -f \u0026lt;上記のファイル名\u0026gt; 結果とまとめ それぞれのログファイルを Tag を Google-Fluentd で付けた形で Google Cloud Logging へ転送出来ました。ログ毎に結果を Cloud Logging UI 上で確認できます。 本来、Docker なので標準出力にログを出力し Kubernetes がその標準出力を Cloud Logging へ転送してくれるのですが、それだと Tag が付けられないため、ログを分離するのが一苦労だと思います。ですが、今回紹介した方法では Google-Fluentd で Tag を付けてログ転送出来たため、その心配はありません。\nこの Kubernetes Volumes は他にも利用方法がありそうです。\n本来、GKE や Kubernetes を利用される方は Microservice Architecture が採用出来ている方々だと思うのですが、fluentd をアプリコンテナから分離するのは結構悩むところじゃないかと思うので、今回紹介した方法はそう言った場合に有用かと思います。\n","permalink":"https://jedipunkz.github.io/post/fluentd-sidecar-gcp/","summary":"\u003cp\u003eこんにちは @jedipunkz です。\u003c/p\u003e\n\u003cp\u003e今回は Kubernetes を使った構成で Google-Fluentd をどのコンテナに載せるか？ってことを考えてみたので書きたいと思います。\u003c/p\u003e\n\u003cp\u003eKubernetes は Docker を利用したソフトウェアなので Docker と同じく \u0026ldquo;1コンテナ, 1プロセス\u0026rdquo; というポリシがあります。つまり、コンテナ上のプロセスが停止したら Kubernetes がそれを検知してコンテナを起動しなおしてくれます。ですが、複数プロセスを1コンテナに稼働させると、それが出来ません。そうは言っても中には複数のプロセスを稼働させたい場面があります。その場面として考えられる具体的な例として HTTPD サーバのログを Google-Fluentd を使って GCP Cloud Logging に転送したい場合があります。\u003c/p\u003e\n\u003cp\u003e今回は上記の例を fluentd-sidecar-gcp と kubernetes volumes を使って解決する方法を記したいと思います。\u003c/p\u003e\n\u003ch2 id=\"構成のシナリオ\"\u003e構成のシナリオ\u003c/h2\u003e\n\u003cp\u003eシナリオとしては下記のとおりです。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eマルチコンテナポッドを扱う\u003c/li\u003e\n\u003cli\u003e1つの Kubernetes Volumes を複数コンテナで共有する\u003c/li\u003e\n\u003cli\u003eHTTPD ログをその Volume に出力\u003c/li\u003e\n\u003cli\u003e隣接する Google-Fluentd コンテナでその Volume に出力されたログを読み込みログ転送\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"fluentd-sidecar-gcp-とは\"\u003efluentd-sidecar-gcp とは\u003c/h2\u003e\n\u003cp\u003e次に説明するのは fluentd-sidecar-gcp の概略です。これは Kubernetes が contrib で扱っているコンテナです。下記の URL にあります。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/kubernetes/contrib/tree/master/logging/fluentd-sidecar-gcp\"\u003ehttps://github.com/kubernetes/contrib/tree/master/logging/fluentd-sidecar-gcp\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eGoogle-Fluentd を稼働させる Dockerfile が用意されているのですが、下記の記述を確認するとこのコンテナに環境変数 $FILES_TO_COLLECT を渡すと Google Fluentd でログを取得してくれることが分かります。\u003c/p\u003e","title":"fluentd-sidecar-gcp と Kubernetes Volumes で Cloud Logging ログ転送"},{"content":"こんにちは。@jedipunkz です。\n今回は Google Cloud Platform の Google CloudCDN について調べてみたので記したいと思います。\nCloudCDN は GCP のロードバランサのバックエンドサービスに紐付けられるサービスです。このバックエンドサービスで CloudCDN を有効にしていると CDN サービスを機能させることが出来ます。先に書いておくとこの CloudCDN はとてもシンプルで扱いやすいサービスだと判りました。高機能な他の CDN サービスと比べると機能が足らない感ありますが、必要最低限なところを抑えているのと、価格がとても安いです。(価格は下記の URL 参照)\n価格表 : https://cloud.google.com/cdn/pricing\n構成 構成と構成の特徴です。\n+----------+ +---------+ | instance |--+ +-| EndUser | +----------+ | +------------+ +----------+ | +---------+ +--|LoadBalancer|--| CloudCDN |-+-| EndUser | +----------+ | +------------+ +----------+ | +---------+ | instance |--+ +-| EndUser | +----------+ +---------+ コンテンツが初めてリクエストされた場合キャッシュミスする キャッシュミスした際に近くにあるキャッシュからコンテンツを取得しようと試みる 近くのキャッシュがコンテンツがある場合、最初のキャッシュにコンテンツが送信される 近くのキャッシュにコンテンツがない場合、HTTP ロードバランサにリクエストが転送される その後のリクエストはキャッシュが応答する(キャッシュヒット) キャッシュ間のフィルは EndUser のリクエストに応じて実行される キャッシュを事前に読み込むことできない キャッシュは世界各地に配置されている CloudCDN を導入する方法 導入する方法は簡単で下記のとおりです。\n新規 LB を WebUI で作成 バックエンドサービスを作成 右パネルの下にある [Cloud CDN を有効にする] チェックボックスをオンに 作成ボタンを押下 HTTPD サーバ側 (今回は Apache を使います) でキャッシュに関する設定を行います。 下記は例です。3600秒、キャッシュさせる設定になります。\nHeader set Cache-control \u0026#34;public, max-age=3600\u0026#34; キャッシュされる条件は RFC7324 に規定されているとおりです。\n特定のキャッシュを削除する方法 Google Cloud SDK https://cloud.google.com/sdk/ を使うと CLI でコンテンツを指定してキャッシュのクリアが出来ます。また WebUI でもクリアは可能です。\n$ gcloud compute url-maps invalidate-cdn-cache \u0026lt;url-map の名前\u0026gt; --path \u0026#34;コンテンツのパス\u0026#34; このときに \u0026ndash;path で指定できるコンテンツの記し方は下記のように指定することも可能です。\n--path \u0026#34;/cat.png # \u0026lt;--- 1つのコンテンツキャッシュを削除 --path \u0026#34;/*\u0026#34; # \u0026lt;--- 全てのコンテンツキャッシュを削除 --path \u0026#34;/pix/*\u0026#34; # \u0026lt;--- ディレクトリ指定で削除 まとめ 価格が低価格で必要最低限な機能に絞られている印象です。シンプルな分、理解し易いですしキャッシュクリアについても Hubot 化などすれば開発者の方に実行してもらいやすいのではないでしょうか。\n","permalink":"https://jedipunkz.github.io/post/cloud-cdn/","summary":"\u003cp\u003eこんにちは。@jedipunkz です。\u003c/p\u003e\n\u003cp\u003e今回は Google Cloud Platform の Google CloudCDN について調べてみたので記したいと思います。\u003c/p\u003e\n\u003cp\u003eCloudCDN は GCP のロードバランサのバックエンドサービスに紐付けられるサービスです。このバックエンドサービスで CloudCDN を有効にしていると CDN サービスを機能させることが出来ます。先に書いておくとこの CloudCDN はとてもシンプルで扱いやすいサービスだと判りました。高機能な他の CDN サービスと比べると機能が足らない感ありますが、必要最低限なところを抑えているのと、価格がとても安いです。(価格は下記の URL 参照)\u003c/p\u003e\n\u003cp\u003e価格表 : \u003ca href=\"https://cloud.google.com/cdn/pricing\"\u003ehttps://cloud.google.com/cdn/pricing\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"構成\"\u003e構成\u003c/h2\u003e\n\u003cp\u003e構成と構成の特徴です。\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e+----------+                                    +---------+\n| instance |--+                               +-| EndUser |\n+----------+  |  +------------+  +----------+ | +---------+\n              +--|LoadBalancer|--| CloudCDN |-+-| EndUser |\n+----------+  |  +------------+  +----------+ | +---------+\n| instance |--+                               +-| EndUser |\n+----------+                                    +---------+\n\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\n\u003cli\u003eコンテンツが初めてリクエストされた場合キャッシュミスする\u003c/li\u003e\n\u003cli\u003eキャッシュミスした際に近くにあるキャッシュからコンテンツを取得しようと試みる\u003c/li\u003e\n\u003cli\u003e近くのキャッシュがコンテンツがある場合、最初のキャッシュにコンテンツが送信される\u003c/li\u003e\n\u003cli\u003e近くのキャッシュにコンテンツがない場合、HTTP ロードバランサにリクエストが転送される\u003c/li\u003e\n\u003cli\u003eその後のリクエストはキャッシュが応答する(キャッシュヒット)\u003c/li\u003e\n\u003cli\u003eキャッシュ間のフィルは EndUser のリクエストに応じて実行される\u003c/li\u003e\n\u003cli\u003eキャッシュを事前に読み込むことできない\u003c/li\u003e\n\u003cli\u003eキャッシュは世界各地に配置されている\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"cloudcdn-を導入する方法\"\u003eCloudCDN を導入する方法\u003c/h2\u003e\n\u003cp\u003e導入する方法は簡単で下記のとおりです。\u003c/p\u003e","title":"Google Cloud CDN を使ってみた"},{"content":"こんにちは @jedipunkz です。\n今日は某アドベントカレンダーに参加していて記事を書いています。 記事だけ先出ししちゃいます..。\n今日は最近 coreos がリリースした \u0026rsquo;etcd-operator\u0026rsquo; を触ってみようかと思います。ほぼ、README に書かれている手順通りに実施するのですが、所感を交えながら作業して記事にしてみたいと思います。\ncoreos が提供している etcd についてご存知ない方もいらっしゃると思いますが etcd は KVS ストレージでありながら Configuration Management Store として利用できる分散型ストレージです。Docker 等の環境を提供する coreos という軽量 OS 内でも etcd が起動していてクラスタで管理された情報をクラスタ内の各 OS が読み書きできる、といった機能を etcd が提供しています。 詳細については公式サイトを御覧ください。\netcd 公式サイト : https://coreos.com/etcd/docs/latest/\netcd-operator はこの etcd クラスタを kubernetes 上でクラスタ管理するための簡単に運用管理するためのソフトウェアになります。\netcd-operator 公式アナウンス : https://coreos.com/blog/introducing-the-etcd-operator.html\n後に実際に触れていきますが下記のような管理が可能になります。\netcd クラスタの構築 etcd クラスタのスケールアップ・ダウン etcd Pod の障害時自動復旧 etcd イメージをオンラインで最新のモノにアップグレード では早速利用してみたいと思います。\n必要な環境 下記の環境が事前に用意されている必要があります。\nDocker Kubernetes or minikube+kubernetes (https://github.com/kubernetes/minikube) etcdctl : https://github.com/coreos/etcd/tree/master/etcdctl 作業準備 下記のレポジトリをクローンします。\n$ git clone https://github.com/coreos/etcd-operator.git Operator のデプロイ 下記のような内容のファイルが記さているファイルを利用します。中身を確認しましょう。\n$ cat example/deployment.yaml apiVersion: extensions/v1beta1 kind: Deployment metadata: name: etcd-operator spec: replicas: 1 template: metadata: labels: name: etcd-operator spec: containers: - name: etcd-operator image: quay.io/coreos/etcd-operator env: - name: MY_POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace kind: Deployment で etcd-operator のイメージを使ってレプリカ数1のポッドを起動しているのが分かると思います。実際にデプロイします。\n$ kubectl create -f example/deployment.yaml どんなポッドが起動したか確認します。\nkubectl get pods NAME READY STATUS RESTARTS AGE etcd-operator-217127005-futo0 1/1 Running 0 1m あと、これは自分も知りませんでしたがサードパーティリソースという枠があるらしく下記のように確認することができます。\nkubectl get thirdpartyresources NAME DESCRIPTION VERSION(S) etcd-cluster.coreos.com Managed etcd clusters v1 replica数1 ですが、ポッドなのでポッド内のプロセスに問題があれば kubernetes が自動で修復 (場合によってポッドの再構築) されます。また replica 数を増やす運用も考えられそうです。\netcd クラスタの構築 下記の内容のファイルで etcd をデプロイします。中身を確認しましょう。 API は coreos.com/v1 で kind: EtcdCluster という情報が記されています。また version でイメージバージョンが記されているのかなということが後に確認できます。また size でレプリカ数が記されているようです。\n$ cat example/example-etcd-cluster.yaml apiVersion: \u0026#34;coreos.com/v1\u0026#34; kind: \u0026#34;EtcdCluster\u0026#34; metadata: name: \u0026#34;etcd-cluster\u0026#34; spec: size: 3 version: \u0026#34;v3.1.0-alpha.1\u0026#34; デプロイをしてみます。\n$ kubectl create -f example/example-etcd-cluster.yaml クラスタがデプロイされたか見てみます。3つのポッドが確認できます。やはりファイル中 size: 3 はレプリカ数だったようです。\n$ kubectl get pods --show-all NAME READY STATUS RESTARTS AGE etcd-cluster-0000 1/1 Running 0 1m etcd-cluster-0001 1/1 Running 0 36s etcd-cluster-0002 1/1 Running 0 21s 同様に Service について確認します。etcd-cluster-000[012] の3つが今回作った etcd クラスタです。\n$ kubectl get svc NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE etcd-cluster 10.0.0.80 \u0026lt;none\u0026gt; 2379/TCP 1m etcd-cluster-0000 10.0.0.13 \u0026lt;none\u0026gt; 2380/TCP,2379/TCP 1m etcd-cluster-0001 10.0.0.111 \u0026lt;none\u0026gt; 2380/TCP,2379/TCP 1m etcd-cluster-0002 10.0.0.183 \u0026lt;none\u0026gt; 2380/TCP,2379/TCP 50s kubernetes 10.0.0.1 \u0026lt;none\u0026gt; 443/TCP 5d etcd クラスタをスケールアウト では etcd クラスタのレプリカ数を増やすことでスケールアウトしてみます。が、今現在 (2016/12) 時点では一部不具合があるらしく回避策として下記の通り curl を用いてスケールアウトすることが可能なようです。\n下記の内容で body.json ファイルを用意します。size: 5 になっていることが確認できて、レプリカ数5になるのだなと判断できます。\n$ cat body.json { \u0026#34;apiVersion\u0026#34;: \u0026#34;coreos.com/v1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;EtcdCluster\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;etcd-cluster\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;default\u0026#34; }, \u0026#34;spec\u0026#34;: { \u0026#34;size\u0026#34;: 5 } } ここで curl で実行するためプロキシを稼働します。\n$ kubectl proxy --port=8080 curl で先程作った body.json を PUT してみます。\ncurl -H \u0026#39;Content-Type: application/json\u0026#39; -X PUT --data @body.json http://127.0.0.1:8080/apis/coreos.com/v1/namespaces/default/etcdclusters/etcd-cluster クラスタがスケールアウトされたか確認しましょう。\nkubectl get pods --show-all NAME READY STATUS RESTARTS AGE etcd-cluster-0000 1/1 Running 0 5m etcd-cluster-0001 1/1 Running 0 4m etcd-cluster-0002 1/1 Running 0 4m etcd-cluster-0003 1/1 Running 0 32s etcd-cluster-0004 1/1 Running 0 17s etcd-operator-217127005-futo0 1/1 Running 0 9m 5台構成のクラスタにスケールアウトしたことが確認できます。\netcd にアクセス 5台構成の etcd クラスタがデプロイできたので etcd に etcdctl を使ってアクセスしてみましょう。事前に etcdctl をインストールする必要があります。また私の環境もそうだったのですがローカルの Mac に minikube を使って kubernetes 環境を構築しているので下記のように nodePort を作るため作業が必要です。\n$ kubectl create -f example/example-etcd-cluster-nodeport-service.json $ export ETCDCTL_API=3 $ export ETCDCTL_ENDPOINTS=$(minikube service etcd-cluster-client-service --url) $ etcdctl put foo bar $ etcdctl get foo foo bar etcdctl でキー・値を入力・読み込みが可能であることが分かりました！\nポッドの自動復旧 kubernetes を普段使っている方は分かると思うのですがポッドを落としても kubernetes が自動復旧してくれます。ここで一つポッドを削除してみようと思います。\n$kubectl get pods NAME READY STATUS RESTARTS AGE etcd-cluster-0000 1/1 Running 0 11m etcd-cluster-0001 1/1 Running 0 11m etcd-cluster-0002 1/1 Running 0 11m etcd-cluster-0003 1/1 Running 0 6m etcd-cluster-0004 1/1 Running 0 6m etcd-operator-217127005-futo0 1/1 Running 0 16m $kubect delete pod etcd-cluster-0004 しばらくすると下記の通り etcd-cluster-0004 に代わり etcd-cluster-0005 が稼働していることが確認できると思います。\n$ kubectl get pods NAME READY STATUS RESTARTS AGE etcd-cluster-0000 1/1 Running 0 12m etcd-cluster-0001 1/1 Running 0 12m etcd-cluster-0002 1/1 Running 0 11m etcd-cluster-0003 1/1 Running 0 7m etcd-cluster-0005 1/1 Running 0 3s etcd-operator-217127005-futo0 1/1 Running 0 17m まとめ kubernetes の上で構成することでうまく kubernetes のメリットを活かしつつ etcd クラスタを構成できていると言えると思います。記事執筆時ではまだ不具合が幾つかありました (上記の curl で実施した箇所や、イメージのアップグレード) が、etcd を手動で構築するより kubernetes で構成するほうがメリットが多いことは明らかです。また kubernetes のポッドから kubernetes dns を介してサービスネームに直接アクセスできるのでポッドから etcd に情報を読み書きすることも容易になりそうです。\nですが etcd に収めるデータの性質によっては運用面で厳しくなることも想定できます。coreos 内で etcd クラスタを介して互いのコンテナ間でコンフィグを共有し合う使い方は非常に意味があると思うのですが、coreos 外の独自のソフトウェアがいろんな種別のデータを etcd クラスタに外から入出力することの意味はそれほど無いように思えます。であれば高耐久性で運用のし易い軽量な KVS ソフトウェアを使うべきだからです。\nまた今回紹介した etcd-operator とは別に coreos が同時にアナウンスした(https://coreos.com/blog/the-prometheus-operator.html) に関しても興味を持っているので後に触ってみたいと思います。\n何と言うか所感として最後に述べるとサーバレスが叫ばれている中でわざわざクラスタソフトウェアを自前で管理するのか？と疑問も確かに残ります。それこそクラウドプラットフォームが提供すべき機能だと思うからです..。\n","permalink":"https://jedipunkz.github.io/post/etcd-operator/","summary":"\u003cp\u003eこんにちは @jedipunkz です。\u003c/p\u003e\n\u003cp\u003e今日は某アドベントカレンダーに参加していて記事を書いています。\n記事だけ先出ししちゃいます..。\u003c/p\u003e\n\u003cp\u003e今日は最近 coreos がリリースした \u0026rsquo;etcd-operator\u0026rsquo; を触ってみようかと思います。ほぼ、README に書かれている手順通りに実施するのですが、所感を交えながら作業して記事にしてみたいと思います。\u003c/p\u003e\n\u003cp\u003ecoreos が提供している etcd についてご存知ない方もいらっしゃると思いますが etcd は KVS ストレージでありながら Configuration Management Store として利用できる分散型ストレージです。Docker 等の環境を提供する coreos という軽量 OS 内でも etcd が起動していてクラスタで管理された情報をクラスタ内の各 OS が読み書きできる、といった機能を etcd が提供しています。\n詳細については公式サイトを御覧ください。\u003c/p\u003e\n\u003cp\u003eetcd 公式サイト : \u003ca href=\"https://coreos.com/etcd/docs/latest/\"\u003ehttps://coreos.com/etcd/docs/latest/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eetcd-operator はこの etcd クラスタを kubernetes 上でクラスタ管理するための簡単に運用管理するためのソフトウェアになります。\u003c/p\u003e\n\u003cp\u003eetcd-operator 公式アナウンス : \u003ca href=\"https://coreos.com/blog/introducing-the-etcd-operator.html\"\u003ehttps://coreos.com/blog/introducing-the-etcd-operator.html\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e後に実際に触れていきますが下記のような管理が可能になります。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eetcd クラスタの構築\u003c/li\u003e\n\u003cli\u003eetcd クラスタのスケールアップ・ダウン\u003c/li\u003e\n\u003cli\u003eetcd Pod の障害時自動復旧\u003c/li\u003e\n\u003cli\u003eetcd イメージをオンラインで最新のモノにアップグレード\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eでは早速利用してみたいと思います。\u003c/p\u003e\n\u003ch2 id=\"必要な環境\"\u003e必要な環境\u003c/h2\u003e\n\u003cp\u003e下記の環境が事前に用意されている必要があります。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDocker\u003c/li\u003e\n\u003cli\u003eKubernetes or minikube+kubernetes (\u003ca href=\"https://github.com/kubernetes/minikube\"\u003ehttps://github.com/kubernetes/minikube\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003eetcdctl : \u003ca href=\"https://github.com/coreos/etcd/tree/master/etcdctl\"\u003ehttps://github.com/coreos/etcd/tree/master/etcdctl\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"作業準備\"\u003e作業準備\u003c/h2\u003e\n\u003cp\u003e下記のレポジトリをクローンします。\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e$ git clone https://github.com/coreos/etcd-operator.git\n\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"operator-のデプロイ\"\u003eOperator のデプロイ\u003c/h2\u003e\n\u003cp\u003e下記のような内容のファイルが記さているファイルを利用します。中身を確認しましょう。\u003c/p\u003e","title":"coreos の etcd operator を触ってみた"},{"content":"こんにちは。@jedipunkz です。\n今回は Helm という kubernetes のパッケージマネージャ的なソフトウェアを使ってみたので記事にしたいと思います。\n公式サイト : https://helm.sh/ Kubernetes を仕事で使っているのですが \u0026ldquo;レプリケーションコントローラ\u0026rdquo; や \u0026ldquo;サービス\u0026rdquo; といった単位を使って Pod, Service を管理しています。Helm を使うことでこれらの管理方法が変わるのか調べたいと思います。\n依存するソフトウェア 今回は MacOS を使って環境を整えます。\nvirtualbox minikube kubectl これらのソフトウェアをインストールしていきます。\n$ brew cask install virtualbo $ curl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.12.2/minikube-darwin-amd64 \u0026amp;\u0026amp; chmod +x minikube \u0026amp;\u0026amp; sudo mv minikube /usr/local/bin/ $ brew install kubectl minikube を使って簡易的な kubernetes 環境を起動します。\n$ minikube start $ eval $(minikube docker-env) Helm を使ってみる Helm は Charts という単位で Kubernetes をパッケージングします。Charts の一覧を見てみましょう。\n$ helm search NAME VERSION DESCRIPTION stable/drupal 0.3.7 One of the most versatile open source content m... stable/factorio 0.1.1 Factorio dedicated server. stable/ghost 0.4.0 A simple, powerful publishing platform that all... stable/jenkins 0.1.1 A Jenkins Helm chart for Kubernetes. stable/joomla 0.4.0 PHP content management system (CMS) for publish... stable/mariadb 0.5.3 Chart for MariaDB stable/mediawiki 0.4.0 Extremely powerful, scalable software and a fea... stable/memcached 0.4.0 Chart for Memcached stable/minecraft 0.1.0 Minecraft server stable/mysql 0.2.1 Chart for MySQL stable/phpbb 0.4.0 Community forum that supports the notion of use... stable/postgresql 0.2.0 Chart for PostgreSQL stable/prometheus 1.3.1 A Prometheus Helm chart for Kubernetes. Prometh... stable/redis 0.4.1 Chart for Redis stable/redmine 0.3.5 A flexible project management web application. stable/spark 0.1.1 A Apache Spark Helm chart for Kubernetes. Apach... stable/spartakus 1.0.0 A Spartakus Helm chart for Kubernetes. Spartaku... stable/testlink 0.4.0 Web-based test management system that facilitat... stable/traefik 1.1.0-rc3-a A Traefik based Kubernetes ingress controller w... stable/uchiwa 0.1.0 Dashboard for the Sensu monitoring framework stable/wordpress 0.3.2 Web publishing platform for building blogs and ... 各アプリケーションの名前で Charts が管理されていることが分かります。 ここでは stable/mysql を使って kubernetes の中に MySQL 環境を作ってみます。まず stable/mysql に設定できるパラメータ一覧を取得します。\n$ helm inspect stable/mysql Fetched stable/mysql to mysql-0.2.1.tgz description: Chart for MySQL engine: gotpl home: https://www.mysql.com/ keywords: - mysql - database - sql maintainers: - email: viglesias@google.com name: Vic Iglesias name: mysql sources: - https://github.com/kubernetes/charts - https://github.com/docker-library/mysql version: 0.2.1 --- ## mysql image version ## ref: https://hub.docker.com/r/library/mysql/tags/ ## imageTag: \u0026#34;5.7.14\u0026#34; ## Specify password for root user ## ## Default: random 10 character string # mysqlRootPassword: testing ## Create a database user ## # mysqlUser: # mysqlPassword: ## Allow unauthenticated access, uncomment to enable ## # mysqlAllowEmptyPassword: true ## Create a database ## # mysqlDatabase: ## Specify a imagePullPolicy ## \u0026#39;Always\u0026#39; if imageTag is \u0026#39;latest\u0026#39;, else set to \u0026#39;IfNotPresent\u0026#39; ## ref: http://kubernetes.io/docs/user-guide/images/#pre-pulling-images ## # imagePullPolicy: ## Persist data to a persitent volume persistence: enabled: true storageClass: generic accessMode: ReadWriteOnce size: 8Gi ## Configure resource requests and limits ## ref: http://kubernetes.io/docs/user-guide/compute-resources/ ## resources: requests: memory: 256Mi cpu: 100m stable/mysql という Charts を構成するパラメータ一覧が取得できました。今回は DB 上のユーザを作ってパスワードを設定してみようと思います。上記のパラメータの中から \u0026lsquo;mysqlUser\u0026rsquo;, \u0026lsquo;mysqlPasswword\u0026rsquo; を編集してみます。下記の内容を config.yaml というファイル名で保存します。\nmysqlUser: user1 mysqlPassword: password1 この config.yaml を使って stable/mysql を起動してみます。\n$ helm install -f config.yaml stable/mysql $ helm ls # \u0026lt;--- 起動した環境を確認する NAME REVISION UPDATED STATUS CHART dealing-ladybug 1 Sun Nov 20 10:44:00 2016 DEPLOYED mysql-0.2.1 \u0026lsquo;dealing-ladybug\u0026rsquo; という名前で mysql が起動したことが分かります。\nkubectl をつかって Services を確認してみます。\n$ kubectl get svc NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE dealing-ladybug-mysql 10.0.0.24 \u0026lt;none\u0026gt; 3306/TCP 33m \u0026lsquo;dealing-ladybug-mysql\u0026rsquo; という Service が kubernetes 環境に作られました。この名前は Kubernetes 内のコンテナから DNS 名前解決できるアドレスとなります。\nここで mysql-client 環境を作るために下記のようなコンテナを起動して mysql-client をインストール、config.yaml で作成したユーザ・パスワードで mysql サーバにアクセス確認します。\n$ kubectl run -i --tty ubuntu02 --image=ubuntu:16.04 --restart=Never -- bash -il # apt-get update; apt-get install -y mysql-client # mysql -h dealing-ladybug-mysql -u user1 -ppassword1 \u0026lt;省略\u0026gt; mysql\u0026gt; config.yaml に記したユーザ情報で MySQL にアクセスできることを確認できました。\nまとめ Helm を使うことでレプリケーションコントローラやサービスを直接 yaml で作らなくても MySQL 環境構築と設定が行えました。Helm の Charts は自分で開発することも可能です。(参考 URL)\nhttps://github.com/kubernetes/helm/blob/master/docs/charts.md\nパッケージングすることで他人の作った資源を有用に活用することもできるため、こういった何かを抽象化する技術を採用することにはとても意味があると思います。自動化を考える上でも抽象化できる技術は有用だと思います。\nですがレプリケーションコントローラを使っても Helm でも Yaml で管理することに変わりはなく、またレプリケーションコントローラで指定できる詳細なパラメータ (replicas や command, image など) を指定できないためすぐに実用するというわけにはいかないなぁと感じました。\n参考 URL https://github.com/kubernetes/helm/blob/master/docs/charts.md https://github.com/kubernetes/helm/blob/master/docs/quickstart.md ","permalink":"https://jedipunkz.github.io/post/helm/","summary":"\u003cp\u003eこんにちは。@jedipunkz です。\u003c/p\u003e\n\u003cp\u003e今回は Helm という kubernetes のパッケージマネージャ的なソフトウェアを使ってみたので記事にしたいと思います。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e公式サイト : \u003ca href=\"https://helm.sh/\"\u003ehttps://helm.sh/\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eKubernetes を仕事で使っているのですが \u0026ldquo;レプリケーションコントローラ\u0026rdquo; や \u0026ldquo;サービス\u0026rdquo; といった単位を使って Pod, Service を管理しています。Helm を使うことでこれらの管理方法が変わるのか調べたいと思います。\u003c/p\u003e\n\u003ch2 id=\"依存するソフトウェア\"\u003e依存するソフトウェア\u003c/h2\u003e\n\u003cp\u003e今回は MacOS を使って環境を整えます。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003evirtualbox\u003c/li\u003e\n\u003cli\u003eminikube\u003c/li\u003e\n\u003cli\u003ekubectl\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eこれらのソフトウェアをインストールしていきます。\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e$ brew cask install virtualbo\n$ curl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.12.2/minikube-darwin-amd64 \u0026amp;\u0026amp; chmod +x minikube \u0026amp;\u0026amp; sudo mv minikube /usr/local/bin/\n$ brew install kubectl\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eminikube を使って簡易的な kubernetes 環境を起動します。\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e$ minikube start\n$ eval $(minikube docker-env)\n\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"helm-を使ってみる\"\u003eHelm を使ってみる\u003c/h2\u003e\n\u003cp\u003eHelm は Charts という単位で Kubernetes をパッケージングします。Charts の一覧を見てみましょう。\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e$ helm search\nNAME                    VERSION         DESCRIPTION\nstable/drupal           0.3.7           One of the most versatile open source content m...\nstable/factorio         0.1.1           Factorio dedicated server.\nstable/ghost            0.4.0           A simple, powerful publishing platform that all...\nstable/jenkins          0.1.1           A Jenkins Helm chart for Kubernetes.\nstable/joomla           0.4.0           PHP content management system (CMS) for publish...\nstable/mariadb          0.5.3           Chart for MariaDB\nstable/mediawiki        0.4.0           Extremely powerful, scalable software and a fea...\nstable/memcached        0.4.0           Chart for Memcached\nstable/minecraft        0.1.0           Minecraft server\nstable/mysql            0.2.1           Chart for MySQL\nstable/phpbb            0.4.0           Community forum that supports the notion of use...\nstable/postgresql       0.2.0           Chart for PostgreSQL\nstable/prometheus       1.3.1           A Prometheus Helm chart for Kubernetes. Prometh...\nstable/redis            0.4.1           Chart for Redis\nstable/redmine          0.3.5           A flexible project management web application.\nstable/spark            0.1.1           A Apache Spark Helm chart for Kubernetes. Apach...\nstable/spartakus        1.0.0           A Spartakus Helm chart for Kubernetes. Spartaku...\nstable/testlink         0.4.0           Web-based test management system that facilitat...\nstable/traefik          1.1.0-rc3-a     A Traefik based Kubernetes ingress controller w...\nstable/uchiwa           0.1.0           Dashboard for the Sensu monitoring framework\nstable/wordpress        0.3.2           Web publishing platform for building blogs and ...\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e各アプリケーションの名前で Charts が管理されていることが分かります。\nここでは stable/mysql を使って kubernetes の中に MySQL 環境を作ってみます。まず stable/mysql に設定できるパラメータ一覧を取得します。\u003c/p\u003e","title":"Helm を使って Kubernetes を管理する"},{"content":"こんにちは、@jedipunkz です。今回は Kubernetes1.4 から実装された ScheduledJob を試してみたのでその内容を記したいと思います。\nScheduledJob はバッチ的な処理を Kubernetes の pod を使って実行するための仕組みです。現在は alpha バージョンとして実装されています。 kubernetes の pod, service は通常、永続的に立ち上げておくサーバなどを稼働させるものですが、それに対してこの scheduledJob は cron 感覚でバッチ処理を pod に任せることができます。\nAlpha バージョンということで今回の環境構築は minikube を使って簡易的に Mac 上に構築しました。Docker がインストールされていれば Linux でも環境を作れます。\n参考 URL 今回利用する yaml ファイルなどは下記のサイトを参考にしています。\nhttp://kubernetes.io/docs/user-guide/scheduled-jobs/ https://github.com/kubernetes/minikube 前提の環境 私の環境では下記の環境を利用しました。\nMac OSX Docker-machine or Docker for Mac minikube kubernetes 1.4 以降の構成を minikube で構築する まず minikube のインストールを行います。\n$ curl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.12.0/minikube-darwin-amd64 \u0026amp;\u0026amp; chmod +x minikube \u0026amp;\u0026amp; sudo mv minikube /usr/local/bin/ 早速 minikube を起動します。\n$ docker-machine start default $ minikube start $ eval $(minikube docker-env) Google Cloud の GKE を利用する場合 今回は minikube を使ってローカルマシンに kubernetes 1.4 環境を作りましたが Google Cloud の GKE を用いている場合下記のように gcloud SDK を用いて GKE クラスターノードを構築することで対応できます。ですが Google に確認したところこの構築方法を取った場合には Google からのサポートを得られないのと SLA も対象外になるとのことでした。リスクは大きいと思います。(2016/11現在)\ngcloud alpha container clusters create alpha-test-cluster --zone asia-northeast1-b --enable-kubernetes-alpha gcloud container clusters get-credentials alpha-test-cluster gcloud container node-pools create alpha-test-pool --zone asia-northeast1-b --cluster alpha-test-cluster scheduledjob を試す yaml ファイルの生成 scheduledjob を実行するための yaml ファイルを生成します。公式サイト (http://kubernetes.io/docs/user-guide/scheduled-jobs/) にあるものを一部修正して記述しています。ファイル名は sj.yaml とします。\napiVersion: batch/v2alpha1 kind: ScheduledJob metadata: name: hello spec: schedule: 0/1 * * * ? jobTemplate: spec: template: spec: containers: - name: hello image: busybox args: - /bin/sh - -c - date; echo Hello from the Kubernetes cluster restartPolicy: OnFailure concurrencyPolicy: Allow #suspend: true scheduledJob の実行 生成した yaml ファイルを元に kubectl コマンドで scheduledjob を実行します。\n$ kubectl create -f ./sj.yaml 実行したジョブがどのような状況か確認します。下記のコマンドで生成した scheduledjob 一覧が確認できます。 yaml ファイルの通り、1分間隔で \u0026lsquo;hello\u0026rsquo; という scheduledJob が実行されることが確認できると思います。\n$ kubectl get scheduledjob NAME SCHEDULE SUSPEND ACTIVE LAST-SCHEDULE hello 0/1 * * * ? False 0 \u0026lt;none\u0026gt; scheduledjob を元に実行された(実行される) ジョブ(job) 一覧を確認します。 1分間隔で実行されている様子が確認できます。また、DESIRED:1 で SUCCESSFUL:0 の行は、1分間隔で実行される直前のジョブとして認識されていることがわかります。\n$ kubectl get jobs --watch NAME DESIRED SUCCESSFUL AGE hello-1856276298 1 1 59s NAME DESIRED SUCCESSFUL AGE hello-1780451143 1 0 0s hello-1780451143 1 0 0s hello-1780451143 1 1 5s hello-1476429627 1 0 0s hello-1476429627 1 0 0s hello-1476429627 1 1 5s hello-1628211009 1 0 0s hello-1628211009 1 0 0s hello-1628211009 1 1 5s hello-1552385854 1 0 0s hello-1552385854 1 0 0s hello-1552385854 1 1 5s 下記のコマンドで実行されたジョブ一覧を取得できます。こちらは結果のみですので SUCCESSFULL=1 のみが表示されています。\n$ kubectl get job NAME DESIRED SUCCESSFUL AGE hello-1476429627 1 1 6m hello-1552385854 1 1 4m hello-1552516926 1 1 1m hello-1628211009 1 1 5m hello-1628342081 1 1 2m hello-1704167236 1 1 3m hello-1704298308 1 1 49s hello-1780451143 1 1 7m hello-1856276298 1 1 8m この際に pod の様子も確認してみます。\u0026ndash;show-all オプションで過去の pod を一覧表示します。今回のジョブは一瞬で実行可能な内容なのでこのオプションを付けないと pod 一覧が取得できない可能性が高いです。\nkubectl get pod --show-all NAME READY STATUS RESTARTS AGE hello-1476429627-pxr06 0/1 Completed 0 8m hello-1552385854-y68ci 0/1 Completed 0 6m hello-1552516926-ggjpk 0/1 Completed 0 3m hello-1628211009-iih0i 0/1 Completed 0 7m hello-1628342081-ig5lg 0/1 Completed 0 4m hello-1628473153-9wvn4 0/1 Completed 0 1m hello-1704167236-zqg94 0/1 Completed 0 5m hello-1704298308-eaq8m 0/1 Completed 0 2m hello-1780254535-64mah 0/1 Completed 0 28s hello-1780451143-tjg8r 0/1 Completed 0 9m hello-1856276298-y2o65 0/1 Completed 0 10m 最終行のジョブ \u0026lsquo;hello-1856276298-y2o65\u0026rsquo; の内容を確認します。\n$ kubectl logs hello-1856276298-y2o65 Thu Oct 27 01:18:11 UTC 2016 Hello from the Kubernetes cluster sj.yaml 内に記述したジョブ内容 \u0026rsquo;echo \u0026hellip;\u0026rsquo; の実行結果が表示されていることが確認できます。\ndocker のイメージも確認します。ジョブ内で指定した image: busybox が確認できます。 その他のイメージは minikube を構成するものです。\n$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE busybox latest e02e811dd08f 2 weeks ago 1.093 MB gcr.io/google_containers/kubernetes-dashboard-amd64 v1.4.0 436faaeba2e2 5 weeks ago 86.27 MB gcr.io/google-containers/kube-addon-manager v5.1 735ce4344f7f 3 months ago 255.8 MB gcr.io/google_containers/pause-amd64 3.0 99e59f495ffa 5 months ago 746.9 kB ジョブの削除 ジョブを削除します。削除する対象は scheduledjob とそれを元に生成・実行された jobs です。\n$ kubectl delete scheduledjob hello $ kubectl delete jobs $(kubectl get jobs | awk \u0026#39;{print $1}\u0026#39; | grep -v NAME) job \u0026#34;hello-1476429627\u0026#34; deleted job \u0026#34;hello-1552385854\u0026#34; deleted job \u0026#34;hello-1552516926\u0026#34; deleted job \u0026#34;hello-1628211009\u0026#34; deleted job \u0026#34;hello-1628342081\u0026#34; deleted job \u0026#34;hello-1628473153\u0026#34; deleted job \u0026#34;hello-1628604225\u0026#34; deleted job \u0026#34;hello-1704167236\u0026#34; deleted job \u0026#34;hello-1704298308\u0026#34; deleted job \u0026#34;hello-1704429380\u0026#34; deleted job \u0026#34;hello-1780254535\u0026#34; deleted job \u0026#34;hello-1780385607\u0026#34; deleted job \u0026#34;hello-1780451143\u0026#34; deleted job \u0026#34;hello-1856276298\u0026#34; deleted ここで確認できたのは scheduledjob を削除するとそれ以降の jobs は新規実行されませんでした。が、実行された後の jobs は情報として残ったままでした。\nその他のオプション concurrencyPolicy 下記のように spec.concurrencyPolicy オプションが指定できます。下記のような value を渡すと実行されるジョブの動作を変えることが可能です。\nAllow (Default) : 重複するジョブ実行を許可 Forbid : 直前のジョブが終了していない場合ジョブ実行をスキップする Replace : 直前のジョブが終了していない場合新しいジョブを上書きする suspend \u0026lsquo;spec.suspend: true\u0026rsquo; に設定すると ScheduledJob は生成されますが次回実行時のジョブがサスペンドされ実行されません。デフォルトは false です。\n$ kubectl get scheduledjob NAME SCHEDULE SUSPEND ACTIVE LAST-SCHEDULE hello 0/1 * * * ? True 0 \u0026lt;none\u0026gt; $ kubectl get job $ オプションの適用例ファイル apiVersion: batch/v2alpha1 kind: ScheduledJob metadata: name: hello spec: schedule: 0/1 * * * ? jobTemplate: spec: template: spec: containers: - name: hello image: busybox args: - /bin/sh - -c - date; echo Hello from the Kubernetes cluster restartPolicy: OnFailure concurrencyPolicy: Forbid suspend: true まとめと考察 今回試してみて、バッチ的な処理を kubernetes で行うならこの scheduledJob しかないな、と思いました。 ですが、2016/11 現在ではまだ Alpha バージョンということで下記のようなリスクを含んでいます。\nこのバージョンはバギーです アナウンス無しに機能が削られることがあります アナウンス無しにそれまでの互換性を変更することがあります テスト用としての利用を勧めます。 参考サイト : http://kubernetes.io/docs/api/\nまた上記にも記しましたが、ジョブの結果が残っていくため、通常使う cron のように数分間隔で実行しているとあっという間に job の量が大量に肥大化することが予想されます。この job には実行結果も含まれているため、消されるものではないのは理解できるのですが kubernetes api が持っている DB 上に大量のデータが生成され続けてしまうため、kubernetes api/サーバを管理している場合には問題になると思います。この辺り、仕様の修正が入ることを期待しています。\n","permalink":"https://jedipunkz.github.io/post/kubernetes-scheduledjob/","summary":"\u003cp\u003eこんにちは、@jedipunkz です。今回は Kubernetes1.4 から実装された ScheduledJob を試してみたのでその内容を記したいと思います。\u003c/p\u003e\n\u003cp\u003eScheduledJob はバッチ的な処理を Kubernetes の pod を使って実行するための仕組みです。現在は alpha バージョンとして実装されています。\nkubernetes の pod, service は通常、永続的に立ち上げておくサーバなどを稼働させるものですが、それに対してこの scheduledJob は cron 感覚でバッチ処理を pod に任せることができます。\u003c/p\u003e\n\u003cp\u003eAlpha バージョンということで今回の環境構築は minikube を使って簡易的に Mac 上に構築しました。Docker がインストールされていれば Linux でも環境を作れます。\u003c/p\u003e\n\u003ch2 id=\"参考-url\"\u003e参考 URL\u003c/h2\u003e\n\u003cp\u003e今回利用する yaml ファイルなどは下記のサイトを参考にしています。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"http://kubernetes.io/docs/user-guide/scheduled-jobs/\"\u003ehttp://kubernetes.io/docs/user-guide/scheduled-jobs/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/kubernetes/minikube\"\u003ehttps://github.com/kubernetes/minikube\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"前提の環境\"\u003e前提の環境\u003c/h2\u003e\n\u003cp\u003e私の環境では下記の環境を利用しました。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMac OSX\u003c/li\u003e\n\u003cli\u003eDocker-machine or Docker for Mac\u003c/li\u003e\n\u003cli\u003eminikube\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"kubernetes-14-以降の構成を-minikube-で構築する\"\u003ekubernetes 1.4 以降の構成を minikube で構築する\u003c/h2\u003e\n\u003cp\u003eまず minikube のインストールを行います。\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e$ curl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.12.0/minikube-darwin-amd64 \u0026amp;\u0026amp; chmod +x minikube \u0026amp;\u0026amp; sudo mv minikube /usr/local/bin/\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e早速 minikube を起動します。\u003c/p\u003e","title":"kubernetes1.4 で実装された ScheduledJob を試してみた！"},{"content":"こんにちは。@jedipunkz です。\nkubernetes の環境を簡易的に作れる Minikube (https://github.com/kubernetes/minikube) が2ヶ月前ほどにリリースになっていました。簡単ですが少し触ってみたのでその際のメモを記したいと思います。VirtualBox もしくは VMware Fusion がインストールされていればすぐにでも稼働可能です。私は Kubernetes 初心者ですが何も考えずに kubernetes を動かすことが出来ました。\n前提 前提として下記の環境が必要になります。\nMac OSX がインストールされていること VirtualBox もしくは VMware Fusion がインストールされていること minikube をインストール minikube をインストールします。\n$ curl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.6.0/minikube-darwin-amd64 \u0026amp;\u0026amp; chmod +x minikube \u0026amp;\u0026amp; sudo mv minikube /usr/local/bin/ kubetl をインストール 次に kubectl をインストールします。\n$ curl -k -o kubectl https://kuar.io/darwin/kubectl \u0026amp;\u0026amp; chmod +x kubectl \u0026amp;\u0026amp; sudo mv kubectl /usr/local/bin/ Minikube で Kurbernates を稼働 Minikube を使って Kubernetes を稼働してみます。下記のコマンドを実行すると Virtualbox 上で仮想マシンが稼働し Kubernetes 一式も立ち上がります。\n$ minikube start Kurbernates を使ってみる Pods を立ち上げてみましょう。下記の内容を redis-django.yaml ファイルに保存します。\napiVersion: v1 kind: Pod metadata: name: redis-django labels: app: web spec: containers: - name: key-value-store image: redis ports: - containerPort: 6379 - name: frontend image: django ports: - containerPort: 8000 kubectl コマンドで Pod を立ち上げます。\n$ kubectl create -f ./redis-django.yaml Pod の様子を確認します。\n$ kubectl get pods NAME READY STATUS RESTARTS AGE redis-django 1/2 CrashLoopBackOff 7 15m Minikube はクラスタといってもノードが1つなので READY 1/2 となるようです。Nodes の様子を見てみます。\n$ kubectl get nodes NAME LABELS STATUS minikubevm beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=minikubevm Ready Docker ホスト上の様子を見てみましょう。Kubernetes を形成するコンテナと共に redis のコンテナが稼働していることが確認できます。\n$ eval $(minikube docker-env) $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 550285614e33 redis \u0026#34;docker-entrypoint.sh\u0026#34; 20 minutes ago Up 20 minutes k8s_key-value-store.a3b8356e_redis-django_default_4440a1d8-5272-11e6-9f19-6e0006e7fb51_90c3fec8 aba3a8c040d4 gcr.io/google_containers/pause-amd64:3.0 \u0026#34;/pause\u0026#34; 20 minutes ago Up 20 minutes k8s_POD.822b267d_redis-django_default_4440a1d8-5272-11e6-9f19-6e0006e7fb51_5bef1d2a 9ea96a3f3e10 gcr.io/google-containers/kube-addon-manager-amd64:v2 \u0026#34;/opt/kube-addons.sh\u0026#34; 48 minutes ago Up 48 minutes k8s_kube-addon-manager.a1c58ca2_kube-addon-manager-minikubevm_kube-system_48abed82af93bb0b941173334110923f_84f4fd38 192e886a5795 gcr.io/google_containers/pause-amd64:3.0 \u0026#34;/pause\u0026#34; 48 minutes ago Up 48 minutes k8s_POD.d8dbe16c_kube-addon-manager-minikubevm_kube-system_48abed82af93bb0b941173334110923f_6c65b482 7b005c68d9d4 gcr.io/google_containers/pause-amd64:3.0 \u0026#34;/pause\u0026#34; 48 minutes ago Up 48 minutes k8s_POD.2225036b_kubernetes-dashboard-pzdxy_kube-system_7005dce1-479a-11e6-a0ce-86b669e45864_c08bd009 まとめ Kubernetes のことを殆ど知らない私でもなんとなくですが稼働させて基本的な操作が出来ました。2016/5/31 にリリースされたツールなのでまだ安定しないところもありますが、より容易に Kubernetes が稼働できるようになったのでエンジニアの敷居が下がったのではないでしょうか。\n","permalink":"https://jedipunkz.github.io/post/minikube/","summary":"\u003cp\u003eこんにちは。@jedipunkz です。\u003c/p\u003e\n\u003cp\u003ekubernetes の環境を簡易的に作れる Minikube (\u003ca href=\"https://github.com/kubernetes/minikube\"\u003ehttps://github.com/kubernetes/minikube\u003c/a\u003e) が2ヶ月前ほどにリリースになっていました。簡単ですが少し触ってみたのでその際のメモを記したいと思います。VirtualBox もしくは VMware Fusion がインストールされていればすぐにでも稼働可能です。私は Kubernetes 初心者ですが何も考えずに kubernetes を動かすことが出来ました。\u003c/p\u003e\n\u003ch2 id=\"前提\"\u003e前提\u003c/h2\u003e\n\u003cp\u003e前提として下記の環境が必要になります。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMac OSX がインストールされていること\u003c/li\u003e\n\u003cli\u003eVirtualBox もしくは VMware Fusion がインストールされていること\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"minikube-をインストール\"\u003eminikube をインストール\u003c/h2\u003e\n\u003cp\u003eminikube をインストールします。\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e$ curl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.6.0/minikube-darwin-amd64 \u0026amp;\u0026amp; chmod +x minikube \u0026amp;\u0026amp; sudo mv minikube /usr/local/bin/\n\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"kubetl-をインストール\"\u003ekubetl をインストール\u003c/h2\u003e\n\u003cp\u003e次に kubectl をインストールします。\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e$ curl -k -o kubectl https://kuar.io/darwin/kubectl \u0026amp;\u0026amp; chmod +x kubectl \u0026amp;\u0026amp; sudo mv kubectl /usr/local/bin/\n\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"minikube-で-kurbernates-を稼働\"\u003eMinikube で Kurbernates を稼働\u003c/h2\u003e\n\u003cp\u003eMinikube を使って Kubernetes を稼働してみます。下記のコマンドを実行すると Virtualbox 上で仮想マシンが稼働し Kubernetes 一式も立ち上がります。\u003c/p\u003e","title":"Minikube で簡易 kubernetes 環境構築"},{"content":"こんにちは。@jedipunkz です。\n今日は Go 言語でサーバのメトリクスデータを InfluxDB に入れてリソース監視を行う方法について書きます。\nAnsible, Terraform, Chef などのソフトウェアを使ってインフラを定義するのが当たり前になった現在ですが、本当の意味でのソフトウェアによるインフラの定義ってなんだろと最近考えています。aws-sdk や fog などを使ったネイティブな言語でインフラを定義することの意味もあるように感じているからです。某サービスプロバイダのエンジニアはこうした言語によるインフラの定義の一番大きなメリットとして \u0026ldquo;再利用性\u0026rdquo; をあげていました。こうしたソフトウェアによるインフラの定義や構成を行う上で監視についてもコード化できるのでは？と考えて今回の記事に至りました。\n使うモノ https://github.com/influxdata/influxdb/tree/master/client 公式の InfluxDB Go Client です。InfluxDB 自体が Go 言語で書かれていますがクライアントも Go 言語で記述することができます。ここにあるサンプルコードをすこしいじって、今回の記事を書こうと思います。\nhttps://github.com/shirou/gopsutil @shirou さんが作られた psutil の Go 言語版です。CPU, Mem などリソースをモニタするのに便利なので利用します。\n環境構築 環境を作っていきます。InfluxDB と Chronograf を構築するのですが Docker で構築するのが簡単なのでこれを利用します。Chronograf は InfluxDB 内のデータを可視化するためのソフトウェアです。\nInfluxDB の起動 InfluxDB のコンテナを起動します。\ndocker run -p 8083:8083 -p 8086:8086 \\ -v $PWD:/var/lib/influxdb \\ influxdb Chronograf の起動 Chronograf のコンテナを起動します。\ndocker run -p 10000:10000 chronograf この時点で http://${DOCKER_HOST}:10000/ にアクセスすると Chronograf の UI を確認できます。\nInfluxDB にユーザ・データベースを作成する InfluxDB 上にユーザとデータベースを作成します。言語の中でも作ることが出来ますが、今回は手動で。 Mac OSX を使っている場合 homebrew で influxdb をインストールすることが簡単にできます。\nbrew install influxdb ユーザを作ります。\ninflux -host 192.168.99.100 -port 8086 \u0026gt; create user foo with password \u0026#39;foo\u0026#39; \u0026gt; grant all privileges to foo データベースを作ります。\ninflux -host 192.168.99.100 -port 8086 \u0026gt; CREATE DATABASE IF NOT EXISTS square_holes; Go言語で CPU 時間を取得し InfluxDB にメトリクスデータを挿入 Go 言語でメモリー使用率を取得し得られたメトリクスデータを InfluxDB に挿入するコードを書きます。\npackage main import ( \u0026#34;log\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/influxdata/influxdb/client/v2\u0026#34; \u0026#34;github.com/shirou/gopsutil/cpu\u0026#34; ) const ( MyDB = \u0026#34;square_holes\u0026#34; username = \u0026#34;foo\u0026#34; password = \u0026#34;foo\u0026#34; ) func main() { for { // Make client c, err := client.NewHTTPClient(client.HTTPConfig{ Addr: \u0026#34;http://192.168.99.100:8086\u0026#34;, Username: username, Password: password, }) if err != nil { log.Fatalln(\u0026#34;Error: \u0026#34;, err) } // Create a new point batch bp, err := client.NewBatchPoints(client.BatchPointsConfig{ Database: MyDB, Precision: \u0026#34;s\u0026#34;, }) if err != nil { log.Fatalln(\u0026#34;Error: \u0026#34;, err) } // get CPU info cp, _ := cpu.Times(true) // get CPU status info for each core var user, system, idle float64 = 0, 0, 0 for _, sub_cpu := range cp { user = user + sub_cpu.User system = system + sub_cpu.System idle = idle + sub_cpu.Idle } // Create a point and add to batch tags := map[string]string{\u0026#34;cpu\u0026#34;: \u0026#34;cpu\u0026#34;} fields := map[string]interface{}{ \u0026#34;User\u0026#34;: user / float64(len(cp)), \u0026#34;System\u0026#34;: system / float64(len(cp)), \u0026#34;Idle\u0026#34;: idle / float64(len(cp)), } pt, err := client.NewPoint(\u0026#34;cpu\u0026#34;, tags, fields, time.Now()) if err != nil { log.Fatalln(\u0026#34;Error: \u0026#34;, err) } bp.AddPoint(pt) // Write the batch c.Write(bp) time.Sleep(1 * time.Second) } } ビルドして実行すると下記のように influxdb 上のデータベースにメトリクスデータが挿入されていることを確認できます。\ninflux -host 192.168.99.100 -port 8086 -execute \u0026#39;SELECT * FROM cpu\u0026#39; -database=square_holes -precision=s | head -8 name: cpu --------- time Idle System User cpu 1469342272 20831.04296875 3700.185546875 3544.90234375 cpu 1469342273 20831.666015625 3700.302734375 3544.966796875 cpu 1469342274 20832.2109375 3700.447265625 3545.068359375 cpu 1469342275 20832.828125 3700.546875 3545.13671875 cpu 1469342291 20841.728515625 3702.482421875 3546.806640625 cpu Chronograf の UI で確認してみましょう。\n得られた CPU に関するデータが可視化されていることが確認できます。変化に乏しいグラフですが\u0026hellip;。 この辺りは CPU 時間から CPU 使用率を得るコードに書き換えるといいかもしれません。\nまとめと考察 InfluxDB の提供元が出している Telegraf というメトリクスデータの送信エージェントがありますが、同じような動きを Go 言語で簡単に開発できることが分かりました。ネイティブな言語で開発するとより柔軟にデータの送信ができることも期待できます。また冒頭に述べた通り再利用も用意になるのではと思います。インフラの状態をメトリクスデータとして時系列 DB に挿入して可視化するということは監視のコード化とも言えると思います。ただし、フレームワークが出てきてもっと簡単に書ける仕組みが出てこないと厳しい気もしますが。果たしてこれらインフラを言語で記述していくことがどれだけ有用なのかまだわかりませんが、いつか現場で実践してみたいと思います。\n","permalink":"https://jedipunkz.github.io/post/influxdb-go/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003e今日は Go 言語でサーバのメトリクスデータを InfluxDB に入れてリソース監視を行う方法について書きます。\u003c/p\u003e\n\u003cp\u003eAnsible, Terraform, Chef などのソフトウェアを使ってインフラを定義するのが当たり前になった現在ですが、本当の意味でのソフトウェアによるインフラの定義ってなんだろと最近考えています。aws-sdk や fog などを使ったネイティブな言語でインフラを定義することの意味もあるように感じているからです。某サービスプロバイダのエンジニアはこうした言語によるインフラの定義の一番大きなメリットとして \u0026ldquo;再利用性\u0026rdquo; をあげていました。こうしたソフトウェアによるインフラの定義や構成を行う上で監視についてもコード化できるのでは？と考えて今回の記事に至りました。\u003c/p\u003e\n\u003ch2 id=\"使うモノ\"\u003e使うモノ\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/influxdata/influxdb/tree/master/client\"\u003ehttps://github.com/influxdata/influxdb/tree/master/client\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e公式の InfluxDB Go Client です。InfluxDB 自体が Go 言語で書かれていますがクライアントも Go 言語で記述することができます。ここにあるサンプルコードをすこしいじって、今回の記事を書こうと思います。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/shirou/gopsutil\"\u003ehttps://github.com/shirou/gopsutil\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e@shirou さんが作られた psutil の Go 言語版です。CPU, Mem などリソースをモニタするのに便利なので利用します。\u003c/p\u003e\n\u003ch2 id=\"環境構築\"\u003e環境構築\u003c/h2\u003e\n\u003cp\u003e環境を作っていきます。InfluxDB と Chronograf を構築するのですが Docker で構築するのが簡単なのでこれを利用します。Chronograf は InfluxDB 内のデータを可視化するためのソフトウェアです。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eInfluxDB の起動\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eInfluxDB のコンテナを起動します。\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003edocker run -p 8083:8083 -p 8086:8086 \\\n      -v $PWD:/var/lib/influxdb \\\n      influxdb\n\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\n\u003cli\u003eChronograf の起動\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eChronograf のコンテナを起動します。\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003edocker run -p 10000:10000 chronograf\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eこの時点で http://${DOCKER_HOST}:10000/ にアクセスすると Chronograf の UI を確認できます。\u003c/p\u003e","title":"Go言語とInfluxDBで監視のコード化"},{"content":"こんにちは。@jedipunkz です。\n私もインフラのプロビジョニングツールとして Chef ではなく Ansible を使うことが増えたのですが、Chef を使っていた頃に同じく利用していた test-kitchen が便利だったので ansible と併用できないかと思い試してみました。test-kitchen は Docker コンテナや EC2 等を起動して Chef, Ansible 等で構成をデプロイし serverspec 等のテストツールで構成をテストできるソフトウェアです。AWS EC2 でデプロイしてもいいのですが、EC2 を起動してデプロイして失敗したら削除してのサイクルを回すことを考えるとだいぶ面倒なので Docker + test-kitchen を使ってこのサイクルを高速に回す方がメリットが大きそうです。今回は Docker + test-kitchen を使って Ansible Role (Playbook) を開発するサイクルを高速化する方法を記したいと思います。\nソフトウェアの構成 構成は、私の場合 Mac OSX を使っているので下記のとおりです。\ntest-kitchen kitchen-ansible (test-kitchen ドライバ) kitchen-docker (test-kitchen ドライバ) serverspec ansible docker (Docker-machine) VirtualBox Linux でネイティブな Docker を使っている方は以降、読み替えて下さい。読み替えるのはそれほど難しくないと思います。\nソフトウェアのインストール 今回は上記ソフトウェアのインストール方法は省きます。test-kitchen, kitchen-ansible, kitchen-docker, serverspec は Ruby で開発されたソフトウェアなので Gemfile 等で管理、ansible は pip 等でインストールしてください。\n環境作成 test-kitchen が稼働するように環境を作っていきます。 作業ディレクトリで kitchen コマンドを使って初期設定を行います。今回は試しに nginx のデプロイを実施したいと思います。\n$ mkdir -p test-kitchen/nginx test-kitchen/roles $ cd test-kitchen/nginx $ kitchen init また上記で作成した roles ディレクトリに ansible-galaxy で nginx の role を取得します。\n$ ansible-galaxy install geerlingguy.nginx -p ../roles/nginx 下記の内容を .kitchen.local.yml として保存してください。 Docker ホストの指定、Provisioner として ansible の指定、Platform として \u0026lsquo;ubuntu:16.04\u0026rsquo; の Docker コンテナの指定を行っています。\n--- driver: name: docker binary: /usr/local/bin/docker socker: tcp://192.168.99.100:2376 provisioner: name: ansible_playbook playbook: ./site.yml roles_path: ../roles host_vars_path: ./host_vars hosts: kitchen-deploy require_ansible_omnibus: false ansible_platform: ubuntu require_chef_for_busser: true platforms: - name: ubuntu driver_config: image: ubuntu:16.04 platform: ubuntu require_chef_omnibus: false suites: - name: default run_list: attributes: ここからは上記 .kitchen.local.yml ファイル内で指定したファイルの準備を行っていきます。\nsite.yml ファイルの内容を下記のように書いてください。\n--- - hosts: kitchen-deploy sudo: yes roles: - { role: geerlingguy.nginx, tags: nginx } host_vars/hosts ファイルを作成します。\u0026lsquo;host_vars\u0026rsquo; ディレクトリは手動で作成してください。\nlocalhost ansible_connection=local [kitchen-deploy] localhost 次に serverspec で行うテストの内容を作成します。 serverspec-init コマンドではインタラクティブに回答しますが、SSH ではなく EXEC(Local) を選択することに注意してください。\n$ mkdir -p test/integration/default/serverspec $ cd test/integration/default/serverspec $ serverspec-init # \u0026lt;--- インタラクティブに回答 : 1) UNIX, 2) EXEC(Local) を選択 $ rm localhost/sample_spec.rb # \u0026lt;--- 必要ないので削除 test/integration/default/serverspec/localhost/nginx_spec.rb として下記の内容を試しに書いてみましょう。\nrequire \u0026#39;spec_helper\u0026#39; describe package(\u0026#39;nginx\u0026#39;) do it { should be_installed } end describe service(\u0026#39;nginx\u0026#39;) do it { should be_enabled } it { should be_running } end describe file(\u0026#39;/etc/nginx/nginx.conf\u0026#39;) do it { should be_file } end 下記のようなファイルとディレクトリ構成になっていることを確認しましょう。\n. ├── nginx │ ├── chefignore │ ├── host_vars │ │ └── hosts │ ├── site.yml │ └── test │ └── integration │ └── default │ ├── Rakefile │ └── serverspec │ ├── localhost │ │ └── nginx_spec.rb │ └── spec_helper.rb └── roles └── geerlingguy.nginx ├── README.md \u0026lt;省略\u0026gt; デプロイ・テストを実行する 環境作成が完了したの Docker コンテナを起動し Ansible でデプロイ、その後 Serverspec でテストしてみます。\n$ cd test-kitchen $ kitchen create # \u0026lt;--- Docker コンテナ起動 $ kitchen setup # \u0026lt;--- Ansible デプロイ $ kitchen verify # \u0026lt;--- Serverspec テスト $ kitchen destroy # \u0026lt;--- コンテナ削除 $ kitchen test # \u0026lt;--- 上の4つのコマンドを一気に実行 まとめ Ansible でも test-kitchen を使ってデプロイ・テストが出来ることが分かりました。インスタンスを使ってデプロイ・テストを実施するよりコンテナを使うほうが失敗した際に削除・起動するのも一瞬で終わりますし Ansible 開発が高速化できることも実際に触っていただいてわかっていただけると思います。\nただ上記の手順ではコンテナの中に Ruby, Chef も一緒にインストールされてしまいます。 test-kitchen 的には下記の記述を .kitchen.local.yml の provisioner: の欄に記述すると Chef のインストールは省けるはず (Ruby は Serverspec で用いる) のですが今現在 (2016/7中旬) では NG でした。これが正常に機能するようになるともっと高速にコンテナデプロイが完了すると思うので残念です。\nrequire_chef_for_busser: false require_ruby_for_busser: true ","permalink":"https://jedipunkz.github.io/post/test-kitchen-with-ansible/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003e私もインフラのプロビジョニングツールとして Chef ではなく Ansible を使うことが増えたのですが、Chef を使っていた頃に同じく利用していた test-kitchen が便利だったので ansible と併用できないかと思い試してみました。test-kitchen は Docker コンテナや EC2 等を起動して Chef, Ansible 等で構成をデプロイし serverspec 等のテストツールで構成をテストできるソフトウェアです。AWS EC2 でデプロイしてもいいのですが、EC2 を起動してデプロイして失敗したら削除してのサイクルを回すことを考えるとだいぶ面倒なので Docker + test-kitchen を使ってこのサイクルを高速に回す方がメリットが大きそうです。今回は Docker + test-kitchen を使って Ansible Role (Playbook) を開発するサイクルを高速化する方法を記したいと思います。\u003c/p\u003e\n\u003ch2 id=\"ソフトウェアの構成\"\u003eソフトウェアの構成\u003c/h2\u003e\n\u003cp\u003e構成は、私の場合 Mac OSX を使っているので下記のとおりです。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003etest-kitchen\u003c/li\u003e\n\u003cli\u003ekitchen-ansible (test-kitchen ドライバ)\u003c/li\u003e\n\u003cli\u003ekitchen-docker (test-kitchen ドライバ)\u003c/li\u003e\n\u003cli\u003eserverspec\u003c/li\u003e\n\u003cli\u003eansible\u003c/li\u003e\n\u003cli\u003edocker (Docker-machine)\u003c/li\u003e\n\u003cli\u003eVirtualBox\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eLinux でネイティブな Docker を使っている方は以降、読み替えて下さい。読み替えるのはそれほど難しくないと思います。\u003c/p\u003e\n\u003ch2 id=\"ソフトウェアのインストール\"\u003eソフトウェアのインストール\u003c/h2\u003e\n\u003cp\u003e今回は上記ソフトウェアのインストール方法は省きます。test-kitchen, kitchen-ansible, kitchen-docker, serverspec は Ruby で開発されたソフトウェアなので Gemfile 等で管理、ansible は pip 等でインストールしてください。\u003c/p\u003e","title":"Test-Kitchen, Docker で Ansible Role 開発サイクル高速化！"},{"content":"こんにちは。@jedipunkz です。\n今回は StackStorm (https://stackstorm.com/) というイベントドリブンオートメーションツールを使ってみましたので 紹介したいと思います。\nクラウドとプロビジョニングツールの登場で昨今はエンジニアがほぼ全ての操作を自動化出来るようになりました。 ですが監視についてはどうでしょうか？監視システムを自動で構築することが出来ても障害発生時に対応を行う のは手動になっていませんでしょうか。もちろんクラスタ組んでいれば大抵のアラートは放置出来ますが、クラスタ を組むことが出来ないような箇所はクラウドを使ってもどうしても出てきます。\nそこで登場するのが今回紹介する StackStorm のようなツールかなぁと考えるようになりました。\nインストール インストール手順は下記の URL にあります。\nhttps://docs.stackstorm.com/install/index.html\n自分は CentOS7 を使ったので下記のワンライナーでインストールできました。 password は任意のものを入れてください。\ncurl -sSL https://stackstorm.com/packages/install.sh | bash -s -- --user=st2admin --password=foo MongoDB, postgreSQL が依存してインストールされます。\n80番ポートで下記のような WEB UI も起動します。\nStackStorm の基本 基本を知るために幾つかの要素について説明していきます。\nまず CLI で操作するために TOKEN を取得して環境変数にセットする必要があります。 上記で設定したユーザ名・パスワードを入力してください。\nexport ST2_AUTH_TOKEN=`st2 auth -t -p foo st2admin` Action Action はイベントが発生した際に実行できるアクションになります。早速アクションの一覧を取得してみましょう。\n$ st2 action list +---------------------------------+---------+-------------------------------------------------------------+ | ref | pack | description | +---------------------------------+---------+-------------------------------------------------------------+ | chatops.format_execution_result | chatops | Format an execution result for chatops | | chatops.post_message | chatops | Post a message to stream for chatops | | chatops.post_result | chatops | Post an execution result to stream for chatops | \u0026lt;省略\u0026gt; | core.http | core | Action that performs an http request. | | core.local | core | Action that executes an arbitrary Linux command on the | | | | localhost. | | core.local_sudo | core | Action that executes an arbitrary Linux command on the | | | | localhost. | | core.remote | core | Action to execute arbitrary linux command remotely. | | core.remote_sudo | core | Action to execute arbitrary linux command remotely. | | core.sendmail | core | This sends an email | | core.windows_cmd | core | Action to execute arbitrary Windows command remotely. | \u0026lt;省略\u0026gt; | linux.cp | linux | Copy file(s) | | linux.diag_loadavg | linux | Diagnostic workflow for high load alert | | linux.dig | linux | Dig action | \u0026lt;省略\u0026gt; | st2.kv.get | st2 | Get value from datastore | | st2.kv.set | st2 | Set value in datastore | +---------------------------------+---------+-------------------------------------------------------------+ 上記のように Linux のコマンドや ChatOps, HTTP でクエリを投げるもの、Key Value の読み書きを行うモノまであります。 上記はかなり咲楽して貼り付けたので本来はもっと沢山のアクションがあります。\nTrigger Trigger は Action を実行する際のトリガになります。同様に一覧を見てみましょう。\n$ st2 trigger list +--------------------------------------+-------+----------------------------------------------------------------+ | ref | pack | description | +--------------------------------------+-------+----------------------------------------------------------------+ | core.st2.generic.actiontrigger | core | Trigger encapsulating the completion of an action execution. | | core.st2.IntervalTimer | core | Triggers on specified intervals. e.g. every 30s, 1week etc. | | core.st2.generic.notifytrigger | core | Notification trigger. | | core.st2.DateTimer | core | Triggers exactly once when the current time matches the | | | | specified time. e.g. timezone:UTC date:2014-12-31 23:59:59. | | core.st2.action.file_writen | core | Trigger encapsulating action file being written on disk. | | core.st2.CronTimer | core | Triggers whenever current time matches the specified time | | | | constaints like a UNIX cron scheduler. | | core.st2.key_value_pair.create | core | Trigger encapsulating datastore item creation. | | core.st2.key_value_pair.update | core | Trigger encapsulating datastore set action. | | core.st2.key_value_pair.value_change | core | Trigger encapsulating a change of datastore item value. | | core.st2.key_value_pair.delete | core | Trigger encapsulating datastore item deletion. | | core.st2.sensor.process_spawn | core | Trigger indicating sensor process is started up. | | core.st2.sensor.process_exit | core | Trigger indicating sensor process is stopped. | | core.st2.webhook | core | Trigger type for registering webhooks that can consume | | | | arbitrary payload. | | linux.file_watch.line | linux | Trigger which indicates a new line has been detected | +--------------------------------------+-------+----------------------------------------------------------------+ CronTimer はその名の通り Cron であることが分かります。IntervalTimer は同じように一定時間間隔で実行するようです。 その他 webhook や Key Value のペアが生成・削除・変更されたタイミング、等あります。\nRule Rule は Trigger が発生して Action を実行する際のルールを記述するものになります。\n$ st2 rule list +----------------+---------+-------------------------------------------------+---------+ | ref | pack | description | enabled | +----------------+---------+-------------------------------------------------+---------+ | chatops.notify | chatops | Notification rule to send results of action | True | | | | executions to stream for chatops | | +----------------+---------+-------------------------------------------------+---------+ 初期では上記の chatops.notify のみがあります。\n実際に使ってみる core.local というアクションを実行してみました。\n$ st2 run core.local -- uname -a id: 5774c022e138230c66f2eefc status: succeeded parameters: cmd: uname -a result: failed: false return_code: 0 stderr: \u0026#39;\u0026#39; stdout: \u0026#39;Linux localhost.localdomain 3.10.0-327.el7.x86_64 #1 SMP Thu Nov 19 22:10:57 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux\u0026#39; succeeded: true stdout に実行結果が出力されています。また下記のように実行結果の一覧を得ることが出来ます。\n$ st2 execution list +----------------------------+---------------+--------------+-------------------------+-----------------------------+-------------------------------+ | id | action.ref | context.user | status | start_timestamp | end_timestamp | +----------------------------+---------------+--------------+-------------------------+-----------------------------+-------------------------------+ | 5774bdbee138230c66f2eeef | core.local | st2admin | succeeded (0s elapsed) | Thu, 30 Jun 2016 06:35:42 | Thu, 30 Jun 2016 06:35:42 UTC | | | | | | UTC | | +----------------------------+---------------+--------------+-------------------------+-----------------------------+-------------------------------+ 応用した使い方 上記のように core.local, core.remote 等でホスト上のコマンドを実行できることが分かりました。 ここで応用した使い方をしてみます。と言いますか、上記の基本的な使い方だけでは StackStorm を 使うメリットが無いように思えます。\n下記のような Rule を作成します。ファイル名は st2_sample_rule_webhook_remote_command.yaml とします。\n--- name: \u0026#34;st2_sample_rule_webhook_remote_command\u0026#34; pack: \u0026#34;examples\u0026#34; description: \u0026#34;Sample rule of webhook.\u0026#34; enabled: true trigger: type: \u0026#34;core.st2.webhook\u0026#34; parameters: url: \u0026#34;restart\u0026#34; criteria: action: ref: \u0026#34;core.remote\u0026#34; parameters: hosts: \u0026#34;10.0.1.250\u0026#34; username: \u0026#34;thirai\u0026#34; private_key: \u0026#34;/root/.ssh/id_rsa\u0026#34; cmd: \u0026#34;sudo service cron restart\u0026#34; StackStorm の基本要素 Action, Criteria(Rule の基準), Trigger から成っていることが分かります。 Triger は webhoook です。url: \u0026ldquo;restart\u0026rdquo; となっているのは URL : https://\u0026lt;stackstorm_ip_addr\u0026gt;/api/v1/webhooks/restart という名前で アクセスを受けるようになるという意味です。criteria は今回は無条件で action を実行したいので空行にします。 action では core.remote が選択されていて hosts: \u0026lsquo;10.0.1.250\u0026rsquo; に username で SSH してコマンドを実行しています。\n要するに https://\u0026lt;stackstorm_ip_addr\u0026gt;/api/v1/webhooks/restart というアドレスでリクエストを受けると 10.0.1.250 に \u0026lsquo;foo\u0026rsquo; というユーザで SSH してコマンドを実行する、というルールになっています。\n下記のコマンドで上記の yaml ファイルをルールに読み込みます。\nst2 rule create st2_sample_rule_webhook_remote_command.yaml 実際にリクエストを投げてみましょう。Token は読み替えてください。\ncurl -k https://localhost/api/v1/webhooks/restart -d \u0026#39;{}\u0026#39; -H \u0026#39;Content-Type: application/json\u0026#39; -H \u0026#39;X-Auth-Token: \u0026lt;Your_Token\u0026gt;\u0026#39; するとリモートホストで \u0026lsquo;cron\u0026rsquo; プロセスの再起動が掛かります。\nまとめと考察 StackStorm は紹介した以外にも沢山のアクションがあり応用が効きます。また監視ツールでアラートが発生した際に webhook 通知するようにして 障害対応を自動で行うといった操作も可能な事がわかりました。ChatOps でも応用が可能なことが分かります。従来、ChatOps ではリモートホストで コマンドなどを実行しようとした場合には Hubot 等のプロセスが稼働しているホストもしくはそのホストから SSH 出来るホストで実行する必要がありましたが StackStorm を介すことで実行結果の閲覧やルールに従った実行等が可能になります。\n自分はまだ少しのアクション・ルールを試用しただけなのですが、他に良い運用上の応用例がないか探してみようと思います。\n","permalink":"https://jedipunkz.github.io/post/stackstorm/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003e今回は StackStorm (\u003ca href=\"https://stackstorm.com/\"\u003ehttps://stackstorm.com/\u003c/a\u003e) というイベントドリブンオートメーションツールを使ってみましたので\n紹介したいと思います。\u003c/p\u003e\n\u003cp\u003eクラウドとプロビジョニングツールの登場で昨今はエンジニアがほぼ全ての操作を自動化出来るようになりました。\nですが監視についてはどうでしょうか？監視システムを自動で構築することが出来ても障害発生時に対応を行う\nのは手動になっていませんでしょうか。もちろんクラスタ組んでいれば大抵のアラートは放置出来ますが、クラスタ\nを組むことが出来ないような箇所はクラウドを使ってもどうしても出てきます。\u003c/p\u003e\n\u003cp\u003eそこで登場するのが今回紹介する StackStorm のようなツールかなぁと考えるようになりました。\u003c/p\u003e\n\u003ch2 id=\"インストール\"\u003eインストール\u003c/h2\u003e\n\u003cp\u003eインストール手順は下記の URL にあります。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://docs.stackstorm.com/install/index.html\"\u003ehttps://docs.stackstorm.com/install/index.html\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e自分は CentOS7 を使ったので下記のワンライナーでインストールできました。\npassword は任意のものを入れてください。\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003ecurl -sSL https://stackstorm.com/packages/install.sh | bash -s -- --user=st2admin --password=foo\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eMongoDB, postgreSQL が依存してインストールされます。\u003c/p\u003e\n\u003cp\u003e80番ポートで下記のような WEB UI も起動します。\u003c/p\u003e\n\u003cimg src=\"http://jedipunkz.github.io/pix/stackstorm.png\" width=\"70%\"\u003e\n\u003ch2 id=\"stackstorm-の基本\"\u003eStackStorm の基本\u003c/h2\u003e\n\u003cp\u003e基本を知るために幾つかの要素について説明していきます。\u003c/p\u003e\n\u003cp\u003eまず CLI で操作するために TOKEN を取得して環境変数にセットする必要があります。\n上記で設定したユーザ名・パスワードを入力してください。\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eexport ST2_AUTH_TOKEN=`st2 auth -t -p foo st2admin`\n\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\n\u003cli\u003eAction\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAction はイベントが発生した際に実行できるアクションになります。早速アクションの一覧を取得してみましょう。\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e$ st2 action list\n+---------------------------------+---------+-------------------------------------------------------------+\n| ref                             | pack    | description                                                 |\n+---------------------------------+---------+-------------------------------------------------------------+\n| chatops.format_execution_result | chatops | Format an execution result for chatops                      |\n| chatops.post_message            | chatops | Post a message to stream for chatops                        |\n| chatops.post_result             | chatops | Post an execution result to stream for chatops              |\n\u0026lt;省略\u0026gt;\n| core.http                       | core    | Action that performs an http request.                       |\n| core.local                      | core    | Action that executes an arbitrary Linux command on the      |\n|                                 |         | localhost.                                                  |\n| core.local_sudo                 | core    | Action that executes an arbitrary Linux command on the      |\n|                                 |         | localhost.                                                  |\n| core.remote                     | core    | Action to execute arbitrary linux command remotely.         |\n| core.remote_sudo                | core    | Action to execute arbitrary linux command remotely.         |\n| core.sendmail                   | core    | This sends an email                                         |\n| core.windows_cmd                | core    | Action to execute arbitrary Windows command remotely.       |\n\u0026lt;省略\u0026gt;\n| linux.cp                        | linux   | Copy file(s)                                                |\n| linux.diag_loadavg              | linux   | Diagnostic workflow for high load alert                     |\n| linux.dig                       | linux   | Dig action                                                  |\n\u0026lt;省略\u0026gt;\n| st2.kv.get                      | st2     | Get value from datastore                                    |\n| st2.kv.set                      | st2     | Set value in datastore                                      |\n+---------------------------------+---------+-------------------------------------------------------------+\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e上記のように Linux のコマンドや ChatOps, HTTP でクエリを投げるもの、Key Value の読み書きを行うモノまであります。\n上記はかなり咲楽して貼り付けたので本来はもっと沢山のアクションがあります。\u003c/p\u003e","title":"イベントドリブンな StackStorm で運用自動化"},{"content":"こんにちは。@jedipunkz です。\n今回は DC/OS (https://dcos.io/) を Vagrant を使って構築し評価してみようと思います。 DC/OS はその名の通りデータセンタ OS として利用されることを期待され開発された OS で内部で Docker と Mesos が稼働しています。\n一昔前に Mesos のマルチノード構成は構築したことあったのですが、DC/OS はデプロイ方法が全く変わっていました。 はじめに想定する構成から説明していきます。\n想定する構成 本来 DC/OS は public, private ネットワーク構成ですが利用するレポジトリではこのような構成が想定されていました。\n+----+ +----+ +----+ +------+ | m1 | | a1 | | p1 | | boot | +----+ +----+ +----+ +------+ | | | | +------+------+------+--------- 192.168.65/24 各ノードは下記の通り動作します。\nm1 : Mesos マスタ, Marathon a1 : Mesos スレーブ(Private Agent) p1 : Mesos スレーブ(Public Agent) boot : DC/OS インストレーションノード 前提の環境 Vagrant が動作するマシンであれば問題ないと思いますが私は下記の構成で利用しました。 比較的たくさんのマシンリソースを使うのでメモリ 8GB はほしいところです。\nMac OSX Vagrant Virtualbox 事前の準備 事前の手順を記します。\n予め用意された dcos-vagrant を取得する $ git clone https://github.com/dcos/dcos-vagrant Mac OSX の hosts ファイルをダイナミック編集する Vagrant プラグインをインストール $ vagrant plugin install vagrant-hostmanager 構築手順 それでは構築手順です。\nDC/OS が利用する config を環境変数に指定指定 $ export DCOS_CONFIG_PATH=etc/config-1.7.yaml DC/OS をレポジトリのルートディレクトリに保存 DC/OS 1.7.0 (Early Access)(2016/06/21現在) をダウンロードしてレポジトリのルートディレクトリにインストール\n$ cd dcos-vagrant $ wget https://downloads.dcos.io/dcos/EarlyAccess/dcos_generate_config.sh VagrantConfig を作成 \u0026lsquo;VagrantConfig.yaml.example\u0026rsquo; というファイルがレポジトリ内ルートディレクトリにあるのでこれを元に下記の通りファイルを生成。元のファイルのままだと比較的大きな CPU/Mem リソースが必要になるので環境に合わせてリソース量を指定。\nm1: ip: 192.168.65.90 cpus: 1 memory: 512 type: master a1: ip: 192.168.65.111 cpus: 1 memory: 1024 memory-reserved: 512 type: agent-private p1: ip: 192.168.65.60 cpus: 1 memory: 1024 memory-reserved: 512 type: agent-public aliases: - spring.acme.org - oinker.acme.org boot: ip: 192.168.65.50 cpus: 1 memory: 1024 type: boot デプロイを実施 $ vagrant up m1 a1 p1 boot DC/OS の UI インストールが完了すると下記のアドレスで DC/OS の UI にアクセスできます。\nhttp://m1.dcos/\nMarathon の UI 下記のアドレスで Marathon の UI にアクセスできます Marathon は分散型の Linux Init 機構のようなものです。\nhttp://m1.dcos:8080/\nChronos の UI 下記のアドレスで Chronos の UI にアクセスできる Chronos は分散型のジョブスケジューラーであり cron のようなものです。\nhttp://a1.dcos:1370/\nMarathon で redis サーバを立ち上げる テストで redis サーバを立ち上げてみる。Mesos-Slave (今回の環境だと a1 ホスト) 上に Docker コンテナとして redis サーバが立ち上がることになる。\nMarathon の UI にて \u0026ldquo;Create Application\u0026rdquo; を選択 General タブの ID に任意の名前を入力 General タブの Command 欄に \u0026lsquo;redis-server\u0026rsquo; を入力 Docker Container タブの Image 欄に \u0026lsquo;redis\u0026rsquo; を入力 \u0026lsquo;Create Application\u0026rsquo; を選択 結果、下記の通りアプリケーションが生成される\n※ 20160625 下記追記\n構成 ここからは Mesosphere DC/OS の内部構成を理解していきます。主となる mesos-master, mesos-slave の構成は下記の通り。\nMesos-Master Node 構成 +--------------+ | mesos-master | +--------------+ +-----------+ +----------+ +-------------+ +-----------+ +-----------+ | zookeeper | | mesos-dns | | marathon | | adminRouter | | minuteman | | exhibitor | +--------------+ +-----------+ +----------+ +-------------+ +-----------+ +-----------+ | mesos-master node | +-------------------------------------------------------------------------------------+ Mesos-Slave (Mesos-Agent) Node 構成 +-------------+ +---+---+---+---+ | m-executor | | c | c | c | c | +-------------+ +---+---+---+---+ | mesos-slave | | docker-daemon | +-------------------------------+ | mesos-slave node | +-------------------------------+ 各プロセスの役割 上記の図の各要素を参考資料を元にまとめました。\nmesos-master masos-slave node からの情報を受け取り mesos-slave へプロセスを分配する 役割。mesos-master は zookeeper によってリーダー選出により冗長構成が保 たれている。\nmesos-dns mesos フレームワーク内での DNS 解決を行うプロセス。各 mesos-master ノー ド上に稼働している。通常の DNS でいうコンテンツ DNS (Authoritative DNS)になっており mesos-master からクラスタ情報を受け取って DNS レコー ド登録、それを mesos-slave が DNS 参照する。mesos-slave が内部レコード に無い DNS 名を解決しに来た際にはインターネット上の root DNS へ問い合 わせ実施。\nmarathon コンテナオーケストレーションを司り mesos-slave へ支持を出しコンテナを 稼働する役割。各 mesos-master 上で稼働し zookeeper 越しに mesos-master のカレントリーダを探しだしリクエストを創出。他に下記の機能を持っている。 \u0026lsquo;HA\u0026rsquo;, \u0026lsquo;ロードバランス\u0026rsquo;, \u0026lsquo;アプリのヘルスチェック\u0026rsquo;, \u0026lsquo;Web UI\u0026rsquo;, \u0026lsquo;REST API\u0026rsquo;, \u0026lsquo;Metrics\u0026rsquo;。\nadminRouter 実態は nginx。各サービスの認証と Proxy の役割を担っている。\nminuteman L4 レイヤのロードバランサ。各 mesos-master ノードで稼働。\nzookeeper mesos-master プロセスを冗長構成させるためのソフトウェア。\nexhibitor zookeeper のコンフィギュレーションを実施。\nmesos-slave Task を実行する役割。内部で meosos-executor (上記 m-executor) を実行し ている。\nm-executor (mesos-executor) mesos-slave ノード上でサービスのための TASK を稼働させる。\n起動シーケンス ここからは mesos-master, mesos-slave の起動シーケンスについて、まとめてきます。\nmesos-master\nexhibitor が起動し zookeeper のコンフィギュレーションを実施し zookeeper を起動 mesos-master が起動。自分自身をレジスト後、他の mesos-master ノードを探索 mesos-dns が起動 mesos-dns が leader.mesos にカレントリーダの mesos-master を登録 marathon が起動し zookeeper 越しに mesos-master を探索。 adminRouter が起動し各 UI (mesos, marathon, exhibitor) が閲覧可能に。 mesos-slave\nleader.mesos に ping を打って mesos-master のカレントリーダを見つけ出し mesos-slave 稼働。 mesos-master に対して自分自身を \u0026lsquo;agent\u0026rsquo; として登録。 mesos-master はその登録された IP アドレスを元に接続を試みる mesos-slave が TASK 実行可能な状態に まとめと考察 一昔前の Mesos + Marathon + Chronos とはだいぶデプロイ方法が変わっていた。だが構成には大きな変化は見られない。 AWS のような public, private ネットワークが別れたプラットフォームでは mesos-slave (DC/OS 的には Agent とも呼ぶ)は public agent, private agent として別けて管理される模様。public agent は AWS の ELB 等で分散され各コンテナ上のアプリにクエリがインターネットからのリクエストに応える。private agent はプライベートネットワーク上に配置されて public agent からのリクエストにも応える。また、mesos-master 達は別途 admin なネットワークに配置するのが Mesosphare の推奨らしい。\nだがしかし public, private を別けずに DC/OS を構成することも可能なように思えた。下記のように p1 を削除して構成して物理・仮想ロードバランサでリクエストを private agent に送出することでも DC/OS は機能するので。\n$ vagrant up m1 a1 boot ちなみに a2, a3, \u0026hellip; と数を増やすことで private agent ノードを増やすことが可能。\nあとマニュアルインストール手順(公式)を実施してみて解ったが、pulic, private ネットワークを各ノードにアタッチして mesos-master, mesos-slave, またその他の各機能はプライベートネットワークを、外部からのリクエストに応えるためのパブリックネットワーク、といった構成も可能でした。\n参考 URL 手順は右記のものを利用。 https://github.com/dcos/dcos-vagrant/blob/master/docs/deploy.md https://docs.mesosphere.com/1.7/overview/architecture/ https://docs.mesosphere.com/1.7/overview/security/ ","permalink":"https://jedipunkz.github.io/post/mesos-dcos-vagrant/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003e今回は DC/OS (\u003ca href=\"https://dcos.io/\"\u003ehttps://dcos.io/\u003c/a\u003e) を Vagrant を使って構築し評価してみようと思います。\nDC/OS はその名の通りデータセンタ OS として利用されることを期待され開発された OS で内部で\nDocker と Mesos が稼働しています。\u003c/p\u003e\n\u003cp\u003e一昔前に Mesos のマルチノード構成は構築したことあったのですが、DC/OS はデプロイ方法が全く変わっていました。\nはじめに想定する構成から説明していきます。\u003c/p\u003e\n\u003ch2 id=\"想定する構成\"\u003e想定する構成\u003c/h2\u003e\n\u003cp\u003e本来 DC/OS は public, private ネットワーク構成ですが利用するレポジトリではこのような構成が想定されていました。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e+----+ +----+ +----+ +------+\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e| m1 | | a1 | | p1 | | boot |\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e+----+ +----+ +----+ +------+\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e|      |      |      |\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e+------+------+------+--------- 192.168.65/24\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e各ノードは下記の通り動作します。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003em1 : Mesos マスタ, Marathon\u003c/li\u003e\n\u003cli\u003ea1 : Mesos スレーブ(Private Agent)\u003c/li\u003e\n\u003cli\u003ep1 : Mesos スレーブ(Public Agent)\u003c/li\u003e\n\u003cli\u003eboot : DC/OS インストレーションノード\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"前提の環境\"\u003e前提の環境\u003c/h2\u003e\n\u003cp\u003eVagrant が動作するマシンであれば問題ないと思いますが私は下記の構成で利用しました。\n比較的たくさんのマシンリソースを使うのでメモリ 8GB はほしいところです。\u003c/p\u003e","title":"Vagrant で Mesosphere DC/OS を構築"},{"content":"こんにちは。@jedipunkz です。\nInfluxdb が Influxdata (https://influxdata.com/) として生まれ変わり公式の メトリクス送信エージェント Telegraf と可視化ツール Chronograf をリリースしたので 使ってみました。\n3つのツールの役割は下記のとおりです。\nChronograf : 可視化ツール, Grafana 相当のソフトウェアです Telegraf : メトリクス情報を Influxdb に送信するエージェント Influxdb : メトリクス情報を格納する時系列データベース 以前に cAdvisor, influxdb, grafana を使って Docker コンテナのメトリクスを可視 化する記事を書きましたが telegraf を使うとサーバ情報と合わせて Docker コンテナ のメトリクスも influxdb に送信することが出来ます。個人的にはそのコンテナ情報の 扱いもサーバ情報と同様に扱ってくれる点に期待しつつ、評価してみました。\n今回の環境 今回は Ubuntu 15.04 vivid64 を使ってテストしています。\ninfluxdb をインストールして起動 最新リリース版の deb パッケージが用意されていたのでこれを使いました。\nwget http://influxdb.s3.amazonaws.com/influxdb_0.9.5.1_amd64.deb sudo dpkg -i influxdb_0.9.5.1_amd64.deb sudo service influxdb start telegraf のインストールと起動 こちらも deb パッケージで。\nwget http://get.influxdb.org/telegraf/telegraf_0.2.4_amd64.deb sudo dpkg -i telegraf_0.2.4_amd64.deb コンフィギュレーションですが今回は CPU, Disk, Net, Docker のメトリクス情報を送 信するようにしました。\n[agent] interval = \u0026#34;0.1s\u0026#34; [outputs] [outputs.influxdb] urls = [\u0026#34;http://localhost:8086\u0026#34;] database = \u0026#34;telegraf-test\u0026#34; user_agent = \u0026#34;telegraf\u0026#34; [plugins] [[plugins.cpu]] percpu = true totalcpu = false drop = [\u0026#34;cpu_time*\u0026#34;] [[plugins.disk]] [plugins.disk.tagpass] fstype = [ \u0026#34;ext4\u0026#34;, \u0026#34;xfs\u0026#34; ] #path = [ \u0026#34;/home*\u0026#34; ] [[plugins.disk]] pass = [ \u0026#34;disk_inodes*\u0026#34; ] [[plugins.docker]] [[plugins.net]] interfaces = [\u0026#34;eth0\u0026#34;] 他にも色々メトリクス情報を取得できそうです、下記のサイトを参考にしてみてください。 https://github.com/influxdb/telegraf/blob/0.3.0/CONFIGURATION.md\ntelegraf を起動します。Docker コンテナのメトリクスを取得するために root ユーザ で起動する必要があります。\nsudo telegraf -config telegraf.conf chronograf のインストールと起動 こちらも deb でインストールします。\nwget https://s3.amazonaws.com/get.influxdb.org/chronograf/chronograf_0.4.0_amd64.deb sudo dpkg -i chronograf_0.4.0_amd64.deb sudo /opt/chronograf/chronograf -sample-config \u0026gt; /opt/chronograf/config.toml sudo service chronograf start グラフの描画 この状態でブラウザでアクセスしてみましょう。\nhttp://\u0026lt;ホストのIPアドレス\u0026gt;:10000/\nアクセスすると簡単なガイドが走りますのでここでは設定方法は省きます。Grafana を使った場合と 同様に気をつけるポイントは下記のとおりです。\n\u0026lsquo;filter by\u0026rsquo; に描画したいリソース名を選択(CPU,Disk,Net,Dockerの各リソース) Database に telegraf.conf に記した \u0026rsquo;telegraf-test\u0026rsquo; を選択 すると下記のようなグラフやダッシュボードが作成されます。下記は CPU 使用率をグ ラフ化したものです。\nこちらは Docker 関連のグラフ。\n複数のグラフを1つのダッシュボードにまとめることもできるようです。\nまとめ +++\n個人的には Grafana の UI はとてもわかりずらかったので公式の可視化ツールが出てきて良かった と思っています。操作もとても理解しやすくなっています。Telegraf についても公式のメトリクス 情報送信エージェントということで安心感があります。また Grafana は別途 HTTP サー バが必要でしたが Chronograf は HTTP サーバも内包しているのでセットアップが簡単 でした。\nただ configuration guide がまだまだ説明不十分なので凝ったことをしようすとする とソースを読まなくてはいけないかもしれません。\nいずれにしてもサーバのメトリクス情報と共に cAdvisor 等のソフトウェアを用いなく てもサーバ上で稼働しているコンテナ周りの情報も取得できたので個人的にはハッピー。 cAdvisor でしか取得できない情報もありそうですが今後、導入を検討する上で評価し ていきたいと思います。\n","permalink":"https://jedipunkz.github.io/post/2015/12/28/chronograf-telegraf-influxdb/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003eInfluxdb が Influxdata (\u003ca href=\"https://influxdata.com/\"\u003ehttps://influxdata.com/\u003c/a\u003e) として生まれ変わり公式の\nメトリクス送信エージェント Telegraf と可視化ツール Chronograf をリリースしたので\n使ってみました。\u003c/p\u003e\n\u003cp\u003e3つのツールの役割は下記のとおりです。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eChronograf : 可視化ツール, Grafana 相当のソフトウェアです\u003c/li\u003e\n\u003cli\u003eTelegraf : メトリクス情報を Influxdb に送信するエージェント\u003c/li\u003e\n\u003cli\u003eInfluxdb : メトリクス情報を格納する時系列データベース\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e以前に cAdvisor, influxdb, grafana を使って Docker コンテナのメトリクスを可視\n化する記事を書きましたが telegraf を使うとサーバ情報と合わせて Docker コンテナ\nのメトリクスも influxdb に送信することが出来ます。個人的にはそのコンテナ情報の\n扱いもサーバ情報と同様に扱ってくれる点に期待しつつ、評価してみました。\u003c/p\u003e\n\u003ch2 id=\"今回の環境\"\u003e今回の環境\u003c/h2\u003e\n\u003cp\u003e今回は Ubuntu 15.04 vivid64 を使ってテストしています。\u003c/p\u003e\n\u003ch2 id=\"influxdb-をインストールして起動\"\u003einfluxdb をインストールして起動\u003c/h2\u003e\n\u003cp\u003e最新リリース版の deb パッケージが用意されていたのでこれを使いました。\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003ewget http://influxdb.s3.amazonaws.com/influxdb_0.9.5.1_amd64.deb\nsudo dpkg -i influxdb_0.9.5.1_amd64.deb\nsudo service influxdb start\n\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"telegraf-のインストールと起動\"\u003etelegraf のインストールと起動\u003c/h2\u003e\n\u003cp\u003eこちらも deb パッケージで。\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003ewget http://get.influxdb.org/telegraf/telegraf_0.2.4_amd64.deb\nsudo dpkg -i telegraf_0.2.4_amd64.deb\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eコンフィギュレーションですが今回は CPU, Disk, Net, Docker のメトリクス情報を送\n信するようにしました。\u003c/p\u003e","title":"Chronograf, Telegraf, Influxdbでサーバとコンテナ情報を可視化する"},{"content":"こんにちは。@jedipunkz です。\n今回は Weave というコンテナ間のネットワークを提供してくれる Docker のネットワークプラ グインを使ってみました。下記のような沢山の機能があるようです。\nFast Data Path Docker Network Plugin Security Dynamic Netwrok Attachment Service Binding Fault Tolerance etc \u0026hellip; この記事では上から幾つか抜粋して、Weave ってどのように動かせるのか？を解説します。 そこから Weave が一体ナニモノなのか理解できればなぁと思います。\nVagrant を使った構成 この記事では下記の構成を作って色々と試していきます。使う技術は\nVagrant Docker Weave です。\n+---------------------+ +---------------------+ +---------------------+ | docker container a1 | | docker container a2 | | docker container a3 | +---------------------+ +---------------------+ +---------------------+ | vagrant host 1 | | vagrant host 2 | | vagrant host 3 | +---------------------+-+---------------------+-+---------------------+ | Mac or Windows | +---------------------------------------------------------------------+ 特徴としては\n作業端末(Mac or Windows or Linux)上で Vagrant を動作させる 各 Vagrant VM 同士はホスト OS のネットワークインターフェース上で疎通が取れる です。\nVagrantfile の作成と host1,2,3 の起動 上記の3台の構成を下記の Vagrantfile で構築します。\nVagrant.configure(2) do |config| config.vm.box = \u0026#34;ubuntu/vivid64\u0026#34; config.vm.define \u0026#34;host1\u0026#34; do |server| server.vm.network \u0026#34;private_network\u0026#34;, ip: \u0026#34;192.168.33.11\u0026#34; end config.vm.define \u0026#34;host2\u0026#34; do |server| server.vm.network \u0026#34;private_network\u0026#34;, ip: \u0026#34;192.168.33.12\u0026#34; end config.vm.define \u0026#34;host3\u0026#34; do |server| server.vm.network \u0026#34;private_network\u0026#34;, ip: \u0026#34;192.168.33.13\u0026#34; end config.vm.provision :shell, inline: \u0026lt;\u0026lt;-SHELL apt-get update apt-get install -y libsqlite3-dev docker.io curl -L git.io/weave -o /usr/local/bin/weave chmod a+x /usr/local/bin/weave SHELL end vagrant コマンドを使って host1, host2, host3 を起動します。\n$ vagrant up $ vagrant ssh host1 # \u0026lt;--- host1 に SSH する場合 $ vagrant ssh host2 # \u0026lt;--- host2 に SSH する場合 $ vagrant ssh host3 # \u0026lt;--- host3 に SSH する場合 物理ノードまたがったコンテナ間で通信をする weave でまず物理ノードをまたがったコンテナ間で通信をさせてみましょう。ここでは 上図の host1, host2 を使います。通常、物理ノードまたがっていると各々のホストで 稼働する Docker コンテナは通信し合えませんが weave を使うと通信しあうことが出 来ます。\nまず weave が用いる Docker コンテナを稼働します。下記のように /16 でレンジを切って 更にそこからデフォルトのレンジを指定することが出来ます。\nhost1# weave launch --ipalloc-range 10.2.0.0/16 --ipalloc-default-subnet 10.2.1.0/24 host1# eval $(weave env) host2# weave launch --ipalloc-range 10.2.0.0/16 --ipalloc-default-subnet 10.2.1.0/24 192.168.33.11 host2# eval $(weave env) この状態で下記のようなコンテナが稼働します。\nhost1# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES c55e96b4bdf9 weaveworks/weaveexec:1.4.0 \u0026#34;/home/weave/weavepr 4 seconds ago Up 3 seconds weaveproxy 394382c9c5d9 weaveworks/weave:1.4.0 \u0026#34;/home/weave/weaver 5 seconds ago Up 4 seconds weave host1, host2 でそれぞれテスト用コンテナを稼働させます。名前を \u0026ndash;name オプションで付けるのを 忘れないようにしてください。\nhost1# docker run --name a1 -ti ubuntu host2# docker run --name a2 -ti ubuntu どちらか一方から ping をもう一方に打ってみましょう。下記では a2 -\u0026gt; a1 の流れで ping を実行しています。\nroot@a2:/# ping 10.2.1.1 -c 3 PING 10.2.1.1 (10.2.1.1) 56(84) bytes of data. 64 bytes from 10.2.1.1: icmp_seq=1 ttl=64 time=0.316 ms 64 bytes from 10.2.1.1: icmp_seq=2 ttl=64 time=0.501 ms 64 bytes from 10.2.1.1: icmp_seq=3 ttl=64 time=0.619 ms --- 10.2.1.1 ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 1999ms rtt min/avg/max/mdev = 0.316/0.478/0.619/0.127 ms また docker コンテナを起動する時に指定した \u0026ndash;name a1, \u0026ndash;name a2 の名前で ping を実行してみましょう。これも weave の機能の１つで dns lookup が行えます。\nroot@b2:/# ping a1 -c 3 PING b1.weave.local (10.2.1.1) 56(84) bytes of data. 64 bytes from a1.weave.local (10.2.1.1): icmp_seq=1 ttl=64 time=1.14 ms 64 bytes from a1.weave.local (10.2.1.1): icmp_seq=2 ttl=64 time=0.446 ms 64 bytes from a1.weave.local (10.2.1.1): icmp_seq=3 ttl=64 time=0.364 ms --- b1.weave.local ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 2001ms rtt min/avg/max/mdev = 0.364/0.653/1.149/0.352 ms 結果から、異なる物理ノード(今回は VM)上で動作させた Docker コンテナ同士が通信し合えた ことがわかります。またコンテナ名の DNS 的は名前解決も可能になりました。\nダイナミックにネットワークをアタッチ・デタッチする 次に weave のネットワークを動的(コンテナがオンラインのまま)にアタッチ・デタッ チすることが出来るので試してみます。\n最初に weave のネットワークに属さない a1-1 という名前のコンテナを作ります。 docker exec で IP アドレスを確認すると eth0, lo のインターフェースしか持っていない ことが判ります。\nhost1# C=$(docker run --name a1-1 -e WEAVE_CIDR=none -dti ubuntu) host1# docker exec -it a1-1 ip a # \u0026lt;--- docker コンテナ内でコマンド実行 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN group default link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 25: eth0: \u0026lt;BROADCAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP group default link/ether 02:42:ac:11:00:05 brd ff:ff:ff:ff:ff:ff inet 172.17.0.5/16 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::42:acff:fe11:5/64 scope link valid_lft forever preferred_lft forever では weave のネットワークを a1-1 コンテナにアタッチしてみましょう。\nhost1# weave attach $C 10.2.1.1 host1# docker exec -it a1-1 ip a 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN group default link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 25: eth0: \u0026lt;BROADCAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP group default link/ether 02:42:ac:11:00:05 brd ff:ff:ff:ff:ff:ff inet 172.17.0.5/16 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::42:acff:fe11:5/64 scope link valid_lft forever preferred_lft forever 27: ethwe: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1410 qdisc pfifo_fast state UP group default qlen 1000 link/ether aa:15:06:51:6a:3b brd ff:ff:ff:ff:ff:ff inet 10.2.1.1/24 scope global ethwe valid_lft forever preferred_lft forever inet6 fe80::a815:6ff:fe51:6a3b/64 scope link valid_lft forever preferred_lft forever 上記のようにインターフェース ethwe が付与され最初に指定したデフォルトのサブネッ ト上の IP アドレスが付きました。\n次に weave ネットワークを複数アタッチしてみましょう。default, 10.2.2.0/24, 10.2.3.0/24 のネットワーク(サブネット)をアタッチします。\nhost1# weave attach net:default net:10.2.2.0/24 net:10.2.3.0/24 $C 10.2.1.1 10.2.2.1 10.2.3.1 root@vagrant-ubuntu-vivid-64:~# docker exec -it b3 ip a 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN group default link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 25: eth0: \u0026lt;BROADCAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP group default link/ether 02:42:ac:11:00:05 brd ff:ff:ff:ff:ff:ff inet 172.17.0.5/16 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::42:acff:fe11:5/64 scope link valid_lft forever preferred_lft forever 33: ethwe: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1410 qdisc pfifo_fast state UP group default qlen 1000 link/ether 9a:74:73:1b:24:a9 brd ff:ff:ff:ff:ff:ff inet 10.2.1.1/24 scope global ethwe valid_lft forever preferred_lft forever inet 10.2.2.1/24 scope global ethwe valid_lft forever preferred_lft forever inet 10.2.3.1/24 scope global ethwe valid_lft forever preferred_lft forever inet6 fe80::9874:73ff:fe1b:24a9/64 scope link valid_lft forever preferred_lft forever 結果、ethwe インターフェースに3つの IP アドレスが付与されました。 この様にダイナミックにコンテナに対して weave ネットワークをアタッチすることが出来ます。\nコンテナ外部から情報を取得する 下記のようにコンテナを起動しているホスト上 (Vagrant VM) からコンテナの情報を取 得する事もできます。シンプルですがオーケストレーション・自動化を行う上で重要な機能に なりそうな予感がします。\nhost1# weave expose 10.2.1.1 host1# weave dns-lookup a2 10.2.1.128 ダイナミックに物理ノードを追加し weave ネットワークへ 物理ノード(今回の場合 vagrant vm)を追加し上記で作成した weave ネットワークへ参 加させることも可能です。なお、今回は上記の vagrant up の時点で追加分の vm (host3) を既に稼働させています。\nhost1 で新しい物理ノードを接続します。\nhost1# weave connect 192.168.33.12 host1# weave status targets 192.168.33.13 192.168.33.12 host3 で weave コンテナ・テストコンテナを起動します。 下記で指定している 192.168.33.11 は host1 の IP アドレスです。\nhost3# weave launch --ipalloc-range 10.2.0.0/16 --ipalloc-default-subnet 10.2.1.0/24 192.168.33.11 host3# docker run --name a3 -ti ubuntu host2 の a2 コンテナに ping を打ってみます。\nroota3:/# ping a2 -c 3 PING a2.weave.local (10.2.1.128) 56(84) bytes of data. 64 bytes from a2.weave.local (10.2.1.128): icmp_seq=1 ttl=64 time=0.366 ms 64 bytes from a2.weave.local (10.2.1.128): icmp_seq=2 ttl=64 time=0.709 ms 64 bytes from a2.weave.local (10.2.1.128): icmp_seq=3 ttl=64 time=0.569 ms host3 上の a3 コンテナが既存の weave ネットワークに参加し通信出来たことが確認 できました。\nまとめと考察 コンフィギュレーションらしきモノを記述することなく Docker コンテナ間の通信 が出来ました。これは自動化する際に優位になるでしょう。また今回紹介したのは \u0026lsquo;weave net\u0026rsquo; と呼ばれるモノですが他にも \u0026lsquo;weave scope\u0026rsquo;, \u0026lsquo;weave run\u0026rsquo; といったモノ があります。\nhttp://weave.works/product/\nまた Docker Swarm, Compose と組み合わせる構成も組めるようです。試してみたい方 がいましたら是非。\nhttp://weave.works/guides/weave-and-docker-platform/compose-scalable-swarm-cluster-with-weave.html\nですが結果、まだ weave をどう自分たちのサービスに組み込めるかは検討が付いてい ません。\u0026lsquo;出来る\u0026rsquo; と \u0026lsquo;運用できる\u0026rsquo; が別物であることと、コンテナまわりのネットワー ク機能全般に理解して選定する必要がありそうです。\n参考サイト +++\nhttp://weave.works/docs/\n","permalink":"https://jedipunkz.github.io/post/2015/12/22/weave-docker-network/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003e今回は Weave というコンテナ間のネットワークを提供してくれる Docker のネットワークプラ\nグインを使ってみました。下記のような沢山の機能があるようです。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFast Data Path\u003c/li\u003e\n\u003cli\u003eDocker Network Plugin\u003c/li\u003e\n\u003cli\u003eSecurity\u003c/li\u003e\n\u003cli\u003eDynamic Netwrok Attachment\u003c/li\u003e\n\u003cli\u003eService Binding\u003c/li\u003e\n\u003cli\u003eFault Tolerance\u003c/li\u003e\n\u003cli\u003eetc \u0026hellip;\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eこの記事では上から幾つか抜粋して、Weave ってどのように動かせるのか？を解説します。\nそこから Weave が一体ナニモノなのか理解できればなぁと思います。\u003c/p\u003e\n\u003ch2 id=\"vagrant-を使った構成\"\u003eVagrant を使った構成\u003c/h2\u003e\n\u003cp\u003eこの記事では下記の構成を作って色々と試していきます。使う技術は\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eVagrant\u003c/li\u003e\n\u003cli\u003eDocker\u003c/li\u003e\n\u003cli\u003eWeave\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eです。\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e+---------------------+ +---------------------+ +---------------------+\n| docker container a1 | | docker container a2 | | docker container a3 |\n+---------------------+ +---------------------+ +---------------------+\n|    vagrant host 1   | |    vagrant host 2   | |    vagrant host 3   |\n+---------------------+-+---------------------+-+---------------------+\n|                          Mac or Windows                             |\n+---------------------------------------------------------------------+\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e特徴としては\u003c/p\u003e","title":"Weave を使った Docker ネットワーク"},{"content":"こんにちは。@jedipunkz です。\n最近、業務で CircleCI を扱っていて、だいぶ \u0026ldquo;出来ること・出来ないこと\u0026rdquo; や \u0026ldquo;出来ないこと に対する回避方法\u0026rdquo; 等のノウハウが若干溜まってきたので共有したいなと思います。\nこの記事の前提\u0026hellip; ここでは CodeDeploy の設定方法や、CircleCIの設定方法等に関しては記述しませ ん。あくまで、Tips 的な内容にしています。また運用する上で想定できる問題点と、 それの回避方法等\u0026hellip;についてまとめています。\nCirlceCI と併用するサービスについて CircleCI は Github と連携してレポジトリ内の制御ファイル circle.yml に従ってテ スト・ビルド・デプロイを実施してくれる CI サービスです。ただ CircleCI は自分た ちの管理しているシステム外にあるので、AWS VPC を用いていると VPC 内のプライベー トインスタンスにデプロイするのが難しいです。プロキシ挟んで・・ってことは出来そ うですがプロキシの運用もしたくない、AWS のインフラリソースに任せることが出来た らインスタンス・インスタンス上のミドルウェアを運用しなくて済むので運用コストが 省けそう。ってことで\nAWS S3 (https://aws.amazon.com/jp/s3/) AWS CodeDeploy (https://aws.amazon.com/jp/codedeploy/) を併用することを考えました。\nS3 は皆さんご存知のオブジェクトストレージです。CircleCI 用のバケットを作って、 ビルドした結果を格納します。私の務めている会社ではプログラミング言語として Scala を用いているので SBT というツールを使ってビルドします。その結果もバージョ ニングしつつ S3 バケットに格納できれば、万が一問題が発生した時にバイナリ毎切り 戻すことが出来そうです。\nまた CodeDeploy は EC2 インスタンス・またオンプレのインスタンスへコードのデプ ロイが可能になるサービスです。東京リージョンでは 2015/08 から利用が可能になり ました。これの便利なところは CircleCI 等の CI サービスから簡単に叩けて、VPC 内 のインスタンスに対してもデプロイが可能なところです。\nTips 的な情報として +++\ncircle.yml という CircleCI の制御ファイルがあります。Git レポジトリ内に格納することで CircleCI の動作を制御することが出来ます。この記事では circle.yml の紹介をメインとしたい と思います。\nGit push からデプロイまでを自動で行う circle.yml Github への push, merge をトリガーとしてデプロイまでの流れを自動で行う流れを組む場合の circle.yml を紹介します。全体の流れとしては\u0026hellip;\nレポジトリに git push, merge ことがトリガで処理が走る circle.yml を元にテスト・ビルド(場合によってはテストのみ) が走る S3 バケットにビルドされた結果が格納される CodeDeploy が実行され S3 バケット内のビルドされた成果物を対象のインスタンスにデプロイする となります。\nmachine: environment: SBT_VERSION: 0.13.9 SBT_OPTS: \u0026#34;-Xms512M -Xmx1536M -Xss1M -XX:+CMSClassUnloadingEnabled -XX:MaxPermSize=256M\u0026#34; services: - docker dependencies: pre: - (事前に実行したいコマンド・スクリプトを記述) cache_directories: - \u0026#34;~/.sbt\u0026#34; test: override: - sbt compile deployment: production: branch: master codedeploy: codedeploy-sample: application_root: / region: ap-northeast-1 revision_location: revision_type: S3 s3_location: bucket: circleci-sample-bucket key_pattern: filename-{CIRCLE_BRANCH}-{CIRCLE_BUILD_NUM}.zip deployment_group: codedeploy-sample-group それぞれのパラメータの意味 上記 circle.yml の重要なパラメータのみ説明していきます。 私が務めている会社は Scala を使っていると冒頭に説明しましたがテスト・ビルドに SBT を使うのでこのような記述になっています。Ruby や Python でも同様に記述でき ると思いますので読み替えてください。\nmachine -\u0026gt; environment : 全体で適用できる環境変数を定義します dependencies -\u0026gt; pre : 事前に実行したいコマンド等を定義できます test -\u0026gt; overide : テストを実行するコマンドを書きます。 deployment -\u0026gt; production -\u0026gt; branch : 適用するブランチ名と本番環境であることを記述します。 \u0026lsquo;codedeploy-sample\u0026rsquo; : CodeDeploy 上にサンプルで作成した \u0026lsquo;Application\u0026rsquo; 名です s3_location -\u0026gt; bucket : ビルドした成果物を S3 へ格納する際のバケット名を記します s3_location -\u0026gt; key_pattern : S3 バケットに収めるファイル名指定です deployment_group : CodeDeploy で定義する \u0026lsquo;Deployment-Group\u0026rsquo; 名です より詳細な説明を読みたい場合は下記の URL に描いてあります。\nhttps://circleci.com/docs/configuration\nS3 のみににデプロイする例 上記の circle.yml ではビルドとデプロイを一気に処理するのですが、テスト・ビルドとデプロイを別けて 実行したい場面もありそうです。流れとしては\u0026hellip;\nレポジトリに git push, merge ことがトリガで処理が走る circle.yml を元にテスト・ビルド(場合によってはテストのみ) が走る S3 バケットにビルドされた結果が格納される です。S3 のバケットに格納されたアプリを CodeDeploy を使ってデプロイするのは CodeDeploy の API を直接叩けば出来そうです。\nhttp://docs.aws.amazon.com/codedeploy/latest/APIReference/API_CreateDeployment.html\nこのリファレンスにある \u0026ldquo;CreateDeployment\u0026rdquo; については後に例をあげます。\nただ、同様のサービスとして TravisCI 等は S3 にのみデプロイを実施する仕組みが用意されているのですが CircleCI にはこの機能はありませんでした。サポートに問い合わせもしたのですが、あまり良い回答ではありませんでした。\nよって、下記のように awscli をテストコンテナ起動の度にインストールして S3 にアクセスすれば 上記の流れが組めそうです。\nmachine: environment: SBT_VERSION: 0.13.9 SBT_OPTS: \u0026#34;-Xms512M -Xmx1536M -Xss1M -XX:+CMSClassUnloadingEnabled -XX:MaxPermSize=256M\u0026#34; services: - docker dependencies: pre: - sudo pip install awscli cache_directories: - \u0026#34;~/.sbt\u0026#34; test: override: - sbt compile deployment: master: branch: master commands: - zip -r sample-code-${CIRCLE_BRANCH}-${CIRCLE_BUILD_NUM}.zip . - aws s3 cp sample-code-${CIRCLE_PROJECT_REPONAME}-${CIRCLE_BRANCH}-${CIRCLE_BUILD_NUM}.zip s3://\u0026lt;バケット名\u0026gt;/\u0026lt;ディレクトリ\u0026gt;/ --region ap-northeast-1 事前に awscli をインストールしているだけです。\nS3 バケットに格納された成果物を CodeDeploy を使って手動でデプロイするには下記 のコマンドで実施できます。\n$ aws deploy create-deployment \\ --application-name codedeploy-sample \\ --deployment-config-name CodeDeployDefault.OneAtATime \\ --deployment-group-name codedeploy-sample-group \\ --description \u0026#34;deploy test\u0026#34; \\ --s3-location bucket=\u0026lt;バケット名\u0026gt;,bundleType=zip,key=\u0026lt;ファイル名\u0026gt; { \u0026#34;deploymentId\u0026#34;: \u0026#34;d-2B4OAMT0B\u0026#34; } deploymentId は CodeDeploy 上の Application に紐付いた ID です。CodeDeploy の API を叩くか AWS コンソールで確認可能です。\nCircleCI の問題点とそれの回避方法 production と staging 1つのブランチで管理できる circle.yml は1つです。このファイルの中で定義できる \u0026lsquo;本番用\u0026rsquo;, \u0026lsquo;開発用\u0026rsquo; の定義は deployment -\u0026gt; production, staging の2種類になります。この2つで管理しきれない環境がある場合(例えば staging 以前の development 環境がある) は、レポジトリのブランチを別けて circle.yml を管理する方法があると思います。\n複数のデプロイ先があるレポジトリの運用 同一のレポジトリ内で管理しているコードのデプロイ先が複数ある場合は CodeDeploy 上で1つの Application に対して複数の Deployment-Group を作成することで対応できます。ただ、cirlce.yml で定義できるデプロイ先は deployment_group: の1つ( 厳密に言うと production, staging の2つ) になるので、こちらもブランチによる circle.yml の別管理で回避できそうです。\nこちらの問題については CircleCI 的にはおそらく「1つのレポジトリで管理するデプロイ先は1つに」というコンセプトなのかもしれません。\nAWS IAM ユーザにアタッチする Policy 作成 IAM ユーザを CircleCI に事前に設定しておくことで直接 AWS のリソースを操作出来るのですが、 そのユーザにアタッチしておくべき Policy について例をあげておきます。\n特定の S3 バケットにオブジェクト Put する Policy\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;Stmt1444196633000\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:PutObject\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::\u0026lt;S3 バケット名\u0026gt;/*\u0026#34; ] } ] } CodeDeploy の各 Action を実行する Policy\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;codedeploy:RegisterApplicationRevision\u0026#34;, \u0026#34;codedeploy:GetApplicationRevision\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;*\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;codedeploy:CreateDeployment\u0026#34;, \u0026#34;codedeploy:GetDeployment\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;*\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;codedeploy:GetDeploymentConfig\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;*\u0026#34; ] } ] } まとめ CodeDeploy, S3 を併用することで CircleCI を使っても VPC 内のプライベートインス タンスにデプロイできることが判りました。もし EC2 インスタンスを使っている場合 は他の方法も取れることが判っています。circle.yml 内の pre: で指定出来るコマン ド・スクリプトで EC2 インスタンスに紐付いているセキュリティグループに穴あけ処 理を記述すれば良さそうです。デプロイが終わったら穴を塞げばいいですね。この辺の 例については国内でもブログ記事にされている方がいらっしゃいますので参考にしてくだ さい。\n","permalink":"https://jedipunkz.github.io/post/2015/11/15/circleci-codedeploy/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003e最近、業務で CircleCI を扱っていて、だいぶ \u0026ldquo;出来ること・出来ないこと\u0026rdquo; や \u0026ldquo;出来ないこと\nに対する回避方法\u0026rdquo; 等のノウハウが若干溜まってきたので共有したいなと思います。\u003c/p\u003e\n\u003ch2 id=\"この記事の前提\"\u003eこの記事の前提\u0026hellip;\u003c/h2\u003e\n\u003cp\u003eここでは CodeDeploy の設定方法や、CircleCIの設定方法等に関しては記述しませ\nん。あくまで、Tips 的な内容にしています。また運用する上で想定できる問題点と、\nそれの回避方法等\u0026hellip;についてまとめています。\u003c/p\u003e\n\u003ch2 id=\"cirlceci-と併用するサービスについて\"\u003eCirlceCI と併用するサービスについて\u003c/h2\u003e\n\u003cp\u003eCircleCI は Github と連携してレポジトリ内の制御ファイル circle.yml に従ってテ\nスト・ビルド・デプロイを実施してくれる CI サービスです。ただ CircleCI は自分た\nちの管理しているシステム外にあるので、AWS VPC を用いていると VPC 内のプライベー\nトインスタンスにデプロイするのが難しいです。プロキシ挟んで・・ってことは出来そ\nうですがプロキシの運用もしたくない、AWS のインフラリソースに任せることが出来た\nらインスタンス・インスタンス上のミドルウェアを運用しなくて済むので運用コストが\n省けそう。ってことで\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAWS S3 (\u003ca href=\"https://aws.amazon.com/jp/s3/\"\u003ehttps://aws.amazon.com/jp/s3/\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003eAWS CodeDeploy (\u003ca href=\"https://aws.amazon.com/jp/codedeploy/\"\u003ehttps://aws.amazon.com/jp/codedeploy/\u003c/a\u003e)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eを併用することを考えました。\u003c/p\u003e\n\u003cp\u003eS3 は皆さんご存知のオブジェクトストレージです。CircleCI 用のバケットを作って、\nビルドした結果を格納します。私の務めている会社ではプログラミング言語として\nScala を用いているので SBT というツールを使ってビルドします。その結果もバージョ\nニングしつつ S3 バケットに格納できれば、万が一問題が発生した時にバイナリ毎切り\n戻すことが出来そうです。\u003c/p\u003e\n\u003cp\u003eまた CodeDeploy は EC2 インスタンス・またオンプレのインスタンスへコードのデプ\nロイが可能になるサービスです。東京リージョンでは 2015/08 から利用が可能になり\nました。これの便利なところは CircleCI 等の CI サービスから簡単に叩けて、VPC 内\nのインスタンスに対してもデプロイが可能なところです。\u003c/p\u003e","title":"CodeDeploy, S3 を併用して CircleCI により VPC にデプロイ"},{"content":"こんにちは。@jedipunkz です。\n今回は Docker ネタです。Docker 導入するにしても監視はどうする？という話になる と思うのですが、各 Monitoring as a Service を使うにしてもエージェント入れない といけないしお金掛かることもあるし..で、調べていたら cAdvisor というキーワード が出てきました。今回は cAdvisor を使ってコンテナの監視が出来ないか、について書 いていきたいと想います。\ncAdvisor とは ? cAdvisor は Kubernates で用いられているコンポーネントで単体でも利用可能とのこ と。Google が開発しています。また Docker コンテナの監視においてこの cAdvisor は一般化しつつあるようです。\nhttps://github.com/google/cadvisor\n収集したメトリクスの保存 cAdvisor 自体も Docker で起動して、同ホスト上に起動している Docker コンテナの リソースをモニタリングしてくれます。そのメトリクスデータは幾つかの DB に保存出 来るのですが、そのうちの一つが influxDB です。influxDB は時系列データベースで す。システムのメトリクスデータを収めるのにちょうどいいデータベースになります。\nhttps://influxdb.com/\nDB に収めたメトリクスの可視化 influxDB に収めたメトリクスデータを可視化するのが Grafana です。Grafana のデー タソースは influxDB の他にも幾つかあり Elasticsearch, KairosDB, Graphite 等が それです。\nhttp://grafana.org/\nでは早速試してみましょう。\n前提の環境 今回は Vagrant を使います。また Vagrant 上で上記の3つのソフトウェアを Docker で稼働します。またどうせなので docker-compose を使って3つのコンテナを一斉に立 ち上げてみましょう。\nVagrantFile の準備 下記のような VagrantFile を作成します。各ソフトウェアはそれぞれ WebUI を持って いて、そこに手元のコンピュータから接続するため forwarded_port しています。\n# -*- mode: ruby -*- # vi: set ft=ruby : Vagrant.configure(2) do |config| config.vm.box = \u0026#34;ubuntu/trusty64\u0026#34; config.vm.network \u0026#34;forwarded_port\u0026#34;, guest: 8080, host: 8080 config.vm.network \u0026#34;forwarded_port\u0026#34;, guest: 8083, host: 8083 config.vm.network \u0026#34;forwarded_port\u0026#34;, guest: 3000, host: 3000 config.vm.network \u0026#34;private_network\u0026#34;, ip: \u0026#34;192.168.33.10\u0026#34; end Docker コンテナの起動と docker-compose.yml の準備 Vagrant を起動し docker, docker-compose のインストールを行います。\n$ vagrant up $ vagrant ssh vagrant$ sudo apt-get update ; sudo apt-get -y install curl vagrant$ curl -sSL https://get.docker.com/ | sh vagrant$ sudo -i vagrant# export VERSION_NUM=1.4.0 vagrant# curl -L https://github.com/docker/compose/releases/download/VERSION_NUM/docker-compose-`uname -s`-`uname -m` \u0026gt; /usr/local/bin/docker-compose vagrant# chmod +x /usr/local/bin/docker-compose 次に docker-compose.yml を作成します。上記3つのソフトウェアが稼働するコンテナ を起動するため下記のように記述しましょう。カレントディレクトリに作成します。\nInfluxSrv: image: \u0026#34;tutum/influxdb:0.8.8\u0026#34; ports: - \u0026#34;8083:8083\u0026#34; - \u0026#34;8086:8086\u0026#34; expose: - \u0026#34;8090\u0026#34; - \u0026#34;8099\u0026#34; environment: - PRE_CREATE_DB=cadvisor cadvisor: image: \u0026#34;google/cadvisor:0.16.0\u0026#34; volumes: - \u0026#34;/:/rootfs:ro\u0026#34; - \u0026#34;/var/run:/var/run:rw\u0026#34; - \u0026#34;/sys:/sys:ro\u0026#34; - \u0026#34;/var/lib/docker/:/var/lib/docker:ro\u0026#34; links: - \u0026#34;InfluxSrv:influxsrv\u0026#34; ports: - \u0026#34;8080:8080\u0026#34; command: \u0026#34;-storage_driver=influxdb -storage_driver_db=cadvisor -storage_driver_host=influxsrv:8086 -storage_driver_user=root -storage_driver_password=root -storage_driver_secure=False\u0026#34; grafana: image: \u0026#34;grafana/grafana:2.1.3\u0026#34; ports: - \u0026#34;3000:3000\u0026#34; environment: - INFLUXDB_HOST=localhost - INFLUXDB_PORT=8086 - INFLUXDB_NAME=cadvisor - INFLUXDB_USER=root - INFLUXDB_PASS=root links: - \u0026#34;InfluxSrv:influxsrv\u0026#34; コンテナの起動 +++\ndocker コンテナを立ち上げます。\nvagrant$ docker-compose -d influxDB の WebUI に接続する それでは起動したコンテナのうち一つ influxDB の WebUI に接続していましょう。 上記の VagrantFile では IP アドレスを 192.168.33.10 と指定しました。\nURL : http://192.168.33.10:8083\nデータベースに接続します。\nユーザ名 : root パスワード : root\n接続するとデータベース作成画面に飛びますので Database Datails 枠に \u0026ldquo;cadvisor\u0026rdquo; と入力、その他の項目はデフォルトのままで \u0026ldquo;Create Database\u0026rdquo; をクリックします。\ncAdvisor の WebUI に接続する 続いて cAdvisor の WebUI に接続してみましょう。\nURL : http://192.168.33.10:8080\nここでは特に作業の必要はありません。コンテナの監視が行われグラフが描画されてい ることを確認します。\nGrafana の WebUI に接続する 最後に Grafana の WebUI です。\nURL : http://192.168.33.10:3000 ユーザ名 : admin パスワード : admin\nまずデータソースの設定を行います。左上のアイコンをクリックし \u0026ldquo;Data Sources\u0026rdquo; を 選択します。次に \u0026ldquo;Add New Data Source\u0026rdquo; ボタンをクリックします。\n下記の情報を入力しましょう。\nName : influxdb Type : influxDB 0.8.x Url : http://influxsrv:8086 Access : proxy Basic Auth User admin Basic Auth Password admin Database : cadvisor User : root Password : root さて最後にグラフを作成していきます。左メニューの \u0026ldquo;Dashboard\u0026rdquo; を選択し上部の \u0026ldquo;Home\u0026rdquo; ボランを押し \u0026ldquo;+New\u0026rdquo; を押します。\n下記の画面を参考にし値に入力していきます。\nMetrics を選択しネットワークの受信転送量をグラフにしています。\nseries : \u0026lsquo;stats\u0026rsquo; alias : RX Bytes select mean(rx_bytes) 同じく送信転送量もグラフにします。Add Query を押すと追加できます。\nseries : \u0026lsquo;stats\u0026rsquo; alias : TX Bytes select mean(tx_bytes) 時間が経過すると下記のようにグラフが描画されます。\nまとめと考察 3つのソフトウェア共に開発が活発であり、cAdvisor は特に Docker コンテナの監視と して一般化しつつあるよう。Kubernates の一部ということもありそう簡単には廃れな いと想います。コンテナの中にエージェント等を入れることもなく、これで Docker コ ンテナのリソース監視が出来そう。ただサービス監視は別途考えなくてはいけないなぁ という印象です。また、今回 docker-compose に記した各コンテナのバージョンは Docker Hub を確認すると別バージョンもあるので時期が経ってこのブログ記事をご覧 になった方は修正すると良いと想います。ただこの記事を書いている時点では influxDB の 0.9.x 系では動作しませんでした。よって latest ではなくバージョン指 定で記してあります。\n参考にしたサイト http://qiita.com/atskimura/items/4c4aaaaa554e2814e938 https://www.brianchristner.io/how-to-setup-docker-monitoring/ ","permalink":"https://jedipunkz.github.io/post/2015/09/12/cadvisor-influxdb-grafana-docker/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003e今回は Docker ネタです。Docker 導入するにしても監視はどうする？という話になる\nと思うのですが、各 Monitoring as a Service を使うにしてもエージェント入れない\nといけないしお金掛かることもあるし..で、調べていたら cAdvisor というキーワード\nが出てきました。今回は cAdvisor を使ってコンテナの監視が出来ないか、について書\nいていきたいと想います。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ecAdvisor とは ?\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003ecAdvisor は Kubernates で用いられているコンポーネントで単体でも利用可能とのこ\nと。Google が開発しています。また Docker コンテナの監視においてこの cAdvisor\nは一般化しつつあるようです。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/google/cadvisor\"\u003ehttps://github.com/google/cadvisor\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e収集したメトリクスの保存\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003ecAdvisor 自体も Docker で起動して、同ホスト上に起動している Docker コンテナの\nリソースをモニタリングしてくれます。そのメトリクスデータは幾つかの DB に保存出\n来るのですが、そのうちの一つが influxDB です。influxDB は時系列データベースで\nす。システムのメトリクスデータを収めるのにちょうどいいデータベースになります。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://influxdb.com/\"\u003ehttps://influxdb.com/\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDB に収めたメトリクスの可視化\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003einfluxDB に収めたメトリクスデータを可視化するのが Grafana です。Grafana のデー\nタソースは influxDB の他にも幾つかあり Elasticsearch, KairosDB, Graphite 等が\nそれです。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://grafana.org/\"\u003ehttp://grafana.org/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eでは早速試してみましょう。\u003c/p\u003e\n\u003ch2 id=\"前提の環境\"\u003e前提の環境\u003c/h2\u003e\n\u003cp\u003e今回は Vagrant を使います。また Vagrant 上で上記の3つのソフトウェアを Docker\nで稼働します。またどうせなので docker-compose を使って3つのコンテナを一斉に立\nち上げてみましょう。\u003c/p\u003e","title":"cAdvisor/influxDB/GrafanaでDockerリソース監視"},{"content":"こんにちは。@jedipunkz です。\n前回 OpenStack Kilo のオールインワン構成を Chef-Zero を使ってデプロイする方法 を書きましたが、複数台構成についても調べたので結果をまとめていきます。\n使うのは openstack/openstack-chef-repo です。下記の URL にあります。\nhttps://github.com/openstack/openstack-chef-repo\nこの中に Vagrant を使ったファイルが存在しますが、実機でのデプロイには全く役に 立ちません。自分で Environment ファイルを作成する必要があります。今回は前提の 構成を作って、それに合わせた Environment ファイルを記します。ほぼスタンダード な構成にしましたので、自分の環境に合わせて修正するのも比較的簡単だと想います。 参考にしてください。\n今回は knife-zero を使ってデプロイします。Chef サーバが必要なく、knife-zero を 使うホスト上のオンメモリで Chef サーバが稼働するので準備がほとんど必要ありません。\n早速ですが、構成と準備・そしてデプロイ作業を記していきます。\n前提の構成 +------------+ | GW Router | +--+------------+ | | | +--------------+--------------+---------------------------- public network | | eth0 | eth0 | +------------+ +------------+ +------------+ +------------+ | | Controller | | Network | | Compute | | Knife-Zero | | +------------+ +-------+----+ +------+-----+ +------------+ | | eth1 | eth1 | | eth1 | | eth1 +--+--------------+-------)------+------)-------+------------- api/management network | eth2 | eth2 +-------------+--------------------- guest network 特徴としては\u0026hellip;\npublic, api/management, guest の3つのネットワークに接続された OpenStack ホスト Controller, Network, Compute の最小複数台構成 knife-zero を実行する \u0026lsquo;Knife-Zero\u0026rsquo; ホスト Knife-zero ホストは api/management network のみに接続で可 デプロイは api/management network を介して行う public, api/management network はインターネットへの疎通が必須 OS は Ubuntu 14.04 amd64 とくに api/management network がインターネットへの疎通が必要なところに注意して ください。デプロイは knife-zero ホストで実行しますが、各ノードへログインしデプ ロイする際にインターネット上からパッケージの取得を試みます。\nまた api/management network を2つに分離するのも一般的ですが、ここでは一本にま とめています。\nIP アドレス IP アドレスは下記を前提にします。\ninterface IP addr Controller eth0 10.0.1.10 Controller eth1 10.0.2.10 Network eth0 10.0.1.11 Network eth1 10.0.2.11 Network eth2 10.0.3.11 Compute eth1 10.0.2.12 Compute eth2 10.0.3.12 Knife-Zero eth1 10.0.2.13 ネットワークインターフェース設定 それぞれのホストで下記のようにネットワークインターフェースを設定します。\nController ホスト eth0, 1 を使用します。\nauto eth0 iface eth0 inet static address 10.0.1.10 netmask 255.255.255.0 gateway 10.0.1.254 dns-nameservers 8.8.8.8 dns-search jedihub.com auto eth1 iface eth1 inet static address 10.0.2.10 netmask 255.255.255.0 auto eth2 iface eth2 inet manual Network ホスト eth0, 1, 2 全てを使用します。\nauto eth0 iface eth0 inet static up ifconfig $IFACE 0.0.0.0 up up ip link set $IFACE promisc on down ip link set $IFACE promisc off down ifconfig $IFACE down address 10.0.1.11 netmask 255.255.255.0 auto eth1 iface eth1 inet static address 10.0.2.11 netmask 255.255.255.0 gateway 10.0.2.248 dns-nameservers 8.8.8.8 dns-search jedihub.com auto eth2 iface eth2 inet static address 10.0.3.11 netmask 255.255.255.0 Compute ホスト eth1, 2 を使用します。\nauto eth0 iface eth0 inet manual auto eth1 iface eth1 inet static address 10.0.2.12 netmask 255.255.255.0 gateway 10.0.2.248 dns-nameservers 8.8.8.8 dns-search jedihub.com auto eth2 iface eth2 inet static address 10.0.3.12 netmask 255.255.255.0 これらの作業は knife-zero からログインし eth1 を介して行ってください。でないと 接続が切断される可能性があります。\n準備 knife-zero ホストに chef, knife-zero, berkshelf が入っている必要があるので、こ こでインストールしていきます。\nknife-zero ホストに chef をインストールします。Omnibus パッケージを使って手っ 取り早く環境を整えます。\nsudo -i curl -L https://www.opscode.com/chef/install.sh | bash Berkshelf をインストールするのに必要なソフトウェアをインストールします。\napt-get -y install build-essential zlib1g-dev libssl-dev libreadline-dev ruby-dev libxml2-dev libxslt-dev g++ Berkshelf をインストールします。\n/opt/chef/embedded/bin/gem install berkshelf --no-ri --no-rdoc 最後に knife-zero をインストールします。\n/opt/chef/embedded/bin/gem install knife-zero --no-ri --no-rdoc デプロイ作業 それでは openstack-chef-repo を取得してデプロイの準備を行います。 ブランチの指定は行わず master ブランチを取得します。Kilo は master ブランチで 管理されています。次のバージョンの開発が始まるタイミングで \u0026lsquo;stable/kilo\u0026rsquo; ブラ ンチに管理が移されます。\nsudo -i cd ~/ git clone https://github.com/openstack/openstack-chef-repo.git 次に Berkshelf を使って必要な Cookbooks をダウンロードします。\ncd ~/openstack-chef-repo /opt/chef/embedded/bin/berks vendor ./cookbooks Environment を作成します。これは各環境に合わせた設定ファイルのようなもので、各 Cookbooks の Attributes を上書きする仕組みになっています。下記の内容を\nopenstack-chef-repo/environments/multi-neutron-kilo.json というファイル名で保存してください。\n{ \u0026#34;name\u0026#34;: \u0026#34;multi-neutron-kilo\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;test\u0026#34;, \u0026#34;cookbook_versions\u0026#34;: { }, \u0026#34;json_class\u0026#34;: \u0026#34;Chef::Environment\u0026#34;, \u0026#34;chef_type\u0026#34;: \u0026#34;environment\u0026#34;, \u0026#34;default_attributes\u0026#34;: { }, \u0026#34;override_attributes\u0026#34;: { \u0026#34;mysql\u0026#34;: { \u0026#34;bind_address\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;server_root_password\u0026#34;: \u0026#34;mysqlroot\u0026#34;, \u0026#34;server_debian_password\u0026#34;: \u0026#34;mysqlroot\u0026#34;, \u0026#34;server_repl_password\u0026#34;: \u0026#34;mysqlroot\u0026#34;, \u0026#34;allow_remote_root\u0026#34;: true, \u0026#34;root_network_acl\u0026#34;: [\u0026#34;10.0.0.0/8\u0026#34;] }, \u0026#34;rabbitmq\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;5672\u0026#34;, \u0026#34;loopback_users\u0026#34;: [] }, \u0026#34;openstack\u0026#34;: { \u0026#34;auth\u0026#34;: { \u0026#34;validate_certs\u0026#34;: false }, \u0026#34;dashboard\u0026#34;: { \u0026#34;session_backend\u0026#34;: \u0026#34;file\u0026#34; }, \u0026#34;block-storage\u0026#34;: { \u0026#34;syslog\u0026#34;: { \u0026#34;use\u0026#34;: false }, \u0026#34;api\u0026#34;: { \u0026#34;ratelimit\u0026#34;: \u0026#34;False\u0026#34; }, \u0026#34;debug\u0026#34;: true, \u0026#34;image_api_chef_role\u0026#34;: \u0026#34;os-image\u0026#34;, \u0026#34;identity_service_chef_role\u0026#34;: \u0026#34;os-identity\u0026#34;, \u0026#34;rabbit_server_chef_role\u0026#34;: \u0026#34;os-ops-messaging\u0026#34; }, \u0026#34;compute\u0026#34;: { \u0026#34;rabbit\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34; }, \u0026#34;novnc_proxy\u0026#34;: { \u0026#34;bind_interface\u0026#34;: \u0026#34;eth1\u0026#34; }, \u0026#34;libvirt\u0026#34;: { \u0026#34;virt_type\u0026#34;: \u0026#34;qemu\u0026#34;, \u0026#34;bind_interface\u0026#34;: \u0026#34;eth1\u0026#34; }, \u0026#34;novnc_proxy\u0026#34;: { \u0026#34;bind_interface\u0026#34;: \u0026#34;eth1\u0026#34; }, \u0026#34;xvpvnc_proxy\u0026#34;: { \u0026#34;bind_interface\u0026#34;: \u0026#34;eth1\u0026#34; }, \u0026#34;image_api_chef_role\u0026#34;: \u0026#34;os-image\u0026#34;, \u0026#34;identity_service_chef_role\u0026#34;: \u0026#34;os-identity\u0026#34;, \u0026#34;nova_setup_chef_role\u0026#34;: \u0026#34;os-compute-api\u0026#34;, \u0026#34;rabbit_server_chef_role\u0026#34;: \u0026#34;os-ops-messaging\u0026#34;, \u0026#34;network\u0026#34;: { \u0026#34;public_interface\u0026#34;: \u0026#34;eth1\u0026#34;, \u0026#34;service_type\u0026#34;: \u0026#34;neutron\u0026#34; } }, \u0026#34;network\u0026#34;: { \u0026#34;debug\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;dhcp\u0026#34;: { \u0026#34;enable_isolated_metadata\u0026#34;: \u0026#34;True\u0026#34; }, \u0026#34;metadata\u0026#34;: { \u0026#34;nova_metadata_ip\u0026#34;: \u0026#34;10.0.2.10\u0026#34; }, \u0026#34;openvswitch\u0026#34;: { \u0026#34;tunnel_id_ranges\u0026#34;: \u0026#34;1:1000\u0026#34;, \u0026#34;enable_tunneling\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;tenant_network_type\u0026#34;: \u0026#34;gre\u0026#34;, \u0026#34;tunnel_types\u0026#34;: \u0026#34;gre\u0026#34;, \u0026#34;tunnel_type\u0026#34;: \u0026#34;gre\u0026#34;, \u0026#34;bridge_mappings\u0026#34;: \u0026#34;physnet1:br-eth2\u0026#34;, \u0026#34;bridge_mapping_interface\u0026#34;: \u0026#34;br-eth2:eth2\u0026#34; }, \u0026#34;ml2\u0026#34;: { \u0026#34;tenant_network_types\u0026#34;: \u0026#34;gre\u0026#34;, \u0026#34;mechanism_drivers\u0026#34;: \u0026#34;openvswitch\u0026#34;, \u0026#34;tunnel_id_ranges\u0026#34;: \u0026#34;1:1000\u0026#34;, \u0026#34;enable_security_group\u0026#34;: \u0026#34;True\u0026#34; }, \u0026#34;api\u0026#34;: { \u0026#34;bind_interface\u0026#34;: \u0026#34;eth1\u0026#34; }, \u0026#34;l3\u0026#34;: { \u0026#34;external_network_bridge_interface\u0026#34;: \u0026#34;eth0\u0026#34; }, \u0026#34;service_plugins\u0026#34;: [\u0026#34;neutron.services.l3_router.l3_router_plugin.L3RouterPlugin\u0026#34;] }, \u0026#34;db\u0026#34;: { \u0026#34;bind_interface\u0026#34;: \u0026#34;eth1\u0026#34;, \u0026#34;compute\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34; }, \u0026#34;identity\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34; }, \u0026#34;image\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34; }, \u0026#34;network\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34; }, \u0026#34;volume\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34; }, \u0026#34;dashboard\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34; }, \u0026#34;telemetry\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34; }, \u0026#34;orchestration\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34; } }, \u0026#34;developer_mode\u0026#34;: true, \u0026#34;endpoints\u0026#34;: { \u0026#34;network-openvswitch\u0026#34;: { \u0026#34;bind_interface\u0026#34;: \u0026#34;eth1\u0026#34; }, \u0026#34;compute-api-bind\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8774\u0026#34; }, \u0026#34;compute-api\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8774\u0026#34; }, \u0026#34;compute-ec2-admin-bind\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8773\u0026#34; }, \u0026#34;compute-ec2-admin\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8773\u0026#34; }, \u0026#34;compute-ec2-api-bind\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8773\u0026#34; }, \u0026#34;compute-ec2-api\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8773\u0026#34; }, \u0026#34;compute-xvpvnc\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;6081\u0026#34; }, \u0026#34;compute-novnc-bind\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;6080\u0026#34; }, \u0026#34;compute-novnc\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;6080\u0026#34; }, \u0026#34;compute-vnc\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;6080\u0026#34; }, \u0026#34;image-api\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;9292\u0026#34; }, \u0026#34;image-api-bind\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;9292\u0026#34; }, \u0026#34;image-registry\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;9191\u0026#34; }, \u0026#34;image-registry-bind\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;9191\u0026#34; }, \u0026#34;identity-api\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;5000\u0026#34; }, \u0026#34;identity-bind\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;5000\u0026#34; }, \u0026#34;identity-admin\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;35357\u0026#34; }, \u0026#34;identity-internal\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;35357\u0026#34; }, \u0026#34;volume-api-bind\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8776\u0026#34; }, \u0026#34;volume-api\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8776\u0026#34; }, \u0026#34;telemetry-api\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8777\u0026#34; }, \u0026#34;network-api-bind\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.11\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;9696\u0026#34; }, \u0026#34;network-api\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.11, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;9696\u0026#34; }, \u0026#34;block-storage-api-bind\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8776\u0026#34;, \u0026#34;bind_interface\u0026#34;: \u0026#34;eth1\u0026#34; }, \u0026#34;block-storage-api\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8776\u0026#34; }, \u0026#34;orchestration-api\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8004\u0026#34; }, \u0026#34;orchestration-api-cfn\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8000\u0026#34; }, \u0026#34;db\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;3306\u0026#34; }, \u0026#34;bind-host\u0026#34;: \u0026#34;0.0.0.0\u0026#34; }, \u0026#34;identity\u0026#34;: { \u0026#34;admin_user\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;bind_interface\u0026#34;: \u0026#34;eth1\u0026#34;, \u0026#34;debug\u0026#34;: true }, \u0026#34;image\u0026#34;: { \u0026#34;api\u0026#34;: { \u0026#34;bind_interface\u0026#34;: \u0026#34;eth1\u0026#34; }, \u0026#34;debug\u0026#34;: true, \u0026#34;identity_service_chef_role\u0026#34;: \u0026#34;os-identity\u0026#34;, \u0026#34;rabbit_server_chef_role\u0026#34;: \u0026#34;os-ops-messaging\u0026#34;, \u0026#34;registry\u0026#34;: { \u0026#34;bind_interface\u0026#34;: \u0026#34;eth1\u0026#34; }, \u0026#34;syslog\u0026#34;: { \u0026#34;use\u0026#34;: false } }, \u0026#34;mq\u0026#34;: { \u0026#34;bind_interface\u0026#34;: \u0026#34;eth1\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;guest\u0026#34;, \u0026#34;vhost\u0026#34;: \u0026#34;/nova\u0026#34;, \u0026#34;network\u0026#34;: { \u0026#34;rabbit\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;5672\u0026#34; } }, \u0026#34;compute\u0026#34;: { \u0026#34;service_type\u0026#34;: \u0026#34;rabbitmq\u0026#34;, \u0026#34;rabbit\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;5672\u0026#34; } }, \u0026#34;block-storage\u0026#34;: { \u0026#34;service_type\u0026#34;: \u0026#34;rabbitmq\u0026#34;, \u0026#34;rabbit\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;5672\u0026#34; } } } }, \u0026#34;queue\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;guest\u0026#34;, \u0026#34;vhost\u0026#34;: \u0026#34;/nova\u0026#34; } } } 上記ファイルでは virt_type : qemu に設定していますが、KVM リソースを利用出来る 環境であればここを削除してください。デフォルトの \u0026lsquo;kvm\u0026rsquo; が適用されます。また気 をつけることは IP アドレスとネットワークインターフェース名です。環境に合わせて 設定していきましょう。今回は前提構成に合わせて environemnt ファイルを作ってい ます。\n次に openstack-chef-repo/.chef/encrypted_data_bag_secret というファイルが knife-zero ホストにあるはずです。これをデプロイ対象の3ノードに事前に転送してお く必要があります。\nscp openstack-chef-repo/.chef/encrypted_data_bag_secret 10.0.2.10:/tmp/ scp openstack-chef-repo/.chef/encrypted_data_bag_secret 10.0.2.11:/tmp/ scp openstack-chef-repo/.chef/encrypted_data_bag_secret 10.0.2.12:/tmp/ 対象ホストにて\nmkdir /etc/chef mv /tmp/encrypted_data_bag_secret /etc/chef/openstack_data_bag_secret ではいよいよデプロイです。\nController ホストへのデプロイ\nknife zero bootstrap 10.0.2.10 -N kilo01 -r \u0026#39;role[os-compute-single-controller-no-network]\u0026#39; -E multi-neutron-kilo -x \u0026lt;USERNAME\u0026gt; --sudo Network ホストへのデプロイ\nknife zero bootstrap 10.0.2.11 -N kilo02 -r \u0026#39;role[os-client]\u0026#39;,\u0026#39;role[os-network]\u0026#39; -E multi-neutron-kilo -x \u0026lt;USERNAME\u0026gt; --sudo Compute ノードへのデプロイ\nknife zero bootstrap 10.0.2.12 -N kilo03 -r \u0026#39;role[os-compute-worker]\u0026#39; -E multi-neutron-kilo -x \u0026lt;USERNAME\u0026gt; --sudo これで完了です。admin/mypass というユーザ・パスワードでログインが可能です。\nまとめ openstack-chef-repo を使って OpenStack Kilo の複数台構成をデプロイ出来ました。重要なのは Environment をどうやって作るか？ですが、 私は 作成 -\u0026gt; デプロイ -\u0026gt; 修正 -\u0026gt; デプロイ -\u0026gt;\u0026hellip;. を繰り返して作成しています。何度実行しても不具合は発生しない設計なクックブックに なっていますので、このような作業が可能になります。また、「ここの設定を追加したい」という時は\u0026hellip;\n該当の template を探す 該当のパラメータを確認する recipe 内で template にどうパラメータを渡しているか確認する attribute なり、変数なりを修正するための方法を探す と行います。比較的難しい作業になるのですが、自らの環境に合わせた Environment を作成するにはこれらの作業が必須となってきます。\n以上、複数台構成のデプロイ方法についてでした。\n","permalink":"https://jedipunkz.github.io/post/2015/07/20/knife-zero-openstack-kilo/","summary":"\u003cp\u003eこんにちは。@jedipunkz です。\u003c/p\u003e\n\u003cp\u003e前回 OpenStack Kilo のオールインワン構成を Chef-Zero を使ってデプロイする方法\nを書きましたが、複数台構成についても調べたので結果をまとめていきます。\u003c/p\u003e\n\u003cp\u003e使うのは openstack/openstack-chef-repo です。下記の URL にあります。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/openstack/openstack-chef-repo\"\u003ehttps://github.com/openstack/openstack-chef-repo\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eこの中に Vagrant を使ったファイルが存在しますが、実機でのデプロイには全く役に\n立ちません。自分で Environment ファイルを作成する必要があります。今回は前提の\n構成を作って、それに合わせた Environment ファイルを記します。ほぼスタンダード\nな構成にしましたので、自分の環境に合わせて修正するのも比較的簡単だと想います。\n参考にしてください。\u003c/p\u003e\n\u003cp\u003e今回は knife-zero を使ってデプロイします。Chef サーバが必要なく、knife-zero を\n使うホスト上のオンメモリで Chef サーバが稼働するので準備がほとんど必要ありません。\u003c/p\u003e\n\u003cp\u003e早速ですが、構成と準備・そしてデプロイ作業を記していきます。\u003c/p\u003e\n\u003ch2 id=\"前提の構成\"\u003e前提の構成\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003e   +------------+\n   | GW Router  |\n+--+------------+\n|  |\n|  +--------------+--------------+---------------------------- public network\n|  | eth0         | eth0\n|  +------------+ +------------+ +------------+ +------------+\n|  | Controller | |  Network   | |  Compute   | | Knife-Zero | \n|  +------------+ +-------+----+ +------+-----+ +------------+\n|  | eth1         | eth1  |      | eth1 |       | eth1 \n+--+--------------+-------)------+------)-------+------------- api/management network\n                          | eth2        | eth2\n                          +-------------+--------------------- guest network\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e特徴としては\u0026hellip;\u003c/p\u003e","title":"Knife-ZeroでOpenStack Kiloデプロイ(複数台編)"},{"content":"こんにちは。@jedipunkz です。\n久々に openstack-chef-repo を覗いてみたら \u0026lsquo;openstack/openstack-chef-repo\u0026rsquo; とし て公開されていました。今まで stackforge 側で管理されていましたが \u0026lsquo;openstack\u0026rsquo; の方に移動したようです。\nhttps://github.com/openstack/openstack-chef-repo\n結構安定してきているのかな？と想い、ちらっと試したのですが案の定、簡単に動作さ せることが出来ました。\n今回はこのレポジトリを使ってオールインワン構成の OpenStack Kilo を作る方法をま とめていきます。\n前提の構成 このレポジトリは Vagrant で OpenStack を作るための環境一式が最初から用意されて いますが、Vagrant では本番環境を作ることは出来ないため、Ubuntu ホストを前提と した記述に差し替えて説明していきます。前提にする構成は下記のとおりです。\nUuntu Linux 14.04 x 1 台 ネットワークインターフェース x 3 つ eth0 : External ネットワーク用 eth1 : Internal (API, Manage) ネットワーク用 eth2 : Guest ネットワーク用 特徴としては上記なのですが、eth2 に関してはオールインワンなので必ずしも必要と いうわけではありません。複数台構成を考慮した設定になっています。\n前提のIP アドレス この記事では下記の IP アドレスを前提にします。お手持ちの環境の IP アドレスが違 い場合はそれに合わせて後に示す json ファイルを変更してください。\n10.0.1.10 (eth0) : external ネットワーク 10.0.2.10 (eth1) : api/management ネットワーク 10.0.3.10 (eth2) : Guest ネットワーク 事前の準備 事前に対象ホスト (OpenStack ホスト) に chef, berkshelf をインストールします。\nsudo -i curl -L https://www.opscode.com/chef/install.sh | bash Berkshelf をインストールするのに必要なソフトウェアをインストールします。\napt-get -y install build-essential zlib1g-dev libssl-dev libreadline-dev ruby-dev libxml2-dev libxslt-dev g++ Berkshelf をインストールします。\n/opt/chef/embedded/bin/gem install berkshelf --no-ri --no-rdoc デプロイ作業 それでは openstack-chef-repo を取得してデプロイの準備を行います。 ブランチの指定は行わず master ブランチを取得します。Kilo は master ブランチで 管理されています。次のバージョンの開発が始まるタイミングで \u0026lsquo;stable/kilo\u0026rsquo; ブラ ンチに管理が移されます。\nsudo -i cd ~/ git clone https://github.com/openstack/openstack-chef-repo.git 次に Berkshelf を使って必要な Cookbooks をダウンロードします。\ncd ~/openstack-chef-repo /opt/chef/embedded/bin/berks vendor ./cookbooks Environment を作成します。これは各環境に合わせた設定ファイルのようなもので、各 Cookbooks の Attributes を上書きする仕組みになっています。下記の内容を\nopenstack-chef-repo/environments/aio-neutron-kilo.json というファイル名で保存してください。\n{ \u0026#34;name\u0026#34;: \u0026#34;aio-neutron-kilo\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;test\u0026#34;, \u0026#34;cookbook_versions\u0026#34;: { }, \u0026#34;json_class\u0026#34;: \u0026#34;Chef::Environment\u0026#34;, \u0026#34;chef_type\u0026#34;: \u0026#34;environment\u0026#34;, \u0026#34;default_attributes\u0026#34;: { }, \u0026#34;override_attributes\u0026#34;: { \u0026#34;mysql\u0026#34;: { \u0026#34;bind_address\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;server_root_password\u0026#34;: \u0026#34;mysqlroot\u0026#34;, \u0026#34;server_debian_password\u0026#34;: \u0026#34;mysqlroot\u0026#34;, \u0026#34;server_repl_password\u0026#34;: \u0026#34;mysqlroot\u0026#34;, \u0026#34;allow_remote_root\u0026#34;: true, \u0026#34;root_network_acl\u0026#34;: [\u0026#34;10.0.0.0/8\u0026#34;] }, \u0026#34;rabbitmq\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;5672\u0026#34;, \u0026#34;loopback_users\u0026#34;: [] }, \u0026#34;openstack\u0026#34;: { \u0026#34;auth\u0026#34;: { \u0026#34;validate_certs\u0026#34;: false }, \u0026#34;dashboard\u0026#34;: { \u0026#34;session_backend\u0026#34;: \u0026#34;file\u0026#34; }, \u0026#34;block-storage\u0026#34;: { \u0026#34;syslog\u0026#34;: { \u0026#34;use\u0026#34;: false }, \u0026#34;api\u0026#34;: { \u0026#34;ratelimit\u0026#34;: \u0026#34;False\u0026#34; }, \u0026#34;debug\u0026#34;: true, \u0026#34;image_api_chef_role\u0026#34;: \u0026#34;os-image\u0026#34;, \u0026#34;identity_service_chef_role\u0026#34;: \u0026#34;os-identity\u0026#34;, \u0026#34;rabbit_server_chef_role\u0026#34;: \u0026#34;os-ops-messaging\u0026#34; }, \u0026#34;compute\u0026#34;: { \u0026#34;rabbit\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34; }, \u0026#34;novnc_proxy\u0026#34;: { \u0026#34;bind_interface\u0026#34;: \u0026#34;eth1\u0026#34; }, \u0026#34;libvirt\u0026#34;: { \u0026#34;virt_type\u0026#34;: \u0026#34;qemu\u0026#34;, \u0026#34;bind_interface\u0026#34;: \u0026#34;eth1\u0026#34; }, \u0026#34;novnc_proxy\u0026#34;: { \u0026#34;bind_interface\u0026#34;: \u0026#34;eth1\u0026#34; }, \u0026#34;xvpvnc_proxy\u0026#34;: { \u0026#34;bind_interface\u0026#34;: \u0026#34;eth1\u0026#34; }, \u0026#34;image_api_chef_role\u0026#34;: \u0026#34;os-image\u0026#34;, \u0026#34;identity_service_chef_role\u0026#34;: \u0026#34;os-identity\u0026#34;, \u0026#34;nova_setup_chef_role\u0026#34;: \u0026#34;os-compute-api\u0026#34;, \u0026#34;rabbit_server_chef_role\u0026#34;: \u0026#34;os-ops-messaging\u0026#34;, \u0026#34;network\u0026#34;: { \u0026#34;public_interface\u0026#34;: \u0026#34;eth1\u0026#34;, \u0026#34;service_type\u0026#34;: \u0026#34;neutron\u0026#34; } }, \u0026#34;network\u0026#34;: { \u0026#34;debug\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;dhcp\u0026#34;: { \u0026#34;enable_isolated_metadata\u0026#34;: \u0026#34;True\u0026#34; }, \u0026#34;metadata\u0026#34;: { \u0026#34;nova_metadata_ip\u0026#34;: \u0026#34;10.0.2.10\u0026#34; }, \u0026#34;openvswitch\u0026#34;: { \u0026#34;tunnel_id_ranges\u0026#34;: \u0026#34;1:1000\u0026#34;, \u0026#34;enable_tunneling\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;tenant_network_type\u0026#34;: \u0026#34;gre\u0026#34;, \u0026#34;local_ip_interface\u0026#34;: \u0026#34;eth2\u0026#34; }, \u0026#34;api\u0026#34;: { \u0026#34;bind_interface\u0026#34;: \u0026#34;eth1\u0026#34; }, \u0026#34;l3\u0026#34;: { \u0026#34;external_network_bridge_interface\u0026#34;: \u0026#34;eth0\u0026#34; }, \u0026#34;service_plugins\u0026#34;: [\u0026#34;neutron.services.l3_router.l3_router_plugin.L3RouterPlugin\u0026#34;] }, \u0026#34;db\u0026#34;: { \u0026#34;bind_interface\u0026#34;: \u0026#34;eth1\u0026#34;, \u0026#34;compute\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34; }, \u0026#34;identity\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34; }, \u0026#34;image\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34; }, \u0026#34;network\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34; }, \u0026#34;volume\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34; }, \u0026#34;dashboard\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34; }, \u0026#34;telemetry\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34; }, \u0026#34;orchestration\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34; } }, \u0026#34;developer_mode\u0026#34;: true, \u0026#34;endpoints\u0026#34;: { \u0026#34;compute-api-bind\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8774\u0026#34; }, \u0026#34;compute-api\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8774\u0026#34; }, \u0026#34;compute-ec2-admin-bind\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8773\u0026#34; }, \u0026#34;compute-ec2-admin\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8773\u0026#34; }, \u0026#34;compute-ec2-api-bind\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8773\u0026#34; }, \u0026#34;compute-ec2-api\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8773\u0026#34; }, \u0026#34;compute-xvpvnc\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;6081\u0026#34; }, \u0026#34;compute-novnc-bind\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;6080\u0026#34; }, \u0026#34;compute-novnc\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;6080\u0026#34; }, \u0026#34;compute-vnc\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;6080\u0026#34; }, \u0026#34;image-api\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;9292\u0026#34; }, \u0026#34;image-api-bind\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;9292\u0026#34; }, \u0026#34;image-registry\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;9191\u0026#34; }, \u0026#34;image-registry-bind\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;9191\u0026#34; }, \u0026#34;identity-api\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;5000\u0026#34; }, \u0026#34;identity-bind\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;5000\u0026#34; }, \u0026#34;identity-admin\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;35357\u0026#34; }, \u0026#34;identity-internal\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;35357\u0026#34; }, \u0026#34;volume-api-bind\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8776\u0026#34; }, \u0026#34;volume-api\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8776\u0026#34; }, \u0026#34;telemetry-api\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8777\u0026#34; }, \u0026#34;network-api-bind\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;9696\u0026#34; }, \u0026#34;network-api\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;9696\u0026#34; }, \u0026#34;orchestration-api\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8004\u0026#34; }, \u0026#34;orchestration-api-cfn\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8000\u0026#34; }, \u0026#34;db\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;3306\u0026#34; }, \u0026#34;bind-host\u0026#34;: \u0026#34;0.0.0.0\u0026#34; }, \u0026#34;identity\u0026#34;: { \u0026#34;admin_user\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;bind_interface\u0026#34;: \u0026#34;eth1\u0026#34;, \u0026#34;debug\u0026#34;: true }, \u0026#34;image\u0026#34;: { \u0026#34;api\u0026#34;: { \u0026#34;bind_interface\u0026#34;: \u0026#34;eth1\u0026#34; }, \u0026#34;debug\u0026#34;: true, \u0026#34;identity_service_chef_role\u0026#34;: \u0026#34;os-identity\u0026#34;, \u0026#34;rabbit_server_chef_role\u0026#34;: \u0026#34;os-ops-messaging\u0026#34;, \u0026#34;registry\u0026#34;: { \u0026#34;bind_interface\u0026#34;: \u0026#34;eth1\u0026#34; }, \u0026#34;syslog\u0026#34;: { \u0026#34;use\u0026#34;: false } }, \u0026#34;mq\u0026#34;: { \u0026#34;bind_interface\u0026#34;: \u0026#34;eth1\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;guest\u0026#34;, \u0026#34;vhost\u0026#34;: \u0026#34;/nova\u0026#34;, \u0026#34;network\u0026#34;: { \u0026#34;rabbit\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;5672\u0026#34; } }, \u0026#34;compute\u0026#34;: { \u0026#34;service_type\u0026#34;: \u0026#34;rabbitmq\u0026#34;, \u0026#34;rabbit\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;5672\u0026#34; } }, \u0026#34;block-storage\u0026#34;: { \u0026#34;service_type\u0026#34;: \u0026#34;rabbitmq\u0026#34;, \u0026#34;rabbit\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;5672\u0026#34; } } } }, \u0026#34;queue\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.2.10\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;guest\u0026#34;, \u0026#34;vhost\u0026#34;: \u0026#34;/nova\u0026#34; } } } 上記ファイルは KVM が使えない環境用に virt_type : qemu にしていますが、KVM が 利用できる環境をご利用であれば該当行を削除してください。デフォルト値の \u0026lsquo;kvm\u0026rsquo; が入るはずです。\n次にデプロイ前に databag 関連の事前操作を行います。Vagrant 用に作成されたファ イルを除くと\u0026hellip;\nmachine \u0026#39;controller\u0026#39; do add_machine_options vagrant_config: controller_config role \u0026#39;allinone-compute\u0026#39; role \u0026#39;os-image-upload\u0026#39; chef_environment env file(\u0026#39;/etc/chef/openstack_data_bag_secret\u0026#39;, \u0026#34;#{File.dirname(__FILE__)}/.chef/encrypted_data_bag_secret\u0026#34;) converge true end となっていて /etc/chef/openstack_data_bag_secret というファイルを事前にコピー する必要がありそうです。下記のように操作します。\ncp .chef/encrypted_data_bag_secret /etc/chef/openstack_data_bag_secret デプロイを実行します。\nこの openstack-chef-repo には .chef ディレクトリが存在していてノード名が記され ています。\u0026rsquo;nodienode\u0026rsquo; というノード名です。これを利用してそのままデプロイを実行 します。\nchef-client -z knife node -z run_list add nodienode \u0026#39;role[allinone-compute]\u0026#39; chef-client -z -E aio-neutron-kilo 上記の説明を行います。 １行目 chef-client -z で Chef-Zero サーバをメモリ上に起動し、2行目で自ノードへ run_list を追加しています。最後、3行目でデプロイ実行、となります。\n数分待つと OpenStack Kilo が構成されているはずです。\nまとめ Chef-Zero を用いることで Chef サーバを利用せずに楽に構築が行えました。ですが、 OpenStack の複数台構成となるとそれぞれのノードのパラメータを連携させる必要が出 てくるので Chef サーバを用いたほうが良さそうです。今度、時間を見つけて Kilo の 複数台構成についても調べておきます。\nまた、master ブランチを使用していますので、まだ openstack-chef-repo 自体が流動 的な状態とも言えます。が launchpad で管理されている Bug リストを見ると、ステー タス Critical, High の Bug が見つからなかったので Kilo に関しては、大きな問題 無く安定してきている感があります。\nhttps://bugs.launchpad.net/openstack-chef\n","permalink":"https://jedipunkz.github.io/post/2015/07/16/chef-zero-openstack-allinone/","summary":"\u003cp\u003eこんにちは。@jedipunkz です。\u003c/p\u003e\n\u003cp\u003e久々に openstack-chef-repo を覗いてみたら \u0026lsquo;openstack/openstack-chef-repo\u0026rsquo; とし\nて公開されていました。今まで stackforge 側で管理されていましたが \u0026lsquo;openstack\u0026rsquo;\nの方に移動したようです。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/openstack/openstack-chef-repo\"\u003ehttps://github.com/openstack/openstack-chef-repo\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e結構安定してきているのかな？と想い、ちらっと試したのですが案の定、簡単に動作さ\nせることが出来ました。\u003c/p\u003e\n\u003cp\u003e今回はこのレポジトリを使ってオールインワン構成の OpenStack Kilo を作る方法をま\nとめていきます。\u003c/p\u003e\n\u003ch2 id=\"前提の構成\"\u003e前提の構成\u003c/h2\u003e\n\u003cp\u003eこのレポジトリは Vagrant で OpenStack を作るための環境一式が最初から用意されて\nいますが、Vagrant では本番環境を作ることは出来ないため、Ubuntu ホストを前提と\nした記述に差し替えて説明していきます。前提にする構成は下記のとおりです。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eUuntu Linux 14.04 x 1 台\u003c/li\u003e\n\u003cli\u003eネットワークインターフェース x 3 つ\u003c/li\u003e\n\u003cli\u003eeth0 : External ネットワーク用\u003c/li\u003e\n\u003cli\u003eeth1 : Internal (API, Manage) ネットワーク用\u003c/li\u003e\n\u003cli\u003eeth2 : Guest ネットワーク用\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e特徴としては上記なのですが、eth2 に関してはオールインワンなので必ずしも必要と\nいうわけではありません。複数台構成を考慮した設定になっています。\u003c/p\u003e\n\u003ch2 id=\"前提のip-アドレス\"\u003e前提のIP アドレス\u003c/h2\u003e\n\u003cp\u003eこの記事では下記の IP アドレスを前提にします。お手持ちの環境の IP アドレスが違\nい場合はそれに合わせて後に示す json ファイルを変更してください。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e10.0.1.10 (eth0) : external ネットワーク\u003c/li\u003e\n\u003cli\u003e10.0.2.10 (eth1) : api/management ネットワーク\u003c/li\u003e\n\u003cli\u003e10.0.3.10 (eth2) : Guest ネットワーク\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"事前の準備\"\u003e事前の準備\u003c/h2\u003e\n\u003cp\u003e事前に対象ホスト (OpenStack ホスト) に chef, berkshelf をインストールします。\u003c/p\u003e","title":"Chef-ZeroでOpenStack Kiloデプロイ(オールインワン編)"},{"content":"こんにちは、@jedipunkz です。\n久々にブログ更新になりましたが、ウォーミングアップで minio というオブジェクト ストレージを使ってみたメモを記事にしたいと想います。\nminio は Minimal Object Storage の名の通り、最小限の小さなオブジェクトストレー ジになります。公式サイトは下記のとおりです。\nhttp://minio.io/\nGolang で記述されていて Apache License v2 の元に公開されています。\n最近、資金調達の話も挙がっていたので、これから一般的になってくるのかもしれません。\n早速ですが、minio を動かしてみます。\nMinio を起動する 方法は mithub.com/minio/minio の README に書かれていますが、バイナリを持ってき て実行権限を与えるだけのシンプルな手順になります。\nLinux でも Mac でも動作しますが、今回私は Mac 上で動作させました。\n% wget https://dl.minio.io:9000/updates/2015/Jun/darwin-amd64/minio % chmod +x minio % ./minio mode memory limit 512MB Starting minio server on: http://127.0.0.1:9000 Starting minio server on: http://192.168.1.123:9000 起動すると Listening Port と共に EndPoint の URL が表示されます。\n次に mc という minio client を使って動作確認します。\nMc を使ってアクセスする mc は下記の URL にあります。\nhttps://github.com/minio/mc\nこちらもダウンロードして実行権限を付与するのみです。mc は minio だけではなく、 Amazon S3 とも互換性がありアクセス出来ますが、せっかくなので上記で起動した minio にアクセスします。\n% wget https://dl.minio.io:9000/updates/2015/Jun/darwin-amd64/mc % chmod +x mc % ./mc config generate /mc ls http://127.0.0.1:9000/bucket01 [2015-06-25 16:21:37 JST] 0B testfile 上記では予め作っておいた bucket01 という名前のバケットの中身を表示しています。 作り方はこれから minio の Golang ライブラリである minio-go を使って作りました。 これから説明します。\nまた ls コマンドの他にも Usage を確認すると幾つかのサブコマンドが見つかります。\nMinio の Golang ライブラリ minio-go を使ってアクセスする さて、せっかくのオブジェクトストレージも手作業でファイルやバケットのアクセスを 行うのはもったいないです。ソフトウェアを使って操作してす。\nminio のサンプルのコードを参考にして、下記のコードを作成してみました。\npackage main import ( \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; \u0026#34;github.com/minio/minio-go\u0026#34; ) func main() { config := minio.Config{ // AccessKeyID: \u0026#34;YOUR-ACCESS-KEY-HERE\u0026#34;, // SecretAccessKey: \u0026#34;YOUR-PASSWORD-HERE\u0026#34;, Endpoint: \u0026#34;http://127.0.0.1:9000\u0026#34;, } s3Client, err := minio.New(config) if err != nil { log.Fatalln(err) } err = s3Client.MakeBucket(\u0026#34;bucket01\u0026#34;, minio.BucketACL(\u0026#34;public-read-write\u0026#34;)) if err != nil { log.Fatalln(err) } log.Println(\u0026#34;Success: I made a bucket.\u0026#34;) object, err := os.Open(\u0026#34;testfile\u0026#34;) if err != nil { log.Fatalln(err) } defer object.Close() objectInfo, err := object.Stat() if err != nil { object.Close() log.Fatalln(err) } err = s3Client.PutObject(\u0026#34;bucket01\u0026#34;, \u0026#34;testfile\u0026#34;, \u0026#34;application/octet-stream\u0026#34;, objectInfo.Size(), object) if err != nil { log.Fatalln(err) } for bucket := range s3Client.ListBuckets() { if bucket.Err != nil { log.Fatalln(bucket.Err) } log.Println(bucket.Stat) } for object := range s3Client.ListObjects(\u0026#34;bucket01\u0026#34;, \u0026#34;\u0026#34;, true) { if object.Err != nil { log.Fatalln(object.Err) } log.Println(object.Stat) } } 簡単ですがコードの説明をします。\n11行目で config の上書きをします。先ほど起動した minio の EndPoint を記します。 17行目で minio にセッションを張り接続を行っています。 22行目で \u0026lsquo;bucket01\u0026rsquo; というバケットを生成しています。その際にACLも設定 28行目から42行目で \u0026rsquo;testfile\u0026rsquo; というローカルファイルをストレージにPUTしています。 44行目でバケット一覧を表示しています。 51行目で上記で作成したバケットの中のオブジェクト一覧を表示しています。 実行結果は下記のとおりです。\n2015/06/25 16:56:21 Success: I made a bucket. 2015/06/25 16:56:21 {bucket01 2015-06-25 07:56:21.155 +0000 UTC} 2015/06/25 16:56:21 {\u0026#34;d41d8cd98f00b204e9800998ecf8427e\u0026#34; testfile 2015-06-25 07:56:21.158 +0000 UTC 0 {minio minio} STANDARD} バケットの作成とオブジェクトの PUT が正常に行えたことをログから確認できます。\nまとめ 上記の通り、今現在出来ることは少ないですが冒頭にも記したとおり資金調達の話も挙 がってきていますので、これからどのような方向に向かうか楽しみでもあります。また 最初から Golang, Python 等のライブラリが用意されているところが今どきだなぁと想 いました。オブジェクトストレージを手作業で操作するケースは現場では殆ど無いと想 いますので、その辺は現在では当たり前になりつつあるかもしれません。ちなみに Python のライブラリは下記の URL にあります。\nhttps://github.com/minio/minio-py\n以上です。\n","permalink":"https://jedipunkz.github.io/post/2015/06/25/minio/","summary":"\u003cp\u003eこんにちは、@jedipunkz です。\u003c/p\u003e\n\u003cp\u003e久々にブログ更新になりましたが、ウォーミングアップで minio というオブジェクト\nストレージを使ってみたメモを記事にしたいと想います。\u003c/p\u003e\n\u003cp\u003eminio は Minimal Object Storage の名の通り、最小限の小さなオブジェクトストレー\nジになります。公式サイトは下記のとおりです。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://minio.io/\"\u003ehttp://minio.io/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eGolang で記述されていて Apache License v2 の元に公開されています。\u003c/p\u003e\n\u003cp\u003e最近、資金調達の話も挙がっていたので、これから一般的になってくるのかもしれません。\u003c/p\u003e\n\u003cp\u003e早速ですが、minio を動かしてみます。\u003c/p\u003e\n\u003ch2 id=\"minio-を起動する\"\u003eMinio を起動する\u003c/h2\u003e\n\u003cp\u003e方法は mithub.com/minio/minio の README に書かれていますが、バイナリを持ってき\nて実行権限を与えるだけのシンプルな手順になります。\u003c/p\u003e\n\u003cp\u003eLinux でも Mac でも動作しますが、今回私は Mac 上で動作させました。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e% wget https://dl.minio.io:9000/updates/2015/Jun/darwin-amd64/minio\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e% chmod +x minio\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e% ./minio mode memory limit 512MB\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eStarting minio server on: http://127.0.0.1:9000\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eStarting minio server on: http://192.168.1.123:9000\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e起動すると Listening Port と共に EndPoint の URL が表示されます。\u003c/p\u003e\n\u003cp\u003e次に mc という minio client を使って動作確認します。\u003c/p\u003e","title":"オブジェクトストレージ minio を使ってみる"},{"content":"こんにちは。@jedipunkz です。\nVyOS に VXLAN が実装されたと聞いて少し触ってみました。この情報を知ったきっかけ は @upaa さんの下記の資料です。\n参考資料 : http://www.slideshare.net/upaa/vyos-users-meeting-2-vyosvxlan\nVyOS は御存知の通り実体は Debian Gnu/Linux 系の OS でその上に OSS なミドル ウェアが搭載されていて CLI でミドルウェアのコンフィギュレーション等が行えるモ ノになっています。Linux で VXLAN といえば OVS を使ったモノがよく知られています が VyOS の VXLAN 機能は Linux Kernel の実装を使っているようです。\n要件 トンネルを張るためのセグメントを用意 VyOS 1.1.1 (現在最新ステーブルバージョン) が必要 Ubuntu Server 14.04 LTS (同じく Linux VXLAN 搭載バージョン) 構成 特徴\nマネージメント用セグメント 10.0.1.0/24 を用意 GRE と同じくトンネル終端が必要なのでそのためのセグメント 10.0.2.0/24 を用意 各 eth1 は IP reachable である必要があるので予め IP アドレスの設定と疎通を確認 VXLAN を喋れる Ubuntu 14.04 LTS x 1 台と VyOS 1.1.1 x 2 台で相互に疎通確認 +-------------+-------------+------------ Management 10.0.1.0/24 |10.0.0.254 |10.0.0.253 |10.0.0.1 |eth0 |eth0 |eth0 +----------+ +----------+ +----------+ | vyos01 | | vyos02 | | ubuntu | +-+--------+ +----------+ +----------+ | |eth1 | |eth1 | |eth1 | |10.0.2.254 | |10.0.2.253 | |10.0.2.1 | +-----------)-+-----------)-+---------- Tunneling 10.0.2.0/24 | | | +-------------+-------------+------------ VXLAN(eth1にlink) 10.0.1.0/24 10.0.1.254 10.0.1.253 10.0.1.1 設定を投入 vyos01 の設定を行う。VXLAN の設定に必要なものは\u0026hellip;\nVNI (VXLAN Network Ideintity)という識別子 Multicast Group Address 互いに IP reachable なトンネルを張るためのインターフェース です。これらを意識して下記の設定を vyos01 に投入します。\n$ configure % set interfaces vxlan vxlan0 % set interfaces vxlan vxlan0 group 239.1.1.1 % set interfaces vxlan vxlan0 vni 42 % set interfaces vxlan vxlan0 address \u0026#39;10.0.1.254/24\u0026#39; % set interfaces vxlan vxlan0 link eth1 設定を確認します\n% exit $ show int ...\u0026lt;省略\u0026gt;... vxlan vxlan0 { address 10.0.1.254/24 group 239.1.1.1 link eth1 vni 42 } VyOS の CLI を介さず直 Linux の設定を iproute2 で確認してみましょう。 VNI, Multicast Group Address と共に \u0026rsquo;link eth1\u0026rsquo; で設定したトンネルを終端するための物理 NIC が確認できます。\nvyos@vyos01# ip -d link show vxlan0 5: vxlan0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1450 qdisc noqueue state UNKNOWN mode DEFAULT group default link/ether 86:24:26:b2:11:5c brd ff:ff:ff:ff:ff:ff promiscuity 0 vxlan id 42 group 239.1.1.1 dev eth1 port 32768 61000 ttl 16 ageing 300 vyos02 の設定を同様に行います。\n$ congigure % set interfaces vxlan vxlan0 address \u0026#39;10.0.1.253/24\u0026#39; % set interfaces vxlan vxlan0 vni 42 % set interfaces vxlan vxlan0 group 239.1.1.1 % set interfaces vxlan vxlan0 link eth1 設定の確認を行います。\n... 省略 ... vxlan vxlan0 { address 10.0.1.254/24 group 239.1.1.1 link eth1 vni 42 } 同じく Linux の iproute2 で確認します。\nvyos@vyos01# ip -d link show vxlan0 5: vxlan0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1450 qdisc noqueue state UNKNOWN mode DEFAULT group default link/ether 86:24:26:b2:11:5c brd ff:ff:ff:ff:ff:ff promiscuity 0 vxlan id 42 group 239.1.1.1 dev eth1 port 32768 61000 ttl 16 ageing 300 ubuntu ホストの設定を行っていきます。\nUbuntu Server 14.04 LTS であればパッチを当てること無く Linux Kernel の VXLAN 機能を使うことができます。 設定内容は VyOS と同等です。VyOS がこの Linux の実装を使っているのがよく分かります。\nsudo modprobe vxlan sudo ip link add vxlan0 type vxlan id 42 group 239.1.1.1 dev eth1 sudo ip link set up vxlan0 sudo ip a add 10.0.1.1/24 dev vxlan0 同じく Linux iproute2 で確認を行います。\nip -d link show vxlan0 5: vxlan0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1450 qdisc noqueue state UNKNOWN mode DEFAULT group default link/ether d6:ff:c1:27:69:a0 brd ff:ff:ff:ff:ff:ff promiscuity 0 vxlan id 42 group 239.1.1.1 dev eth1 port 32768 61000 ageing 300 疎通確認 疎通確認を行います。\nubuntu -\u0026gt; vyos01 の疎通確認です。ICMP で疎通が取れることを確認できます。\nthirai@ubuntu:~$ ping 10.0.1.254 -c 3 PING 10.0.1.254 (10.0.1.254) 56(84) bytes of data. 64 bytes from 10.0.1.254: icmp_seq=1 ttl=64 time=0.272 ms 64 bytes from 10.0.1.254: icmp_seq=2 ttl=64 time=0.336 ms 64 bytes from 10.0.1.254: icmp_seq=3 ttl=64 time=0.490 ms --- 10.0.1.254 ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 1999ms rtt min/avg/max/mdev = 0.272/0.366/0.490/0.091 ms 次に ubuntu -\u0026gt; vyos02 の疎通確認です。\nthirai@ubuntu:~$ ping 10.0.1.253 -c 3 PING 10.0.1.253 (10.0.1.253) 56(84) bytes of data. 64 bytes from 10.0.1.253: icmp_seq=1 ttl=64 time=0.272 ms 64 bytes from 10.0.1.253: icmp_seq=2 ttl=64 time=0.418 ms 64 bytes from 10.0.1.253: icmp_seq=3 ttl=64 time=0.451 ms --- 10.0.1.253 ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 1998ms rtt min/avg/max/mdev = 0.272/0.380/0.451/0.079 ms この時点で ubuntu ホストの fdb (forwarding db) の内容を確認します。\n$ bridge fdb show dev vxlan0 00:00:00:00:00:00 dst 239.1.1.1 via eth1 self permanent 4e:69:a4:a7:ef:1c dst 10.0.2.253 self 86:24:26:b2:11:5c dst 10.0.2.254 self vyos01, vyos02 のトンネル終端 IP アドレスと Mac アドレスが確認できます。ubuntu ホストから見ると 送信先は vyos0[12] の VXLAN インターフェースではなく、あくまでもトンネル終端を行っているインターフェース になることがわかります。\nまとめ VyOS ver 1.1.0 には VXLAN を物理インターフェースに link する機能に不具合がありそうなので今ら ver 1.1.1 を使うしか なさそう。とは言え、ver 1.1.1 なら普通に動作しました。\nVyOS は仮想ルータという位置付けなので今回紹介したようにインターフェースを VXLAN ネットワークに所属させる 機能があるのみです。VXLAN Trunk を行うような設定はありません。これはハイパーバイザ上で動作させることを前提 に設計されているので仕方ないです..というかスイッチで行うべき機能ですよね..。VM を接続して云々するには OVS のようなソフトウェアスイッチを使えばできます。\nまた fdb は時間が経つと情報が消えます。これは VXLAN のメッシュ構造なトンネルがその都度張られているのかどうか 気になるところです。ICMP の送信で一発目のみマルチキャストでその後ユニキャストになることを確認しましたが、その 一発目のマルチキャストでトンネリングがされるものなのでしょうか\u0026hellip;。あとで調べてみます。OVS のように CLI で トンネルがどのように張られているか確認する手段があれば良いのですが。\n以上です。\n","permalink":"https://jedipunkz.github.io/post/2014/12/16/vyos-vxlan/","summary":"\u003cp\u003eこんにちは。@jedipunkz です。\u003c/p\u003e\n\u003cp\u003eVyOS に VXLAN が実装されたと聞いて少し触ってみました。この情報を知ったきっかけ\nは @upaa さんの下記の資料です。\u003c/p\u003e\n\u003cp\u003e参考資料 : \u003ca href=\"http://www.slideshare.net/upaa/vyos-users-meeting-2-vyosvxlan\"\u003ehttp://www.slideshare.net/upaa/vyos-users-meeting-2-vyosvxlan\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eVyOS は御存知の通り実体は Debian Gnu/Linux 系の OS でその上に OSS なミドル\nウェアが搭載されていて CLI でミドルウェアのコンフィギュレーション等が行えるモ\nノになっています。Linux で VXLAN といえば OVS を使ったモノがよく知られています\nが VyOS の VXLAN 機能は Linux Kernel の実装を使っているようです。\u003c/p\u003e\n\u003ch2 id=\"要件\"\u003e要件\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eトンネルを張るためのセグメントを用意\u003c/li\u003e\n\u003cli\u003eVyOS 1.1.1 (現在最新ステーブルバージョン) が必要\u003c/li\u003e\n\u003cli\u003eUbuntu Server 14.04 LTS (同じく Linux VXLAN 搭載バージョン)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"構成\"\u003e構成\u003c/h2\u003e\n\u003cp\u003e特徴\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eマネージメント用セグメント 10.0.1.0/24 を用意\u003c/li\u003e\n\u003cli\u003eGRE と同じくトンネル終端が必要なのでそのためのセグメント 10.0.2.0/24 を用意\u003c/li\u003e\n\u003cli\u003e各 eth1 は IP reachable である必要があるので予め IP アドレスの設定と疎通を確認\u003c/li\u003e\n\u003cli\u003eVXLAN を喋れる Ubuntu 14.04 LTS x 1 台と VyOS 1.1.1 x 2 台で相互に疎通確認\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e+-------------+-------------+------------ Management 10.0.1.0/24\n|10.0.0.254   |10.0.0.253   |10.0.0.1\n|eth0         |eth0         |eth0\n+----------+  +----------+  +----------+ \n|  vyos01  |  |  vyos02  |  |  ubuntu  |\n+-+--------+  +----------+  +----------+ \n| |eth1       | |eth1       | |eth1\n| |10.0.2.254 | |10.0.2.253 | |10.0.2.1\n| +-----------)-+-----------)-+---------- Tunneling 10.0.2.0/24\n|             |             |\n+-------------+-------------+------------ VXLAN(eth1にlink) 10.0.1.0/24\n10.0.1.254     10.0.1.253    10.0.1.1\n\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"設定を投入\"\u003e設定を投入\u003c/h2\u003e\n\u003cp\u003evyos01 の設定を行う。VXLAN の設定に必要なものは\u0026hellip;\u003c/p\u003e","title":"VyOS で VXLAN を使ってみる"},{"content":"こんにちは。@jedipunkz です。\n自分は Ruby を普段使うのでいつも Fog というライブラリを使って OpenStack, AWS を操作していました。Fog を使うとクラウドの操作が Ruby のネイティブコードで行え るのでシステムコマンド打つよりミス無く済みます。\nFog より後発で Aviator というライブラリが登場してきたので少し使ってみたのです がまだ未完成なところがあるものの便利な点もあって今後に期待だったので紹介します。\n認証情報を yaml ファイルに記す 接続に必要な認証情報を yaml ファイルで記述します。名前を \u0026lsquo;aviator.yml\u0026rsquo; として 保存。この時に下記のように環境毎に認証情報を別けて書くことができます。こうする ことでコードの中で開発用・サービス用等と使い分けられます。\nproduction: provider: openstack auth_service: name: identity host_uri: \u0026lt;Auth URL\u0026gt; request: create_token validator: list_tenants auth_credentials: username: \u0026lt;User Name\u0026gt; password: \u0026lt;Password\u0026gt; tenant_name: \u0026lt;Tenant Name\u0026gt; development: provider: openstack auth_service: name: identity host_uri: \u0026lt;Auth URL\u0026gt; request: create_token validator: list_tenants auth_credentials: username: \u0026lt;User Name\u0026gt; password: \u0026lt;Password\u0026gt; tenant_name: \u0026lt;Tenant Name\u0026gt; シンタックス確認 +++\n次に aviator のシンタックスを確認します。Fog に無い機能で、コマンドラインでシ ンタックスを確認できてしかも指定可能はパラメータと必須なパラメータと共にサンプ ルコードまで提供してくれます。公式サイトに\u0026rsquo;サーバ作成\u0026rsquo;のメソッドが掲載されてい るので、ここでは仮想ディスクを作るシンタックスを確認してみます。\n% gem install aviator % aviator describe openstack volume # \u0026lt;-- 利用可能な機能を確認 Available requests for openstack volume_service: v1 public list_volume_types v1 public list_volumes v1 public delete_volume v1 public create_volume v1 public get_volume v1 public update_volume v1 public root % aviator describe openstack volume v1 public create_volume # \u0026lt;-- シンタックスを確認 :Request =\u0026gt; create_volume Parameters: +---------------------+-----------+ | NAME | REQUIRED? | +---------------------+-----------+ | availability_zone | N | | display_description | Y | | display_name | Y | | metadata | N | | size | Y | | snapshot_id | N | | volume_type | N | +---------------------+-----------+ Sample Code: session.volume_service.request(:create_volume) do |params| params.volume_type = value params.availability_zone = value params.snapshot_id = value params.metadata = value params.display_name = value params.display_description = value params.size = value end このように create_volume というメソッドが用意されていて、指定出来るパラメータ・ 必須なパラメータが確認できます。必須なモノには \u0026ldquo;Y\u0026rdquo; が REQUIRED に付いています。 またサンプルコードが出力されるので、めちゃ便利です。\nでは create_volume のシンタックスがわかったので、コードを書いてみましょう。\nコードを書いてみる +++\n#!/usr/bin/env ruby require \u0026#39;aviator\u0026#39; require \u0026#39;json\u0026#39; volume_session = Aviator::Session.new( :config_file =\u0026gt; \u0026#39;/home/thirai/aviator/aviator.yml\u0026#39;, :environment =\u0026gt; :production, :log_file =\u0026gt; \u0026#39;/home/thirai/aviator/aviator.log\u0026#39; ) volume_session.authenticate volume_session.volume_service.request(:create_volume) do |params| params.display_description = \u0026#39;testvol\u0026#39; params.display_name = \u0026#39;testvol01\u0026#39; params.size = 1 end puts volume_session.volume_service.request(:list_volumes).body 6行目で先ほど作成した認証情報ファイル aviator.yml とログ出力ファイル aviator.log を指定します。12行目で実際に OpenStack にログインしています。\n14-18行目はサンプルコードそのままです。必須パラメータの display_description, display_name, size のみを指定し仮想ディスクを作成しました。最後の puts \u0026hellip; は 実際に作成した仮想ディスク一覧を出力しています。\n結果は下記のとおりです。\n{ volumes: [{ status: \u0026#39;available\u0026#39;, display_name: \u0026#39;testvol01\u0026#39;, attachments: [], availability_zone: \u0026#39;az3\u0026#39;, bootable: \u0026#39;false\u0026#39;, created_at: description = \u0026#39;testvol\u0026#39;, volume_type: \u0026#39;standard\u0026#39;, snapshot_id: nil, source_volid: nil, metadata: }, id: \u0026#39;3a5f616e-a732-4442-a419-10369111bd4c\u0026#39;, size: 1 }] } まとめ +++\nサンプルコードやパラメータ一覧等がひと目でわかる aviator はとても便利です。ま だ利用できるクラウドプラットフォームが OpenStack しかないのと、Neutron の機能 がスッポリ抜けているので、まだ利用するには早いかもです\u0026hellip;。逆に言えばコントリ ビューションするチャンスなので、もし気になった方がいたら開発に参加してみるのも いいかもしれません。\n","permalink":"https://jedipunkz.github.io/post/2014/12/13/aviator-openstack/","summary":"\u003cp\u003eこんにちは。@jedipunkz です。\u003c/p\u003e\n\u003cp\u003e自分は Ruby を普段使うのでいつも Fog というライブラリを使って OpenStack, AWS\nを操作していました。Fog を使うとクラウドの操作が Ruby のネイティブコードで行え\nるのでシステムコマンド打つよりミス無く済みます。\u003c/p\u003e\n\u003cp\u003eFog より後発で Aviator というライブラリが登場してきたので少し使ってみたのです\nがまだ未完成なところがあるものの便利な点もあって今後に期待だったので紹介します。\u003c/p\u003e\n\u003ch2 id=\"認証情報を-yaml-ファイルに記す\"\u003e認証情報を yaml ファイルに記す\u003c/h2\u003e\n\u003cp\u003e接続に必要な認証情報を yaml ファイルで記述します。名前を \u0026lsquo;aviator.yml\u0026rsquo; として\n保存。この時に下記のように環境毎に認証情報を別けて書くことができます。こうする\nことでコードの中で開発用・サービス用等と使い分けられます。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-yaml\" data-lang=\"yaml\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eproduction\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#f92672\"\u003eprovider\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003eopenstack\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#f92672\"\u003eauth_service\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003ename\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003eidentity\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003ehost_uri\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003e\u0026lt;Auth URL\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003erequest\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003ecreate_token\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003evalidator\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003elist_tenants\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#f92672\"\u003eauth_credentials\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003eusername\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003e\u0026lt;User Name\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003epassword\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003e\u0026lt;Password\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003etenant_name\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003e\u0026lt;Tenant Name\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003edevelopment\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#f92672\"\u003eprovider\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003eopenstack\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#f92672\"\u003eauth_service\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003ename\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003eidentity\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003ehost_uri\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003e\u0026lt;Auth URL\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003erequest\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003ecreate_token\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003evalidator\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003elist_tenants\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#f92672\"\u003eauth_credentials\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003eusername\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003e\u0026lt;User Name\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003epassword\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003e\u0026lt;Password\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003etenant_name\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003e\u0026lt;Tenant Name\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eシンタックス確認\n+++\u003c/p\u003e\n\u003cp\u003e次に aviator のシンタックスを確認します。Fog に無い機能で、コマンドラインでシ\nンタックスを確認できてしかも指定可能はパラメータと必須なパラメータと共にサンプ\nルコードまで提供してくれます。公式サイトに\u0026rsquo;サーバ作成\u0026rsquo;のメソッドが掲載されてい\nるので、ここでは仮想ディスクを作るシンタックスを確認してみます。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e% gem install aviator\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e% aviator describe openstack volume \u003cspan style=\"color:#75715e\"\u003e# \u0026lt;-- 利用可能な機能を確認\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eAvailable requests \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e openstack volume_service:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ev1 public list_volume_types\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ev1 public list_volumes\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ev1 public delete_volume\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ev1 public create_volume\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ev1 public get_volume\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ev1 public update_volume\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  v1 public root\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e% aviator describe openstack volume v1 public create_volume \u003cspan style=\"color:#75715e\"\u003e# \u0026lt;-- シンタックスを確認\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e:Request \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u0026gt; create_volume\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eParameters:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e +---------------------+-----------+\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e | NAME                | REQUIRED? |\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e +---------------------+-----------+\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e | availability_zone   |     N     |\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e | display_description |     Y     |\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e | display_name        |     Y     |\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e | metadata            |     N     |\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e | size                |     Y     |\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e | snapshot_id         |     N     |\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e | volume_type         |     N     |\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e +---------------------+-----------+\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eSample Code:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  session.volume_service.request\u003cspan style=\"color:#f92672\"\u003e(\u003c/span\u003e:create_volume\u003cspan style=\"color:#f92672\"\u003e)\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003edo\u003c/span\u003e |params|\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    params.volume_type \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e value\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    params.availability_zone \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e value\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    params.snapshot_id \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e value\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    params.metadata \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e value\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    params.display_name \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e value\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    params.display_description \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e value\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    params.size \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e value\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  end\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eこのように create_volume というメソッドが用意されていて、指定出来るパラメータ・\n必須なパラメータが確認できます。必須なモノには \u0026ldquo;Y\u0026rdquo; が REQUIRED に付いています。\nまたサンプルコードが出力されるので、めちゃ便利です。\u003c/p\u003e","title":"Aviator でモダンに OpenStack を操作する"},{"content":"こんにちは。@jedipunkz です。\nOpenStack Juno がリリースされましたが、今日は Icehouse ネタです。\nicehouse 以降、自分の中で OpenStack を自動で作る仕組みが無くなりつつあり、気軽 に OpenStack を作って色々試したい！ッていう時に手段が無く困っていました。例え ば仕事でちょっと OpenStack 弄りたい！って時に DevStack, RDO しかなく。DevStack は御存知の通り動かない可能性が結構あるし RDO は Ubuntu/Debian Gnu Linux ベース じゃないし。\nってことで、以前にも紹介した stackforge 管理の openstack-chef-repo と Chef-Zero を使って OpenStack Icehouse (Neutron) のオールインワン構成を作る方法 を書きます。ちなみに最近 Chef-Solo が Chef-Zero に置き換わりつつあるらしいです。 Chef-Zero はオンメモリで Chef サーバを起動する仕組みです。Chef-Solo と違って Chef サーバを扱う時と何も変更無く操作が出来るのでとても楽です。また、Chef サーバを 別途構、構築・管理しなくて良いので、気軽に OpenStack が作れます。\nちなみに stackforge/openstack-chef-repo の README.md に Chef-Zero での構築方法 が書いてありますが、沢山の問題があります。\nnova-network 構成 API の Endpoint が全て localhost に向いてしまうため外部から操作不可能 各コンポーネントの bind_address が localhost を向いてしまう berkshelf がそのままでは入らない よって、今回はこれらの問題を解決しつつ \u0026ldquo;オールインワンな Neutron 構成の Icehouse OpenStack を作る方法\u0026rdquo; を書いていきます。\n構成 +----------------- 10.0.0.0/24 (api/management network) | +----------------+ | OpenStack Node | | Controller | | Compute | +----------------+ | | +--(-------------- 10.0.1.0/24 (external network) | +-------------- 10.0.2.0/24 (guest vm network) IP address 達\n10.0.0.10 (api/manageent network) : eth0 10.0.1.10 (external network) : eth1 10.0.2.10 (guest vm network) : eth2 注意 : 操作は全て eth0 経由で行う\n前提の環境 stackforge/openstack-chef-repo の依存している Cookbooks の関係上、upstart 周り がうまく制御できていないので Ubuntu Server 12.04.x を使います。\nインストール方法 上記のように3つのネットワークインターフェースが付いたサーバを1台用意します。 KVM が利用出来たほうがいいですが使えないくても構いません。KVM リソースが使えな い場合の修正方法を後に記します。\nサーバにログインし root ユーザになります。その後 Chef をオムニバスインストーラ でインストールします。\n% sudo -i # curl -L https://www.opscode.com/chef/install.sh | bash 次に stable/icehose ブランチを指定して openstack-chef-repo をクローンします。\n# cd ~ # git clone -b stable/icehouse https://github.com/stackforge/openstack-chef-repo # berkshelf をインストールするのですが依存パッケージが足らないのでここでインストー ルします。\n# apt-get -y install build-essential zlib1g-dev libssl-dev libreadline-dev \\ ruby-dev libxml2-dev libxslt-dev g++ berkshelf をインストールします。\n# /opt/chef/embedded/bin/gem install berkshelf --no-ri --no-rdoc 次に openstack-chef-repo に依存する Cookbooks を取得します。\n# cd ~/openstack-chef-repo # /opt/chef/embedded/bin/berks vendor ./cookbooks ~/openstack-chef-repo/environments ディレクトリ配下に neutron-allinone.json と いうファイル名で作成します。内容は下記の通りです。\n{ [0/215] \u0026#34;name\u0026#34;: \u0026#34;neutron-allinone\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;test\u0026#34;, \u0026#34;cookbook_versions\u0026#34;: { }, \u0026#34;json_class\u0026#34;: \u0026#34;Chef::Environment\u0026#34;, \u0026#34;chef_type\u0026#34;: \u0026#34;environment\u0026#34;, \u0026#34;default_attributes\u0026#34;: { }, \u0026#34;override_attributes\u0026#34;: { \u0026#34;mysql\u0026#34;: { \u0026#34;bind_address\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;server_root_password\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;server_debian_password\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;server_repl_password\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;allow_remote_root\u0026#34;: true, \u0026#34;root_network_acl\u0026#34;: [\u0026#34;10.0.0.0/8\u0026#34;] }, \u0026#34;rabbitmq\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;10.0.1.10\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;5672\u0026#34; }, \u0026#34;openstack\u0026#34;: { \u0026#34;auth\u0026#34;: { \u0026#34;validate_certs\u0026#34;: false }, \u0026#34;dashboard\u0026#34;: { \u0026#34;session_backend\u0026#34;: \u0026#34;file\u0026#34; }, \u0026#34;block-storage\u0026#34;: { \u0026#34;syslog\u0026#34;: { \u0026#34;use\u0026#34;: false }, \u0026#34;api\u0026#34;: { \u0026#34;ratelimit\u0026#34;: \u0026#34;False\u0026#34; }, \u0026#34;debug\u0026#34;: true, \u0026#34;image_api_chef_role\u0026#34;: \u0026#34;os-image\u0026#34;, \u0026#34;identity_service_chef_role\u0026#34;: \u0026#34;os-identity\u0026#34;, \u0026#34;rabbit_server_chef_role\u0026#34;: \u0026#34;os-ops-messaging\u0026#34; }, \u0026#34;compute\u0026#34;: { \u0026#34;rabbit\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34; }, \u0026#34;novnc_proxy\u0026#34;: { \u0026#34;bind_interface\u0026#34;: \u0026#34;eth0\u0026#34; }, \u0026#34;libvirt\u0026#34;: { \u0026#34;bind_interface\u0026#34;: \u0026#34;eth0\u0026#34;, }, \u0026#34;novnc_proxy\u0026#34;: { \u0026#34;bind_interface\u0026#34;: \u0026#34;eth0\u0026#34; }, \u0026#34;xvpvnc_proxy\u0026#34;: { \u0026#34;bind_interface\u0026#34;: \u0026#34;eth0\u0026#34; }, \u0026#34;image_api_chef_role\u0026#34;: \u0026#34;os-image\u0026#34;, \u0026#34;identity_service_chef_role\u0026#34;: \u0026#34;os-identity\u0026#34;, \u0026#34;nova_setup_chef_role\u0026#34;: \u0026#34;os-compute-api\u0026#34;, \u0026#34;rabbit_server_chef_role\u0026#34;: \u0026#34;os-ops-messaging\u0026#34;, \u0026#34;network\u0026#34;: { \u0026#34;public_interface\u0026#34;: \u0026#34;eth0\u0026#34;, \u0026#34;service_type\u0026#34;: \u0026#34;neutron\u0026#34; } }, \u0026#34;network\u0026#34;: { \u0026#34;debug\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;dhcp\u0026#34;: { \u0026#34;enable_isolated_metadata\u0026#34;: \u0026#34;True\u0026#34; }, \u0026#34;metadata\u0026#34;: { \u0026#34;nova_metadata_ip\u0026#34;: \u0026#34;10.0.1.10\u0026#34; }, \u0026#34;openvswitch\u0026#34;: { \u0026#34;tunnel_id_ranges\u0026#34;: \u0026#34;1:1000\u0026#34;, \u0026#34;enable_tunneling\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;tenant_network_type\u0026#34;: \u0026#34;gre\u0026#34;, \u0026#34;local_ip_interface\u0026#34;: \u0026#34;eth2\u0026#34; }, \u0026#34;api\u0026#34;: { \u0026#34;bind_interface\u0026#34;: \u0026#34;eth0\u0026#34; }, \u0026#34;l3\u0026#34;: { \u0026#34;external_network_bridge_interface\u0026#34;: \u0026#34;eth1\u0026#34; }, \u0026#34;service_plugins\u0026#34;: [\u0026#34;neutron.services.l3_router.l3_router_plugin.L3RouterPlugin\u0026#34;] }, \u0026#34;db\u0026#34;: { \u0026#34;bind_interface\u0026#34;: \u0026#34;eth0\u0026#34;, \u0026#34;compute\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34; }, \u0026#34;identity\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34; }, \u0026#34;image\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34; }, \u0026#34;network\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34; }, \u0026#34;volume\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34; }, \u0026#34;dashboard\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34; }, \u0026#34;telemetry\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34; }, \u0026#34;orchestration\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34; } }, \u0026#34;developer_mode\u0026#34;: true, \u0026#34;endpoints\u0026#34;: { \u0026#34;compute-api-bind\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8774\u0026#34; }, \u0026#34;compute-api\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8774\u0026#34; }, \u0026#34;compute-ec2-admin-bind\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8773\u0026#34; }, \u0026#34;compute-ec2-admin\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8773\u0026#34; }, \u0026#34;compute-ec2-api-bind\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8773\u0026#34; }, \u0026#34;compute-ec2-api\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8773\u0026#34; }, \u0026#34;compute-xvpvnc\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;6081\u0026#34; }, \u0026#34;compute-novnc-bind\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;6080\u0026#34; }, \u0026#34;compute-novnc\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;6080\u0026#34; }, \u0026#34;compute-vnc\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;6080\u0026#34; }, \u0026#34;image-api\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;9292\u0026#34; }, \u0026#34;image-api-bind\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;9292\u0026#34; }, \u0026#34;image-registry\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;9191\u0026#34; }, \u0026#34;image-registry-bind\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;9191\u0026#34; }, \u0026#34;identity-api\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;5000\u0026#34; }, \u0026#34;identity-bind\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;5000\u0026#34; }, \u0026#34;identity-admin\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;35357\u0026#34; }, \u0026#34;volume-api-bind\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8776\u0026#34; }, \u0026#34;volume-api\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8776\u0026#34; }, \u0026#34;telemetry-api\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8777\u0026#34; }, \u0026#34;network-api-bind\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;9696\u0026#34; }, \u0026#34;network-api\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;9696\u0026#34; }, \u0026#34;orchestration-api\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8004\u0026#34; }, \u0026#34;orchestration-api-cfn\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;8000\u0026#34; } }, \u0026#34;identity\u0026#34;: { \u0026#34;admin_user\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;bind_interface\u0026#34;: \u0026#34;eth0\u0026#34;, \u0026#34;debug\u0026#34;: true }, \u0026#34;image\u0026#34;: { \u0026#34;api\u0026#34;: { \u0026#34;bind_interface\u0026#34;: \u0026#34;eth0\u0026#34; }, \u0026#34;debug\u0026#34;: true, \u0026#34;identity_service_chef_role\u0026#34;: \u0026#34;os-identity\u0026#34;, \u0026#34;rabbit_server_chef_role\u0026#34;: \u0026#34;os-ops-messaging\u0026#34;, \u0026#34;registry\u0026#34;: { \u0026#34;bind_interface\u0026#34;: \u0026#34;eth0\u0026#34; }, \u0026#34;syslog\u0026#34;: { \u0026#34;use\u0026#34;: false }, \u0026#34;upload_images\u0026#34;: [ \u0026#34;precise\u0026#34; ] }, \u0026#34;mq\u0026#34;: { \u0026#34;bind_interface\u0026#34;: \u0026#34;eth0\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;guest\u0026#34;, \u0026#34;vhost\u0026#34;: \u0026#34;/nova\u0026#34;, \u0026#34;network\u0026#34;: { \u0026#34;rabbit\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;5672\u0026#34; } }, \u0026#34;compute\u0026#34;: { \u0026#34;service_type\u0026#34;: \u0026#34;rabbitmq\u0026#34;, \u0026#34;rabbit\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;5672\u0026#34; } }, \u0026#34;block-storage\u0026#34;: { \u0026#34;service_type\u0026#34;: \u0026#34;rabbitmq\u0026#34;, \u0026#34;rabbit\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;5672\u0026#34; } } } }, \u0026#34;queue\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.0.1.10\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;guest\u0026#34;, \u0026#34;vhost\u0026#34;: \u0026#34;/nova\u0026#34; } } } 内容について全て説明するのは難しいですが、このファイルを作成するのが今回一番苦 労した点です。と言うのは、構成を作りつつそれぞれのコンポーネントのコンフィギュ レーション、エンドポイントのアドレス、バインドアドレス、リスンポート等など、全 てが正常な値になるように Cookbooks を読みつつ作業するからです。この json ファ イルが完成してしまえば、あとは簡単なのですが。\n前述しましたが KVM リソースが使えない環境の場合 Qemu で仮想マシンを稼働するこ とができます。その場合、下記のように \u0026ldquo;libvirt\u0026rdquo; の項目に \u0026ldquo;virt_type\u0026rdquo; を追記して ください。\n\u0026#34;libvirt\u0026#34;: { \u0026#34;bind_interface\u0026#34;: \u0026#34;eth0\u0026#34;, \u0026#34;virt_type\u0026#34;: \u0026#34;qemu\u0026#34; # \u0026lt;------ 追記 }, それではデプロイしていきます。\nここで \u0026lsquo;allinone\u0026rsquo; はホスト名、\u0026lsquo;allinone-compute\u0026rsquo; は Role 名、neutron-allinone は先ほど作成した json で指定している environment 名です。\n# chef-client -z # knife node -z run_list add allinone \u0026#39;role[allinone-compute]\u0026#39; # chef-client -z -E neutron-allinone 環境にもよりますが、数分でオールインワンな OpenStack Icehouse が完成します。\nまとめ +++\nChef サーバを使わなくて良いのでお手軽に OpenStack が構築出来ました。この json ファイルは実は他にも応用出来ると思っています。複数台構成の OpenStack も指定 Role を工夫すれば構築出来るでしょう。が、その場合は chef-zero は使えません。 Chef サーバ構成にする必要が出てきます。\nちなみに OpenStack Paris Summit 2014 で「OpenStack のデプロイに何を使っている か？」という調査結果が下記になります。Chef が2位ですが Pueppet に大きく離され ている感があります。Juno 版の openstack-chef-repo も開発が進んでいますので、頑 張って広めていきたいです。\n1位 Puppet 2位 Chef 3位 Ansible 4位 DevStack 5位 PackStack 6位 Salt 7位 Juju 8位 Crowbar 9位 CFEngine 参考 URL : http://superuser.openstack.org/articles/openstack-user-survey-insights-november-2014\nちなみに、Puppet を使った OpenStack デプロイも個人的に色々試しています。\n","permalink":"https://jedipunkz.github.io/post/2014/11/15/chef-zero-openstack-icehouse/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003eOpenStack Juno がリリースされましたが、今日は Icehouse ネタです。\u003c/p\u003e\n\u003cp\u003eicehouse 以降、自分の中で OpenStack を自動で作る仕組みが無くなりつつあり、気軽\nに OpenStack を作って色々試したい！ッていう時に手段が無く困っていました。例え\nば仕事でちょっと OpenStack 弄りたい！って時に DevStack, RDO しかなく。DevStack\nは御存知の通り動かない可能性が結構あるし RDO は Ubuntu/Debian Gnu Linux ベース\nじゃないし。\u003c/p\u003e\n\u003cp\u003eってことで、以前にも紹介した stackforge 管理の openstack-chef-repo と\nChef-Zero を使って OpenStack Icehouse (Neutron) のオールインワン構成を作る方法\nを書きます。ちなみに最近 Chef-Solo が Chef-Zero に置き換わりつつあるらしいです。\nChef-Zero はオンメモリで Chef サーバを起動する仕組みです。Chef-Solo と違って Chef\nサーバを扱う時と何も変更無く操作が出来るのでとても楽です。また、Chef サーバを\n別途構、構築・管理しなくて良いので、気軽に OpenStack が作れます。\u003c/p\u003e\n\u003cp\u003eちなみに stackforge/openstack-chef-repo の README.md に Chef-Zero での構築方法\nが書いてありますが、沢山の問題があります。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003enova-network 構成\u003c/li\u003e\n\u003cli\u003eAPI の Endpoint が全て localhost に向いてしまうため外部から操作不可能\u003c/li\u003e\n\u003cli\u003e各コンポーネントの bind_address が localhost を向いてしまう\u003c/li\u003e\n\u003cli\u003eberkshelf がそのままでは入らない\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eよって、今回はこれらの問題を解決しつつ \u0026ldquo;オールインワンな Neutron 構成の\nIcehouse OpenStack を作る方法\u0026rdquo; を書いていきます。\u003c/p\u003e","title":"Chef-Zero でお手軽に OpenStack Icehouse を作る"},{"content":"こんにちは。@jedipunkz です。\n昨晩 Midokura さんが Midonet を OSS 化したとニュースになりました。公式サイトは 下記の URL になっています。Midonet は OpenStack Neutron のプラグインとして動作 するソフトウェアです。\nhttp://www.midonet.org\n下記のGithub 上でソースを公開しています。\nhttps://github.com/midonet\n本体の midonet と共に midostack というレポジトリがあってどうやら公式サイトの QuickStart を見ても devstack を交えての簡単な midonet の動作が確認できそう。\nhttps://github.com/midonet/midostack\n早速使ってみる 早速 midostack を使って midonet を体験してみましょう。QuickStart には Vagrant + VirtualBox を用いた使い方が改定ありますが手元の PC 端末だとリソース が足らなくて CirrOS VM 一個すら立ち上がりませんでした。よって普通にリソースの 沢山あるサーバで稼働してみます。Vagrantfile 見ても\nconfig.vm.synced_folder \u0026#34;./\u0026#34;, \u0026#34;/midostack\u0026#34; としているだけなので、Vagrant ではなくても大丈夫でしょう。\nUbuntu Server 14.04 をインストールしたマシンを用意して midostack を取得します。\n% git clone https://github.com/midonet/midostack.git midonet_stack.sh を実行します。\n% cd midostack % ./midonet_stack.sh 暫く待つと Neutron Middonet Plugin が有効になった OpenStack が立ち上がります。 Horizon にアクセスしましょう。ユーザ名 : admin, パスワード : gogomid0 (デフォ ルト) です。\nVM も普通に立ち上がりますし VM 同士の通信も良好です。\nNeutron プロセスを確認する Neutron-Server は下記のように立ち上がっています。\n16229 pts/13 S+ 0:06 python /usr/local/bin/neutron-server --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugins/midonet/midonet.ini /etc/neutron/neutron.conf の midonet の指定はこんな感じ。\ncore_plugin = midonet.neutron.plugin.MidonetPluginV2 api_extensions_path = /opt/stack/midonet/python-neutron-plugin-midonet/midonet/neutron/extensions 次に /etc/neutron/plugins/midonet/midonet.ini を確認してみましょう。\n[midonet] # MidoNet API server URI # midonet_uri = http://localhost:8080/midonet-api # MidoNet admin username # username = admin # MidoNet admin password # password = passw0rd # ID of the project that MidoNet admin user belongs to # project_id = 77777777-7777-7777-7777-777777777777 # Virtual provider router ID # provider_router_id = 00112233-0011-0011-0011-001122334455 # Path to midonet host uuid file # midonet_host_uuid_path = /etc/midolman/host_uuid.properties [MIDONET] project_id = admin password = gogomid0 username = admin midonet_uri = http://localhost:8081/midonet-api Midonet API にアクセスする Midonet API のリファレンスが下記の URL で公開されていました。\nhttp://docs.midonet.org/docs/v1.8/rest-api/api/rest-api-specification.html\n早速使ってみましょう。まず Token を得ます。\ncurl -i \u0026#39;http://127.0.0.1:5000/v2.0/tokens\u0026#39; -X POST -H \u0026#34;Content-Type: application/json\u0026#34; -H \u0026#34;Accept: application/json\u0026#34; -d \u0026#39;{\u0026#34;auth\u0026#34;: {\u0026#34;tenantName\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;passwordCredentials\u0026#34;: {\u0026#34;username\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;gogomid0\u0026#34;}}}\u0026#39; Token ID を取得したら \u0026ldquo;/\u0026rdquo; に対してアクセスしてみましょう。\n% curl -i -X GET http://localhost:8081/midonet-api/ -H \u0026#34;User-Agent: python-keystoneclient\u0026#34; -H \u0026#34;X-Auth-Token: \u0026lt;TokenID\u0026gt;\u0026#34; レスポンス\n{ \u0026#34;routerTemplate\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/routers/{id}\u0026#34;, \u0026#34;portTemplate\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/ports/{id}\u0026#34;, \u0026#34;vipTemplate\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/vips/{id}\u0026#34;, \u0026#34;poolTemplate\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/pools/{id}\u0026#34;, \u0026#34;healthMonitorTemplate\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/health_monitors/{id}\u0026#34;, \u0026#34;healthMonitors\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/health_monitors\u0026#34;, \u0026#34;loadBalancers\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/load_balancers\u0026#34;, \u0026#34;ipAddrGroupTemplate\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/ip_addr_groups/{id}\u0026#34;, \u0026#34;tenants\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/tenants\u0026#34;, \u0026#34;tenantTemplate\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/tenants/{id}\u0026#34;, \u0026#34;portGroupTemplate\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/port_groups/{id}\u0026#34;, \u0026#34;loadBalancerTemplate\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/load_balancers/{id}\u0026#34;, \u0026#34;poolMemberTemplate\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/pool_members/{id}\u0026#34;, \u0026#34;hostVersions\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/versions\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;v1.7\u0026#34;, \u0026#34;bridgeTemplate\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/bridges/{id}\u0026#34;, \u0026#34;hostTemplate\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/hosts/{id}\u0026#34;, \u0026#34;uri\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/\u0026#34;, \u0026#34;vteps\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/vteps\u0026#34;, \u0026#34;tunnelZoneTemplate\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/tunnel_zones/{id}\u0026#34;, \u0026#34;ipAddrGroups\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/ip_addr_groups\u0026#34;, \u0026#34;writeVersion\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/write_version\u0026#34;, \u0026#34;chainTemplate\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/chains/{id}\u0026#34;, \u0026#34;vtepTemplate\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/vteps/{ipAddr}\u0026#34;, \u0026#34;adRouteTemplate\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/ad_routes/{id}\u0026#34;, \u0026#34;bgpTemplate\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/bgps/{id}\u0026#34;, \u0026#34;hosts\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/hosts\u0026#34;, \u0026#34;routeTemplate\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/routes/{id}\u0026#34;, \u0026#34;ruleTemplate\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/rules/{id}\u0026#34;, \u0026#34;systemState\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/system_state\u0026#34;, \u0026#34;vips\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/vips\u0026#34;, \u0026#34;pools\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/pools\u0026#34;, \u0026#34;routers\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/routers\u0026#34;, \u0026#34;bridges\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/bridges\u0026#34;, \u0026#34;chains\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/chains\u0026#34;, \u0026#34;portGroups\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/port_groups\u0026#34;, \u0026#34;poolMembers\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/pool_members\u0026#34;, \u0026#34;tunnelZones\u0026#34;: \u0026#34;http://localhost:8081/midonet-api/tunnel_zones\u0026#34; } なんとなく引数にこれらの文字列を渡せばいいのだなと分かります。\n次に neutron の管理している subnets を確認してみましょう。\n% curl -i -X GET http://localhost:8081/midonet-api/neutron/subnets -H \u0026#34;User-Agent: python-keystoneclient\u0026#34; -H \u0026#34;X-Auth-Token: \u0026lt;TokenID\u0026gt;\u0026#34; レスポンス\n[ { \u0026#34;enable_dhcp\u0026#34;: false, \u0026#34;tenant_id\u0026#34;: \u0026#34;65f7012145d84ac5afc36572eabe5b09\u0026#34;, \u0026#34;host_routes\u0026#34;: [], \u0026#34;dns_nameservers\u0026#34;: [], \u0026#34;id\u0026#34;: \u0026#34;3dbe5cff-8a8c-4790-85b5-b789d8ede863\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;public-subnet\u0026#34;, \u0026#34;cidr\u0026#34;: \u0026#34;200.200.200.0/24\u0026#34;, \u0026#34;shared\u0026#34;: false, \u0026#34;ip_version\u0026#34;: 4, \u0026#34;network_id\u0026#34;: \u0026#34;45269fba-e32f-40b0-a542-f5cfe34ce1a1\u0026#34;, \u0026#34;gateway_ip\u0026#34;: \u0026#34;200.200.200.1\u0026#34;, \u0026#34;allocation_pools\u0026#34;: [ { \u0026#34;last_ip\u0026#34;: null, \u0026#34;first_ip\u0026#34;: null } ] }, { \u0026#34;enable_dhcp\u0026#34;: true, \u0026#34;tenant_id\u0026#34;: \u0026#34;f34b4398015546b8b84f50c731ed6c51\u0026#34;, \u0026#34;host_routes\u0026#34;: [], \u0026#34;dns_nameservers\u0026#34;: [], \u0026#34;id\u0026#34;: \u0026#34;3dbcf04a-9738-4b1f-b084-76f2a4b17cbc\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;private-subnet\u0026#34;, \u0026#34;cidr\u0026#34;: \u0026#34;10.0.0.0/24\u0026#34;, \u0026#34;shared\u0026#34;: false, \u0026#34;ip_version\u0026#34;: 4, \u0026#34;network_id\u0026#34;: \u0026#34;2edb78c3-0f23-4e29-a3e6-cc97f55baa6a\u0026#34;, \u0026#34;gateway_ip\u0026#34;: \u0026#34;10.0.0.1\u0026#34;, \u0026#34;allocation_pools\u0026#34;: [ { \u0026#34;last_ip\u0026#34;: null, \u0026#34;first_ip\u0026#34;: null } ] } ] ２つのサブネットが確認出来ました。\nまとめ 勉強不足でまだ全く midonet で出来る事がわからない..汗。でもとりあえず動かせた し、API も引っ張れるのでこれから色々試せそうですね。OSS 化されたことで、コミュ ニティの間でも使われていくことも想像出来ますし、自分たち技術者としてはとても有 り難いことでした。\n","permalink":"https://jedipunkz.github.io/post/2014/11/04/midostack/","summary":"\u003cp\u003eこんにちは。@jedipunkz です。\u003c/p\u003e\n\u003cp\u003e昨晩 Midokura さんが Midonet を OSS 化したとニュースになりました。公式サイトは\n下記の URL になっています。Midonet は OpenStack Neutron のプラグインとして動作\nするソフトウェアです。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://www.midonet.org\"\u003ehttp://www.midonet.org\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e下記のGithub 上でソースを公開しています。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/midonet\"\u003ehttps://github.com/midonet\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e本体の midonet と共に midostack というレポジトリがあってどうやら公式サイトの\nQuickStart を見ても devstack を交えての簡単な midonet の動作が確認できそう。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/midonet/midostack\"\u003ehttps://github.com/midonet/midostack\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"早速使ってみる\"\u003e早速使ってみる\u003c/h2\u003e\n\u003cp\u003e早速 midostack を使って midonet を体験してみましょう。QuickStart には\nVagrant + VirtualBox を用いた使い方が改定ありますが手元の PC 端末だとリソース\nが足らなくて CirrOS VM 一個すら立ち上がりませんでした。よって普通にリソースの\n沢山あるサーバで稼働してみます。Vagrantfile 見ても\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003econfig.vm.synced_folder \u0026#34;./\u0026#34;, \u0026#34;/midostack\u0026#34;\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eとしているだけなので、Vagrant ではなくても大丈夫でしょう。\u003c/p\u003e\n\u003cp\u003eUbuntu Server 14.04 をインストールしたマシンを用意して midostack を取得します。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e% git clone https://github.com/midonet/midostack.git\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003emidonet_stack.sh を実行します。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e% cd midostack\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e% ./midonet_stack.sh\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e暫く待つと Neutron Middonet Plugin が有効になった OpenStack が立ち上がります。\nHorizon にアクセスしましょう。ユーザ名 : admin, パスワード : gogomid0 (デフォ\nルト) です。\u003c/p\u003e","title":"MidoStack を動かしてみる"},{"content":"こんにちは。@jedipunkz です。\n昨晩 Chef が Chef-Container を発表しました。\nhttp://www.getchef.com/blog/2014/07/15/release-chef-container-0-2-0-beta/ http://docs.opscode.com/containers.html まだ Beta リリースでバージョンは 0.2.0 です。(gem だと 0.1.1)\nDocker を代表とするコンテナ周りの技術が最近、盛んにリリースされていますし、今 後クラウドプラットフォーム上でコンテナを使ってアプリを動かすケースも増えてくる のではないでしょうか。Dockerfile を使っても Chef-Solo を使ってソフトウェアをデ プロイ出来るのだけどそれだけだとしんどいので、コンテナに特化した Chef が出てき たってことだと思います。特徴として SSH でログインしてブートストラップするので はなくて Runit + Chef-init を用いてコンテナにデプロイすることが挙げられます。\nでは実際に使ってみたのでその時の手順をまとめてみます。\n事前に用意する環境 下記のソフトウェアを予めインストールしておきましょう。\ndocker chef berkshelf ここで注意なのですが後に knife コマンドを使って Docker イメージをビルドします。 つまり root 権限が必要です。rbenv 等を使って ruby, chef をインストールすると、 辛いかもしれませんので OS のパッケージを使ってインストールすると良いと思います。 この辺りは今後改善策が出てくるかも\u0026hellip;。\n尚、インストール方法はここでは割愛します。\nChef-Container のインストール 下記の2つの Gems をインストールします。\nknife-container chef-container % sudo gem install knife-container % sudo gem install chef-container 使用方法 まず knife コマンドを使って操作に必要なディレクトリとファイルを生成します。\n% knife container docker init chef/ubuntu-12.04 -r \u0026#39;recipe[apache2]\u0026#39; -z -b ここで \u0026lsquo;chef/ubuntu-12.04\u0026rsquo; は Docker のイメージ名です。chef-init 等の環境が予 め入っていました。このイメージ以外では今のところ動作を確認していません..。これは後にまとめで触れます。\n上記のコマンドの結果で得られるディレクトリとファイル達です。\n. └── dockerfiles └── chef └── ubuntu-12.04 ├── Berksfile ├── chef │ ├── first-boot.json │ └── zero.rb └── Dockerfile また dockerfiles/chef/ubuntu-12.04/Dockerfile を確認すると\u0026hellip;\n# BASE chef/ubuntu-12.04:latest FROM chef/ubuntu-12.04 ADD chef/ /etc/chef/ RUN chef-init --bootstrap RUN rm -rf /etc/chef/secure/* ENTRYPOINT [\u0026#34;chef-init\u0026#34;] CMD [\u0026#34;--onboot\u0026#34;] イメージを取得 -\u0026gt; ディレクトリ同期 -\u0026gt; chef-init 実行 -\u0026gt; /etc/chef/secure 配下削除、と 実行しているようです。\n次に first-boot.json という名前のファイルを生成します。chef-init が解釈するファ イルです。\n{ \u0026#34;run_list\u0026#34;: [ \u0026#34;recipe[apache2]\u0026#34; ], \u0026#34;container_service\u0026#34;: { \u0026#34;apache2\u0026#34;: { \u0026#34;command\u0026#34;: \u0026#34;/usr/sbin/apache2 -k start\u0026#34; } } } ではいよいよ knife コマンドで Docker イメージをビルドします。\n% sudo knife container docker build chef/ubuntu-12.04 -z すると、下記のように Docker イメージが出来上がります。\n% sudo docker images REPOSITORY TAG IMAGE ID CREATED VIRTUAL SIZE chef/ubuntu-12.04 11 03fd2357596f 4 days ago 397.7 MB chef/ubuntu-12.04 11.12 03fd2357596f 4 days ago 397.7 MB chef/ubuntu-12.04 11.12.8 03fd2357596f 4 days ago 397.7 MB 出来上がったイメージを利用してコンテナを稼働します。\n% sudo docker run chef/ubuntu-12.04 % sudo docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 191cfdaf0bdb 650a89f73ed8 chef-init --onboot 39 minutes ago Up 39 minutes agitated_almeida まとめ コンテナと言っても今現在は Docker のみに対応しているようです。また init の際に指定する Docker イメージ の中に chef-init が入っている必要がありそうです。Build する前に予めイメージを作っておく必要があるという のはしんどいので、今後改善されるかもしれません。\nそもそも Docker やコンテナ技術の登場で Puppet, Chef を代表とするツール類が不要になるのでは？という議論が 幾つかの場面であったように思います。つまりコンテナのイメージに予めソフトウェアを配布しそれを用いて稼働 することで、マシンが起動した後にデプロイすることが必要ないよね？という発想です。今回紹介したようにコンテナの イメージを生成するのに Chef を用いるということであれば、また別の議論になりそうです。また稼働したコンテナに ソフトウェアをデプロイすることも場合によっては必要なので、この辺りの技術の完成度が上がることを期待したいです。\n参考 URL CreationLine さんブログ http://www.creationline.com/lab/5346 公式サイト http://docs.opscode.com/containers.html ","permalink":"https://jedipunkz.github.io/post/2014/07/16/chef-container/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003e昨晩 Chef が Chef-Container を発表しました。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"http://www.getchef.com/blog/2014/07/15/release-chef-container-0-2-0-beta/\"\u003ehttp://www.getchef.com/blog/2014/07/15/release-chef-container-0-2-0-beta/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://docs.opscode.com/containers.html\"\u003ehttp://docs.opscode.com/containers.html\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eまだ Beta リリースでバージョンは 0.2.0 です。(gem だと 0.1.1)\u003c/p\u003e\n\u003cp\u003eDocker を代表とするコンテナ周りの技術が最近、盛んにリリースされていますし、今\n後クラウドプラットフォーム上でコンテナを使ってアプリを動かすケースも増えてくる\nのではないでしょうか。Dockerfile を使っても Chef-Solo を使ってソフトウェアをデ\nプロイ出来るのだけどそれだけだとしんどいので、コンテナに特化した Chef が出てき\nたってことだと思います。特徴として SSH でログインしてブートストラップするので\nはなくて Runit + Chef-init を用いてコンテナにデプロイすることが挙げられます。\u003c/p\u003e\n\u003cp\u003eでは実際に使ってみたのでその時の手順をまとめてみます。\u003c/p\u003e\n\u003ch2 id=\"事前に用意する環境\"\u003e事前に用意する環境\u003c/h2\u003e\n\u003cp\u003e下記のソフトウェアを予めインストールしておきましょう。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003edocker\u003c/li\u003e\n\u003cli\u003echef\u003c/li\u003e\n\u003cli\u003eberkshelf\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eここで注意なのですが後に knife コマンドを使って Docker イメージをビルドします。\nつまり root 権限が必要です。rbenv 等を使って ruby, chef をインストールすると、\n辛いかもしれませんので OS のパッケージを使ってインストールすると良いと思います。\nこの辺りは今後改善策が出てくるかも\u0026hellip;。\u003c/p\u003e\n\u003cp\u003e尚、インストール方法はここでは割愛します。\u003c/p\u003e\n\u003ch2 id=\"chef-container-のインストール\"\u003eChef-Container のインストール\u003c/h2\u003e\n\u003cp\u003e下記の2つの Gems をインストールします。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eknife-container\u003c/li\u003e\n\u003cli\u003echef-container\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e% sudo gem install knife-container\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e% sudo gem install chef-container\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"使用方法\"\u003e使用方法\u003c/h2\u003e\n\u003cp\u003eまず knife コマンドを使って操作に必要なディレクトリとファイルを生成します。\u003c/p\u003e","title":"Chef-Container Beta を使ってみる"},{"content":"こんにちは。@jedipunkz です。\n今日、JTF2014 (July Tech Festa 2014) というイベントで Ceph のことを話してきま した。Ceph ユーザ会の会員として話してきたのですが Ceph ユーザ会は実は最近発足 したばかりのユーザ会で、まだまだ活動が活発ではありません。もし興味がある方いらっ しゃいましたら是非参加よろしくお願いしますー。下記の Google Groups になります。\nhttps://groups.google.com/forum/#!forum/ceph-jp\nユーザ会としての勉強会として初になるのですが、今回このイベントで自分は Ceph-Deploy について話してきました。とりあえず皆さんに使ってもらいたかったので この話をしてきました。が、予定時間がメチャ短かったので超絶早口で頑張った分、皆 さんに理解してもらえなかった気がしてちょっと反省\u0026hellip;。なので、このブログを利用 して少し細くさせてもらいます。\n今日の発表資料はこちらです！\n今日のテーマは 「Ceph-Deploy を使って Ceph を構築してみる」だったのですが、下 記のテーマを持って資料を作っています。\n単にミニマム構成ではなく運用を考慮した実用性のある構成 OSD, MON, MDS の各プロセスとノード・ディスクの数の関係を知ってもらう 特に「実用性のある..」は意識したつもりでした。そのために前提とした構成に下記の 特徴を持たせています。(資料 6 ページ目に構成図があります。確認してみてください。)\nオブジェクト格納用ディスクは複数/ノードを前提 OSD レプリケーションのためのクラスタネットワークを用いる構成 OSD の扱うジャーナル格納用ディスクは高速な SSD を用いる MDS は利用する HW リソースの特徴が異なるので別ノードへ配置 ストレージ全体を拡張したければ\n図中 ceph01-03 の様なノードを増設する ceph01-03 にディスクとそれに対する OSD を増設する ですが、前者がベストでしょう。ノード増設の場合 ceph-deploy を用いて\nceph-deploy mon create \u0026lt;新規ホスト名\u0026gt; で MON を稼働 ceph-dploy disk zap, osd create で OSD を稼働 で簡単に可能です。MDS の増設も負荷状況を見ながらするといいでしょう。自分はまだ Ceph を運用していないので、各プロセスがどのようなリソースの消費の仕方をするの か知りません。MDS がどのような数で運用していくべきなのか。早く運用から得たノウ ハウが共有されないかなぁと期待しています。\nまた今回話すのを忘れたのですが SSD をジャーナル格納用ディスクとして用いたのは ハードディスクに対して高速でアクセス出来ること・またメタデータはファイルオブジェ クトに対して小容量で済む、といった理由からです。メタデータを扱うのに適している と思います。また将来的には幾つかの KVS データベースソフトウェアをメタデータ管 理に使う実装がされるそうです。\n以上です。皆さん、是非 Ceph を使ってみてください！ また興味のある方はユーザ会 への加入をご検討くださいー。\n","permalink":"https://jedipunkz.github.io/post/2014/06/22/jtf2014-ceph/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003e今日、JTF2014 (July Tech Festa 2014) というイベントで Ceph のことを話してきま\nした。Ceph ユーザ会の会員として話してきたのですが Ceph ユーザ会は実は最近発足\nしたばかりのユーザ会で、まだまだ活動が活発ではありません。もし興味がある方いらっ\nしゃいましたら是非参加よろしくお願いしますー。下記の Google Groups になります。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://groups.google.com/forum/#!forum/ceph-jp\"\u003ehttps://groups.google.com/forum/#!forum/ceph-jp\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eユーザ会としての勉強会として初になるのですが、今回このイベントで自分は\nCeph-Deploy について話してきました。とりあえず皆さんに使ってもらいたかったので\nこの話をしてきました。が、予定時間がメチャ短かったので超絶早口で頑張った分、皆\nさんに理解してもらえなかった気がしてちょっと反省\u0026hellip;。なので、このブログを利用\nして少し細くさせてもらいます。\u003c/p\u003e\n\u003cp\u003e今日の発表資料はこちらです！\u003c/p\u003e\n\u003cscript async class=\"speakerdeck-embed\"\ndata-id=\"592a0b90ceb30131a5d25ae3f95c3a1a\" data-ratio=\"1.33333333333333\" src=\"//speakerdeck.com/assets/embed.js\"\u003e\u003c/script\u003e\n\u003cp\u003e今日のテーマは 「Ceph-Deploy を使って Ceph を構築してみる」だったのですが、下\n記のテーマを持って資料を作っています。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e単にミニマム構成ではなく運用を考慮した実用性のある構成\u003c/li\u003e\n\u003cli\u003eOSD, MON, MDS の各プロセスとノード・ディスクの数の関係を知ってもらう\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e特に「実用性のある..」は意識したつもりでした。そのために前提とした構成に下記の\n特徴を持たせています。(資料 6 ページ目に構成図があります。確認してみてください。)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eオブジェクト格納用ディスクは複数/ノードを前提\u003c/li\u003e\n\u003cli\u003eOSD レプリケーションのためのクラスタネットワークを用いる構成\u003c/li\u003e\n\u003cli\u003eOSD の扱うジャーナル格納用ディスクは高速な SSD を用いる\u003c/li\u003e\n\u003cli\u003eMDS は利用する HW リソースの特徴が異なるので別ノードへ配置\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eストレージ全体を拡張したければ\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e図中 ceph01-03 の様なノードを増設する\u003c/li\u003e\n\u003cli\u003eceph01-03 にディスクとそれに対する OSD を増設する\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eですが、前者がベストでしょう。ノード増設の場合 ceph-deploy を用いて\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eceph-deploy mon create \u0026lt;新規ホスト名\u0026gt; で MON を稼働\u003c/li\u003e\n\u003cli\u003eceph-dploy disk zap, osd create で OSD を稼働\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eで簡単に可能です。MDS の増設も負荷状況を見ながらするといいでしょう。自分はまだ\nCeph を運用していないので、各プロセスがどのようなリソースの消費の仕方をするの\nか知りません。MDS がどのような数で運用していくべきなのか。早く運用から得たノウ\nハウが共有されないかなぁと期待しています。\u003c/p\u003e","title":"JTF2014 で Ceph について話してきた！"},{"content":"こんにちは。@jedipunkz です。\n以前 Mesos, Docker について記事にしました。\nhttp://jedipunkz.github.io/blog/2013/09/28/mesos-architecture-number-1/ http://jedipunkz.github.io/blog/2013/10/01/methos-architecture-number-2-docker-on-mesos/\nTwitter で Docker 関連のオーケストレーションツールについて呟いていたら @everpeace さんから こんな情報をもらいました。\n@jedipunkz 元々meos-dockerっていうmesos executorがあったんですけど、mesosがcontainer部分をpluggableにしたので、それに合わせてdeimosっていうmesos用のexternal containerizer が作られました。\n\u0026mdash; Shingo Omura (@everpeace) 2014, 6月 12 Deimos !!! 知らなかった。Mesos の Docker プラグインらしく下記の場所にありました。\nhttps://github.com/mesosphere/deimos\n色々調べいたら、こんな資料が見つかりました。どうやらまだ公開されて4日しか経っていないようです。\nhttp://mesosphere.io/learn/run-docker-on-mesosphere/\nMesos + Marathon + Deimos + Docker をオールインワン構成で構築する手順が書かれています。\n内容はほぼ同じですが、一応自分がやってみて理解したことをまとめたいので下記に記していきます。\n構築してみる 手順をまとめてスクリプトにしました。パッケージは Ubuntu 13.10 用のようですが 14.04 のホスト で実行出来ました。14.04 のパッケージはまだ見つかっていません。\n#!/bin/bash # disable ipv6 echo \u0026#39;net.ipv6.conf.all.disable_ipv6 = 1\u0026#39; | sudo tee -a /etc/sysctl.conf echo \u0026#39;net.ipv6.conf.default.disable_ipv6 = 1\u0026#39; | sudo tee -a /etc/sysctl.conf sudo sysctl -p # install related tools sudo apt-get update sudo apt-get -y install curl python-setuptools python-pip python-dev python-protobuf # install zookeeper sudo apt-get -y install zookeeperd echo 1 | sudo dd of=/var/lib/zookeeper/myid # install docker sudo apt-get -y install docker.io sudo ln -sf /usr/bin/docker.io /usr/local/bin/docker sudo sed -i \u0026#39;$acomplete -F _docker docker\u0026#39; /etc/bash_completion.d/docker.io sudo docker pull libmesos/ubuntu # install mesos curl -fL http://downloads.mesosphere.io/master/ubuntu/13.10/mesos_0.19.0-xcon3_amd64.deb -o /tmp/mesos.deb sudo dpkg -i /tmp/mesos.deb sudo mkdir -p /etc/mesos-master echo in_memory | sudo dd of=/etc/mesos-master/registry curl -fL http://downloads.mesosphere.io/master/ubuntu/13.10/mesos_0.19.0-xcon3_amd64.egg -o /tmp/mesos.egg sudo easy_install /tmp/mesos.egg # install marathon curl -fL http://downloads.mesosphere.io/marathon/marathon_0.5.0-xcon2_noarch.deb -o /tmp/marathon.deb sudo dpkg -i /tmp/marathon.deb # restart each services sudo service docker.io restart sudo service zookeeper restart sudo service mesos-master restart sudo service mesos-slave restart # install deimos sudo pip install deimos sudo mkdir -p /etc/mesos-slave ## Configure Deimos as a containerizer echo /usr/bin/deimos | sudo dd of=/etc/mesos-slave/containerizer_path echo external | sudo dd of=/etc/mesos-slave/isolation プロセスの確認 実行が終わると各プロセスが確認出来ます。オプションでどのプロセスが何を見ているか大体 わかりますので見ていきます。\nmesos-master mesos-master は zookeeper を参照して 5050 番ポートで起動しているようです。\n% ps ax | grep mesos-master 1224 ? Ssl 0:30 /usr/local/sbin/mesos-master --zk=zk://localhost:2181/mesos --port=5050 --log_dir=/var/log/mesos --registry=in_memory mesos-slave mesos-slave は同じく zookeeper を参照して containerizer を deimos として稼働していることが わかります。\n% ps ax | grep mesos-slave 1225 ? Ssl 0:12 /usr/local/sbin/mesos-slave --master=zk://localhost:2181/mesos --log_dir=/var/log/mesos --containerizer_path=/usr/bin/deimos --isolation=external zookeeper zookeeper は OpenJDK7 で稼働している Java プロセスです。\n% ps ax | grep zookeeper 1073 ? Ssl 1:07 /usr/bin/java -cp /etc/zookeeper/conf:/usr/share/java/jline.jar:/usr/share/java/log4j-1.2.jar:/usr/share/java/xercesImpl.jar:/usr/share/java/xmlParserAPIs.jar:/usr/share/java/netty.jar:/usr/share/java/slf4j-api.jar:/usr/share/java/slf4j-log4j12.jar:/usr/share/java/zookeeper.jar -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.local.only=false -Dzookeeper.log.dir=/var/log/zookeeper -Dzookeeper.root.logger=INFO,ROLLINGFILE org.apache.zookeeper.server.quorum.QuorumPeerMain /etc/zookeeper/conf/zoo.cfg docker docker が起動していることも確認できます。設定は特にしていないです。\n% ps axuw | grep docker root 831 0.0 0.3 364776 14924 ? Sl 01:30 0:01 /usr/bin/docker.io -d Marathon の WebUI にアクセス Marathon の WebUI にアクセスしてみましょう。\nまだ何も Tasks が実行されていないので一覧には何も表示されないと思います。\nTasks の実行 Marathon API に対してクエリを発行することで Mesos の Tasks として Docker コンテナを稼働させることが出来ます。 下記のファイルを ubuntu.json として保存。\n{ \u0026#34;container\u0026#34;: { \u0026#34;image\u0026#34;: \u0026#34;docker:///libmesos/ubuntu\u0026#34; }, \u0026#34;id\u0026#34;: \u0026#34;ubuntu\u0026#34;, \u0026#34;instances\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;cpus\u0026#34;: \u0026#34;.5\u0026#34;, \u0026#34;mem\u0026#34;: \u0026#34;512\u0026#34;, \u0026#34;uris\u0026#34;: [ ] } 下記の通り localhost:8080 が Marathon API の Endpoint になっているのでここに対して作成した JSON を POST します。\n% curl -X POST -H \u0026#34;Content-Type: application/json\u0026#34; localhost:8080/v2/apps -d@ubuntu.json Tasks の一覧を取得してみます。\n% curl -X GET -H \u0026#34;Content-Type: application/json\u0026#34; localhost:8080/v2/apps {\u0026#34;apps\u0026#34;:[{\u0026#34;id\u0026#34;:\u0026#34;ubuntu\u0026#34;,\u0026#34;cmd\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;env\u0026#34;:{},\u0026#34;instances\u0026#34;:1,\u0026#34;cpus\u0026#34;:0.5,\u0026#34;mem\u0026#34;:512.0,\u0026#34;executor\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;constraints\u0026#34;:[],\u0026#34;uris\u0026#34;:[],\u0026#34;ports\u0026#34;:[13049],\u0026#34;taskRateLimit\u0026#34;:1.0,\u0026#34;container\u0026#34;:{\u0026#34;image\u0026#34;:\u0026#34;docker:///libmesos/ubuntu\u0026#34;,\u0026#34;options\u0026#34;:[]},\u0026#34;version\u0026#34;:\u0026#34;2014-06-13T01:45:58.693Z\u0026#34;,\u0026#34;tasksStaged\u0026#34;:1,\u0026#34;tasksRunning\u0026#34;:0}]} Tasks の一覧が JSON で返ってきます。id : ubuntu, インスタンス数 : 1, CPU 0.5, メモリー : 512MB で Task が稼働していることが確認出来ます。\nここで WebUI 側も見てみましょう。\n一つ Task が稼働していることが確認出来ると思います。\nその Task をクリックすると詳細が表示されます。\n次に Tasks のスケーリングを行ってみましょう。 下記の通り ubuntu.json を修正し instances : 2 とする。これによってインスタンス数が2に増えます。\n{ \u0026#34;container\u0026#34;: { \u0026#34;image\u0026#34;: \u0026#34;docker:///libmesos/ubuntu\u0026#34; }, \u0026#34;id\u0026#34;: \u0026#34;ubuntu\u0026#34;, \u0026#34;instances\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;cpus\u0026#34;: \u0026#34;.5\u0026#34;, \u0026#34;mem\u0026#34;: \u0026#34;512\u0026#34;, \u0026#34;uris\u0026#34;: [ ] } 修正した JSON を POST します。\n% curl -X PUT -H \u0026#34;Content-Type: application/json\u0026#34; localhost:8080/v2/apps/ubuntu -d@ubuntu.json Tasks の一覧を取得し containers が 2 になっていることが確認できます。\n% curl -X GET -H \u0026#34;Content-Type: application/json\u0026#34; localhost:8080/v2/apps {\u0026#34;apps\u0026#34;:[{\u0026#34;id\u0026#34;:\u0026#34;ubuntu\u0026#34;,\u0026#34;cmd\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;env\u0026#34;:{},\u0026#34;instances\u0026#34;:2,\u0026#34;cpus\u0026#34;:0.5,\u0026#34;mem\u0026#34;:512.0,\u0026#34;executor\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;constraints\u0026#34;:[],\u0026#34;uris\u0026#34;:[],\u0026#34;ports\u0026#34;:[17543],\u0026#34;taskRateLimit\u0026#34;:1.0,\u0026#34;container\u0026#34;:{\u0026#34;image\u0026#34;:\u0026#34;docker:///libmesos/ubuntu\u0026#34;,\u0026#34;options\u0026#34;:[]},\u0026#34;version\u0026#34;:\u0026#34;2014-06-13T02:40:04.536Z\u0026#34;,\u0026#34;tasksStaged\u0026#34;:3,\u0026#34;tasksRunning\u0026#34;:0}]} 最後に Tasks を削除してみましょう。\n% curl -X DELETE -H \u0026#34;Content-Type: application/json\u0026#34; localhost:8080/v2/apps/ubuntu Tasks が削除されたことを確認します。\n% curl -X GET -H \u0026#34;Content-Type: application/json\u0026#34; localhost:8080/v2/apps {\u0026#34;apps\u0026#34;:[]} Marathon API v2 Marathon API v2 について下記の URL に仕様が載っています。上記に記したクエリ以外にも色々載っているので 動作を確認してみるといいと思います。\nhttps://github.com/mesosphere/marathon/blob/master/docs/api/http/REST_template.md\nまとめ オールインワン構成が出来ました。また動作確認も無事出来ています。 以前試した時よりも大分、手順が簡潔になった印象があります。また参考資料中に\n\u0026ldquo;checkout our other multi-node tutorials on how to scale Docker in your data center.\u0026rdquo;\nとありますが、まだ見つかっていません(´・ω・｀)見つかった方教えてくださいー。\n以前試した時は Mesos-Master の冗長化が出来なかったので今回こそ Multi Mesos-Masters, Multi Mesos-Slaves の構成を作ってみたいと思います。\nまた今月？になって続々と Docker のオーケストレーションツールを各社が公開しています。\ncenturion New Relic が開発したオーケストレーションツール。 https://github.com/newrelic/centurion\nhelios Spotify が開発したオーケストレーションツール。 https://github.com/spotify/helios\nfleet CoreOS 標準搭載。 https://github.com/coreos/fleet\ngeard RedHat が Red Hat Enterprise Linux Atomic Host に搭載しているツール。 http://openshift.github.io/geard/\nKubernetes Google が開発したオーケストレーションツール。 https://github.com/GoogleCloudPlatform/kubernetes\nshipper Python のコードで Docker をオーケストレーション出来るツール。 https://github.com/mailgun/shipper\n幾つか試したのですが、まだまだ動く所までいかないツールがありました。github の README にも \u0026ldquo;絶賛開発中なのでプロダクトレディではない\u0026rdquo; と書かれています。これからでしょう。\n","permalink":"https://jedipunkz.github.io/post/2014/06/13/mesos-marathon-deimos-docker/","summary":"\u003cp\u003eこんにちは。@jedipunkz です。\u003c/p\u003e\n\u003cp\u003e以前 Mesos, Docker について記事にしました。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://jedipunkz.github.io/blog/2013/09/28/mesos-architecture-number-1/\"\u003ehttp://jedipunkz.github.io/blog/2013/09/28/mesos-architecture-number-1/\u003c/a\u003e\n\u003ca href=\"http://jedipunkz.github.io/blog/2013/10/01/methos-architecture-number-2-docker-on-mesos/\"\u003ehttp://jedipunkz.github.io/blog/2013/10/01/methos-architecture-number-2-docker-on-mesos/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eTwitter で Docker 関連のオーケストレーションツールについて呟いていたら @everpeace さんから\nこんな情報をもらいました。\u003c/p\u003e\n\u003cblockquote class=\"twitter-tweet\" lang=\"ja\"\u003e\u003cp\u003e\u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e 元々meos-dockerっていうmesos executorがあったんですけど、mesosがcontainer部分をpluggableにしたので、それに合わせてdeimosっていうmesos用のexternal containerizer が作られました。\u003c/p\u003e\u0026mdash; Shingo Omura (@everpeace) \u003ca href=\"https://twitter.com/everpeace/statuses/476998842383347712\"\u003e2014, 6月 12\u003c/a\u003e\u003c/blockquote\u003e\n\u003cscript async src=\"//platform.twitter.com/widgets.js\" charset=\"utf-8\"\u003e\u003c/script\u003e\n\u003cp\u003eDeimos !!! 知らなかった。Mesos の Docker プラグインらしく下記の場所にありました。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/mesosphere/deimos\"\u003ehttps://github.com/mesosphere/deimos\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e色々調べいたら、こんな資料が見つかりました。どうやらまだ公開されて4日しか経っていないようです。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://mesosphere.io/learn/run-docker-on-mesosphere/\"\u003ehttp://mesosphere.io/learn/run-docker-on-mesosphere/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eMesos + Marathon + Deimos + Docker をオールインワン構成で構築する手順が書かれています。\u003c/p\u003e\n\u003cp\u003e内容はほぼ同じですが、一応自分がやってみて理解したことをまとめたいので下記に記していきます。\u003c/p\u003e\n\u003ch2 id=\"構築してみる\"\u003e構築してみる\u003c/h2\u003e\n\u003cp\u003e手順をまとめてスクリプトにしました。パッケージは Ubuntu 13.10 用のようですが 14.04 のホスト\nで実行出来ました。14.04 のパッケージはまだ見つかっていません。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#!/bin/bash\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e\u003c/span\u003e\u003cspan style=\"color:#75715e\"\u003e# disable ipv6\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eecho \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;net.ipv6.conf.all.disable_ipv6 = 1\u0026#39;\u003c/span\u003e | sudo tee -a /etc/sysctl.conf\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eecho \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;net.ipv6.conf.default.disable_ipv6 = 1\u0026#39;\u003c/span\u003e | sudo tee -a /etc/sysctl.conf\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003esudo sysctl -p\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# install related tools\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003esudo apt-get update\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003esudo apt-get -y install curl python-setuptools python-pip python-dev python-protobuf\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# install zookeeper\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003esudo apt-get -y install zookeeperd\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eecho \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e | sudo dd of\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e/var/lib/zookeeper/myid\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# install docker\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003esudo apt-get -y install docker.io\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003esudo ln -sf /usr/bin/docker.io /usr/local/bin/docker\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003esudo sed -i \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;$acomplete -F _docker docker\u0026#39;\u003c/span\u003e /etc/bash_completion.d/docker.io\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003esudo docker pull libmesos/ubuntu\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# install mesos\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ecurl -fL http://downloads.mesosphere.io/master/ubuntu/13.10/mesos_0.19.0-xcon3_amd64.deb -o /tmp/mesos.deb\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003esudo dpkg -i /tmp/mesos.deb\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003esudo mkdir -p /etc/mesos-master\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eecho in_memory  | sudo dd of\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e/etc/mesos-master/registry\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ecurl -fL http://downloads.mesosphere.io/master/ubuntu/13.10/mesos_0.19.0-xcon3_amd64.egg -o /tmp/mesos.egg\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003esudo easy_install /tmp/mesos.egg\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# install marathon\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ecurl -fL http://downloads.mesosphere.io/marathon/marathon_0.5.0-xcon2_noarch.deb -o /tmp/marathon.deb\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003esudo dpkg -i /tmp/marathon.deb\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# restart each services\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003esudo service docker.io restart\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003esudo service zookeeper restart\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003esudo service mesos-master restart\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003esudo service mesos-slave restart\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# install deimos\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003esudo pip install deimos\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003esudo mkdir -p /etc/mesos-slave\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e## Configure Deimos as a containerizer\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eecho /usr/bin/deimos  | sudo dd of\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e/etc/mesos-slave/containerizer_path\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eecho external     | sudo dd of\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e/etc/mesos-slave/isolation\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"プロセスの確認\"\u003eプロセスの確認\u003c/h2\u003e\n\u003cp\u003e実行が終わると各プロセスが確認出来ます。オプションでどのプロセスが何を見ているか大体\nわかりますので見ていきます。\u003c/p\u003e","title":"Mesos + Marathon + Deimos + Docker を試してみた!"},{"content":"こんにちは。@jedipunkz です。\n最近 OpenStack でサービスを開発！.. じゃなくて AWS でプロトタイプサービス作っ ているのですが、Ruby で開発したかったので Fog を使っています。EC2 と ELB の API を叩くコードになりつつあるのですが、サンプルコードって世の中に中々無いと気 がついたので、このブログ記事にサンプルコードを載せたいと思います。\nFog とは ? Fog http://fog.io/ はクラウドライブラリソフトウェアです。AWS, Rackspace, CloudStack, OpenStack .. と数ある世の中のクラウドプラットフォームを扱うために 用意されたソフトウェアです。対応しているプラットフォームの種別は下記を見ると参 考になります。\nhttp://fog.io/about/provider_documentation.html\nドキュメントがまだまだ揃っていなく、Fog のコードを覗きながら実装するしかない状 況です。なので「こう使えば良い！」というお手本があまりネット上にも無い気がしま す。\nドキュメントは一応下記にあります。 が使い方がよくわからない・・！(´；ω；｀)ﾌﾞﾜｯ\nhttp://rubydoc.info/gems/fog/frames/index\nEC2 インスタンスを使ってみる まずは AWS EC2 の API を叩いて t1.micro インスタンスを立ち上げてみましょう。\nrequire \u0026#39;fog\u0026#39; compute = Fog::Compute.new({ :provider =\u0026gt; \u0026#39;AWS\u0026#39;, :aws_access_key_id =\u0026gt; \u0026#39;....\u0026#39;, :aws_secret_access_key =\u0026gt; \u0026#39;....\u0026#39;, :region =\u0026gt; \u0026#39;ap-northeast-1\u0026#39; }) server = compute.servers.create( :image_id =\u0026gt; \u0026#39;ami-cedaa2bc\u0026#39;, :flavor_id =\u0026gt; \u0026#39;t1.micro\u0026#39;, :key_name =\u0026gt; \u0026#39;test_key\u0026#39;, :tags =\u0026gt; {\u0026#39;Name\u0026#39; =\u0026gt; \u0026#39;test\u0026#39;}, :groups =\u0026gt; \u0026#39;ssh-secgroup\u0026#39; ) server.wait_for { print \u0026#34;.\u0026#34;; ready? } puts \u0026#34;created instance name :\u0026#34;, server.dns_name 解説 compute = \u0026hellip; とあるところで接続情報を記しています。 \u0026ldquo;ACCESS_KEY_ID\u0026rdquo; や \u0026ldquo;SECRET_ACCESS_KEY\u0026rdquo; はみなさん接続する時にお持ちですよね。それ とリージョン名やプロバイダ名 (ここでは AWS) を記して AWS の API に接続します。\nserver = \u0026hellip; とあるところで実際にインスタンスを作成しています。 ここではインスタンス生成に必要な情報を盛り込んでいます。Flavor 名や AMI イメー ジ名・SSH 鍵の名前・セキュリティグループ名等です。\n便利なメソッド server = \u0026hellip; でインスタンスを生成すると便利なメソッドを扱って情報を読み込むこ とが出来ます。\nserver.dns_name # =\u0026gt; public な DNS 名を取得 server.private_dns_name # =\u0026gt; private な DNS 名を取得 server.id # =\u0026gt; インスタンス ID を取得 server.availability_zone # =\u0026gt; Availability Zone を取得 server.public_ip_address # =\u0026gt; public な IP アドレスを取得 server.private_ip_address # =\u0026gt; private な IP アドレスを取得 これは便利\u0026hellip;\nモジュール化して利用 +++\n毎回コードの中でこれらの接続情報を書くのはしんどいので、Ruby のモジュールを作 りましょう。\nmodule AWSCompute def self.connect() conn = Fog::Compute.new({ :provider =\u0026gt; \u0026#39;AWS\u0026#39;, :aws_access_key_id =\u0026gt; \u0026#39;...\u0026#39;, :aws_secret_access_key =\u0026gt; \u0026#39;...\u0026#39;, :region =\u0026gt; \u0026#39;...\u0026#39; }) begin yield conn ensure # conn.close end rescue Errno::ECONNREFUSED end end こう書いておくと例えば\u0026hellip;\nインスタンスのターミネイト AWSCompute.connect() do |sock| server = sock.servers.get(instance_id) server.destroy return server.id end インスタンスの起動 AWSCompute.connect() do |sock| server = sock.servers.get(instance_id) server.start return server.id end インスタンスの停止 AWSCompute.connect() do |sock| server = sock.servers.get(instance_id) server.stop return server.id end 等と出来ます。\nELB (Elastic LoadBalancer) を使ってみる 同様に ELB を扱うコードのサンプルも載せておきます。同じくモジュール化して書くと\nmodule AWSELB def self.connect() conn = Fog::AWS::ELB.new( :aws_access_key_id =\u0026gt; \u0026#39;...\u0026#39;, :aws_secret_access_key =\u0026gt; \u0026#39;...\u0026#39;, :region =\u0026gt; \u0026#39;...\u0026#39;, ) begin yield conn ensure # conn.close end rescue Errno::ECONNREFUSED end end としておいて\u0026hellip;\nELB の新規作成 下記のコードで ELB を新規作成出来ます。\nAWSELB.connect() do |sock| availability_zone = \u0026#39;...\u0026#39; elb_name = \u0026#39;...\u0026#39; listeners = [{ \u0026#34;Protocol\u0026#34; =\u0026gt; \u0026#34;HTTP\u0026#34;, \u0026#34;LoadBalancerPort\u0026#34; =\u0026gt; 80, \u0026#34;InstancePort\u0026#34; =\u0026gt; 80, \u0026#34;InstanceProtocol\u0026#34; =\u0026gt; \u0026#34;HTTP\u0026#34; }] result = sock.create_load_balancer(availability_zone, elb_name, listeners) p result end この状態では ELB に対してインスタンスが紐付けられていないので使えません。下記の操作で インスタンスを紐付けてみましょう。\nAWSELB.connect() do |sock| insntance_id = \u0026#39;...\u0026#39; elb_name = \u0026#39;...\u0026#39; result = sock.register_instances_with_load_balancer(instance_id, elbname) p result end insntance_id には紐付けたいインスタンスの ID を、elb_name には先ほど作成した ELB の名前を 入力します。 この操作を繰り返せば AWS 上にクラスタが構成出来ます。\n逆にクラスタからインスタンスの削除したい場合は下記の通り実行します。\nAWSELB.connect() do |sock| result = sock.deregister_instances_from_load_balancer(instance_id, elbname) p result end まとめ 今回は Fog を紹介しましたが Python 使いの方には libcloud をおすすめします。\nhttps://libcloud.apache.org/\nApache ファウンデーションが管理しているクラウドライブラリです。こちらも複数の クラウドプラットフォームに対応しているようです。\nFog で OpenStack も操作したことがあるのですが、AWS 用のコードの方が完成度が高 いのか、戻り値などが綺麗に整形されていて扱いやすかったり、メソッドも豊富に用意 されていたりという印象でした。これは\u0026hellip; OpenStack 用の Fog コードにコントリビュー トするチャンス・・！\n皆さんもサンプルコードお持ちでしたら、ブログ等で公開していきましょうー。 ではでは。\n","permalink":"https://jedipunkz.github.io/post/2014/05/29/fog-aws-ec2-elb/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003e最近 OpenStack でサービスを開発！.. じゃなくて AWS でプロトタイプサービス作っ\nているのですが、Ruby で開発したかったので Fog を使っています。EC2 と ELB の\nAPI を叩くコードになりつつあるのですが、サンプルコードって世の中に中々無いと気\nがついたので、このブログ記事にサンプルコードを載せたいと思います。\u003c/p\u003e\n\u003ch2 id=\"fog-とは-\"\u003eFog とは ?\u003c/h2\u003e\n\u003cp\u003eFog \u003ca href=\"http://fog.io/\"\u003ehttp://fog.io/\u003c/a\u003e はクラウドライブラリソフトウェアです。AWS, Rackspace,\nCloudStack, OpenStack .. と数ある世の中のクラウドプラットフォームを扱うために\n用意されたソフトウェアです。対応しているプラットフォームの種別は下記を見ると参\n考になります。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://fog.io/about/provider_documentation.html\"\u003ehttp://fog.io/about/provider_documentation.html\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eドキュメントがまだまだ揃っていなく、Fog のコードを覗きながら実装するしかない状\n況です。なので「こう使えば良い！」というお手本があまりネット上にも無い気がしま\nす。\u003c/p\u003e\n\u003cp\u003eドキュメントは一応下記にあります。\nが使い方がよくわからない・・！(´；ω；｀)ﾌﾞﾜｯ\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://rubydoc.info/gems/fog/frames/index\"\u003ehttp://rubydoc.info/gems/fog/frames/index\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"ec2-インスタンスを使ってみる\"\u003eEC2 インスタンスを使ってみる\u003c/h2\u003e\n\u003cp\u003eまずは AWS EC2 の API を叩いて t1.micro インスタンスを立ち上げてみましょう。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-ruby\" data-lang=\"ruby\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003erequire \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;fog\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ecompute \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eFog\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eCompute\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003enew({\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#e6db74\"\u003e:provider\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u0026gt;\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;AWS\u0026#39;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#e6db74\"\u003e:aws_access_key_id\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u0026gt;\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;....\u0026#39;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#e6db74\"\u003e:aws_secret_access_key\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u0026gt;\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;....\u0026#39;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#e6db74\"\u003e:region\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u0026gt;\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;ap-northeast-1\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e})\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eserver \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e compute\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eservers\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ecreate(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#e6db74\"\u003e:image_id\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u0026gt;\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;ami-cedaa2bc\u0026#39;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#e6db74\"\u003e:flavor_id\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u0026gt;\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;t1.micro\u0026#39;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#e6db74\"\u003e:key_name\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u0026gt;\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;test_key\u0026#39;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#e6db74\"\u003e:tags\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u0026gt;\u003c/span\u003e {\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;Name\u0026#39;\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u0026gt;\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;test\u0026#39;\u003c/span\u003e},\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#e6db74\"\u003e:groups\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u0026gt;\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;ssh-secgroup\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eserver\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ewait_for { print \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;.\u0026#34;\u003c/span\u003e; ready? }\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eputs \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;created instance name :\u0026#34;\u003c/span\u003e, server\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edns_name\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch4 id=\"解説\"\u003e解説\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003ecompute = \u0026hellip; とあるところで接続情報を記しています。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u0026ldquo;ACCESS_KEY_ID\u0026rdquo; や \u0026ldquo;SECRET_ACCESS_KEY\u0026rdquo; はみなさん接続する時にお持ちですよね。それ\nとリージョン名やプロバイダ名 (ここでは AWS) を記して AWS の API に接続します。\u003c/p\u003e","title":"クラウドライブラリ Fog で AWS を操作！..のサンプル"},{"content":"こんにちは。@jedipunkz です。\nまたまた OpenStack のデプロイをどうするか？についてです。\n今まで自分の中では Rackspace Private Cloud で使われている Rackspace 管理の rcbops/chef-cookbooks が今現在使うならベストの選択だと思っていました。これは内 部で Chef が使われていてしかも Cookbooks が Github 上で公開されています。 Apache ライセンスで使えるので、サービス構築にも使えちゃうというモノ。\n先日、ある OpenStack コアデベロッパーの方から「jedipunkz さん、やっぱり rcbops がいいですか？運営とかどうなっているんでしょう？マージの規準とかどうなのかな？」 と質問受けました。確かにマージの基準は Rackspace Private Cloud がベースになり ますし、管理しているエンジニアの一覧を見ていると Rackspace 社のエンジニアがメ インですし、今後どうなるのか分からない\u0026hellip;。\n逃げ道は用意したほうが良さそう。\nということで、以前自分も暑かったことのある StackForge の openstack-chef-repo を久々に使ってみました。Icehouse 構成がこの時点で既に組めるようになっていて、 以前よりだい〜ぶ完成度増した感があります。今回は nova-network 構成を作ってみた のですが、Neutron 構成ももちろん出来そうなので後に調べてまた公開したいです。\nStackForge とは StackForge は OpenStack のデプロイ・CI の仕組みとして公式に用いられているもの。 公式サイトは下記の場所にある。\nhttp://ci.openstack.org/stackforge.html\nStackForge の openstack-chef-repo は下記の場所にある。\nhttps://github.com/stackforge/openstack-chef-repo\nopenstack-chef-repo はまだ \u0026lsquo;stable/icehouse\u0026rsquo; ブランチが生成されていない。が直 ちに master からブランチが切られる様子。\n目的 StackForge の openstack-chef-repo を用いて Icehouse リリース版の OpenStack を デプロイするための方法を記す。今回は未だ \u0026lsquo;stable/icehouse\u0026rsquo; ブランチが無いので master ブランチを用いて Icehouse リリース版 OpenStack を構築する。\n構成 構成はこんな感じ。\n+-----------------+ | GW Router | +--+-----------------+ | | | +-------------------+-------------------+-------------------------------------- | |eth0 |eth0 |eth0 VM Network (fixed) | +-----------------+ +-----------------+ +-----------------+ +-----------------+ | | Controller Node | | Compute Node | | Compute Node | | Chef Workstation| | +-----------------+ +-----------------+ +-----------------+ +-----------------+ | |eth1 |eth1 |eth1 | +--+-------------------+-------------------+-------------------+------------------ API/Management Network Nova-Network 構成 今回は fixed network 用の NIC を eth0(物理 NIC) にアサイン fixed network 用のネットワークをパブリックにする API/Management Network 側に全ての API を出す。またここから The Internet に迂回出来るようにする VM Network も GW を介して The Internet へ迂回出来るようにする 全ての操作は \u0026lsquo;Chef Workstaion\u0026rsquo; から行う Compute ノードはキャパシティの許す限り何台でも可 IP 一覧 (この記事での例)\nController : 10.200.9.46 (eth0), 10.200.10.46 (eth1) Compute : 10.200.9.47 (eth0), 10.200.10.47 (eth1) Compute : 10.200.9.48 (eth0), 10.200.10.48 (eth1) 手順 openstack-chef-repo をワークステーションノード上で取得する。\n% git clone https://github.com/stackforge/openstack-chef-repo.git % cd openstack-chef-repo Berksfile があるのでこれを用いて Chef Cookbooks を取得する。\n% berks install --path=./cookbooks Roles, Cookbooks を Chef サーバにアップロードする。\n% knife cookbook upload -o cookbooks -a % knife role from file roles/*.rb 1 Environment に対して 1 OpenStack クラスタである。今回構築するクラスタのため の Environment を作成する。\n下記を environments/icehouse-nova-network.rb として生成する。\nname \u0026#34;icehouse-nova-network\u0026#34; description \u0026#34;separated nodes environment\u0026#34; override_attributes( \u0026#34;release\u0026#34; =\u0026gt; \u0026#34;icehouse\u0026#34;, \u0026#34;osops_networks\u0026#34; =\u0026gt; { \u0026#34;management\u0026#34; =\u0026gt; \u0026#34;10.200.10.0/24\u0026#34;, \u0026#34;public\u0026#34; =\u0026gt; \u0026#34;10.200.10.0/24\u0026#34;, \u0026#34;nova\u0026#34; =\u0026gt; \u0026#34;10.200.10.0/24\u0026#34; }, \u0026#34;mysql\u0026#34; =\u0026gt; { \u0026#34;bind_address\u0026#34; =\u0026gt; \u0026#34;0.0.0.0\u0026#34;, \u0026#34;root_network_acl\u0026#34; =\u0026gt; \u0026#34;%\u0026#34;, \u0026#34;allow_remote_root\u0026#34; =\u0026gt; true, \u0026#34;server_root_password\u0026#34; =\u0026gt; \u0026#34;secrete\u0026#34;, \u0026#34;server_repl_password\u0026#34; =\u0026gt; \u0026#34;secrete\u0026#34;, \u0026#34;server_debian_password\u0026#34; =\u0026gt; \u0026#34;secrete\u0026#34; }, \u0026#34;nova\u0026#34; =\u0026gt; { \u0026#34;network\u0026#34; =\u0026gt; { \u0026#34;fixed_range\u0026#34; =\u0026gt; \u0026#34;172.18.0.0/24\u0026#34;, \u0026#34;public_interface\u0026#34; =\u0026gt; \u0026#34;eth0\u0026#34; } }, \u0026#34;rabbitmq\u0026#34; =\u0026gt; { \u0026#34;address\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;5672\u0026#34; }, \u0026#34;openstack\u0026#34; =\u0026gt; { \u0026#34;developer_mode\u0026#34; =\u0026gt; true, \u0026#34;compute\u0026#34; =\u0026gt; { \u0026#34;rabbit\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34; }, \u0026#34;novnc_proxy\u0026#34; =\u0026gt; { \u0026#34;bind_interface\u0026#34; =\u0026gt; \u0026#34;eth1\u0026#34; }, \u0026#34;libvirt\u0026#34; =\u0026gt; { \u0026#34;bind_interface\u0026#34; =\u0026gt; \u0026#34;eth1\u0026#34; }, \u0026#34;network\u0026#34; =\u0026gt; { \u0026#34;fixed_range\u0026#34; =\u0026gt; \u0026#34;10.200.9.0/24\u0026#34; }, \u0026#34;rabbit_server_chef_role\u0026#34; =\u0026gt; \u0026#34;os-ops-messaging\u0026#34;, \u0026#34;networks\u0026#34; =\u0026gt; [ { \u0026#34;label\u0026#34; =\u0026gt; \u0026#34;private\u0026#34;, \u0026#34;ipv4_cidr\u0026#34; =\u0026gt; \u0026#34;10.200.9.0/24\u0026#34;, \u0026#34;num_networks\u0026#34; =\u0026gt; \u0026#34;1\u0026#34;, \u0026#34;network_size\u0026#34; =\u0026gt; \u0026#34;255\u0026#34;, \u0026#34;bridge\u0026#34; =\u0026gt; \u0026#34;br200\u0026#34;, \u0026#34;bridge_dev\u0026#34; =\u0026gt; \u0026#34;eth0\u0026#34;, \u0026#34;dns1\u0026#34; =\u0026gt; \u0026#34;8.8.8.8\u0026#34;, \u0026#34;dns2\u0026#34; =\u0026gt; \u0026#34;8.8.4.4\u0026#34;, \u0026#34;multi_host\u0026#34; =\u0026gt; \u0026#34;T\u0026#34; } ] }, \u0026#34;identity\u0026#34; =\u0026gt; { \u0026#34;bind_interface\u0026#34; =\u0026gt; \u0026#34;eth1\u0026#34;, \u0026#34;users\u0026#34; =\u0026gt; { \u0026#34;demo\u0026#34; =\u0026gt; { \u0026#34;password\u0026#34; =\u0026gt; \u0026#34;demo\u0026#34;, \u0026#34;default_tenant\u0026#34; =\u0026gt; \u0026#34;service\u0026#34;, \u0026#34;roles\u0026#34; =\u0026gt; { \u0026#34;Member\u0026#34; =\u0026gt; [ \u0026#34;Member\u0026#34; ] } } } }, \u0026#34;image\u0026#34; =\u0026gt; { \u0026#34;api\u0026#34; =\u0026gt; { \u0026#34;bind_interface\u0026#34; =\u0026gt; \u0026#34;eth1\u0026#34; }, \u0026#34;debug\u0026#34; =\u0026gt; true, \u0026#34;identity_service_chef_role\u0026#34; =\u0026gt; \u0026#34;os-identity\u0026#34;, \u0026#34;rabbit_server_chef_role\u0026#34; =\u0026gt; \u0026#34;os-ops-messaging\u0026#34;, \u0026#34;registry\u0026#34; =\u0026gt; { \u0026#34;bind_interface\u0026#34; =\u0026gt; \u0026#34;eth1\u0026#34; }, \u0026#34;syslog\u0026#34; =\u0026gt; { \u0026#34;use\u0026#34; =\u0026gt; false }, \u0026#34;upload_image\u0026#34; =\u0026gt; { \u0026#34;cirros\u0026#34; =\u0026gt; \u0026#34;http://hypnotoad/cirros-0.3.0-x86_64-disk.img\u0026#34;, }, \u0026#34;upload_images\u0026#34; =\u0026gt; [ \u0026#34;cirros\u0026#34; ] }, \u0026#34;network\u0026#34; =\u0026gt; { \u0026#34;api\u0026#34; =\u0026gt; { \u0026#34;bind_interface\u0026#34; =\u0026gt; \u0026#34;eth1\u0026#34;, } }, \u0026#34;db\u0026#34; =\u0026gt; { \u0026#34;bind_interface\u0026#34; =\u0026gt; \u0026#34;eth1\u0026#34;, \u0026#34;compute\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34; }, \u0026#34;identity\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34; }, \u0026#34;image\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34; }, \u0026#34;network\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34; }, \u0026#34;volume\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34; }, \u0026#34;dashboard\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34; } }, \u0026#34;mq\u0026#34; =\u0026gt; { \u0026#34;bind_interface\u0026#34; =\u0026gt; \u0026#34;eth1\u0026#34;, \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34;, \u0026#34;user\u0026#34; =\u0026gt; \u0026#34;guest\u0026#34;, \u0026#34;vhost\u0026#34; =\u0026gt; \u0026#34;/nova\u0026#34;, \u0026#34;servers\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34;, \u0026#34;compute\u0026#34; =\u0026gt; { \u0026#34;service_type\u0026#34; =\u0026gt; \u0026#34;rabbitmq\u0026#34;, \u0026#34;rabbit\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;5672\u0026#34; } }, \u0026#34;block-storage\u0026#34; =\u0026gt; { \u0026#34;service_type\u0026#34; =\u0026gt; \u0026#34;rabbitmq\u0026#34;, \u0026#34;rabbit\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;5672\u0026#34; } } }, \u0026#34;endpoints\u0026#34; =\u0026gt; { \u0026#34;compute-api-bind\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34;, \u0026#34;scheme\u0026#34; =\u0026gt; \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;8774\u0026#34;, \u0026#34;path\u0026#34; =\u0026gt; \u0026#34;/v2/%(tenant_id)s\u0026#34; }, \u0026#34;compute-api\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34;, \u0026#34;scheme\u0026#34; =\u0026gt; \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;8774\u0026#34;, \u0026#34;path\u0026#34; =\u0026gt; \u0026#34;/v2/%(tenant_id)s\u0026#34; }, \u0026#34;compute-ec2-admin-bind\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34;, \u0026#34;scheme\u0026#34; =\u0026gt; \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;8773\u0026#34;, \u0026#34;path\u0026#34; =\u0026gt; \u0026#34;/services/Admin\u0026#34; }, \u0026#34;compute-ec2-admin\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34;, \u0026#34;scheme\u0026#34; =\u0026gt; \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;8773\u0026#34;, \u0026#34;path\u0026#34; =\u0026gt; \u0026#34;/services/Admin\u0026#34; }, \u0026#34;compute-ec2-api-bind\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34;, \u0026#34;scheme\u0026#34; =\u0026gt; \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;8773\u0026#34;, \u0026#34;path\u0026#34; =\u0026gt; \u0026#34;/services/Cloud\u0026#34; }, \u0026#34;compute-ec2-api\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34;, \u0026#34;scheme\u0026#34; =\u0026gt; \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;8773\u0026#34;, \u0026#34;path\u0026#34; =\u0026gt; \u0026#34;/services/Cloud\u0026#34; }, \u0026#34;compute-xvpvnc-bind\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34;, \u0026#34;scheme\u0026#34; =\u0026gt; \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;6081\u0026#34;, \u0026#34;path\u0026#34; =\u0026gt; \u0026#34;/console\u0026#34; }, \u0026#34;compute-xvpvnc\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34;, \u0026#34;scheme\u0026#34; =\u0026gt; \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;6081\u0026#34;, \u0026#34;path\u0026#34; =\u0026gt; \u0026#34;/console\u0026#34; }, \u0026#34;compute-novnc-bind\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34;, \u0026#34;scheme\u0026#34; =\u0026gt; \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;6080\u0026#34;, \u0026#34;path\u0026#34; =\u0026gt; \u0026#34;/vnc_auto.html\u0026#34; }, \u0026#34;compute-novnc\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34;, \u0026#34;scheme\u0026#34; =\u0026gt; \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;6080\u0026#34;, \u0026#34;path\u0026#34; =\u0026gt; \u0026#34;/vnc_auto.html\u0026#34; }, \u0026#34;compute-vnc\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;0.0.0.0\u0026#34;, \u0026#34;scheme\u0026#34; =\u0026gt; \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;6080\u0026#34;, \u0026#34;path\u0026#34; =\u0026gt; \u0026#34;/vnc_auto.html\u0026#34; }, \u0026#34;image-api-bind\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34;, \u0026#34;scheme\u0026#34; =\u0026gt; \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;9292\u0026#34;, \u0026#34;path\u0026#34; =\u0026gt; \u0026#34;/v2\u0026#34; }, \u0026#34;image-api\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34;, \u0026#34;scheme\u0026#34; =\u0026gt; \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;9292\u0026#34;, \u0026#34;path\u0026#34; =\u0026gt; \u0026#34;/v2\u0026#34; }, \u0026#34;image-registry-bind\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34;, \u0026#34;scheme\u0026#34; =\u0026gt; \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;9191\u0026#34;, \u0026#34;path\u0026#34; =\u0026gt; \u0026#34;/v2\u0026#34; }, \u0026#34;image-registry\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34;, \u0026#34;scheme\u0026#34; =\u0026gt; \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;9191\u0026#34;, \u0026#34;path\u0026#34; =\u0026gt; \u0026#34;/v2\u0026#34; }, \u0026#34;identity-bind\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34;, \u0026#34;scheme\u0026#34; =\u0026gt; \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;5000\u0026#34;, \u0026#34;path\u0026#34; =\u0026gt; \u0026#34;/v2.0\u0026#34; }, \u0026#34;identity-api\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34;, \u0026#34;scheme\u0026#34; =\u0026gt; \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;5000\u0026#34;, \u0026#34;path\u0026#34; =\u0026gt; \u0026#34;/v2.0\u0026#34; }, \u0026#34;identity-admin\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34;, \u0026#34;scheme\u0026#34; =\u0026gt; \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;35357\u0026#34;, \u0026#34;path\u0026#34; =\u0026gt; \u0026#34;/v2.0\u0026#34; }, \u0026#34;volume-api-bind\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34;, \u0026#34;scheme\u0026#34; =\u0026gt; \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;8776\u0026#34;, \u0026#34;path\u0026#34; =\u0026gt; \u0026#34;/v1/%(tenant_id)s\u0026#34; }, \u0026#34;block-storage-api\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34;, \u0026#34;scheme\u0026#34; =\u0026gt; \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;8776\u0026#34;, \u0026#34;path\u0026#34; =\u0026gt; \u0026#34;/v1/%(tenant_id)s\u0026#34; }, \u0026#34;telemetry-api-bind\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34;, \u0026#34;scheme\u0026#34; =\u0026gt; \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;8777\u0026#34;, \u0026#34;path\u0026#34; =\u0026gt; \u0026#34;/v1\u0026#34; }, \u0026#34;telemetry-api\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34;, \u0026#34;scheme\u0026#34; =\u0026gt; \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;8777\u0026#34;, \u0026#34;path\u0026#34; =\u0026gt; \u0026#34;/v1\u0026#34; }, \u0026#34;network-api-bind\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34;, \u0026#34;scheme\u0026#34; =\u0026gt; \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;9696\u0026#34;, \u0026#34;path\u0026#34; =\u0026gt; \u0026#34;/v2\u0026#34; }, \u0026#34;network-api\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34;, \u0026#34;scheme\u0026#34; =\u0026gt; \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;9696\u0026#34;, \u0026#34;path\u0026#34; =\u0026gt; \u0026#34;/v2\u0026#34; }, \u0026#34;orchestration-api-bind\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34;, \u0026#34;scheme\u0026#34; =\u0026gt; \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;8004\u0026#34;, \u0026#34;path\u0026#34; =\u0026gt; \u0026#34;/v1/%(tenant_id)s\u0026#34; }, \u0026#34;orchestration-api\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34;, \u0026#34;scheme\u0026#34; =\u0026gt; \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;8004\u0026#34;, \u0026#34;path\u0026#34; =\u0026gt; \u0026#34;/v1/%(tenant_id)s\u0026#34; }, \u0026#34;orchestration-api-cfn-bind\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34;, \u0026#34;scheme\u0026#34; =\u0026gt; \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;8000\u0026#34;, \u0026#34;path\u0026#34; =\u0026gt; \u0026#34;/v1\u0026#34; }, \u0026#34;orchestration-api-cfn\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.200.10.46\u0026#34;, \u0026#34;scheme\u0026#34; =\u0026gt; \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;8000\u0026#34;, \u0026#34;path\u0026#34; =\u0026gt; \u0026#34;/v1\u0026#34; }, \u0026#34;mq\u0026#34; =\u0026gt; { \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;5672\u0026#34; } } } ) 生成した environment を Chef サーバにアップロードする。\n% knife environment from file environments/icehouse-nova-network.rb Spiceweasel をインストールする。Spiceweasel は yml ファイルを元に knife の操作 を書き出して、またそれを一気に実行することが出来るツールです。\n% gem install spiceweasel --no-ri --no-rdoc % rbenv rehash infrastructure.yml を下記の通り修正する。\nberksfile: options: \u0026#39;--no-freeze --halt-on-frozen\u0026#39; cookbooks: - apache2: - apt: - aws: - build-essential: - chef_handler: - database: - dmg: - erlang: - git: - homebrew: - iptables: - logrotate: - memcached: - mysql: - openssl: - openstack-block-storage: - openstack-common: - openstack-compute: - openstack-dashboard: - openstack-identity: - openstack-image: - openstack-network: - openstack-object-storage: - openstack-ops-messaging: - openstack-ops-database: - openstack-orchestration: - openstack-telemetry: - pacman: - postgresql: - python: - rabbitmq: - runit: - selinux: - statsd: - windows: - xfs: - yum: - yum-epel: - yum-erlang_solutions: roles: - allinone-compute: - os-compute-single-controller: - os-base: - os-ops-caching: - os-ops-messaging: - os-ops-database: - os-block-storage: - os-block-storage-api: - os-block-storage-scheduler: - os-block-storage-volume: - os-client: - os-compute-api: - os-compute-api-ec2: - os-compute-api-metadata: - os-compute-api-os-compute: - os-compute-cert: - os-compute-conductor: - os-compute-scheduler: - os-compute-setup: - os-compute-vncproxy: - os-compute-worker: - os-dashboard: - os-identity: - os-image: - os-image-api: - os-image-registry: - os-image-upload: - os-telemetry-agent-central: - os-telemetry-agent-compute: - os-telemetry-api: - os-telemetry-collector: - os-network: - os-network-server: - os-network-l3-agent: - os-network-dhcp-agent: - os-network-metadata-agent: - os-network-openvswitch: - os-object-storage: - os-object-storage-account: - os-object-storage-container: - os-object-storage-management: - os-object-storage-object: - os-object-storage-proxy: environments: - icehouse-nova-network: nodes: - 10.200.10.46: run_list: role[os-compute-single-controller] options: -N opstall01 -E icehouse-nova-network --sudo -x thirai - 10.200.10.47: run_list: role[os-compute-worker] options: -N opstall02 -E icehouse-nova-network --sudo -x thirai - 10.200.10.48: run_list: role[os-compute-worker] options: -N opstall03 -E icehouse-nova-network --sudo -x thirai ※ nodes: 項にはデプロイしたいノードと Roles を割り当て列挙する。\nspiceweasel を実行する。この時点ではこれから実行されるコマンドの一覧が表示され るのみである。\n% spiceweasel infrastructure.yml berks upload --no-freeze --halt-on-frozen -b ./Berksfile knife cookbook upload apache2 apt aws build-essential chef_handler database dmg erlang git homebrew iptables logrotate memcached mysql openssl openstack-block-storage openstack-common openstack-compute openstack-dashboard openstack-identity openstack-image openstack-network openstack-object-storage openstack-ops-messaging openstack-ops-database openstack-orchestration openstack-telemetry pacman postgresql python rabbitmq runit selinux statsd windows xfs yum yum-epel yum-erlang_solutions knife environment from file separated.rb knife role from file allinone-compute.rb os-base.rb os-block-storage-api.rb os-block-storage-scheduler.rb os-block-storage-volume.rb os-block-storage.rb os-client.rb os-compute-api-ec2.rb os-compute-api-metadata.rb os-compute-api-os-compute.rb os-compute-api.rb os-compute-cert.rb os-compute-conductor.rb os-compute-scheduler.rb os-compute-setup.rb os-compute-single-controller.rb os-compute-vncproxy.rb os-compute-worker.rb os-dashboard.rb os-identity.rb os-image-api.rb os-image-registry.rb os-image-upload.rb os-image.rb os-network-dhcp-agent.rb os-network-l3-agent.rb os-network-metadata-agent.rb os-network-openvswitch.rb os-network-server.rb os-network.rb os-object-storage-account.rb os-object-storage-container.rb os-object-storage-management.rb os-object-storage-object.rb os-object-storage-proxy.rb os-object-storage.rb os-ops-caching.rb os-ops-database.rb os-ops-messaging.rb os-telemetry-agent-central.rb os-telemetry-agent-compute.rb os-telemetry-api.rb os-telemetry-collector.rb knife bootstrap 10.200.10.46 -N opstall01 -E separated --sudo -x thirai -r \u0026#39;role[os-compute-single-controller]\u0026#39; knife bootstrap 10.200.10.47 -N opstall02 -E separated --sudo -x thirai -r \u0026#39;role[os-compute-worker]\u0026#39; knife bootstrap 10.200.10.48 -N opstall03 -E separated --sudo -x thirai -r \u0026#39;role[os-compute-worker]\u0026#39; -e オプションを付与すると実際にこれらのコマンドが実行される。実行してデプロイを行う。\n% spiceweasel -e infrastructure.yml この時点で、下記の操作も行われる。\nNova-Network 上に仮想ネットワークの生成 OS イメージのダウンロードと登録 (Environment に記したモノ) Cinder の設定 デプロイ完了したところで cinder-volume プロセスは稼働しているが物理ディスクの アサインが済んでいない。これは Chef では指定出来ないので手動で行う。\n予め Cinder 用の物理ディスクを Controller ノードに付与する。(ここでは /dev/sdb1)\ncontroller% sudo -i controller# pvcreate /dev/sdb1 controller# vgcreate cinder-volumes /dev/sdb1 controller# service cinder-volume restart これで完了。\n使ってみる ではデプロイした OpenStack を使って仮想マシンを作ってみる。\ncontroller% sudo -i controller# source openrc controller# nova keypair-add novakey01 \u0026gt; novakey01 controller# chmod 400 novakey01 controller# nova boot --image cirros --flavor 1 --key_name novakey01 cirros01 controller# nova list +--------------------------------------+----------+--------+------------+-------------+--------------------+ | ID | Name | Status | Task State | Power State | Networks | +--------------------------------------+----------+--------+------------+-------------+--------------------+ | e6687359-1aef-4105-a8db-894600001610 | cirros01 | ACTIVE | - | Running | private=10.200.9.2 | +--------------------------------------+----------+--------+------------+-------------+--------------------+ controller# ssh -i novakey01 -l cirros 10.200.9.2 vm# ping www.goo.ne.jp 仮想マシンが生成され The Internet に対して通信が行えたことを確認。\n次に仮想ディスクを生成して上記で作成した仮想マシンに付与する。\ncontroller# cinder create --display-name vol01 1 +--------------------------------------+-----------+--------------+------+-------------+----------+--------------------------------------+ | ID | Status | Display Name | Size | Volume Type | Bootable | Attached to | +--------------------------------------+-----------+--------------+------+-------------+----------+--------------------------------------+ | 0f3726a0-c2f5-45bc-bcf2-1f6eb746f5c8 | available | vol02 | 1 | None | false | b286387c-2311-4134-ab59-850fee3e4650 | +--------------------------------------+-----------+--------------+------+-------------+----------+--------------------------------------+ controller# nova volume-attach e6687359-1aef-4105-a8db-894600001610 0f3726a0-c2f5-45bc-bcf2-1f6eb746f5c8 auto controller# ssh -i novakey01 -l cirros 10.200.9.2 vm# mkfs.ext4 /dev/vdb vm# mount -t ext4 /dev/vdb /mnt vm# df -h | grep mnt /dev/vdb 976M 1.3M 908M 1% /mnt 仮想ディスクを仮想マシンに付与しマウントできることを確認出来た。\nまとめ 今回は Nova-Network 構成の Icehouse を構築出来た。Nova-Network は Havana でサポート終了との話が延期 になっていることは聞いていたが Icehouse でもしっかり動いている。また後に Neutron 構成も調査を行う。 Neutron 構成は下記の Environments を参考にすると動作するかもしれない。\nhttps://github.com/stackforge/openstack-chef-repo/blob/master/environments/vagrant-multi-neutron.json\nキーとなるのは、\n\u0026#34;network\u0026#34;: { \u0026#34;debug\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;dhcp\u0026#34;: { \u0026#34;enable_isolated_metadata\u0026#34;: \u0026#34;True\u0026#34; }, \u0026#34;metadata\u0026#34;: { \u0026#34;nova_metadata_ip\u0026#34;: \u0026#34;192.168.3.60\u0026#34; }, \u0026#34;openvswitch\u0026#34;: { \u0026#34;tunnel_id_ranges\u0026#34;: \u0026#34;1:1000\u0026#34;, \u0026#34;enable_tunneling\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;tenant_network_type\u0026#34;: \u0026#34;gre\u0026#34;, \u0026#34;local_ip_interface\u0026#34;: \u0026#34;eth2\u0026#34; }, \u0026#34;api\u0026#34;: { \u0026#34;bind_interface\u0026#34;: \u0026#34;eth1\u0026#34; } }, この辺り。\n残っている問題点 VNC コンソールにアクセス出来ない。調べたのですが、environment の修正で直せる問 題ではないように見えました。\ncookbooks/openstack-compute/templates/default/nova.conf.rb を確認すると下記の ようになっています。\n##### VNCPROXY ##### novncproxy_base_url=\u0026lt;%= @novncproxy_base_url %\u0026gt; xvpvncproxy_base_url=\u0026lt;%= @xvpvncproxy_base_url %\u0026gt; # This is only required on the server running xvpvncproxy xvpvncproxy_host=\u0026lt;%= @xvpvncproxy_bind_host %\u0026gt; xvpvncproxy_port=\u0026lt;%= @xvpvncproxy_bind_port %\u0026gt; # This is only required on the server running novncproxy novncproxy_host=\u0026lt;%= @novncproxy_bind_host %\u0026gt; novncproxy_port=\u0026lt;%= @novncproxy_bind_port %\u0026gt; vncserver_listen=\u0026lt;%= @vncserver_listen %\u0026gt; vncserver_proxyclient_address=\u0026lt;%= @vncserver_proxyclient_address %\u0026gt; vncserver_listen, vncserver_proxyclient_address はそれぞれ\n@vncserver_listen @vncserver_proxyclient_address という変数が格納されることになっている。\nでは cookbooks/openstack-compute/recipes/nova-common.rb を確認すると、\ntemplate \u0026#39;/etc/nova/nova.conf\u0026#39; do source \u0026#39;nova.conf.erb\u0026#39; owner node[\u0026#39;openstack\u0026#39;][\u0026#39;compute\u0026#39;][\u0026#39;user\u0026#39;] group node[\u0026#39;openstack\u0026#39;][\u0026#39;compute\u0026#39;][\u0026#39;group\u0026#39;] mode 00644 variables( sql_connection: sql_connection, novncproxy_base_url: novnc_endpoint.to_s, xvpvncproxy_base_url: xvpvnc_endpoint.to_s, xvpvncproxy_bind_host: xvpvnc_bind.host, xvpvncproxy_bind_port: xvpvnc_bind.port, novncproxy_bind_host: novnc_bind.host, novncproxy_bind_port: novnc_bind.port, vncserver_listen: vnc_endpoint.host, vncserver_proxyclient_address: vnc_endpoint.host, 以下略 となっている。vncserver_listen, vncserver_proxyclient_address 共に vnc_endpoint.host が格納されることになっている。vnc_endpont.host は\nvnc_endpoint = endpoint \u0026#39;compute-vnc\u0026#39; || {} となっており、Attributes の\n\u0026#34;compute-vnc\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;0.0.0.0\u0026#34;, \u0026#34;scheme\u0026#34; =\u0026gt; \u0026#34;http\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; \u0026#34;6080\u0026#34;, \u0026#34;path\u0026#34; =\u0026gt; \u0026#34;/vnc_auto.html\u0026#34; }, の設定が入ることになる。つまり上記のような Attributes だと vncserver_listen, vncserver_proxyclient_address 共に \u0026lsquo;0.0.0.0\u0026rsquo; のアドレスが controller, compute ノードの双方に入ってしまい、NoVNC が正しく格納しないことになる。\n解決したらまたここに更新版を載せたいと思いまーす！\n以上です。\n","permalink":"https://jedipunkz.github.io/post/2014/04/25/stackforge-openstack-chef-repo-icehouse-deploy/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003eまたまた OpenStack のデプロイをどうするか？についてです。\u003c/p\u003e\n\u003cp\u003e今まで自分の中では Rackspace Private Cloud で使われている Rackspace 管理の\nrcbops/chef-cookbooks が今現在使うならベストの選択だと思っていました。これは内\n部で Chef が使われていてしかも Cookbooks が Github 上で公開されています。\nApache ライセンスで使えるので、サービス構築にも使えちゃうというモノ。\u003c/p\u003e\n\u003cp\u003e先日、ある OpenStack コアデベロッパーの方から「jedipunkz さん、やっぱり rcbops\nがいいですか？運営とかどうなっているんでしょう？マージの規準とかどうなのかな？」\nと質問受けました。確かにマージの基準は Rackspace Private Cloud がベースになり\nますし、管理しているエンジニアの一覧を見ていると Rackspace 社のエンジニアがメ\nインですし、今後どうなるのか分からない\u0026hellip;。\u003c/p\u003e\n\u003cp\u003e逃げ道は用意したほうが良さそう。\u003c/p\u003e\n\u003cp\u003eということで、以前自分も暑かったことのある StackForge の openstack-chef-repo\nを久々に使ってみました。Icehouse 構成がこの時点で既に組めるようになっていて、\n以前よりだい〜ぶ完成度増した感があります。今回は nova-network 構成を作ってみた\nのですが、Neutron 構成ももちろん出来そうなので後に調べてまた公開したいです。\u003c/p\u003e\n\u003ch2 id=\"stackforge-とは\"\u003eStackForge とは\u003c/h2\u003e\n\u003cp\u003eStackForge は OpenStack のデプロイ・CI の仕組みとして公式に用いられているもの。\n公式サイトは下記の場所にある。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://ci.openstack.org/stackforge.html\"\u003ehttp://ci.openstack.org/stackforge.html\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eStackForge の openstack-chef-repo は下記の場所にある。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/stackforge/openstack-chef-repo\"\u003ehttps://github.com/stackforge/openstack-chef-repo\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eopenstack-chef-repo はまだ \u0026lsquo;stable/icehouse\u0026rsquo; ブランチが生成されていない。が直\nちに master からブランチが切られる様子。\u003c/p\u003e","title":"stackforge/openstack-chef-repo で OpenStack Icehouse デプロイ"},{"content":"こんにちは。@jedipunkz です。\n皆さん、Mirantis OpenStack はご存知ですか？ OpenStack ディストリビューションの 1つです。以下、公式サイトです。\nhttp://software.mirantis.com/main/\nこの Mirantis OpenStack を使って OpenStack Havana (Neutron GRE) 構成を作ってみ ました。その時のメモを書いていきたいと思います。\n構成は? 構成は下記の通り。\n※ CreativeCommon\n特徴は\nAdministrative Network : Fuel Node, DHCP + PXE ブート用 Management Network : 各コンポーネント間 API 用 Public/Floating IP Network : パブリック API, VM Floating IP 用 Storage Network : Cinder 配下ストレージ \u0026lt;-\u0026gt; インスタンス間用 要インターネット接続 : Public/Floating Networks Neutron(GRE) 構成 です。タグ VLAN 使って物理ネットワークの本数を減らすことも出来るはずですが、僕 の環境では何故かダメだったので上記の4つの物理ネットワークを別々に用意しました。\nFuel ノードの構築 Fuel ノードとは、OpenStack の各ノードをデプロイするための管理ノードのことです。 DHCP + PXE を管理する Cobbler やデプロイツールの Puppet が内部で稼働し、 Administrative Network 上で稼働したノードを管理・その後デプロイします。\n構築方法は\u0026hellip;\n下記の URL より ISO ファイルをダウンロード。\nhttp://software.mirantis.com/main/\nAdministrative Network に NIC を出したノードを上記の ISO から起動。\nGrub メニューが表示されたタイミングで \u0026ldquo;Tab\u0026rdquo; キーを押下。\n上記画面にてカーネルオプションにて hostname, ip, gw, dns を修正。下記は例。\nvmlinuz initrd=initrd.img biosdevname=0 ks=cdrom:/ks.cfg ip=10.200.10.76 gw=10.200.10.1 dns1=8.8.8.8 netmask=255.255.255.0 hostname=fuel.cpi.ad.jp showmenu=no_ ブラウザで http://10.200.10.76:8080 (上記例)にアクセスし、新しい \u0026lsquo;OpenStack Environment\u0026rsquo; を作成する。\nName : 任意 OpenStack Release : Havana on CentOS6.4 なお、Ubuntu 構成も組めるが、私の環境では途中でエラーが出力した。\nNext を押下し、ネットワーク設定を行う。今回は \u0026lsquo;Nuetron GRE\u0026rsquo; を選択。\n\u0026lsquo;Save Settings\u0026rsquo; を押下し設定を保存。この時点では \u0026lsquo;Verify Networks\u0026rsquo; は行えない。 少なくとも 2 ノードが必要。次のステップで2ノードの追加を行う。\nノードの追加 下記の4つのネットワークセグメントに NIC を出したノードを用意し、起動する。起動 すると Administrative Network 上で稼働している Cobbler によりノードが PXE 起動 しミニマムな OS がインストールされる。これは後に Fuel ノードよりログインがされ、 各インターフェースの Mac アドレス等の情報を知るためです。ネットワークベリファ イ等もこのミニマムな OS 越しに実施されます。\nAdministrative Network Public/Floating IP Network Storage Network Management Network ノードが稼働した時点で Fuel によりノードが認識されるので、ここでは2つのノード をそれぞれ\nController ノード Compute ノード として画面上で割り当てます。\nインターフェースの設定 http://10.200.10.76:8000/#cluster/1/nodes にログインし作成した Environment を選択。その後、\u0026lsquo;Nodes\u0026rsquo; タブを押下。ノードを選択し、\u0026lsquo;Configure Interfaces\u0026rsquo; を 選択。各ノードのインターフェースの Mac アドレスを確認し、各々のネットワークセ グメントを紐付ける。尚、Fuel ノードから \u0026lsquo;root\u0026rsquo; ユーザで SSH(22番ポート) にノン パスフレーズで公開鍵認証ログインが可能となっている。Fuel ノードに対しては SSH (22番ポート) にて下記のユーザにてログインが可能です。\nusername : root password : r00tme ネットワークの確認 http://10.200.10.76:8000/#cluster/1/network にて \u0026lsquo;Networks\u0026rsquo; タブを開き、\u0026lsquo;Verify Networks\u0026rsquo; を押下。ネットワーク設定が正しく行われているか否かを確認。\nデプロイ http://10.200.10.76:8000/#cluster/1/nodes にて \u0026lsquo;Deploy Changes\u0026rsquo; を押下しデプ ロイ開始。kickstart にて OS が自動でインストールされ puppet にて fuel ノードか ら自動で OpenStack がインストールされます。\nOpenStack へのアクセス SSH では下記のステップで OpenStack コントローラノードにログイン。\nfuel ノード (SSH) -\u0026gt; node-1 (OpenStack コントローラノード)(SSH)\nブラウザで Horizon にアクセスするには\nhttp://10.200.10.2\nにアクセス。これは例。Administrative Network に接続している NIC の IP アドレス へ HTTP でログイン。\nまとめ Mirantis OpenStack Neutron (GRE) 構成が組めた。上記構成図を見て疑問に思ってい た \u0026ldquo;VM 間通信のネットワークセグメント\u0026rdquo; であるが、Administrative Network のセグ メントを用いている事が判った。これは利用者が意図しているとは考えづらいので、正 直、あるべき姿では無いように思える。が、上記構成図に VM ネットワークが無い理由 はこれにて判った。\n下記はノード上で ovs-vsctl show コマンドを打った結果の抜粋です。\nBridge \u0026#34;br-eth1\u0026#34; Port \u0026#34;br-eth1\u0026#34; Interface \u0026#34;br-eth1\u0026#34; type: internal Port \u0026#34;br-eth1--br-fw-admin\u0026#34; trunks: [0] Interface \u0026#34;br-eth1--br-fw-admin\u0026#34; type: patch options: {peer=\u0026#34;br-fw-admin--br-eth1\u0026#34;} Port \u0026#34;eth1\u0026#34; Interface \u0026#34;eth1\u0026#34; 今回の構成は eth1 は Administrative Network に割り当てていました。\n一昔前は OS のディストリビュータが有料サポートをビジネスにしていました。Redhat がその代表格だと思いますが、今は OS 上で何かを実現するにもソフトウェアの完成度 が高く、エンジニアが困るシチュエーションがそれほど無くなった気がします。そこで その OS の上で稼働する OpenStack のサポートビジネスが出てきたか！という印象で す。OpenStack はまだまだエンジニアにとって敷居の高いソフトウェアです。自らクラ ウドプラットフォームを構築出来るのは魅力的ですが、サポート無しに構築・運用する には、まだ難しい技術かもしれません。こういったディストリビューションが出てくる 辺りが時代だなぁと感じます。\n尚、ISO をダウンロードして利用するだけでしたら無償で OK です。\n","permalink":"https://jedipunkz.github.io/post/2014/04/23/mirantis-openstack/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003e皆さん、Mirantis OpenStack はご存知ですか？ OpenStack ディストリビューションの\n1つです。以下、公式サイトです。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://software.mirantis.com/main/\"\u003ehttp://software.mirantis.com/main/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eこの Mirantis OpenStack を使って OpenStack Havana (Neutron GRE) 構成を作ってみ\nました。その時のメモを書いていきたいと思います。\u003c/p\u003e\n\u003ch2 id=\"構成は\"\u003e構成は?\u003c/h2\u003e\n\u003cp\u003e構成は下記の通り。\u003c/p\u003e\n\u003cimg src=\"http://jedipunkz.github.io/pix/mirantis_gre.jpg\"\u003e\n\u003cp\u003e※ CreativeCommon\u003c/p\u003e\n\u003cp\u003e特徴は\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdministrative Network : Fuel Node, DHCP + PXE ブート用\u003c/li\u003e\n\u003cli\u003eManagement Network : 各コンポーネント間 API 用\u003c/li\u003e\n\u003cli\u003ePublic/Floating IP Network : パブリック API, VM Floating IP 用\u003c/li\u003e\n\u003cli\u003eStorage Network : Cinder 配下ストレージ \u0026lt;-\u0026gt; インスタンス間用\u003c/li\u003e\n\u003cli\u003e要インターネット接続 : Public/Floating Networks\u003c/li\u003e\n\u003cli\u003eNeutron(GRE) 構成\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eです。タグ VLAN 使って物理ネットワークの本数を減らすことも出来るはずですが、僕\nの環境では何故かダメだったので上記の4つの物理ネットワークを別々に用意しました。\u003c/p\u003e\n\u003ch2 id=\"fuel-ノードの構築\"\u003eFuel ノードの構築\u003c/h2\u003e\n\u003cp\u003eFuel ノードとは、OpenStack の各ノードをデプロイするための管理ノードのことです。\nDHCP + PXE を管理する Cobbler やデプロイツールの Puppet が内部で稼働し、\nAdministrative Network 上で稼働したノードを管理・その後デプロイします。\u003c/p\u003e","title":"Mirantis OpenStack (Neutron GRE)を組んでみた！"},{"content":"こんにちは。@jedipunkz です。\n今週 Redhat が \u0026lsquo;Redhat Enterprise Linux Atomic Host\u0026rsquo; しましたよね。Docker を特 徴としたミニマムな OS だとのこと。その内部で用いられている技術 Geard について 少し調べてみました。複数コンテナの関連付けが可能なようです。ここでは調べた結果 について簡単にまとめていきます。\n参考資料 http://openshift.github.io/geard/deploy_with_geard.html\n利用方法 ここではホスト OS に Fedora20 を用意します。\nまず Geard をインストール\n% sudo yum install --enablerepo=updates-testing geard 下記の json ファイルを作成します。ここにはコンテナをデプロイするための情報と関 連付けのための情報を記します。\n$ ${EDITOR} rockmongo_mongodb.json { \u0026#34;containers\u0026#34;:[ { \u0026#34;name\u0026#34;:\u0026#34;rockmongo\u0026#34;, \u0026#34;count\u0026#34;:1, \u0026#34;image\u0026#34;:\u0026#34;derekwaynecarr/rockmongo\u0026#34;, \u0026#34;publicports\u0026#34;:[{\u0026#34;internal\u0026#34;:80,\u0026#34;external\u0026#34;:6060}], \u0026#34;links\u0026#34;:[{\u0026#34;to\u0026#34;:\u0026#34;mongodb\u0026#34;}] }, { \u0026#34;name\u0026#34;:\u0026#34;mongodb\u0026#34;, \u0026#34;count\u0026#34;:1, \u0026#34;image\u0026#34;:\u0026#34;ccoleman/ubuntu-mongodb\u0026#34;, \u0026#34;publicports\u0026#34;:[{\u0026#34;internal\u0026#34;:27017}] } ] } 上記のファイルの解説。\nコンテナ \u0026lsquo;rockmongo\u0026rsquo; と \u0026lsquo;mongodb\u0026rsquo; を作成 それぞれ1個ずつコンテナを作成 \u0026lsquo;image\u0026rsquo; パラメータにて docker イメージの指定 \u0026lsquo;publicports\u0026rsquo; パラメータにてコンテナ内部とホスト側のポートマッピングを行う \u0026rsquo;links\u0026rsquo; パラメータで \u0026lsquo;rockmongo\u0026rsquo; を \u0026lsquo;mongodb\u0026rsquo; に関連付け では、デプロイ開始します。\n$ sudo gear deploy rockmongo_mongo.json 2014/04/22 07:21:12 Deploying rockmongo_mongo.json.20140422-072112 2014/04/22 07:21:12 appending 27017 on rockmongo-1: \u0026amp;{PortPair:{Internal:27017 External:0} Target:127.0.0.1:27017} \u0026amp;{Id:rockmongo-1 Image:derekwaynecarr/rockmongo From:rockmongo On:0xc2100bb980 Ports:[{PortPair:{Internal:80 External:6060} Target::0}] add:true remove:false container:0xc21000be00 links:[]} 2014/04/22 07:21:12 ports: Releasing 2014/04/22 07:21:12 systemd: Reloading daemon local PortMapping: 80 -\u0026gt; 6060 local Container rockmongo-1 is installed 2014/04/22 07:21:12 ports: Releasing 2014/04/22 07:21:12 systemd: Reloading daemon local PortMapping: 27017 -\u0026gt; 4000 local Container mongodb-1 is installed linking rockmongo: 127.0.0.1:27017 -\u0026gt; localhost:4000 local Links set on local local Container rockmongo-1 starting local Container mongodb-1 starting ブラウザにてホスト OS に接続することで rockmongo の管理画面にアクセスが可能。\nhttp://\u0026lt;ホストOSのIP:6060\u0026gt;/ ポートマッピング管理 デプロイが完了して、実際に RockMongo の管理画面にアクセス出来たと思います。 関連付けが特徴と言えそうなのでその解析をしてみました。\nGeard のコンテナ関連付けはホストとコンテナのポート管理がキモとなっていることが 解ります。これを紐解くことで geard のコンテナ管理を理解します。\n\u0026#39;rockmongo\u0026#39; コンテナ \u0026#39;mongodb\u0026#39; コンテナ +----------+ +--------------------------+ +-------------------+ | Client |-\u0026gt;6060-\u0026gt;|-\u0026gt;80-\u0026gt; RockMongo -\u0026gt;27017-\u0026gt;|-\u0026gt;4000-\u0026gt;|-\u0026gt;27017-\u0026gt; Mongodb | +----------+ +--------------------------+ +-------------------+ |-------------------- docker ホスト --------------------| 上記 \u0026lsquo;gear deploy\u0026rsquo; コマンド発行時のログと json ファイルにより上図のような構成 になっていることが理解できます。一つずつ読み解いていきましょう。\n\u0026lsquo;rockmongo\u0026rsquo; コンテナの内部でリスンしている 80 番ポートはホスト OS の 6060 番へ変換 \u0026lsquo;rockmongo\u0026rsquo; コンテナ内で稼働する RockMongo の config.php から \u0026lsquo;127.0.0.1:27017\u0026rsquo; でリスンしていることが解る 試しに \u0026lsquo;derekwaynecarr/rockmongo:latest\u0026rsquo; に /bin/bash でログインし config.php を確認。\n$ sudo docker run -i -t derekwaynecarr/rockmongo:latest /bin/bash bash-4.1# grep mongo_host /var/www/html/config.php $MONGO[\u0026#34;servers\u0026#34;][$i][\u0026#34;mongo_host\u0026#34;] = \u0026#34;127.0.0.1\u0026#34;;//mongo host $MONGO[\u0026#34;servers\u0026#34;][$i][\u0026#34;mongo_host\u0026#34;] = \u0026#34;127.0.0.1\u0026#34;; デプロイ時のログと json ファイルの \u0026rsquo;links\u0026rsquo; パラメータより、ホストのポートに動的に(ここでは 4000番ポート) に変換されることが解ります。 linking rockmongo: 127.0.0.1:27017 -\u0026gt; localhost:4000 ホストの 4000 番ポートは動的に \u0026lsquo;mongodb\u0026rsquo; コンテナの内部ポート 27017 にマッピングされる これらのポートマッピングによりそれぞれのコンテナの連携が取れている。\nまとめと考察 Geard は下記の2つを特徴とした技術だと言えるとことがわかりました。\nコンテナを json で管理しデプロイする仕組みを提供する コンテナ間の関連付けをホスト OS のポートを動的に管理・マッピングすることで行う 同じような OS に CoreOS https://coreos.com/ がありますが、こちらも docker, sytemd 等を特徴としています。さらに etcd を使ってクラスタの構成等が可能になっ ていますが、Geard はホストのポートを動的に管理することで関連付けが可能なことが わかりました。\n実際に触ってみた感覚から言えば、まだまだ実用化は厳しい状況に思えますが、今後へ の展開に期待したい技術です。\n","permalink":"https://jedipunkz.github.io/post/2014/04/22/geard-port-mapping/","summary":"\u003cp\u003eこんにちは。@jedipunkz です。\u003c/p\u003e\n\u003cp\u003e今週 Redhat が \u0026lsquo;Redhat Enterprise Linux Atomic Host\u0026rsquo; しましたよね。Docker を特\n徴としたミニマムな OS だとのこと。その内部で用いられている技術 Geard について\n少し調べてみました。複数コンテナの関連付けが可能なようです。ここでは調べた結果\nについて簡単にまとめていきます。\u003c/p\u003e\n\u003ch2 id=\"参考資料\"\u003e参考資料\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"http://openshift.github.io/geard/deploy_with_geard.html\"\u003ehttp://openshift.github.io/geard/deploy_with_geard.html\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"利用方法\"\u003e利用方法\u003c/h2\u003e\n\u003cp\u003eここではホスト OS に Fedora20 を用意します。\u003c/p\u003e\n\u003cp\u003eまず Geard をインストール\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e% sudo yum install --enablerepo\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eupdates-testing geard\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e下記の json ファイルを作成します。ここにはコンテナをデプロイするための情報と関\n連付けのための情報を記します。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-json\" data-lang=\"json\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e$\u003c/span\u003e \u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e$\u003c/span\u003e{\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003eEDITOR\u003c/span\u003e} \u003cspan style=\"color:#960050;background-color:#1e0010\"\u003erockmongo_mongodb.json\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e{\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#f92672\"\u003e\u0026#34;containers\u0026#34;\u003c/span\u003e:[\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      \u003cspan style=\"color:#f92672\"\u003e\u0026#34;name\u0026#34;\u003c/span\u003e:\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;rockmongo\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      \u003cspan style=\"color:#f92672\"\u003e\u0026#34;count\u0026#34;\u003c/span\u003e:\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      \u003cspan style=\"color:#f92672\"\u003e\u0026#34;image\u0026#34;\u003c/span\u003e:\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;derekwaynecarr/rockmongo\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      \u003cspan style=\"color:#f92672\"\u003e\u0026#34;publicports\u0026#34;\u003c/span\u003e:[{\u003cspan style=\"color:#f92672\"\u003e\u0026#34;internal\u0026#34;\u003c/span\u003e:\u003cspan style=\"color:#ae81ff\"\u003e80\u003c/span\u003e,\u003cspan style=\"color:#f92672\"\u003e\u0026#34;external\u0026#34;\u003c/span\u003e:\u003cspan style=\"color:#ae81ff\"\u003e6060\u003c/span\u003e}],\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      \u003cspan style=\"color:#f92672\"\u003e\u0026#34;links\u0026#34;\u003c/span\u003e:[{\u003cspan style=\"color:#f92672\"\u003e\u0026#34;to\u0026#34;\u003c/span\u003e:\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;mongodb\u0026#34;\u003c/span\u003e}]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    },\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      \u003cspan style=\"color:#f92672\"\u003e\u0026#34;name\u0026#34;\u003c/span\u003e:\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;mongodb\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      \u003cspan style=\"color:#f92672\"\u003e\u0026#34;count\u0026#34;\u003c/span\u003e:\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      \u003cspan style=\"color:#f92672\"\u003e\u0026#34;image\u0026#34;\u003c/span\u003e:\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;ccoleman/ubuntu-mongodb\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      \u003cspan style=\"color:#f92672\"\u003e\u0026#34;publicports\u0026#34;\u003c/span\u003e:[{\u003cspan style=\"color:#f92672\"\u003e\u0026#34;internal\u0026#34;\u003c/span\u003e:\u003cspan style=\"color:#ae81ff\"\u003e27017\u003c/span\u003e}]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    }\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  ]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e上記のファイルの解説。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eコンテナ \u0026lsquo;rockmongo\u0026rsquo; と \u0026lsquo;mongodb\u0026rsquo; を作成\u003c/li\u003e\n\u003cli\u003eそれぞれ1個ずつコンテナを作成\u003c/li\u003e\n\u003cli\u003e\u0026lsquo;image\u0026rsquo; パラメータにて docker イメージの指定\u003c/li\u003e\n\u003cli\u003e\u0026lsquo;publicports\u0026rsquo; パラメータにてコンテナ内部とホスト側のポートマッピングを行う\u003c/li\u003e\n\u003cli\u003e\u0026rsquo;links\u0026rsquo; パラメータで \u0026lsquo;rockmongo\u0026rsquo; を \u0026lsquo;mongodb\u0026rsquo; に関連付け\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eでは、デプロイ開始します。\u003c/p\u003e","title":"Geard のポートマッピングについて調べてみた"},{"content":"こんにちは！@jedipunkz です。\n今回は Havana 版の OpenStack Glance, Cinder と分散ストレージの Ceph を連携させ る手順を書いていきます。元ネタはこちら。下記の Ceph の公式サイトに手順です。\nhttps://ceph.com/docs/master/rbd/rbd-openstack/\nこの手順から下記の変更を行って、ここでは記していきます。\nNova + Ceph 連携させない cinder-backup は今のところ動作確認出来ていないので省く 諸々の手順がそのままでは実施出来ないので補足を入れていく。 cinder-backup は Cinder で作成した仮想ディスクのバックアップを Ceph ストレージ 上に取ることが出来るのですが、そもそも Ceph 上にある仮想ディスクを Ceph にバッ クアップ取っても意味が薄いですし、まだ動いていないので今回は省きます。LVM やそ の他ストレージを使った Cinder 連携をされている方にとっては cinder-backup の Ceph 連携は意味が大きくなってくると思います。\n構成 下記の通りの物理ネットワーク6つの構成です。 OpenStack, Ceph 共に最小構成を前提にします。\n+--------------------------------------------------------------------- external | +--------------+--(-----------+--------------+------------------------------------------ public | | | | | +------------+ +------------+ +------------+ +------------+ +------------+ +------------+ | controller | | network | | compute | | ceph01 | | ceph02 | | ceph03 | +------------+ +------------+ +------------+ +------------+ +------------+ +------------+ | | | | | | | | | | | | | | | | +--------------+--(-----------+--(-----------+--(--(--------+--(--(--------+--(--(------- management | | | | | | | | | | | +--------------+--(-----------(--(-----------(--(-----------(--(------- guest | | | | | | | | +--------------------------------+-----------+--(-----------+--(-----------+--(------- storage | | | +--------------+--------------+------- cluster 特徴\n最小構成 controller x 1 + network x 1 + compute x 1 + ceph x 3 OpenStack API の相互通信は management ネットワーク OpenStack VM 間通信は guest ネットワーク OpenStack 外部通信は public ネットワーク OpenStack VM の floating-ip (グローバル) 通信は external ネットワーク Ceph と OpenStack 間の通信は storage ネットワーク Ceph の OSD 間通信は cluster ネットワーク ここには記されていないホスト \u0026lsquo;workstation\u0026rsquo; から OpenStack, Ceph をデプロイ 前提の構成 前提構成の OpenStack と Ceph ですが、ここでは構築方法は割愛します。OpenStack は rcbops/chef-cookbooks を。Ceph は ceph-deploy を使って自分は構築しました。 下記の自分のブログに構築手順が載っているので参考にしてみてください。\nOpenStack 構築方法\nhttp://jedipunkz.github.io/blog/2013/11/17/openstack-havana-chef-deploy/\nCeph 構築方法\nhttp://jedipunkz.github.io/blog/2014/02/27/journal-ssd-ceph-deploy/\nOpenStack + Ceph 連携手順 では実際に連携するための手順を記していきます。\nrbd pool の作成 Ceph ノードの何れかで下記の手順を実施し rbd pool を作成する。\nceph01% sudo ceph osd pool create volumes 128 ceph01% sudo ceph osd pool create images 128 ceph01% sudo ceph osd pool create backups 128 ssh 鍵の配置 Ceph ノードから OpenStack の controller, compute ノードへ接続出来るよう鍵を配 布します。\nceph01% ssh-copy-id \u0026lt;username\u0026gt;@\u0026lt;controller_ip\u0026gt; ceph01% ssh-copy-id \u0026lt;username\u0026gt;@\u0026lt;compute_ip\u0026gt; sudoers の設定 controller, compute ノード上で sudoers の設定を予め実施する。 /etc/sudoers.d/ として保存する。\n\u0026lt;username\u0026gt; ALL = (root) NOPASSWD:ALL ceph パッケージのインストール controller ノード, compute ノード上で Ceph パッケージをインストールする。\ncontroller% wget -q -O- \u0026#39;https://ceph.com/git/?p=ceph.git;a=blob_plain;f=keys/release.asc\u0026#39; | sudo apt-key add - controller% echo deb http://ceph.com/debian/ $(lsb_release -sc) main | sudo tee /etc/apt/sources.list.d/ceph.list controller% sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y python-ceph ceph-common compute% wget -q -O- \u0026#39;https://ceph.com/git/?p=ceph.git;a=blob_plain;f=keys/release.asc\u0026#39; | sudo apt-key add - compute% echo deb http://ceph.com/debian/ $(lsb_release -sc) main | sudo tee /etc/apt/sources.list.d/ceph.list compute% sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y python-ceph ceph-common controller ノード, compute ノード上でディレクトリを作成する。\ncontroller% sudo mkdir /etc/ceph compute % sudo mkdir /etc/ceph ceph.conf を controller, compute ノードへ配布する。\nceph01% sudo -i ceph01# ssh \u0026lt;controller_ip\u0026gt; sudo tee /etc/ceph/ceph.conf \u0026lt;/etc/ceph/ceph.conf ceph01# ssh \u0026lt;compute_ip\u0026gt; sudo tee /etc/ceph/ceph.conf \u0026lt;/etc/ceph/ceph.conf ceph 上にユーザを作成 Ceph 上に cinder, glance 用の新しいユーザを作成する。\nceph auth get-or-create client.cinder mon \u0026#39;allow r\u0026#39; osd \u0026#39;allow class-read object_prefix rbd_children, allow rwx pool=volumes, allow rx pool=images\u0026#39; ceph auth get-or-create client.glance mon \u0026#39;allow r\u0026#39; osd \u0026#39;allow class-read object_prefix rbd_children, allow rwx pool=images\u0026#39; ceph auth get-or-create client.cinder-backup mon \u0026#39;allow r\u0026#39; osd \u0026#39;allow class-read object_prefix rbd_children, allow rwx pool=backups\u0026#39; キーリングの作成と配置 client.cinder, client.glance, client.cinder-backup のキーリングを作成する。また作成したキーリングを controller ノードに配布する。\nceph01% sudo ceph auth get-or-create client.glance | ssh {your-glance-api-server} sudo tee /etc/ceph/ceph.client.glance.keyring ceph01% ssh {your-glance-api-server} sudo chown glance:glance /etc/ceph/ceph.client.glance.keyring ceph01% sudo ceph auth get-or-create client.cinder | ssh {your-volume-server} sudo tee /etc/ceph/ceph.client.cinder.keyring ceph01% ssh {your-cinder-volume-server} sudo chown cinder:cinder /etc/ceph/ceph.client.cinder.keyring ceph01% sudo ceph auth get-or-create client.cinder-backup | ssh {your-cinder-backup-server} sudo tee /etc/ceph/ceph.client.cinder-backup.keyring ceph01% ssh {your-cinder-backup-server} sudo chown cinder:cinder /etc/ceph/ceph.client.cinder-backup.keyring client.cinder のキーリングを compute ノードに配置する。\nceph01% sudo ceph auth get-key client.cinder | ssh {your-compute-node} tee client.cinder.key libvirt への secret キー追加 compute ノード上で secret キーを libvirt に追加する。\ncompute% uuidgen 457eb676-33da-42ec-9a8c-9293d545c337 cat \u0026gt; secret.xml \u0026lt;\u0026lt;EOF \u0026lt;secret ephemeral=\u0026#39;no\u0026#39; private=\u0026#39;no\u0026#39;\u0026gt; \u0026lt;uuid\u0026gt;457eb676-33da-42ec-9a8c-9293d545c337\u0026lt;/uuid\u0026gt; \u0026lt;usage type=\u0026#39;ceph\u0026#39;\u0026gt; \u0026lt;name\u0026gt;client.cinder secret\u0026lt;/name\u0026gt; \u0026lt;/usage\u0026gt; \u0026lt;/secret\u0026gt; EOF compute % sudo virsh secret-define --file secret.xml Secret 457eb676-33da-42ec-9a8c-9293d545c337 created compute% sudo virsh secret-set-value --secret 457eb676-33da-42ec-9a8c-9293d545c337 --base64 $(cat client.cinder.key) \u0026amp;\u0026amp; rm client.cinder.key secret.xml glance 連携手順 controller ノードの /etc/glance/glance-api.conf に下記を追記。 default_store=file と標準ではなっているので下記の通り rbd に書き換える。\ndefault_store=rbd rbd_store_user=glance rbd_store_pool=images cinder 連携手順 controller ノードの /etc/cinder/cinder.conf に下記を追記。 volume_driver は予め LVM の設定が入っていると思われるので書き換える。 また rbd_secret_uuid は先ほど生成した uuid を記す。\nvolume_driver=cinder.volume.drivers.rbd.RBDDriver rbd_pool=volumes rbd_ceph_conf=/etc/ceph/ceph.conf rbd_flatten_volume_from_snapshot=false rbd_max_clone_depth=5 glance_api_version=2 rbd_user=cinder rbd_secret_uuid=457eb676-33da-42ec-9a8c-9293d545c337 ceph.conf への追記 上記で配布した ceph.conf にはキーリングのパスが記されていない。controller ノー ド上の /etc/ceph/ceph.conf に下記の通り追記する。\nここは公式サイトには印されていないのでハマりました。ポイントです。\n[client.keyring] keyring = /etc/ceph/ceph.client.cinder.keyring OpenStack のそれぞれのコンポーネントを再起動かける controller% sudo glance-control api restart compute % sudo service nova-compute restart controller% sudo service cinder-volume restart controller% sudo service cinder-backup restart 動作確認 では動作確認を。Glance に OS イメージを登録。その後そのイメージを元にインスタ ンスを作成。Cinder 上に仮想ディスクを作成。その後先ほど作成したインスタンスに 接続しマウント。そのマウントした仮想ディスク上で書き込みが行えるか確認をします。\nテストで Ubuntu Precise 12.04 のイメージを glance を用いて登録する。\ncontroller% wget http://cloud-images.ubuntu.com/precise/current/precise-server-cloudimg-amd64-disk1.img controller% glance image-create --name precise-image --is-public true --container-format bare --disk-format qcow2 \u0026lt; precise-server-cloudimg-amd64-disk1.img テスト VM を稼働する。\ncontroller% nova boot --nic net-id=\u0026lt;net_id\u0026gt; --flavor 2 --image precise-image --key_name novakey01 vm01 controller% cinder create --display-name vol01 1 controller% nova volume-attach \u0026lt;instance_id\u0026gt; \u0026lt;volume_id\u0026gt; auto テスト VM へログインしファイルシステムを作成後、マウントする。\ncontroller% ssh -i \u0026lt;key_file_path\u0026gt; -l ubuntu \u0026lt;instance_ip\u0026gt; vm01% sudo mkfs -t ext4 /dev/vdb vm01% sudo mount -t ext4 /dev/vdb /mnt マウントした仮想ディスク上でデータを書き込んでみる。\nvm01% sudo dd if=/dev/zero of=/mnt/500M bs=1M count=5000 まとめ Ceph, Cinder の Ceph 連携が出来ました！\nOpenStack Grizzly 版時代に Ceph 連携は取っていたのですが Havana では\ncinder-bacup の Ceph 連携 nova の Ceph 連携 が追加されていました。Nova 連携をとるとインスタンスを稼働させる際に通常は controller ノードの /var/lib/nova 配下にファイルとしてインスタンスイメージが作 成されますが、これが Ceph 上に作成されるとのことです。Nova 連携は是非とってみ たいのですが、今のところ動いていません。引き続き調査を行います。\ncinder-backup も少し連携取ってみましたが backup_driver に Ceph ドライバの指定 をしているにも関わらず Swift に接続しにいってしまう有り様でした..。こちらも引 き続き調査します。またステートが \u0026lsquo;in-use\u0026rsquo; の場合バックアップが取れず \u0026lsquo;available\u0026rsquo; なステートでないといけないようです。確かに書き込み中に操作が行われ てしまってもバックアップの整合性が取れないですし、ここは仕方ないところですね。\n","permalink":"https://jedipunkz.github.io/post/2014/04/04/openstack-havana-cinder-glance-ceph/","summary":"\u003cp\u003eこんにちは！\u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003e今回は Havana 版の OpenStack Glance, Cinder と分散ストレージの Ceph を連携させ\nる手順を書いていきます。元ネタはこちら。下記の Ceph の公式サイトに手順です。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://ceph.com/docs/master/rbd/rbd-openstack/\"\u003ehttps://ceph.com/docs/master/rbd/rbd-openstack/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eこの手順から下記の変更を行って、ここでは記していきます。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eNova + Ceph 連携させない\u003c/li\u003e\n\u003cli\u003ecinder-backup は今のところ動作確認出来ていないので省く\u003c/li\u003e\n\u003cli\u003e諸々の手順がそのままでは実施出来ないので補足を入れていく。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003ecinder-backup は Cinder で作成した仮想ディスクのバックアップを Ceph ストレージ\n上に取ることが出来るのですが、そもそも Ceph 上にある仮想ディスクを Ceph にバッ\nクアップ取っても意味が薄いですし、まだ動いていないので今回は省きます。LVM やそ\nの他ストレージを使った Cinder 連携をされている方にとっては cinder-backup の\nCeph 連携は意味が大きくなってくると思います。\u003c/p\u003e\n\u003ch2 id=\"構成\"\u003e構成\u003c/h2\u003e\n\u003cp\u003e下記の通りの物理ネットワーク6つの構成です。\nOpenStack, Ceph 共に最小構成を前提にします。\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e                  +--------------------------------------------------------------------- external\n                  |\n+--------------+--(-----------+--------------+------------------------------------------ public\n|              |  |           |              |\n+------------+ +------------+ +------------+ +------------+ +------------+ +------------+\n| controller | |  network   | |  compute   | |   ceph01   | |   ceph02   | |   ceph03   |\n+------------+ +------------+ +------------+ +------------+ +------------+ +------------+\n|  |           |  |           |  |  |        |  |  |        |  |  |        |  |  |\n+--------------+--(-----------+--(-----------+--(--(--------+--(--(--------+--(--(------- management\n   |              |              |  |           |  |           |  |           |  |\n   |              +--------------+--(-----------(--(-----------(--(-----------(--(------- guest\n   |                                |           |  |           |  |           |  |\n   +--------------------------------+-----------+--(-----------+--(-----------+--(------- storage\n                                                   |              |              |\n                                                   +--------------+--------------+------- cluster\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e特徴\u003c/p\u003e","title":"OpenStack Havana Cinder,Glance の分散ストレージ Ceph 連携"},{"content":"こんにちは。@jedipunkz です。\n追記 2014/03/20 : 一旦削除していた記事なのですが、無事動作が確認出来たので再度アッ プします！\n第17回 OpenStack 勉強会で rcbops/chef-cookbooks の話をしてきたのですが会場から 質問がありました。「Havana の Swift 構成を作る Cookbooks はどこにありますか？」 と。私が試したのが Grizzly 時代のモノで、よく rcbops/chef-cookbooks を見てみる と Havana ブランチ又は Havana に対応したリリースタグのファイル構成に Swift が 綺麗サッパリ消えているではありませんか・・！下記の Swift の Cookbooks は幸い github 上に残っていました。\nhttps://github.com/rcbops-cookbooks/swift\nが rcbops/chef-cookbooks との関連付けが切れています・・。ぐあぁ。\nということで Havana 構成の Keystone 2013.2.2 と Swift 1.10.0 の構成を Chef で 作らねば！と思い色々試していたら結構あっさりと出来ました。今回はその方法を書い ていきたいと思います！\n構成 構成は\u0026hellip;以前の記事 http://jedipunkz.github.io/blog/2013/10/27/swift-chef/ と同じです。\n+-----------------+ | load balancer | +-----------------+ | +-------------------+-------------------+-------------------+-------------------+---------------------- proxy network | | | | | +-----------------+ +-----------------+ +-----------------+ +-----------------+ +-----------------+ | chef server | | chef workstation| | swift-mange | | swift-proxy01 | | swift-proxy02 | +-----------------+ +-----------------+ +-----------------+ +-----------------+ +-----------------+ ...\u0026gt; scaling | | | | | +-------------------+-------------------+-------------------+-------------------+-------------------+-- storage network | | | | | | +-----------------+ +-----------------+ +-----------------+ +-----------------+ +-----------------+ +-----------------+ | swift-storage01 | | swift-storage02 | | swift-storage03 | | swift-account01 | | swift-account02 | | swift-account03 | +-----------------+ +-----------------+ +-----------------+ +-----------------+ +-----------------+ +-----------------+ ..\u0026gt; scaling 手順 では早速手順を記していきますね。毎回なのですが Chef ワークステーション・Chef サーバの環境構築については割愛します。オムニバスインストーラを使えば Chef サー バの構築は簡単ですし、ワークステーションの構築も Ruby インストール -\u0026gt; gem で Chef をインストール -\u0026gt; .chef 配下を整える、で出来ます。\nrcbops/chef-cookbooks の取得。現在 Stable バージョンの refs/tags/v4.2.1 を用いる。\n% git clone https://github.com/rcbops/chef-cookbooks.git ./chef-cookbooks-4.2.1 % cd ./chef-cookbooks-4.2.1 % git checkout -b v4.2.1 refs/tags/v4.2.1 % git submodule init % git submodule sync % git submodule update ここで本家 rcbops/chef-cookbooks と関連付けが消えている rcbops-cookbooks/swift を cookbooks ディレクトリ配下にクローンします。あと念のため \u0026lsquo;havana\u0026rsquo; ブランチ を指定します。コードを確認したところ何も変化はありませんでしたが。\n% git clone https://github.com/rcbops-cookbooks/swift.git cookbooks/swift % cd cookbooks/swift % git checkout -b havana remotes/origin/havana % cd ../.. cookbooks, roles の Chef サーバへのアップロードを行います。\n% knife cookbook upload -o cookbooks -a % knife role from file role/*.rb 今回の構成 (1クラスタ) 用の environments/swift-havana.json を作成します。json ファイルの名前は任意です。\n{ \u0026#34;name\u0026#34;: \u0026#34;swift-havana\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;cookbook_versions\u0026#34;: { }, \u0026#34;json_class\u0026#34;: \u0026#34;Chef::Environment\u0026#34;, \u0026#34;chef_type\u0026#34;: \u0026#34;environment\u0026#34;, \u0026#34;default_attributes\u0026#34;: { }, \u0026#34;override_attributes\u0026#34;: { \u0026#34;package_component\u0026#34;: \u0026#34;havana\u0026#34;, \u0026#34;osops_networks\u0026#34;: { \u0026#34;management\u0026#34;: \u0026#34;10.200.9.0/24\u0026#34;, \u0026#34;public\u0026#34;: \u0026#34;10.200.10.0/24\u0026#34;, \u0026#34;swift\u0026#34;: \u0026#34;10.200.9.0/24\u0026#34; }, \u0026#34;keystone\u0026#34;: { \u0026#34;pki\u0026#34;: { \u0026#34;enabled\u0026#34;: false }, \u0026#34;admin_port\u0026#34;: \u0026#34;35357\u0026#34;, \u0026#34;admin_token\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;admin_user\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;tenants\u0026#34;: [ \u0026#34;admin\u0026#34;, \u0026#34;service\u0026#34; ], \u0026#34;users\u0026#34;: { \u0026#34;admin\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;secrete\u0026#34;, \u0026#34;roles\u0026#34;: { \u0026#34;admin\u0026#34;: [ \u0026#34;admin\u0026#34; ] } }, \u0026#34;demo\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;demo\u0026#34;, \u0026#34;default_tenant\u0026#34; : \u0026#34;service\u0026#34;, \u0026#34;roles\u0026#34;: { \u0026#34;admin\u0026#34;: [ \u0026#34;admin\u0026#34; ] } } }, \u0026#34;db\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;keystone\u0026#34; } }, \u0026#34;mysql\u0026#34;: { \u0026#34;root_network_acl\u0026#34;: \u0026#34;%\u0026#34;, \u0026#34;allow_remote_root\u0026#34;: true, \u0026#34;server_root_password\u0026#34;: \u0026#34;secrete\u0026#34;, \u0026#34;server_repl_password\u0026#34;: \u0026#34;secrete\u0026#34;, \u0026#34;server_debian_password\u0026#34;: \u0026#34;secrete\u0026#34; }, \u0026#34;monitoring\u0026#34;: { \u0026#34;procmon_provider\u0026#34;: \u0026#34;monit\u0026#34;, \u0026#34;metric_provider\u0026#34;: \u0026#34;collectd\u0026#34; }, \u0026#34;vips\u0026#34;: { \u0026#34;keystone-admin-api\u0026#34;: \u0026#34;10.200.9.11\u0026#34;, \u0026#34;keystone-service-api\u0026#34;: \u0026#34;10.200.9.11\u0026#34;, \u0026#34;keystone-internal-api\u0026#34;: \u0026#34;10.200.9.11\u0026#34;, \u0026#34;swift-proxy\u0026#34;: \u0026#34;10.200.9.11\u0026#34;, \u0026#34;config\u0026#34;: { \u0026#34;10.200.9.112\u0026#34;: { \u0026#34;vrid\u0026#34;: 12, \u0026#34;network\u0026#34;: \u0026#34;management\u0026#34; } } }, \u0026#34;developer_mode\u0026#34;: false, \u0026#34;swift\u0026#34;: { \u0026#34;swift_hash\u0026#34;: \u0026#34;307c0568ea84\u0026#34;, \u0026#34;authmode\u0026#34;: \u0026#34;keystone\u0026#34;, \u0026#34;authkey\u0026#34;: \u0026#34;20281b71-ce89-4b27-a2ad-ad873d3f2760\u0026#34; } } } 作成した environment ファイル environments/swift-havana.json を chef-server へアップ ロードする。\n% knife environment from file environments/swift-havana.json Cookbooks の修正 swift cookbooks を修正します。havana からは keystone を扱うクライアントは keystone.middleware.auth_token でなく keystoneclient.middleware.auth_token を 使うように変更掛かっていますので、下記のように修正しました。\n% cd cookbooks/swift/templates/default % diff -u proxy-server.conf.erb.org proxy-server.conf.erb --- proxy-server.conf.erb.org 2014-03-20 16:28:28.000000000 +0900 +++ proxy-server.conf.erb 2014-03-20 16:28:13.000000000 +0900 @@ -243,7 +243,8 @@ use = egg:swift#keystoneauth [filter:authtoken] -paste.filter_factory = keystone.middleware.auth_token:filter_factory +#paste.filter_factory = keystone.middleware.auth_token:filter_factory +paste.filter_factory = keystoneclient.middleware.auth_token:filter_factory auth_host = \u0026lt;%= @keystone_api_ipaddress %\u0026gt; auth_port = \u0026lt;%= @keystone_admin_port %\u0026gt; auth_protocol = \u0026lt;%= @keystone_admin_protocol %\u0026gt; % cd ../../../.. デプロイ かきのとおり knife bootstrap する。\n% knife bootstrap \u0026lt;manage_ip_addr\u0026gt; -N swift-manage -r \u0026#39;role[base]\u0026#39;,\u0026#39;role[mysql-master]\u0026#39;,\u0026#39;role[keystone]\u0026#39;,\u0026#39;role[swift-management-server]\u0026#39; -E swift-havana --sudo -x thirai % knife bootstrap \u0026lt;proxy01_ip_addr\u0026gt; -N swift-proxy01 -r \u0026#34;role[base]\u0026#34;,\u0026#34;role[swift-proxy-server]\u0026#34;,\u0026#39;role[swift-setup]\u0026#39;,\u0026#39;role[openstack-ha]\u0026#39; -E swift-havana --sudo -x thirai % knife bootstrap \u0026lt;proxy02_ip_addr\u0026gt; -N swift-proxy02 -r \u0026#34;role[base]\u0026#34;,\u0026#34;role[swift-proxy-server]\u0026#34;,\u0026#39;role[openstack-ha]\u0026#39; -E swift-havana --sudo -x thirai % knife bootstrap \u0026lt;storage01_ip_addr\u0026gt; -N swift-storage01 -r \u0026#39;role[base]\u0026#39;,\u0026#39;role[swift-object-server]\u0026#39; -E swift-havana --sudo -x thirai % knife bootstrap \u0026lt;storage02_ip_addr\u0026gt; -N swift-storage02 -r \u0026#39;role[base]\u0026#39;,\u0026#39;role[swift-object-server]\u0026#39; -E swift-havana --sudo -x thirai % knife bootstrap \u0026lt;storage03_ip_addr\u0026gt; -N swift-storage03 -r \u0026#39;role[base]\u0026#39;,\u0026#39;role[swift-object-server]\u0026#39; -E swift-havana --sudo -x thirai % knife bootstrap \u0026lt;account01_ip_addr\u0026gt; -N swift-account01 -r \u0026#39;role[base]\u0026#39;,\u0026#39;role[swift-account-server]\u0026#39;,\u0026#39;role[swift-container-server]\u0026#39; -E swift-havana --sudo -x thirai % knife bootstrap \u0026lt;account02_ip_addr\u0026gt; -N swift-account02 -r \u0026#39;role[base]\u0026#39;,\u0026#39;role[swift-account-server]\u0026#39;,\u0026#39;role[swift-container-server]\u0026#39; -E swift-havana --sudo -x thirai % knife bootstrap \u0026lt;account03_ip_addr\u0026gt; -N swift-account03 -r \u0026#39;role[base]\u0026#39;,\u0026#39;role[swift-account-server]\u0026#39;,\u0026#39;role[swift-container-server]\u0026#39; -E swift-havana --sudo -x thirai ここでバグ対策。Swift 1.10.0 にはバグがあるので下記の通り対処します。\nkeystone.middleware.s3_token に既知のバグがあり、下記のように対処します。この 状態ではバグにより swift-proxy が稼働してない状態ですが後の各ノードでの chef-client 実行時に稼働する予定です。\n% diff /usr/lib/python2.7/dist-packages/keystone/exception.py.org /usr/lib/python2.7/dist-packages/keystone/exception.py --- exception.py.org 2014-03-12 16:45:00.181420694 +0900 +++ exception.py 2014-03-12 16:44:47.173177081 +0900 @@ -18,6 +18,7 @@ from keystone.common import config from keystone.openstack.common import log as logging +from keystone.openstack.common.gettextutils import _ from keystone.openstack.common import strutils 上記のバグ報告は下記の URL にあります。\nhttps://bugs.launchpad.net/ubuntu/+source/swift/+bug/1231339\nzone 番号を付与します。\n% knife exec -E \u0026#34;nodes.find(:name =\u0026gt; \u0026#39;swift-storage01\u0026#39;) {|n| n.set[\u0026#39;swift\u0026#39;][\u0026#39;zone\u0026#39;] = \u0026#39;1\u0026#39;; n.save }\u0026#34; % knife exec -E \u0026#34;nodes.find(:name =\u0026gt; \u0026#39;swift-account01\u0026#39;) {|n| n.set[\u0026#39;swift\u0026#39;][\u0026#39;zone\u0026#39;] = \u0026#39;1\u0026#39;; n.save }\u0026#34; % knife exec -E \u0026#34;nodes.find(:name =\u0026gt; \u0026#39;swift-storage02\u0026#39;) {|n| n.set[\u0026#39;swift\u0026#39;][\u0026#39;zone\u0026#39;] = \u0026#39;2\u0026#39;; n.save }\u0026#34; % knife exec -E \u0026#34;nodes.find(:name =\u0026gt; \u0026#39;swift-account02\u0026#39;) {|n| n.set[\u0026#39;swift\u0026#39;][\u0026#39;zone\u0026#39;] = \u0026#39;2\u0026#39;; n.save }\u0026#34; % knife exec -E \u0026#34;nodes.find(:name =\u0026gt; \u0026#39;swift-storage03\u0026#39;) {|n| n.set[\u0026#39;swift\u0026#39;][\u0026#39;zone\u0026#39;] = \u0026#39;3\u0026#39;; n.save }\u0026#34; % knife exec -E \u0026#34;nodes.find(:name =\u0026gt; \u0026#39;swift-account03\u0026#39;) {|n| n.set[\u0026#39;swift\u0026#39;][\u0026#39;zone\u0026#39;] = \u0026#39;3\u0026#39;; n.save }\u0026#34; zone 番号が付与されたこと下記の通りを確認します\naccount-server の確認\n% knife exec -E \u0026#39;search(:node,\u0026#34;role:swift-account-server\u0026#34;) \\ { |n| z=n[:swift][:zone]||\u0026#34;not defined\u0026#34;; puts \u0026#34;#{n.name} has the role \\ [swift-account-server] and is in swift zone #{z}\u0026#34;; }\u0026#39; swift-account01 has the role [swift-account-server] and is in swift zone 1 swift-account02 has the role [swift-account-server] and is in swift zone 2 swift-account03 has the role [swift-account-server] and is in swift zone 3 container-server の確認\n% knife exec -E \u0026#39;search(:node,\u0026#34;role:swift-container-server\u0026#34;) \\ { |n| z=n[:swift][:zone]||\u0026#34;not defined\u0026#34;; puts \u0026#34;#{n.name} has the role \\ [swift-container-server] and is in swift zone #{z}\u0026#34;; }\u0026#39; swift-account01 has the role [swift-container-server] and is in swift zone 1 swift-account02 has the role [swift-container-server] and is in swift zone 2 swift-account03 has the role [swift-container-server] and is in swift zone 3 object-server の確認\n% knife exec -E \u0026#39;search(:node,\u0026#34;role:swift-object-server\u0026#34;) \\ { |n| z=n[:swift][:zone]||\u0026#34;not defined\u0026#34;; puts \u0026#34;#{n.name} has the role \\ [swift-object-server] and is in swift zone #{z}\u0026#34;; }\u0026#39; swift-storage01 has the role [swift-object-server] and is in swift zone 1 swift-storage02 has the role [swift-object-server] and is in swift zone 2 swift-storage03 has the role [swift-object-server] and is in swift zone 3 Chef が各々のノードに搭載された Disk を検知出来るか否かを確認する。\n% knife exec -E \\ \u0026#39;search(:node,\u0026#34;role:swift-object-server OR \\ role:swift-account-server \\ OR role:swift-container-server\u0026#34;) \\ { |n| puts \u0026#34;#{n.name}\u0026#34;; \\ begin; n[:swift][:state][:devs].each do |d| \\ puts \u0026#34;\\tdevice #{d[1][\u0026#34;device\u0026#34;]}\u0026#34;; \\ end; rescue; puts \\ \u0026#34;no candidate drives found\u0026#34;; end; }\u0026#39; swift-storage02 device sdb1 swift-storage03 device sdb1 swift-account01 device sdb1 swift-account02 device sdb1 swift-account03 device sdb1 swift-storage01 device sdb1 swift-manage ノードにて chef-client を実行し /etc/swift/ring-workspace/generate-rings.sh を更新します。\nswift-manage% sudo chef-client generate-rings.sh の \u0026rsquo;exit 0\u0026rsquo; 行をコメントアウトし実行します。\nswift-manage% sudo ${EDITOR} /etc/swift/ring-workspace/generage-rings.sh swift-manage% sudo /etc/swift/ring-workspace/generate-rings.sh この操作で /etc/swift/ring-workspace/rings 配下に account, container, object 用の Rings ファイル群が生成されたことを確認出来るはずです。これらを swift-manage 上で既に稼働している git サーバに push し管理します。\nswift-manage# cd /etc/swift/ring-workspace/rings swift-manage# git add account.builder container.builder object.builder swift-manage# git add account.ring.gz container.ring.gz object.ring.gz swift-manage# git commit -m \u0026#34;initial commit\u0026#34; swift-manage# git push 各々のノードにて chef-client を実行することで git サーバ上の Rings ファイル群 を取得し、swift プロセスを稼働させます。\nswift-proxy01# chef-client swift-proxy02# chef-client swift-storage01# chef-client swift-storage02# chef-client swift-storage03# chef-client swift-account01# chef-client swift-account02# chef-client swift-account03# chef-client 3台のノードが登録されたかどうかを下記の通り確認行います。\nswift-proxy01% sudo swift-recon --md5 [sudo] password for thirai: =============================================================================== --\u0026gt; Starting reconnaissance on 3 hosts =============================================================================== [2013-10-18 11:14:43] Checking ring md5sums 3/3 hosts matched, 0 error[s] while checking hosts. =============================================================================== 動作確認 構築が出来ました！ということで動作の確認をしてみましょう。\nテストコンテナ \u0026lsquo;container01\u0026rsquo; にテストファイル \u0026rsquo;test\u0026rsquo; をアップロードしてみる。\nswift-storage01% swift -V 2 -A http://\u0026lt;ip_addr_keystone\u0026gt;:5000/v2.0/ -U admin:admin -K secrete stat swift-storage01% swift -V 2 -A http://\u0026lt;ip_addr_keystone\u0026gt;:5000/v2.0/ -U admin:admin -K secrete post container01 swift-storage01% echo \u0026#34;test\u0026#34; \u0026gt; test swift-storage01% swift -V 2 -A http://\u0026lt;ip_addr_keystone\u0026gt;:5000/v2.0/ -U admin:admin -K secrete upload container01 test swift-storage01% swift -V 2 -A http://\u0026lt;ip_addr_keystone\u0026gt;:5000/v2.0/ -U admin:admin -K secrete list container01 swift-storage01% swift -V 2 -A http://\u0026lt;ip_addr_keystone\u0026gt;:5000/v2.0/ -U admin:admin -K secrete list container01 test まとめ 前回「実用的な Swift 構成を Chef でデプロイ」の記事で記した内容とほぼ手順は変 わりませんでした。rcbops-cookbooks/rcbops-utils 内にソフトウェアの取得先レポジ トリを記すレシピが下記の場所にあります。\nhttps://github.com/rcbops-cookbooks/osops-utils/blob/master/recipes/packages.rb\nそして havana ブランチの attributes を確認すると Ubuntu Cloud Archive の URL が記されていることが確認出来ます。下記のファイルです。\nhttps://github.com/rcbops-cookbooks/osops-utils/blob/havana/attributes/repos.rb\nファイルの中身の抜粋です。\n\u0026#34;havana\u0026#34; =\u0026gt; { \u0026#34;uri\u0026#34; =\u0026gt; \u0026#34;http://ubuntu-cloud.archive.canonical.com/ubuntu\u0026#34;, \u0026#34;distribution\u0026#34; =\u0026gt; \u0026#34;precise-updates/havana\u0026#34;, \u0026#34;components\u0026#34; =\u0026gt; [\u0026#34;main\u0026#34;], \u0026#34;keyserver\u0026#34; =\u0026gt; \u0026#34;hkp://keyserver.ubuntu.com:80\u0026#34;, \u0026#34;key\u0026#34; =\u0026gt; \u0026#34;EC4926EA\u0026#34; }, これらのことより、rcbops-utils の attibutes で havana (実際には \u0026lsquo;havana-proposed\u0026rsquo;) をレポジトリ指定するように Cookbooks 構成を管理してあげれば Havana 構成の Keystone, Swift が構築出来ることになります。ちなみに havana-proposed で Swift や Keystone のどのバージョンがインストールされるかは、 下記の Packages ファイルを確認すると判断出来ます。\nhttp://ubuntu-cloud.archive.canonical.com/ubuntu/dists/precise-proposed/havana/main/binary-amd64/Packages\n以上です。\n","permalink":"https://jedipunkz.github.io/post/2014/03/16/rcbops-chef-cookbooks-keystone-havana-swift-1-10-0.deploy/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e です。\u003c/p\u003e\n\u003ch4 id=\"追記\"\u003e追記\u003c/h4\u003e\n\u003cp\u003e2014/03/20 : 一旦削除していた記事なのですが、無事動作が確認出来たので再度アッ\nプします！\u003c/p\u003e\n\u003cp\u003e第17回 OpenStack 勉強会で rcbops/chef-cookbooks の話をしてきたのですが会場から\n質問がありました。「Havana の Swift 構成を作る Cookbooks はどこにありますか？」\nと。私が試したのが Grizzly 時代のモノで、よく rcbops/chef-cookbooks を見てみる\nと Havana ブランチ又は Havana に対応したリリースタグのファイル構成に Swift が\n綺麗サッパリ消えているではありませんか・・！下記の Swift の Cookbooks は幸い\ngithub 上に残っていました。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/rcbops-cookbooks/swift\"\u003ehttps://github.com/rcbops-cookbooks/swift\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eが rcbops/chef-cookbooks との関連付けが切れています・・。ぐあぁ。\u003c/p\u003e\n\u003cp\u003eということで Havana 構成の Keystone 2013.2.2 と Swift 1.10.0 の構成を Chef で\n作らねば！と思い色々試していたら結構あっさりと出来ました。今回はその方法を書い\nていきたいと思います！\u003c/p\u003e\n\u003ch2 id=\"構成\"\u003e構成\u003c/h2\u003e\n\u003cp\u003e構成は\u0026hellip;以前の記事 \u003ca href=\"http://jedipunkz.github.io/blog/2013/10/27/swift-chef/\"\u003ehttp://jedipunkz.github.io/blog/2013/10/27/swift-chef/\u003c/a\u003e と同じです。\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e+-----------------+\n|  load balancer  |\n+-----------------+\n|\n+-------------------+-------------------+-------------------+-------------------+---------------------- proxy network\n|                   |                   |                   |                   |                   \n+-----------------+ +-----------------+ +-----------------+ +-----------------+ +-----------------+\n|   chef server   | | chef workstation| |   swift-mange   | |  swift-proxy01  | |  swift-proxy02  | \n+-----------------+ +-----------------+ +-----------------+ +-----------------+ +-----------------+ ...\u0026gt; scaling\n|                   |                   |                   |                   |                   \n+-------------------+-------------------+-------------------+-------------------+-------------------+-- storage network\n|                   |                   |                   |                   |                   |\n+-----------------+ +-----------------+ +-----------------+ +-----------------+ +-----------------+ +-----------------+ \n| swift-storage01 | | swift-storage02 | | swift-storage03 | | swift-account01 | | swift-account02 | | swift-account03 |\n+-----------------+ +-----------------+ +-----------------+ +-----------------+ +-----------------+ +-----------------+ ..\u0026gt; scaling\n\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"手順\"\u003e手順\u003c/h2\u003e\n\u003cp\u003eでは早速手順を記していきますね。毎回なのですが Chef ワークステーション・Chef\nサーバの環境構築については割愛します。オムニバスインストーラを使えば Chef サー\nバの構築は簡単ですし、ワークステーションの構築も Ruby インストール -\u0026gt; gem で\nChef をインストール -\u0026gt; .chef 配下を整える、で出来ます。\u003c/p\u003e","title":"rcbops/chef-cookbooks で Keystone 2013.2.2(Havana) + Swift 1.10.0 を構築"},{"content":"こんにちは。@jedipunkz です。\n今まで監視システムの Sensu やクラウドプラットフォームの OpenStack、コンフィギュ レーションマネージメントツールの Chef やクラウドライブラリの Fog 等使ってきま したが、これらを組み合わせるとオレオレオートスケーラ作れるんじゃないか？と思い、 ちょろっと作ってみました。\nちなみに自分はインフラエンジニアでしかも運用の出身なので Ruby に関しては初心者 レベルです。Chef で扱っているのと Rails アプリを作った経験はありますが、その程 度。Fog というクラウドライブラリにコントリビュートしたことはアリますが..。ちな みに Fog のコントリビュート内容は OpenStack Neutron(当時 Quantum) の仮想ルータ の操作を行う実装です。\nそんな自分ですが\u0026hellip;設計1周間・実装1周間でマネージャと CLI が出来ました。 また暫く放置していたマネージャと CLI に WebUI くっつけようかなぁ？と思って sinatra の学習を始めたのですが、学習を初めて 1.5 日で WebUI が動くところまで行 きました。何故か？Ruby には有用な技術が揃っているから・・！(´；ω；｀)ﾌﾞﾜｯ\nオレオレオートスケーラ \u0026lsquo;sclman\u0026rsquo; の置き場所 https://github.com/jedipunkz/sclman\nスクリーンショット +++\n構成は？ +-------------- public network +-------------+ | |sclman-api.rb| +----+----+---+ | sclman.rb | | vm | vm |.. | |sclman-cli.rb| +-------------+ +-------------+ +-------------+ +-------------+ | openstack | | chef server | | sensu server| | workstation | +-------------+ +-------------+ +-------------+ +-------------+ | | | | +---------------+---------------+---------------+--------------- management network \u0026lsquo;sclman\u0026rsquo; っていう名前です。上図の workstation ノードで稼働します。処理の流れは\n処理の流れ sclman-cli.rb もしくは WebUI から HTTP クラスタのセットを OpenStack 上に生成 生成された VM に対して Chef で nginx をインストール Chef の Roles である \u0026lsquo;LB\u0026rsquo; と \u0026lsquo;Web\u0026quot; が同一 Envrionment になるようにブートストラップ LB VM のバックエンドに Web VM が指し示される bootstrap と同時に sensu-client をインストール Web VM の load を sensu で監視 sclman.rb (マネージャ) は Sensu AP を定期的に叩いて Web VM の load を監視 load が高い environment があれが該当 environment の HTTP クラスタに VM を追加 LB VM は追加された VM へのリクエストを追加 引き続き監視し一定期間 load が上がらなけれ Web VM を削除 LB VM は削除された VM へのリクエストを削除 といった感じです。要約すると CLI/WebUI から HA クラスタを作成。その時に LB, Web ミドルウェアと同時に sensu クライアントを VM に放り込む。監視を継続して負 荷が上昇すれば Web インスタンスの数を増加させ LB のリクエスト振り先にもその追 加した VM のアドレスを追加。逆に負荷が下がれば VM 削除と共にリクエスト振り先も 削除。この間、人の手を介さずに処理してくれる。です。\n使い方 詳細な使い方は github の README.md を見て下さい。ここには簡単な説明だけ書いて おきます。\nsclman を github から取得して bundler で必要な gems をインストールします。 chef-repo に移動して Berkshelf で必要な cookbooks をインストールします。 cookbooks を用いて sensu をデプロイします。 Omnibus インストーラーを使って chef サーバをインストールします。 OpenStack をインストールします。 sclman.conf を環境に合わせて修正します。 sclman.rb (マネージャ) を稼働します。 sclman-api.rb (WebUI/API) を稼働します。 sclman-cli.rb (CLI) もしくは WebUI から最小構成の HTTP クラスタを稼働します。 この状態で \u0026lsquo;Web\u0026rsquo; Role のインスタンスに負荷が掛かると \u0026lsquo;Web\u0026rsquo; Role のインスタンスの数が増加します。 また逆に負荷が下がるとインスタンス数が減ります。 負荷の増減のシビアさは sclman.conf のパラメータ \u0026lsquo;man_sensitivity\u0026rsquo; で決定します。 値が長ければ長いほど増減のし易さは低下します。\nまとめ +++\nこんな僕でも Ruby の周辺技術使ってなんとなくの形が出来ましたー。ただまだまだ課 題はあって、インフラを制御するアプリってエラーハンドリングが難しいっていうこと です。帰ってくるエラーの一覧などがクラウドプラットフォーム・クラウドライブラリ のドキュメントにあればいいのだけど、まだそこまで行ってない。Fog もまだまだ絶賛 開発中と言うかクラウドプラットフォームの進化に必死で追いついている状態なので、 僕らがアプリを作るときには自分でエラーを全部洗い出す等の作業が必要になるかもし れない。大変だけど面白い所でもありますよね！これからも楽しみです。\n","permalink":"https://jedipunkz.github.io/post/2014/03/05/sensu-chef-openstack-fog-autoscaler/","summary":"\u003cp\u003eこんにちは。@jedipunkz です。\u003c/p\u003e\n\u003cp\u003e今まで監視システムの Sensu やクラウドプラットフォームの OpenStack、コンフィギュ\nレーションマネージメントツールの Chef やクラウドライブラリの Fog 等使ってきま\nしたが、これらを組み合わせるとオレオレオートスケーラ作れるんじゃないか？と思い、\nちょろっと作ってみました。\u003c/p\u003e\n\u003cp\u003eちなみに自分はインフラエンジニアでしかも運用の出身なので Ruby に関しては初心者\nレベルです。Chef で扱っているのと Rails アプリを作った経験はありますが、その程\n度。Fog というクラウドライブラリにコントリビュートしたことはアリますが..。ちな\nみに Fog のコントリビュート内容は OpenStack Neutron(当時 Quantum) の仮想ルータ\nの操作を行う実装です。\u003c/p\u003e\n\u003cp\u003eそんな自分ですが\u0026hellip;設計1周間・実装1周間でマネージャと CLI が出来ました。\nまた暫く放置していたマネージャと CLI に WebUI くっつけようかなぁ？と思って\nsinatra の学習を始めたのですが、学習を初めて 1.5 日で WebUI が動くところまで行\nきました。何故か？Ruby には有用な技術が揃っているから・・！(´；ω；｀)ﾌﾞﾜｯ\u003c/p\u003e\n\u003ch2 id=\"オレオレオートスケーラ-sclman-の置き場所\"\u003eオレオレオートスケーラ \u0026lsquo;sclman\u0026rsquo; の置き場所\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/jedipunkz/sclman\"\u003ehttps://github.com/jedipunkz/sclman\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eスクリーンショット\n+++\u003c/p\u003e\n\u003cimg src=\"https://raw.github.com/jedipunkz/sclman/master/pix/sclman.png\" width=\"600\"\u003e\n\u003ch2 id=\"構成は\"\u003e構成は？\u003c/h2\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e+-------------- public network                  +-------------+\n|                                               |sclman-api.rb|\n+----+----+---+                                 |  sclman.rb  |\n| vm | vm |.. |                                 |sclman-cli.rb|\n+-------------+ +-------------+ +-------------+ +-------------+\n|  openstack  | | chef server | | sensu server| | workstation |\n+-------------+ +-------------+ +-------------+ +-------------+\n|               |               |               |\n+---------------+---------------+---------------+--------------- management network\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u0026lsquo;sclman\u0026rsquo; っていう名前です。上図の workstation ノードで稼働します。処理の流れは\u003c/p\u003e","title":"Sensu,Chef,OpenStack,Fog を使ったオレオレオートスケーラを作ってみた！"},{"content":"こんにちは、@jedipunkz です。\n前回、\u0026lsquo;Ceph のプロセス配置ベストプラクティス\u0026rsquo; というタイトルで記事を書きました。\nhttp://jedipunkz.github.io/blog/2014/01/29/ceph-process-best-practice/\n今回はこの記事にあるポリシに従って下記のような特徴を持った構成を作る手順を具体 的に書いていきたいと思います。\nceph01 - ceph04 の4台構成 ノードに HDD 2台搭載されていることを前提 (/dev/sdb, /dev/sdc) ノードに Journal 用 SSD 1台搭載されていることを前提 (/dev/ssd) ceph04 は mds サービス稼働 ceph05 は ceph-deploy を実行するためのワークステーション 最終的に ceph04 から Ceph をマウントする mon は ノード単位で稼働 osd は HDD 単位で稼働 mds は ceph04 に稼働 構成 : ハードウェアとノードとネットワークの関係 public network +-------------------+-------------------+-------------------+-------------------+--------- | | | | | +--------+--------+ +--------+--------+ +--------+--------+ +--------+--------+ +--------+--------+ | ceph01 | | ceph02 | | ceph03 | | ceph04 | | ceph05 | | +-----+ +-----+ | | +-----+ +-----+ | | +-----+ +-----+ | | | | | | | sdb | | sdc | | | | sdb | | sdc | | | | sdb | | sdc | | | | | | | +-----+-+-----+ | | +-----+-+-----+ | | +-----+-+-----+ | | | | | | | ssd | | | | ssd | | | | ssd | | | | | | | +-------------+ | | +-------------+ | | +-------------+ | | | | | +--------+--------+ +--------+--------+ +--------+--------+ +-----------------+ +-----------------+ | | | cluster network +-------------------+-------------------+------------------------------------------------- 構成 : プロセスとノードとネットワークの関係 public network +-------------------+-------------------+-------------------+-------------------+--------- | | | | | +--------+--------+ +--------+--------+ +--------+--------+ +--------+--------+ +--------+--------+ | ceph01 | | ceph02 | | ceph03 | | ceph04 | | ceph05 | | +-----+ +-----+ | | +-----+ +-----+ | | +-----+ +-----+ | | +-------------+ | | | | | osd | | osd | | | | osd | | osd | | | | osd | | osd | | | | mds | | | | | +-----+-+-----+ | | +-----+-+-----+ | | +-----+-+-----+ | | +-------------+ | | | | | mon | | | | mon | | | | mon | | | | | | | +-------------+ | | +-------------+ | | +-------------+ | | | | | +--------+--------+ +--------+--------+ +--------+--------+ +-----------------+ +-----------------+ | | | cluster network +-------------------+-------------------+------------------------------------------------- 注意 : 上記の図だと ssd : mon が対に見えますが、そうではありません。\nでは構築方法について書いていきます。\n作業用ホストの準備 ノンパスフレーズの SSH 公開鍵・秘密鍵を生成する。\n% ssh-keygen 公開鍵をターゲットホスト (ceph01-03) に配置\n% ssh-copy-id ceph@ceph01 % ssh-copy-id ceph@ceph02 % ssh-copy-id ceph@ceph03 ceph-deploy の取得を行う。\n% git clone https://github.com/ceph/ceph-deploy.git ~/ceph-deploy \u0026lsquo;python-virtualenv\u0026rsquo; パッケージをインストールする。\n% sudo apt-get update ; sudo apt-get -y install python-virtualenv ceph-deploy をブートストラップする\n% cd ~/ceph-deploy % ./bootstrap PATH を通す。下記は例。\n% ${EDITOR} ~/.zshrc export PATH=$HOME/ceph-deploy:$PATH ホスト名の解決を行う。IP アドレスは例。\n% sudo ${EDITOR} /etc/hosts 10.200.10.11 ceph01 10.200.10.12 ceph02 10.200.10.13 ceph03 10.200.10.14 ceph04 10.200.10.15 ceph05 上記の構成の構築方法 以前の記事同様に ceph-deploy をデプロイツールとして用いる。\nceph-deploy に関しては下記の URL を参照のこと。\nhttps://github.com/ceph/ceph-deploy\n下記の手順でコンフィギュレーションと鍵の生成を行う。またこれからの操作はすべて public network 上の ceph-deploy 専用 node からの操作とする。\n% mkdir ~/ceph-work % cd ~/ceph-work % ceph-deploy --cluster cluster01 new ceph01 ceph02 ceph03 ceph04 ~/ceph-work ディレクトリ上に cluster01.conf が生成されているので下記の通り cluster network を扱う形へと追記を行う。\npublic network = \u0026lt;public_network_addr\u0026gt;/\u0026lt;netmask\u0026gt; cluster network = \u0026lt;cluster_network_addr\u0026gt;/\u0026lt;netmask\u0026gt; [mon.a] host = ceph01 mon addr = \u0026lt;ceph01_ip_addr\u0026gt;:6789 [mon.b] host = ceph02 mon addr = \u0026lt;ceph02_ip_addr\u0026gt;:6789 [mon.c] host = ceph03 mon addr = \u0026lt;ceph03_ip_addr\u0026gt;:6789 [osd.0] public addr = \u0026lt;ceph01_public_ip_addr\u0026gt; cluster addr = \u0026lt;ceph01_cluster_ip_addr\u0026gt; [osd.1] public addr = \u0026lt;ceph01_public_ip_addr\u0026gt; cluster addr = \u0026lt;ceph01_cluster_ip_addr\u0026gt; [osd.2] public addr = \u0026lt;ceph01_public_ip_addr\u0026gt; cluster addr = \u0026lt;ceph01_cluster_ip_addr\u0026gt; [mds.a] host = ceph04 ceph の各 nodes へのインストールを行う。ceph はワークステーションである ceph05 にも インストールしておきます。後に Ceph ストレージをマウントするためです。\n% ceph-deploy --cluster cluster01 install ceph01 ceph02 ceph03 ceph04 ceph04 mon プロセスを各 nodes で稼働する。\n% ceph-deploy --cluster cluster01 mon create ceph01 ceph02 ceph03 鍵の配布を各 nodes に行う。\n% ceph-deploy --cluster cluster01 gatherkeys ceph01 ceph02 ceph03 ceph04 ceph05 disk のリストを確認。\n各 node 毎に用いることが可能は disk の一覧を確認する。\n% ceph-deploy --cluster cluster01 disk list ceph01 % ceph-deploy --cluster cluster01 disk list ceph02 % ceph-deploy --cluster cluster01 disk list ceph03 disk の初期化を行う。この作業を行うと指定ディスク上のデータは消去される。\n% ceph-deploy --cluster cluster01 disk zap ceph01:/dev/sdb ceph01:/dev/sdc % ceph-deploy --cluster cluster01 disk zap ceph02:/dev/sdb ceph02:/dev/sdc % ceph-deploy --cluster cluster01 disk zap ceph03:/dev/sdb ceph03:/dev/sdc journal 用の ssd のパーティションを切る。ここでは 10GB 毎に切った /dev/ssd1, /dev/ssd2 が存在することを前提に記す。ceph と同時にインストールされた gdisk を用いる。\n% sudo gdisk /dev/ssd (注意) 下記の公式ドキュメントでは osd prepare, osc activate の手順が掲載されて いるがその後の osd create のコマンドにて prepare が実行されるようでこれら2つの 手順を行うと正常に osd create コマンドが実行できなかった。よってこのタイミング にて osd create を行うものとする。\nhttp://ceph.com/docs/master/rados/deployment/ceph-deploy-osd/#prepare-osds http://ceph.com/docs/dumpling/start/quick-ceph-deploy/ 2 つの disk に対してそれぞれ osd を稼働させる。\n% ceph-deploy --cluster cluster01 osd create ceph01:sdb:/dev/ssd1 ceph02:sdb:/dev/ssd1 ceph03:sdb:/dev/ssd1 % ceph-deploy --cluster cluster01 osd create ceph01:sdc:/dev/ssd2 ceph02:sdc:/dev/ssd2 ceph03:sdc:/dev/ssd2 mds の稼働を行う。ここでは1号機にのみ稼働を行う。\n% ceph-deploy --cluster cluster01 mds create ceph04 クライアントからのマウント方法各種 上記で構築した Ceph ストレージを利用する方法を3つ説明する。先に述べたように POSIX 互換 filesystem として利用が可能。それぞれ mds が稼働しているホストに対 して接続を行う。\nBlock Device としてマウントする方法 ストレージ上に block device を生成しそれをマウントする\ncephclient% rbd create foo --size 4096 cephclient% sudo modprobe rbd cephclient% sudo rbd map foo --pool rbd --name client.admin cephclient% sudo mkfs.ext4 -m0 /dev/rbd/rbd/foo cephclient% sudo mkdir /mnt/myrbd cephclinet% sudo mount /dev/rbd/rbd/foo /mnt/myrbd Kernel Driver を用いてマウントする方法 kernel Driver を用いてストレージをマウントする\ncephclient% sudo mkdir /mnt/mycephfs cephclient% sudo mount -t ceph 10.200.10.26:6789:/ /mnt/mycephfs -o \\ name=admin,secret=`sudo ceph-authtool -p /etc/ceph/cluster01.client.admin.keyring` Fuse Driver (ユーザランド) を用いてマウントする方法 ユーザランドソフトウェア FUSE を用いてマウントする方法\ncephclient% sudo mkdir /home/\u0026lt;username\u0026gt;/cephfs cephclient% sudo ceph-fuse -m 10.200.10.26:6789 /home/\u0026lt;username\u0026gt;/cephfs ","permalink":"https://jedipunkz.github.io/post/2014/02/27/journal-ssd-ceph-deploy/","summary":"\u003cp\u003eこんにちは、@jedipunkz です。\u003c/p\u003e\n\u003cp\u003e前回、\u0026lsquo;Ceph のプロセス配置ベストプラクティス\u0026rsquo; というタイトルで記事を書きました。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://jedipunkz.github.io/blog/2014/01/29/ceph-process-best-practice/\"\u003ehttp://jedipunkz.github.io/blog/2014/01/29/ceph-process-best-practice/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e今回はこの記事にあるポリシに従って下記のような特徴を持った構成を作る手順を具体\n的に書いていきたいと思います。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eceph01 - ceph04 の4台構成\u003c/li\u003e\n\u003cli\u003eノードに HDD 2台搭載されていることを前提 (/dev/sdb, /dev/sdc)\u003c/li\u003e\n\u003cli\u003eノードに Journal 用 SSD 1台搭載されていることを前提 (/dev/ssd)\u003c/li\u003e\n\u003cli\u003eceph04 は mds サービス稼働\u003c/li\u003e\n\u003cli\u003eceph05 は ceph-deploy を実行するためのワークステーション\u003c/li\u003e\n\u003cli\u003e最終的に ceph04 から Ceph をマウントする\u003c/li\u003e\n\u003cli\u003emon は ノード単位で稼働\u003c/li\u003e\n\u003cli\u003eosd は HDD 単位で稼働\u003c/li\u003e\n\u003cli\u003emds は ceph04 に稼働\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"構成--ハードウェアとノードとネットワークの関係\"\u003e構成 : ハードウェアとノードとネットワークの関係\u003c/h2\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e                                                                                      public network\n         +-------------------+-------------------+-------------------+-------------------+---------\n         |                   |                   |                   |                   |\n+--------+--------+ +--------+--------+ +--------+--------+ +--------+--------+ +--------+--------+\n|      ceph01     | |      ceph02     | |      ceph03     | |      ceph04     | |      ceph05     |\n| +-----+ +-----+ | | +-----+ +-----+ | | +-----+ +-----+ | |                 | |                 |\n| | sdb | | sdc | | | | sdb | | sdc | | | | sdb | | sdc | | |                 | |                 |\n| +-----+-+-----+ | | +-----+-+-----+ | | +-----+-+-----+ | |                 | |                 |\n| |     ssd     | | | |     ssd     | | | |     ssd     | | |                 | |                 |\n| +-------------+ | | +-------------+ | | +-------------+ | |                 | |                 |\n+--------+--------+ +--------+--------+ +--------+--------+ +-----------------+ +-----------------+\n         |                   |                   |                                    cluster network\n         +-------------------+-------------------+-------------------------------------------------\n\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"構成--プロセスとノードとネットワークの関係\"\u003e構成 : プロセスとノードとネットワークの関係\u003c/h2\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e                                                                                      public network\n         +-------------------+-------------------+-------------------+-------------------+---------\n         |                   |                   |                   |                   |\n+--------+--------+ +--------+--------+ +--------+--------+ +--------+--------+ +--------+--------+\n|      ceph01     | |      ceph02     | |      ceph03     | |      ceph04     | |      ceph05     |\n| +-----+ +-----+ | | +-----+ +-----+ | | +-----+ +-----+ | | +-------------+ | |                 |\n| | osd | | osd | | | | osd | | osd | | | | osd | | osd | | | |     mds     | | |                 |\n| +-----+-+-----+ | | +-----+-+-----+ | | +-----+-+-----+ | | +-------------+ | |                 |\n| |     mon     | | | |     mon     | | | |     mon     | | |                 | |                 |\n| +-------------+ | | +-------------+ | | +-------------+ | |                 | |                 |\n+--------+--------+ +--------+--------+ +--------+--------+ +-----------------+ +-----------------+\n         |                   |                   |                                    cluster network\n         +-------------------+-------------------+-------------------------------------------------\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e注意 : 上記の図だと ssd : mon が対に見えますが、そうではありません。\u003c/p\u003e","title":"Journal 用 SSD を用いた Ceph 構成の構築"},{"content":"Ceph はブロック型の分散ストレージファイルシステムです。POSIX のファイルシステ ムとしてマウント出来ます。Linux の Kernel ドライバや FUSE ドライバを用いてマウ ントします。またブロックデバイスとしてマウントする方法もあります。\nだいぶ前ですが、Ceph に関する記事を以前下記の通り書きました。\nhttp://jedipunkz.github.io/blog/2013/05/25/ceph-cluster-network/ http://jedipunkz.github.io/blog/2013/05/11/ceph-deploy/ Ceph の構築方法について記したブログだったのですが、今まで mon, osd, mds の各プ ロセスをそれぞれ何台のノードに対して配置し、またそれぞれのプロセス幾つを何に対 して配置するのか？という疑問が付きまとわっていました。node, disk, process のそ れぞれの数の関係について知りたいなぁと思っていました。幾つかのドキュメントを読 んでいて、ぼんやり見えてきた事があるのでそれを今回はまとめたいと思います。\nまた、皆さん気になるトコロだと思われる容量設計についても軽く触れたいと思います。\n参考資料 http://ceph.com/docs/master/rados/configuration/mon-config-ref/ http://www.sebastien-han.fr/blog/2013/12/02/ceph-performance-interesting-things-going-on/ 各要素の数の関係 ハードウェア要素である node, disk(hdd), ssd そしてソフトウェア要素である mon, osd, mds の数の関係はどのようにするべきか？基本となる関係は\n1 mds process / node 1 mon process / node 1 osd process / disk n jornal ssd device / disk / node だと考えられます。僕が今のところ理想かなぁと思っている構成をまとめたいと思いま す。\n下記の図がそれです。\n+------------------------+ | client | +------------------------+ | +--------------------------+--------------------------+-------------------------------+------------------------- | | | | public network +------------------------+ +------------------------+ +------------------------+ +------------------------+ | mon | | mon | | mon | | mds | +------+ +------+ +------+ +------+ +------+ +------+ +------+ +------+ +------+ +------------------------+ | osd | | osd | | osd | | osd | | osd | | osd | | osd | | osd | | osd | | | +------+ +------+ +------+ +------+ +------+ +------+ +------+ +------+ +------+ | | | disk | | disk | | disk | | disk | | disk | | disk | | disk | | disk | | disk |....\u0026gt; | | +------+ +------+ +------+ +------+ +------+ +------+ +------+ +------+ +------+scale | node | | ssd | | ssd | | ssd | | | +------------------------+ +------------------------+ +------------------------+ | | | node | | node | | node | | | +------------------------+ +------------------------+ +------------------------+ +------------------------+ | | | | +--------------------------+--------------------------+-------------------------------+------------------------- cluster network mds と node の関係 mds はリモートクライアントへのファイルシステムサービスの提供を行うことや特性が 全く異なることから別ノードに切り出しました。また mds は幾つかのノードで稼働さ せる事も可能。が、mds はそれぞれのサービスを HA 組む仕組みは持っていないので どれか一方の mds をクライアントは指し示す必要があり、その mds が停止すれば直 ちに障害に発展します。\nmon と node の関係 mon は比較的少量のリソースで稼働します。今回 osd と同じノードの搭載しましたが 別ノードに切り出すことも勿論可能です。mon は CRUSH Maps アルゴリズムの元に連携 が取れますので複数のプロセスを稼働することが推奨されていることからも、比較的少 ないノード数のクラスタの場合は osd と同ノードに搭載するのが容易かなと考えまし た。\nosd と node の関係 1 osd プロセスに対して 1 disk が基本となります。osd は実データのレプリケーショ ンを行うことからコンフィギュレーションに対して上図の様にクラスタ用のネットワー クを紐付け、高トラヒックに対応する必要があります。また osd 用の disk device で すが RAID を組まないことが推奨されています。CEPH 自体に HA の仕組みがあること、 また RAID 構成にもよりますがディスクアクセスが遅くなることは Ceph にとってボト ルネックを早く招くことになりますし、小さいディスク容量しか扱えなくなることは Ceph にとって不利になると思います。\njournal 用の ssd device と disk, node の関係 現在の Stable Release 版の Ceph は journal を用いてメタデータを管理します。各 osd の disk 単位に journal 用の disk device を指定出来るようになっています。メ タデータですので実データ用の disk よりだいぶ小さな容量で構わないこと、また比較 的高速なアクセスを要求されることからも SSD を選択することが推奨されつつあるよ うです。実際にストアされるデータの特性にもよりますが 1 node に対して 1 ssd device を配置すれば十分かと思います。また osd のプロセスの数 (disk の数) に対 して一つのパーティションを切ることで対応出来るかと思います。\n設定方法の例を記します。ここに ceph01-03 の3台のノードがありそれぞれ 2 disk, 1 ssd が搭載されているとします。/dev/ssd は gdisk 等を用いて2つ以上のパーティショ ンに切り分けます。\n下記のように /dev/sdb (hdd) に対して /dev/ssd1 (ssd), /dev/sdc (hdd) に対して /dev/ssd2 (ssd) を割り当てることが出来ます。\n% ceph-deploy --cluster cluster01 osd create ceph01:sdb:/dev/ssd1 ceph02:sdb:/dev/ssd1 ceph03:sdb:/dev/ssd1 % ceph-deploy --cluster cluster01 osd create ceph01:sdc:/dev/ssd2 ceph02:sdc:/dev/ssd2 ceph03:sdc:/dev/ssd2 Ceph ストレージ全体の容量設計と mon の ratio の関係 3TB のディスクを持ったノードが 33 台並んでいるとします。各ノードには osd プロ セスが1つ稼働します。合計容量は 99 TB となりますが mon が持っているコンフィギュ レーションである full ratio というパラメータがありデフォルト値が 0.95 となって います。よってこのクラスタで扱える全体のディスク容量は 95TB となります。\nまた、ラックに数台のノードを積むのが通常ですが、電源故障等で一気にラック単位で 障害が発生したとします。この場合 Ceph はすべてのデータに関してレプリカを取り復 旧作業を行います。しかしながら停止するノード数によってはストレージ全体の扱える 容量をオーバーすることも懸念されます。これに対応するために先ほど登場した ratio パラメータを調整することが出来ます。\n[global] mon osd full ratio = .80 mon osd nearfull ratio = .70 上記の例では full ステートの ratio が 0.80, nearfull ステートの ratio が 0.70 となります。想定の障害ノード数を考慮し ratio パラメータにてその台数分を減算す れば良いわけです。\nまとめ 前述した通り上図は比較的少ないノード数のクラスタを組む場合を想定しています。ノー ド数が増える場合は mon は mds, osd とも必要とするリソースの特性が異なりますの で別ノードに切り出すことも考えたほうが良さそうです。2014年の2月には Firefly と いう新しいリリース版が出ます。ここでのブループリント(設計書)を確認すると\u0026hellip;\nhttp://wiki.ceph.com/Planning/Blueprints/Firefly/osd%3A_new_key%2F%2Fvalue_backend\njournal に変わる新たなメタデータ管理方法として KVS データベースを扱うことも視 野に入っているようです。上記の URL 見る限りでは Facebook がオープンソースにし た rocksdb や fusionio の nvmkv, seagate の kinetic 等が挙がっています。2月に 期待しましょう！\n","permalink":"https://jedipunkz.github.io/post/2014/01/29/ceph-process-best-practice/","summary":"\u003cp\u003eCeph はブロック型の分散ストレージファイルシステムです。POSIX のファイルシステ\nムとしてマウント出来ます。Linux の Kernel ドライバや FUSE ドライバを用いてマウ\nントします。またブロックデバイスとしてマウントする方法もあります。\u003c/p\u003e\n\u003cp\u003eだいぶ前ですが、Ceph に関する記事を以前下記の通り書きました。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"http://jedipunkz.github.io/blog/2013/05/25/ceph-cluster-network/\"\u003ehttp://jedipunkz.github.io/blog/2013/05/25/ceph-cluster-network/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://jedipunkz.github.io/blog/2013/05/11/ceph-deploy/\"\u003ehttp://jedipunkz.github.io/blog/2013/05/11/ceph-deploy/\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eCeph の構築方法について記したブログだったのですが、今まで mon, osd, mds の各プ\nロセスをそれぞれ何台のノードに対して配置し、またそれぞれのプロセス幾つを何に対\nして配置するのか？という疑問が付きまとわっていました。node, disk, process のそ\nれぞれの数の関係について知りたいなぁと思っていました。幾つかのドキュメントを読\nんでいて、ぼんやり見えてきた事があるのでそれを今回はまとめたいと思います。\u003c/p\u003e\n\u003cp\u003eまた、皆さん気になるトコロだと思われる容量設計についても軽く触れたいと思います。\u003c/p\u003e\n\u003ch2 id=\"参考資料\"\u003e参考資料\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"http://ceph.com/docs/master/rados/configuration/mon-config-ref/\"\u003ehttp://ceph.com/docs/master/rados/configuration/mon-config-ref/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://www.sebastien-han.fr/blog/2013/12/02/ceph-performance-interesting-things-going-on/\"\u003ehttp://www.sebastien-han.fr/blog/2013/12/02/ceph-performance-interesting-things-going-on/\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"各要素の数の関係\"\u003e各要素の数の関係\u003c/h2\u003e\n\u003cp\u003eハードウェア要素である node, disk(hdd), ssd そしてソフトウェア要素である mon,\nosd, mds の数の関係はどのようにするべきか？基本となる関係は\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e1 mds process / node\u003c/li\u003e\n\u003cli\u003e1 mon process / node\u003c/li\u003e\n\u003cli\u003e1 osd process / disk\u003c/li\u003e\n\u003cli\u003en jornal ssd device / disk / node\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eだと考えられます。僕が今のところ理想かなぁと思っている構成をまとめたいと思いま\nす。\u003c/p\u003e\n\u003cp\u003e下記の図がそれです。\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e+------------------------+\n|         client         |\n+------------------------+\n|\n+--------------------------+--------------------------+-------------------------------+-------------------------\n|                          |                          |                               |            public network\n+------------------------+ +------------------------+ +------------------------+      +------------------------+\n|          mon           | |          mon           | |          mon           |      |          mds           |\n+------+ +------+ +------+ +------+ +------+ +------+ +------+ +------+ +------+      +------------------------+\n| osd  | | osd  | | osd  | | osd  | | osd  | | osd  | | osd  | | osd  | | osd  |      |                        |\n+------+ +------+ +------+ +------+ +------+ +------+ +------+ +------+ +------+      |                        |\n| disk | | disk | | disk | | disk | | disk | | disk | | disk | | disk | | disk |....\u0026gt; |                        |\n+------+ +------+ +------+ +------+ +------+ +------+ +------+ +------+ +------+scale |          node          |\n|          ssd           | |          ssd           | |          ssd           |      |                        |\n+------------------------+ +------------------------+ +------------------------+      |                        |\n|          node          | |          node          | |          node          |      |                        |\n+------------------------+ +------------------------+ +------------------------+      +------------------------+\n|                          |                          |                               |\n+--------------------------+--------------------------+-------------------------------+-------------------------\n                                                                                                  cluster network\n\u003c/code\u003e\u003c/pre\u003e\u003ch4 id=\"mds-と-node-の関係\"\u003emds と node の関係\u003c/h4\u003e\n\u003cp\u003emds はリモートクライアントへのファイルシステムサービスの提供を行うことや特性が\n全く異なることから別ノードに切り出しました。また mds は幾つかのノードで稼働さ\nせる事も可能。が、mds はそれぞれのサービスを HA 組む仕組みは持っていないので\nどれか一方の mds をクライアントは指し示す必要があり、その mds が停止すれば直\nちに障害に発展します。\u003c/p\u003e","title":"Ceph のプロセス配置ベストプラクティス"},{"content":"こんにちは。@jedipunkz です。\n昨晩、第17回 OpenStack 勉強会が開催されました\nhttp://connpass.com/event/4545/\nここで発表をしてきましたぁ！発表タイトルは \u0026ldquo;rcbops/chef-cookbooks\u0026rdquo; です。\n何を発表したかと言うと詳しくは上記のスライドを見ていただくとして、簡単に言うと \u0026ldquo;RackSpace 社のエンジニアが管理している Chef Cookbooks でOpenStack 構成を作ろ う\u0026rdquo; ってことです。\n今日知ったのですがどうも昨晩は初心者向けの勉強会という位置付けだったらしく..少 しだけディープな話題を話してしまったかもしれません！すいません！＞＜\nでもとても楽しく発表出来ましたし、逆に質問のコーナーで最新の情報も教えてもらえ たり！なんと Havana 対応の v4.2.0 以降では Swift の Cookbooks が消えてしまった とか！\u0026hellip; 皆 Swift 好きくないの？\u0026hellip;; ;\nrcbops/chef-cookbooks はずっと追っていますが、ものすごいスピードで開発進んでい るので、今後ぜひみなさん使ってみて下さいー。\n最後に詳しい利用方法を記した僕のブログの URL を貼り付けておきます。\nOpenStack Havana を Chef でデプロイ http://jedipunkz.github.io/blog/2013/11/17/openstack-havana-chef-deploy/\nSwift HA 構成を Chef でデプロイ http://jedipunkz.github.io/blog/2013/07/26/swift-ha-chef-deploy/\n実用的な Swift 構成を Chef でデプロイ http://jedipunkz.github.io/blog/2013/10/27/swift-chef/\n","permalink":"https://jedipunkz.github.io/post/2014/01/21/17th-openstack-study/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003e昨晩、第17回 OpenStack 勉強会が開催されました\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://connpass.com/event/4545/\"\u003ehttp://connpass.com/event/4545/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eここで発表をしてきましたぁ！発表タイトルは \u0026ldquo;rcbops/chef-cookbooks\u0026rdquo; です。\u003c/p\u003e\n\u003cscript async class=\"speakerdeck-embed\"\ndata-id=\"27a2739063d601314bce6a232911c4f0\" data-ratio=\"1.33333333333333\"\nsrc=\"//speakerdeck.com/assets/embed.js\"\u003e\u003c/script\u003e\n\u003cp\u003e何を発表したかと言うと詳しくは上記のスライドを見ていただくとして、簡単に言うと\n\u0026ldquo;RackSpace 社のエンジニアが管理している Chef Cookbooks でOpenStack 構成を作ろ\nう\u0026rdquo; ってことです。\u003c/p\u003e\n\u003cp\u003e今日知ったのですがどうも昨晩は初心者向けの勉強会という位置付けだったらしく..少\nしだけディープな話題を話してしまったかもしれません！すいません！＞＜\u003c/p\u003e\n\u003cp\u003eでもとても楽しく発表出来ましたし、逆に質問のコーナーで最新の情報も教えてもらえ\nたり！なんと Havana 対応の v4.2.0 以降では Swift の Cookbooks が消えてしまった\nとか！\u0026hellip; 皆 Swift 好きくないの？\u0026hellip;; ;\u003c/p\u003e\n\u003cp\u003ercbops/chef-cookbooks はずっと追っていますが、ものすごいスピードで開発進んでい\nるので、今後ぜひみなさん使ってみて下さいー。\u003c/p\u003e\n\u003cp\u003e最後に詳しい利用方法を記した僕のブログの URL を貼り付けておきます。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eOpenStack Havana を Chef でデプロイ\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ca href=\"http://jedipunkz.github.io/blog/2013/11/17/openstack-havana-chef-deploy/\"\u003ehttp://jedipunkz.github.io/blog/2013/11/17/openstack-havana-chef-deploy/\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSwift HA 構成を Chef でデプロイ\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ca href=\"http://jedipunkz.github.io/blog/2013/07/26/swift-ha-chef-deploy/\"\u003ehttp://jedipunkz.github.io/blog/2013/07/26/swift-ha-chef-deploy/\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e実用的な Swift 構成を Chef でデプロイ\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ca href=\"http://jedipunkz.github.io/blog/2013/10/27/swift-chef/\"\u003ehttp://jedipunkz.github.io/blog/2013/10/27/swift-chef/\u003c/a\u003e\u003c/p\u003e","title":"第17回 OpenStack 勉強会で話してきました"},{"content":"こんにちは。@jedipunkz です。\nSerf の登場があったり、ここ最近オーケストレーションについて考える人が増えた気 がします。システムをデプロイしてその後各ノード間の連結だったりも同じ Chef, Puppet 等のコンフィギュレーションツールで行うのか？全く別のツールで？..\n最近 Serf というツールの登場がありました。\n僕も Serf を触ってつい先日ブログに書きました。有用なツールだと思います。シ ンプルだからこそ応用が効きますし、リアルタイム性もあり、将来的に異なるネットワー クセグメント上のノードとも連結出来るようになりそうですし、とても期待です。\n話が少し飛びますが..\nいつも Rebuild.fm を楽しく聞いているのですが Immutable Infrastructure の話題の 時にオーケストレーションの話題になって、どうも \u0026lsquo;Chef でも自律的なクラスタを組 むことが認知されていないのでは？\u0026rsquo; と思うようになりました。もちろん Chef でやる べき！とは言い切りませんし、今後どうなるかわかりません。Opscode の中の人も \u0026lsquo;オー ケストレーションは自分でやってね\u0026rsquo; というスタンスだったとずいぶん前ですが聞きま した。Serf を等のオーケストレーションツールを使う使わないの話は今回はしないの ですが Chef でも自律的クラスタを組むことは出来ますよ〜というのが今回の話題。\nまえがきが長くなりました。\n今回は Chef で自律的クラスタを構成する方法を記したいと思います。\nhaproxy 等を利用すれば尚良いと思いますが、よりクラスタを組むのが簡単な nginx を今回は利用したいと思います。\nhttps://github.com/opscode-cookbooks/nginx\n構成 \u0026lsquo;web\u0026rsquo; という Role 名と \u0026rsquo;lb\u0026rsquo; という Role 名で単純な HTTP サーバとしての nginx ノードを複数台と、ロードバランサとしての nginx ノードを1台でクラスタを構成しま す。また共に environment 名は同じものを利用します。別の environment 名の場合は 別クラスタという区切りです。\n\u0026rsquo;lb\u0026rsquo; node x 1 + \u0026lsquo;web\u0026rsquo; node x n (\u0026lsquo;foo\u0026rsquo; environment) \u0026rsquo;lb\u0026rsquo; node x 1 + \u0026lsquo;web\u0026rsquo; node x n (\u0026lsquo;bar\u0026rsquo; environment) \u0026rsquo;lb\u0026rsquo; nginx ロードバランサのレシピ 下記が \u0026rsquo;lb\u0026rsquo; Role の recipes/cmomnos_conf.rb の修正した内容です。\nenvironment = node.chef_environment webservers = search(:node, \u0026#34;role:web AND chef_environment:#{environment}\u0026#34;) template \u0026#34;#{node[\u0026#39;nginx\u0026#39;][\u0026#39;dir\u0026#39;]}/sites-available/default\u0026#34; do source \u0026#34;default-site.erb\u0026#34; owner \u0026#34;root\u0026#34; group \u0026#34;root\u0026#34; mode 00644 notifies :reload, \u0026#39;service[nginx]\u0026#39; variables ({ :webservers =\u0026gt; webservers }) end 何をやっているかと言うと、environment という変数に自ノードの environment 名を。 webservers という変数に role 名が \u0026lsquo;web\u0026rsquo; で尚且つ自ノードと同じ environment 名 が付いたノード名を入れています。これで自分と同じ environment に所属している \u0026lsquo;web\u0026rsquo; Role なノードを Chef サーバに対して検索しています。また、template 内で webservers という変数をそのまま利用できるように variables で渡しています。\n\u0026rsquo;lb\u0026rsquo; nginx ロードバランサのテンプレート 下記が webservers 変数を受け取った後の template 内の処理です。\n\u0026lt;% if @webservers and ( @webservers != [] ) %\u0026gt; upstream backend { \u0026lt;% @webservers.each do |hostname| -%\u0026gt; server \u0026lt;%= hostname[\u0026#39;ipaddr\u0026#39;] -%\u0026gt;; \u0026lt;% end -%\u0026gt; } \u0026lt;% end %\u0026gt; server { listen 80; server_name \u0026lt;%= node[\u0026#39;hostname\u0026#39;] %\u0026gt;; access_log \u0026lt;%= node[\u0026#39;nginx\u0026#39;][\u0026#39;log_dir\u0026#39;] %\u0026gt;/localhost.access.log; location / { \u0026lt;% if @webservers and ( @webservers != [] ) %\u0026gt; proxy_pass http://backend; \u0026lt;% else %\u0026gt; root /var/www/nginx-default; index index.html index.htm; \u0026lt;% end %\u0026gt; } } upstream backend { \u0026hellip; は皆さん見慣れた記述だと思うのですが、バックエンドの HTTP サーバの IP アドレスを一覧化します。each で回しているので台数分だけ server \u0026lt;ip_addr\u0026gt;; の記述が入ります。\nchef-client をデーモン稼働しておけば、新規に Chef サーバに登録された \u0026lsquo;web\u0026rsquo; Role の HTTP サーバを自動で \u0026rsquo;lb\u0026rsquo; Role のロードバランサが組み込む、つまり自律的 なクラスタが組めることになります。もちろんこの間の手作業は一切ありません。\nちなみに chef-client をデーモン稼働するには\nrecipe[chef-client::service] というレシピをノードに割り当てることで可能です。\nまとめ Chef でも自律的なクラスタが組めました。もちろん chef-client の稼働間隔があるの でリアルタイム性はありません。chef-client の稼働間隔は \u0026lsquo;chef-client\u0026rsquo; レシピの attributes で調整出来ます。その点は serf のほうが確実に勝っていると見るべきで しょう。冒頭に記したようにこの辺りの操作を Chef で行うのか別のツールを使うのか はまだまだ模索が必要そう。ただ、私がいつも使っている \u0026lsquo;OpenStack を Chef で構成 する Cookbooks\u0026rsquo; 等は複数台構成を Chef で構成しています。なので僕にとってはこの 辺りの話は当たり前だと思っていたのだけど、どうも勉強会に出たりすると \u0026ldquo;Chef は 複数台構成を作るのが苦手だ\u0026rdquo; って話があがってくるので気になっていました。\n","permalink":"https://jedipunkz.github.io/post/2013/12/09/chef-autonoumous-cluster/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003eSerf の登場があったり、ここ最近オーケストレーションについて考える人が増えた気\nがします。システムをデプロイしてその後各ノード間の連結だったりも同じ Chef,\nPuppet 等のコンフィギュレーションツールで行うのか？全く別のツールで？..\u003c/p\u003e\n\u003cp\u003e最近 Serf というツールの登場がありました。\u003c/p\u003e\n\u003cp\u003e僕も Serf を触ってつい先日ブログに書きました。有用なツールだと思います。シ\nンプルだからこそ応用が効きますし、リアルタイム性もあり、将来的に異なるネットワー\nクセグメント上のノードとも連結出来るようになりそうですし、とても期待です。\u003c/p\u003e\n\u003cp\u003e話が少し飛びますが..\u003c/p\u003e\n\u003cp\u003eいつも Rebuild.fm を楽しく聞いているのですが Immutable Infrastructure の話題の\n時にオーケストレーションの話題になって、どうも \u0026lsquo;Chef でも自律的なクラスタを組\nむことが認知されていないのでは？\u0026rsquo; と思うようになりました。もちろん Chef でやる\nべき！とは言い切りませんし、今後どうなるかわかりません。Opscode の中の人も \u0026lsquo;オー\nケストレーションは自分でやってね\u0026rsquo; というスタンスだったとずいぶん前ですが聞きま\nした。Serf を等のオーケストレーションツールを使う使わないの話は今回はしないの\nですが Chef でも自律的クラスタを組むことは出来ますよ〜というのが今回の話題。\u003c/p\u003e\n\u003cp\u003eまえがきが長くなりました。\u003c/p\u003e\n\u003cp\u003e今回は Chef で自律的クラスタを構成する方法を記したいと思います。\u003c/p\u003e\n\u003cp\u003ehaproxy 等を利用すれば尚良いと思いますが、よりクラスタを組むのが簡単な nginx\nを今回は利用したいと思います。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/opscode-cookbooks/nginx\"\u003ehttps://github.com/opscode-cookbooks/nginx\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"構成\"\u003e構成\u003c/h2\u003e\n\u003cp\u003e\u0026lsquo;web\u0026rsquo; という Role 名と \u0026rsquo;lb\u0026rsquo; という Role 名で単純な HTTP サーバとしての nginx\nノードを複数台と、ロードバランサとしての nginx ノードを1台でクラスタを構成しま\nす。また共に environment 名は同じものを利用します。別の environment 名の場合は\n別クラスタという区切りです。\u003c/p\u003e","title":"Chef で自律的クラスタを考える"},{"content":"こんにちは。@jedipunkz です。\n皆さん CoreOS は利用されたことありますか？CoreOS は軽量な docker との相性の良 い OS です。下記が公式サイト。\nhttp://coreos.com/\n特徴としては下記の3つがあります。\netcd systemd docker ここではこの中の etcd について注目していきたいと思います。etcd はクラスタエイ ブルな KVS データベースです。コンフィギュレーションをクラスタ間で共有すること がなので、オーケストレーションの分野でも期待出来るのでは？と個人的に感じていま す。今回は etcd のクラスタ構成構築の手順とその基本動作の確認、またどう応用出来 るのか？について記していきたいと思います。\n参考 URL http://coreos.com/using-coreos/etcd/ https://github.com/coreos/etcd ビルド go 1.1 or later をインストールして etcd のコンパイル準備を行います。Ubuntu Saucy のパッケージを用いると容易に行えます。\n% apt-get -y install golang coreos/etcd を取得しビルド\n% git clone https://github.com/coreos/etcd % cd coreos % ./build % ./etcd --version v0.2.0-rc1-60-g73f04d5 CoreOS の用意 ここではたまたま手元にあった OpenStack を用いて CoreOS のイメージを登録してい みます。ベアメタルでも可能ですのでその場合は手順を読み替えて作業してみてくださ い。OpenStack 等クラウドプラットフォームを利用する場合は metadata サービスが必 須となるので注意してください。\n% wget http://storage.core-os.net/coreos/amd64-generic/dev-channel/coreos_production_openstack_image.img.bz2 % bunzip2 coreos_production_openstack_image.img.bz2 % glance image-create --name coreos-image --container-format ovf \\ --disk-format qcow2 --file coreos_production_openstack_image.img nova boot にて CoreOS を起動します。(下記は例)\n% nova keypair-add testkey01 \u0026gt; testkey01.pem % nova boot --nic net-id .... --image coreos-image --flavor 1 --key_name testkey01 coreos01 CoreOS 上での etcd クラスタ起動 上記でコンパイルした etcd のバイナリを起動したインスタンス (CoreOS) に転送しま す。scp 等で転送してください。\nここでは 1 node 上で複数のポート番号を用いて 3 つの etcd を稼働することでクラ スタを構築します。\n7002 番ポートを peer addr として master を起動。listen ポートは 4002\n% ./etcd -peer-addr 127.0.0.1:7002 -addr 127.0.0.1:4002 -data-dir machines/machine1 -name machine1 上記の master を参照する slaves (残り2台) を起動。\n% ./etcd -peer-addr 127.0.0.1:7003 -addr 127.0.0.1:4003 -peers 127.0.0.1:7002 -data-dir machines/machine2 -name machine2 % ./etcd -peer-addr 127.0.0.1:7004 -addr 127.0.0.1:4004 -peers 127.0.0.1:7002 -data-dir machines/machine3 -name machine3 クラスタ構成内のノード情報を確認する。\n% curl -L http://127.0.0.1:4002/v2/machines [etcd] Dec 4 03:46:44.153 INFO | URLs: machine1 / machine1 (http://127.0.0.1:4002,http://127.0.0.1:4003,http://127.0.0.1:4004) http://127.0.0.1:4002, http://127.0.0.1:4003, http://127.0.0.1:4004 leader (master) 情報を確認する。\n% curl -L http://127.0.0.1:4002/v2/leader http://127.0.0.1:7002 上記で起動した master プロセスが leader (master) になっていることを確認出来る と思います。\nキーの投入と参照 テストでキーと値を入力してみましょう。\u0026lsquo;foo\u0026rsquo; キーに \u0026lsquo;bar\u0026rsquo; という値を投入てくだ さい。\n% curl -L http://127.0.0.1:4002/v2/keys/foo -XPUT -d value=bar クラスタ内全てのプロセスから上記のキーの取得できることを確認します。\n% curl -L http://127.0.0.1:4002/v2/keys/foo {\u0026#34;action\u0026#34;:\u0026#34;get\u0026#34;,\u0026#34;node\u0026#34;:{\u0026#34;key\u0026#34;:\u0026#34;/foo\u0026#34;,\u0026#34;value\u0026#34;:\u0026#34;bar\u0026#34;,\u0026#34;modifiedIndex\u0026#34;:4,\u0026#34;createdIndex\u0026#34;:4}} % curl -L http://127.0.0.1:4003/v2/keys/foo {\u0026#34;action\u0026#34;:\u0026#34;get\u0026#34;,\u0026#34;node\u0026#34;:{\u0026#34;key\u0026#34;:\u0026#34;/foo\u0026#34;,\u0026#34;value\u0026#34;:\u0026#34;bar\u0026#34;,\u0026#34;modifiedIndex\u0026#34;:4,\u0026#34;createdIndex\u0026#34;:4}} % curl -L http://127.0.0.1:4004/v2/keys/foo {\u0026#34;action\u0026#34;:\u0026#34;get\u0026#34;,\u0026#34;node\u0026#34;:{\u0026#34;key\u0026#34;:\u0026#34;/foo\u0026#34;,\u0026#34;value\u0026#34;:\u0026#34;bar\u0026#34;,\u0026#34;modifiedIndex\u0026#34;:4,\u0026#34;createdIndex\u0026#34;:4}} master のシャットダウンと master 選挙後の動作確認 テストで master のプロセスをシャットダウンしてみます。\nmaster プロセスのシャットダウン\n% kill \u0026lt;master プロセスの ID\u0026gt; その他 2 つのプロセスから \u0026lsquo;foo\u0026rsquo; キーの確認を行う。\n% curl -L http://127.0.0.1:4004/v2/keys/foo {\u0026#34;action\u0026#34;:\u0026#34;get\u0026#34;,\u0026#34;node\u0026#34;:{\u0026#34;key\u0026#34;:\u0026#34;/foo\u0026#34;,\u0026#34;value\u0026#34;:\u0026#34;bar\u0026#34;,\u0026#34;modifiedIndex\u0026#34;:4,\u0026#34;createdIndex\u0026#34;:4}} % curl -L http://127.0.0.1:4003/v2/keys/foo {\u0026#34;action\u0026#34;:\u0026#34;get\u0026#34;,\u0026#34;node\u0026#34;:{\u0026#34;key\u0026#34;:\u0026#34;/foo\u0026#34;,\u0026#34;value\u0026#34;:\u0026#34;bar\u0026#34;,\u0026#34;modifiedIndex\u0026#34;:4,\u0026#34;createdIndex\u0026#34;:4}} 勿論、旧 master からは確認出来ない。\n% curl -L http://127.0.0.1:4002/v2/keys/foo curl: (7) Failed connect to 127.0.0.1:4002; Connection refused 新 master の確認を行う。選挙の結果 3 つ目のプロセスが master に昇格しているこ とが確認出来る。\n% curl -L http://127.0.0.1:4003/v2/leader http://127.0.0.1:7004 考察とその応用性について とてもシンプルな KVS ではあるけど大きな可能性を秘めていると思っています。オー ケストレーション等への応用です。お互いのノード (今回はプロセス) 間で情報をやり とりできるので自律的なクラスタの構築も可能になるのでは？と思っています。\n\u0026rsquo;etcenv\u0026rsquo; という @mattn さんが開発したツールを見てみましょう。\nhttps://github.com/mattn/etcdenv\n下記、README から引用。\n$ curl http://127.0.0.1:4001/v1/keys/app/db -d value=\u0026#34;newdb\u0026#34; $ curl http://127.0.0.1:4001/v1/keys/app/cache -d value=\u0026#34;new cache\u0026#34; $ curl http://localhost:4001/v1/keys/app [{\u0026#34;action\u0026#34;:\u0026#34;GET\u0026#34;,\u0026#34;key\u0026#34;:\u0026#34;/app/db\u0026#34;,\u0026#34;value\u0026#34;:\u0026#34;newdb\u0026#34;,\u0026#34;index\u0026#34;:4},{\u0026#34;action\u0026#34;:\u0026#34;GET\u0026#34;,\u0026#34;key\u0026#34;:\u0026#34;/app/cache\u0026#34;,\u0026#34;value\u0026#34;:\u0026#34;new cache\u0026#34;,\u0026#34;index\u0026#34;:4}] $ etcdenv -key=/app/ DB=newdb CACHE=new cache $ etcdenv -key=/app/ ruby web.rb クラスタ間の情報を環境変数に落としこむツールです。自ノードの環境変数まで落ちれ ば、クラスタ構築も色々想像出来るのではないでしょうか？\n軽量で docker との相性も良くて etcd 等の仕組みも持っている CoreOS にはこれから も期待です。\n","permalink":"https://jedipunkz.github.io/post/2013/12/09/coreos-etcd-cluster/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003e皆さん CoreOS は利用されたことありますか？CoreOS は軽量な docker との相性の良\nい OS です。下記が公式サイト。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://coreos.com/\"\u003ehttp://coreos.com/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e特徴としては下記の3つがあります。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eetcd\u003c/li\u003e\n\u003cli\u003esystemd\u003c/li\u003e\n\u003cli\u003edocker\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eここではこの中の etcd について注目していきたいと思います。etcd はクラスタエイ\nブルな KVS データベースです。コンフィギュレーションをクラスタ間で共有すること\nがなので、オーケストレーションの分野でも期待出来るのでは？と個人的に感じていま\nす。今回は etcd のクラスタ構成構築の手順とその基本動作の確認、またどう応用出来\nるのか？について記していきたいと思います。\u003c/p\u003e\n\u003ch2 id=\"参考-url\"\u003e参考 URL\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"http://coreos.com/using-coreos/etcd/\"\u003ehttp://coreos.com/using-coreos/etcd/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/coreos/etcd\"\u003ehttps://github.com/coreos/etcd\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"ビルド\"\u003eビルド\u003c/h2\u003e\n\u003cp\u003ego 1.1 or later をインストールして etcd のコンパイル準備を行います。Ubuntu\nSaucy のパッケージを用いると容易に行えます。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e% apt-get -y install golang\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003ecoreos/etcd を取得しビルド\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e% git clone https://github.com/coreos/etcd\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e% cd coreos\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e% ./build\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e% ./etcd --version\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ev0.2.0-rc1-60-g73f04d5\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"coreos-の用意\"\u003eCoreOS の用意\u003c/h2\u003e\n\u003cp\u003eここではたまたま手元にあった OpenStack を用いて CoreOS のイメージを登録してい\nみます。ベアメタルでも可能ですのでその場合は手順を読み替えて作業してみてくださ\nい。OpenStack 等クラウドプラットフォームを利用する場合は metadata サービスが必\n須となるので注意してください。\u003c/p\u003e","title":"CoreOS etcd のクラスタとその応用性"},{"content":"こんにちは。@jedipunkz です。\nアドベントカレンダーの季節がやって参りました。\nIronic を使って OpenStack でベアメタルサーバを扱いたい！ということで色々とやっ ている最中 (今週から始めました..) なのですが、まだまだ incubator プロジェクト ということもあって実装が追い付いていなかったりドキュメントも揃っていなかったり とシンドい状況ｗ ここ2日程で集めた情報を整理するためにも 2013年 OpenStack アド ベントカレンダーに参加させてもらいますー。\n参考資料のまとめ まずは公式 wiki ページ。逆に言うとここに記されている以上の情報は無いんじゃ？あ とはコード読め！の世界かも..。\nhttps://wiki.openstack.org/wiki/Ironic\ndevtest_undercloud です。上の資料の中でも手順の中で度々こちらにジャンプしている。 同じく incubator プロジェクトの TrippleO のデベロッパ用ドキュメントになっている。 上記の公式 wiki の情報を合わせ読むことで Ironic を使ったデプロイの手順に仕上がります。\nhttp://docs.openstack.org/developer/tripleo-incubator/devtest_undercloud.html\nソースコードとドキュメント。あとでドキュメント作成方法を記しますが、こちらを取 得して作成します。\nhttps://github.com/openstack/ironic\nドキュメントサイト。まだ情報が揃っていません。よって上の github から取得したモ ノからドキュメントを作る方法を後で書きます。\nhttp://docs.openstack.org/developer/ironic/\nlaunchpad サイト。全てのバグ情報やブループリント等が閲覧出来ます。まだ絶賛開発 中なので読む必要があると思います。\nhttps://launchpad.net/ironic\nドキュメントを作る +++\n公式 ドキュメントサイトは一応、上記の通りあるのですが、ドキュメントも絶賛執筆 中ということで所々抜けがあります。また公式ドキュメントサイトがどのスパンで更新 されているか分からないので、いち早く情報をゲットしたい場合ドキュメントを作る必 要があると思います。ということで、その作り方を記していきます。尚、公式 wiki サ イトにも手順が載っていますが Vagrant と Apache を用いた方法になっているので、 普通に Ubuntu サーバが手元にある環境を想定して読み替えながら説明していきます。\n必要なパッケージのインストールを行います。\n% sudo apt-get update % sudo apt-get install -y git python-dev swig libssl-dev python-pip \\ libmysqlclient-dev libxml2-dev libxslt-dev libxslt1-dev python-mysqldb \\ libpq-dev % sudo pip install virtualenv setuptools-git flake8 tox % sudo easy_install nose ソースコード・ドキュメントを取得します。\n% git clone git://github.com/openstack/ironic.git Sphinx で構成されているのでビルドします。\n% cd ironic % tox -evenv -- echo \u0026#39;done\u0026#39; % source .tox/venv/bin/activate \u0026gt; python setup.ph build_sphinx \u0026gt; deactivate ironic/doc/build/html ディレクトリ配下に HTML のドキュメントが生成されたはずで す。これを手元の端末に持ってきて開けばブラウザで最新のドキュメントが閲覧出来ま す。\nironic を有効にした devstack による構築 devstack を使って ironic を機能させていきます。私は下記の localrc を用いて ironic の試験をしていました。またブランチは \u0026lsquo;master\u0026rsquo; を使います。\n% git clone https://github.com/openstack-dev/devstack.git % cd devstack % ${EDITOR} localrc # 下記の通り HOST_IP=\u0026lt;your_machine_ip_addr\u0026gt; LOGFILE=stack.sh.log ADMIN_PASSWORD=nomoresecrete MYSQL_PASSWORD=$ADMIN_PASSWORD RABBIT_PASSWORD=$ADMIN_PASSWORD SERVICE_PASSWORD=$ADMIN_PASSWORD SERVICE_TOKEN=admintoken disable_service n-obj # ironic enable_service ir-api enable_service ir-cond # use neutron disable_service n-net enable_service q-svc enable_service q-agt enable_service q-dhcp enable_service q-l3 enable_service q-meta enable_service q-lbaas enable_service neutron ENABLE_TENANT_TUNNELS=True # heat ENABLED_SERVICES+=,heat,h-api,h-api-cfn,h-api-cw,h-eng ## It would also be useful to automatically download and register VM images that Heat can launch. # 64bit image (~660MB) IMAGE_URLS+=\u0026#34;,http://fedorapeople.org/groups/heat/prebuilt-jeos-images/F17-x86_64-cfntools.qcow2\u0026#34; # 32bit image (~640MB) IMAGE_URLS+=\u0026#34;,http://fedorapeople.org/groups/heat/prebuilt-jeos-images/F17-i386-cfntools.qcow2\u0026#34; # syslog SYSLOG=True SYSLOG_HOST=$HOST_IP SYSLOG_PORT=514 stack.sh を実行します。\n% ./stack.sh Ironic を有効にした OpenStack 環境が出来上がったはずです。\ndiskimage-builder を使ったイメージ作成 ベアメタルサーバをデプロイするためのイメージを作成します。元々は TrippleO のプ ロジェクト内に存在していましたが、現在は git レポジトリが別れています。\n先ほど devstack を導入したホストでイメージを作ります。作成には結構時間が掛かります。\n% cd ~ % git clone https://github.com/openstack/diskimage-builder.git % git clone https://github.com/openstack/tripleo-incubator.git % git clone https://github.com/openstack/tripleo-image-elements.git % git clone https://github.com/openstack/tripleo-heat-templates.git % export UNDERCLOUD_DIB_EXTRA_ARGS=\u0026#39;ironic-api ironic-conductor\u0026#39; % export ELEMENTS_PATH=/home/thirai/tripleo-image-elements/elements % ./diskimage-builder/bin/disk-image-create -a amd64 -o ~/undercloud boot-stack \\ nova-baremetal os-collect-config stackuser dhcp-all-interfaces \\ neutron-dhcp-agent ${UNDERCLOUD_DIB_EXTRA_ARGS:-} ubuntu 2\u0026gt;\u0026amp;1 | tee /tmp/undercloud.log イメージが ~/undercloud.qcow2 が生成されたはずです。作成したイメージを Glance に登録します。\n% ~/tripleo-incubator/scripts/load-image undercloud.qcow2 undercloud.yaml と ironic.yaml をマージします。\n% cd ~/tripleo-heat-templates % make undercloud-vm-ironic.yaml パスワードの生成と環境変数への読み込みを行います。\n% cd ~/tripleo-incubator/scripts/ % export PATH=$PATH:. % ./setup-undercloud-passwords % source tripleo-undercloud-passwords UNDERCLOUD_IRONIC_PASSWORD 環境変数にも読み込みます。\n% export UNDERCLOUD_IRONIC_PASSWORD=$(~/tripleo-incubator/scripts/os-make-password) さて、イメージを利用したベアメタルへの稼働ですが、\u0026hellip;\nif [ \u0026#34;$DHCP_DRIVER\u0026#34; = \u0026#34;bm-dnsmasq\u0026#34; ]; then UNDERCLOUD_NATIVE_PXE=\u0026#34;\u0026#34; else UNDERCLOUD_NATIVE_PXE=\u0026#34;;NeutronNativePXE=True\u0026#34; fi heat stack-create -f ./tripleo-heat-templates/undercloud-vm-ironic.yaml \\ -P \u0026#34;PowerUserName=$(whoami);\\ AdminToken=${UNDERCLOUD_ADMIN_TOKEN};\\ AdminPassword=${UNDERCLOUD_ADMIN_PASSWORD};\\ GlancePassword=${UNDERCLOUD_GLANCE_PASSWORD};\\ HeatPassword=${UNDERCLOUD_HEAT_PASSWORD};\\ NeutronPassword=${UNDERCLOUD_NEUTRON_PASSWORD};\\ NovaPassword=${UNDERCLOUD_NOVA_PASSWORD};\\ BaremetalArch=${NODE_ARCH}$UNDERCLOUD_NATIVE_PXE\u0026#34; \\ IronicPassword=${UNDERCLOUD_IRONIC_PASSWORD}\u0026#34; \\ undercloud コケました。\u0026hellip;エラーは下記の通り。\nTRACE heat.engine.resource Error: Creation of server teststack01-WikiDatabase-5nyiqluilnxn failed: No valid host was found. Exceeded max scheduling attempts 3 for instance 733b69df-2b54-44ae-9d61-de766746f21a (500)#0122013-12-03 16:04:14.513 10022 TRACE heat.engine.resource No Valid Host とな\u0026hellip;。うーん。確かに IPMI を積んだベアメタルマシンの情報って どこにも記していないんだよなぁ。しかも heat のテンプレート (underclound-vm-ironic.yaml) 見てもよく理解していない自分がいる\u0026hellip;(´・ω・`)\nというか手順にはその周りのこと何も書いていないのでぇすがぁ！\u0026hellip;\nということで、まだまだコントリビュートするチャンス満載の状態なので、よかったら 皆さん参加されませんか !?!?\nhttps://wiki.openstack.org/wiki/HowToContribute\n","permalink":"https://jedipunkz.github.io/post/2013/12/05/ironic-openstack-beremetal/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003eアドベントカレンダーの季節がやって参りました。\u003c/p\u003e\n\u003cp\u003eIronic を使って OpenStack でベアメタルサーバを扱いたい！ということで色々とやっ\nている最中 (今週から始めました..) なのですが、まだまだ incubator プロジェクト\nということもあって実装が追い付いていなかったりドキュメントも揃っていなかったり\nとシンドい状況ｗ ここ2日程で集めた情報を整理するためにも 2013年 OpenStack アド\nベントカレンダーに参加させてもらいますー。\u003c/p\u003e\n\u003ch2 id=\"参考資料のまとめ\"\u003e参考資料のまとめ\u003c/h2\u003e\n\u003cp\u003eまずは公式 wiki ページ。逆に言うとここに記されている以上の情報は無いんじゃ？あ\nとはコード読め！の世界かも..。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://wiki.openstack.org/wiki/Ironic\"\u003ehttps://wiki.openstack.org/wiki/Ironic\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003edevtest_undercloud です。上の資料の中でも手順の中で度々こちらにジャンプしている。\n同じく incubator プロジェクトの TrippleO のデベロッパ用ドキュメントになっている。\n上記の公式 wiki の情報を合わせ読むことで Ironic を使ったデプロイの手順に仕上がります。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://docs.openstack.org/developer/tripleo-incubator/devtest_undercloud.html\"\u003ehttp://docs.openstack.org/developer/tripleo-incubator/devtest_undercloud.html\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eソースコードとドキュメント。あとでドキュメント作成方法を記しますが、こちらを取\n得して作成します。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/openstack/ironic\"\u003ehttps://github.com/openstack/ironic\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eドキュメントサイト。まだ情報が揃っていません。よって上の github から取得したモ\nノからドキュメントを作る方法を後で書きます。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://docs.openstack.org/developer/ironic/\"\u003ehttp://docs.openstack.org/developer/ironic/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003elaunchpad サイト。全てのバグ情報やブループリント等が閲覧出来ます。まだ絶賛開発\n中なので読む必要があると思います。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://launchpad.net/ironic\"\u003ehttps://launchpad.net/ironic\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eドキュメントを作る\n+++\u003c/p\u003e\n\u003cp\u003e公式 ドキュメントサイトは一応、上記の通りあるのですが、ドキュメントも絶賛執筆\n中ということで所々抜けがあります。また公式ドキュメントサイトがどのスパンで更新\nされているか分からないので、いち早く情報をゲットしたい場合ドキュメントを作る必\n要があると思います。ということで、その作り方を記していきます。尚、公式 wiki サ\nイトにも手順が載っていますが Vagrant と Apache を用いた方法になっているので、\n普通に Ubuntu サーバが手元にある環境を想定して読み替えながら説明していきます。\u003c/p\u003e\n\u003cp\u003e必要なパッケージのインストールを行います。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e% sudo apt-get update\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e% sudo apt-get install -y git python-dev swig libssl-dev python-pip \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  libmysqlclient-dev libxml2-dev libxslt-dev libxslt1-dev python-mysqldb \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  libpq-dev\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e% sudo pip install virtualenv setuptools-git flake8 tox\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e% sudo easy_install nose\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eソースコード・ドキュメントを取得します。\u003c/p\u003e","title":"Ironic でベアメタル OpenStack ！..の一歩手前"},{"content":"こんにちは。@jedipunkz です。\n以前、Sensu を Chef で管理する方法について書きました。\nhttp://jedipunkz.github.io/blog/2013/06/20/sensu-chef-controll/\nこれは今年(2013)の6月頃の記事ですが、この時はまだ sensu-chef を include して使う別の Chef Cookbook が必要でした。また Redis 周りの Cookbooks が完成度あまく、またこれも 公式とは別の Cookbooks を改修して再利用する形でした。この作業は結構しんどかっ た記憶があるのですが、最近 GlideNote さんのブログを読んで( ﾟдﾟ)ﾊｯ!と思い、 sensu-chef を再確認したのですが、だいぶ更新されていました。\n下記が sensu-chef です。\nhttps://github.com/sensu/sensu-chef\nこの Chef Cookbook 単体で利用できる形に更新されていて、plugins, checks 等は Recipe に追記することで対応可能になっていました。早速利用してみたので簡単に使 い方を書いていきます。\n下記が Sensu の管理画面です。最終的にこの画面に監視対象のアラートが上がってきます。\n{% img /pix/sensu.png %}\n使い方 sensu-chef を取得する。chef-repo になっています。\n% git clone https://github.com/sensu/sensu-chef.git ~/chef-repo-sensu bundle にて Gemfile に記述の在る gem パッケージをインストールします。\n% cd ~/chef-repo-sensu % bundle install .chef/ 配下の設定は割愛します。chef サーバの情報に合わせて設定します。\nssl 鍵を生成して data bags に投入します。\n% cd examples/ssl % ./ssl_certs.sh generate % cd ../../ % knife data bag create sensu % knife data bag from file sensu examples/ssl/ssl.json % ./examples/ssl/ssl_certs.sh clean Roles を作成します。chef-repo なのに何も入っていませんでした\u0026hellip;汗 ここで面白いのは \u0026lsquo;sensu-client\u0026rsquo; は sensu で言う subscribers の名前にそのまま利 用されるところです。つまり \u0026lsquo;sensu-client\u0026rsquo; の名前が記された sensu サーバ上の監 視項目 (checks) がこの sensu クライアントに割り当てられます。\n% mkdir roles % ${EDITOR} roles/sensu-server.rb name \u0026#34;sensu-server\u0026#34; description \u0026#34;role applied to sensu server.\u0026#34; run_list \u0026#39;recipe[sensu::default]\u0026#39;, \u0026#39;recipe[sensu::rabbitmq]\u0026#39;, \u0026#39;recipe[sensu::redis]\u0026#39;, \u0026#39;recipe[sensu::server_service]\u0026#39;, \u0026#39;recipe[sensu::api_service]\u0026#39;, \u0026#39;recipe[sensu::dashboard_service]\u0026#39;, \u0026#39;recipe[chef-client::service]\u0026#39; % ${EDITOR} roles/sensu-client.rb name \u0026#34;sensu-client\u0026#34; description \u0026#34;role applied to sensu client.\u0026#34; run_list \u0026#39;recipe[sensu::default]\u0026#39;, \u0026#39;recipe[sensu::client_service]\u0026#39;, \u0026#39;recipe[chef-client::service]\u0026#39; Cheffile に \u0026lsquo;chef-client\u0026rsquo; の Cookbook 名を追記します。\nsite \u0026#39;http://community.opscode.com/api/v1\u0026#39; cookbook \u0026#39;sensu\u0026#39;, path: \u0026#39;./\u0026#39; cookbook \u0026#39;sensu-test\u0026#39;, path: \u0026#39;./test/cookbooks/sensu-test\u0026#39; cookbook \u0026#39;chef-client\u0026#39; # \u0026lt;---- 追記 librarian-chef を実行して Cookbooks を取得します。\n% librarian-chef install Rabbitmq, Redis, API の IP アドレスを設定します。IP アドレスは例です。\n% ${EDITOR} cookbooks/sensu/attributes/default.rb default.sensu.rabbitmq.host = \u0026#34;172.24.19.11\u0026#34; default.sensu.redis.host = \u0026#34;172.24.19.11\u0026#34; default.sensu.api.host = \u0026#34;172.24.19.11\u0026#34; sensu-server 用の Recipe に監視項目を追記します。ここでは cron デーモンの稼働 状況を監視してみました。\n% ${EDITOR} cookbooks/sensu/recipes/server_service.rb # 下記を追記 sensu_check \u0026#34;cron_process\u0026#34; do command \u0026#34;check-procs.rb -p cron -C 1\u0026#34; handlers [\u0026#34;default\u0026#34;] subscribers [\u0026#34;sensu-client\u0026#34;] interval 30 additional(:notification =\u0026gt; \u0026#34;Cron is not running\u0026#34;, :occurrences =\u0026gt; 5) end sensu-client 用の Recipe に client.json (クライアント用設定ファイル) の記述と 上記監視項目に合った plugins 設定の追記を行います。\n% ${EDITOR} cookbooks/sensu/recipes/client_service.rb # 下記を追記 sensu_client node.name do address node.ipaddress subscriptions node.roles + [\u0026#34;all\u0026#34;] end cookbook_file \u0026#39;check-procs.rb\u0026#39; do source \u0026#39;check-procs.rb\u0026#39; mode 0755 path \u0026#39;/etc/sensu/plugins/check-procs.rb\u0026#39; end 上記 check-procs.rb は community サイトからダウンロードする必要があるのですが cookbook_file で対応したので files/ ディレクトリ配下に置いておきます。\n% mkdir -p cookbooks/sensu/files/default % wget -o cookbooks/sensu/files/default/check-procs.rb \\ https://github.com/sensu/sensu-community-plugins/raw/master/plugins/processes/check-procs.rb 上記の check-procs.rb は行頭に \u0026lsquo;#!/usr/bin/env ruby\u0026rsquo; が記されているのですが Sensu インストール時に入る ruby は /opt/sensu/embedded/bin/ruby にあるので行頭 の1行を書き換えます。\n% diff cookbooks/sensu/files/default/check-procs.rb.org cookbooks/sensu/files/default/check-procs.rb - #!/usr/bin/env ruby + #!/opt/sensu/embedded/bin/ruby Roles, Cookbooks を Chef サーバにアップロードします。\n% knife cookbook upload -o ./cookbooks -a % knife role from file roles/*.rb いよいよブートストラップします!\nまずは sensu-server を。\n% knife bootstrap \u0026lt;server_ip_addr\u0026gt; -N sensu-server -r \u0026#39;role[sensu-server]\u0026#39; \\ -x ubuntu --sudo 次に監視対象である sensu-client を。\n% knife bootstrap \u0026lt;client_ip_addr\u0026gt; -N sensu-client01 -r \u0026#39;role[sensu-client]\u0026#39; \\ -x ubuntu --sudo http://\u0026lt;sensu-server の IP アドレス\u0026gt;:8080/ にアクセスすると Sensu の管理画面が 表示されます。認証アカウントは cookbooks/sensu/attributes/default.rb に記述が ありますので確認して下さい。\nまとめ インフラリソースのフルオートメーション化について情報リサーチしていますが監視も 重要なインフラリソースの一部です。Sensu サーバ・クライアントの自動デプロイが出 来たのでこれで一つパーツが揃ったことに。Sensu は API を持っていますのでアプリ から検知することも簡単に出来ますよ。API については下記を参照してください。\nhttp://sensuapp.org/docs/0.12/api\nまた以前抱えていた問題もスッキリクリアになって、これでまた前進できた感があります。\n参考サイト http://sensuapp.org/docs/0.12 http://blog.glidenote.com/blog/2013/11/26/sensu/ https://github.com/sensu/sensu-chef ","permalink":"https://jedipunkz.github.io/post/2013/11/27/sensu-chef-deploy-2/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003e以前、Sensu を Chef で管理する方法について書きました。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://jedipunkz.github.io/blog/2013/06/20/sensu-chef-controll/\"\u003ehttp://jedipunkz.github.io/blog/2013/06/20/sensu-chef-controll/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eこれは今年(2013)の6月頃の記事ですが、この時はまだ sensu-chef を include して使う別の Chef\nCookbook が必要でした。また Redis 周りの Cookbooks が完成度あまく、またこれも\n公式とは別の Cookbooks を改修して再利用する形でした。この作業は結構しんどかっ\nた記憶があるのですが、最近 GlideNote さんのブログを読んで( ﾟдﾟ)ﾊｯ!と思い、\nsensu-chef を再確認したのですが、だいぶ更新されていました。\u003c/p\u003e\n\u003cp\u003e下記が sensu-chef です。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/sensu/sensu-chef\"\u003ehttps://github.com/sensu/sensu-chef\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eこの Chef Cookbook 単体で利用できる形に更新されていて、plugins, checks 等は\nRecipe に追記することで対応可能になっていました。早速利用してみたので簡単に使\nい方を書いていきます。\u003c/p\u003e\n\u003cp\u003e下記が Sensu の管理画面です。最終的にこの画面に監視対象のアラートが上がってきます。\u003c/p\u003e\n\u003cp\u003e{% img /pix/sensu.png %}\u003c/p\u003e\n\u003ch2 id=\"使い方\"\u003e使い方\u003c/h2\u003e\n\u003cp\u003esensu-chef を取得する。chef-repo になっています。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e% git clone https://github.com/sensu/sensu-chef.git ~/chef-repo-sensu\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003ebundle にて Gemfile に記述の在る gem パッケージをインストールします。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e% cd ~/chef-repo-sensu\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e% bundle install\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e.chef/ 配下の設定は割愛します。chef サーバの情報に合わせて設定します。\u003c/p\u003e","title":"sensu-chef で監視システム Sensu を管理 #2"},{"content":"こんにちは。@jedipunkz です。\n自宅ルータを Vyatta で運用しているのですが、諸電力な筐体に交換した際に HDD ス ロットが余っていたので HDD を一本さしてみました。もったいないので Netatalk を インストールして Mac 用の TimeMachine サーバにするか！そんでもってファイルサー バ兼務にしよう！と思い立って作業したら簡単に出来たので共有します。\nVyatta はご存知の通り Debian Gnu/Linux がベースになっているのでパッケージレポ ジトリを追加してちょちょいで設定出来ます。\n手順 電源を通して Disk を追加します。その後起動。私は 3TB Disk が余っていたのでそれ を挿しました。\ndebian wheezy のパッケージレポジトリを Vyatta に追記します。\n% configure # set system package repository debian url http://ftp.jp.debian.org/debian # set system package repository debian distribution wheezy # set system package repository debian components \u0026#34;main contrib\u0026#34; # commit # save # exit netatalk, avahi をインストールする。その際に libgcrypt11 のバージョン 1.5.0 が 必要になるのでインストールすること。\n% sudo apt-get update % sudo apt-get install netatalk avahi-daemon avahi-utils libgcrypt11 ディスクのパーティショニング・フォーマットを行うために e2fsprogs, gdisk を入れ る。2TB オーバーな disk が一般的になったので fdisk ではなく gdisk を使う。\n% sudo apt-get install e2fsprogs gdisk パーティショニングを行う。私は1つの大きなパーティションを作りました。その後ファ イルシステム ext4 にてフォーマットを行います。\n% sudo gdisk /dev/sdb # パーティショニング方法は割愛 % sudo mkfs.ext4 /dev/sdb1 /mnt ディレクトリにマウントします。/mnt/storage をファイルサーバ用, /mnt/timemachine を Mac の TimeMachine 用として稼働させるためにディレクトリを 作成します。\n% sudo mount -t ext4 /dev/sdb1 /mnt % sudo vi /etc/fstab # /mnt の記述追加 % sudo mkdir /mnt/storage ; sudo chown \u0026lt;userid\u0026gt;:users /mnt/storage % sudo mkdir /mnt/timemachine ; sudo chown \u0026lt;userid\u0026gt;:users /mnt/timemachine /etc/netatalk/apfd.conf を修正します。\n% diff /etc/netatalk/apfd.conf.org /etc/netatalk/afpd.conf - # - -tcp -noddp -uamlist uams_dhx.so,uams_dhx2.so -nosavepassword + - -tcp -noddp -uamlist uams_guest.so,uams_dhx.so,uams_dhx2.so -nosavepassword TimeMachine 用のディレクトリパスに下記のファイルを touch します。中身は空で OK です。\n% touch /mnt/timemachine/.com.apple.timemachine.supported /etc/netatalk/AppleVolumes.default ファイルにファイルサーバ, TimeMachine 用の 設定を投入します。TimeMachine 用は下記の通りオプションが必要になります。\n% sudo vi /etc/netatalk/AppleVolumes.default # 下記を追記 /mnt/storage \u0026#34;Storage\u0026#34; /mnt/timemachine \u0026#34;TimeMachine\u0026#34; cnidscheme:dbd options:usedots,upriv,tm netatalk を再起動します。\n% sudo /etc/init.d/netatalk restart TimeMachine を設定する Mac 側の設定 今回の様に Apple 公式の TimeCupsule 以外のマシンを TimeMachine のバックアップ 先として利用する場合、Mac 側で下記の設定が必要になります。\n% defaults write com.apple.systempreferences TMShowUnsupportedNetworkVolumes 1 あとは \u0026lsquo;環境設定\u0026rsquo; -\u0026gt; \u0026lsquo;TimeMachine\u0026rsquo; にて Vyatta を TimeMachine のバックアップ先 に指定すれば OK です。\nまとめ パッケージレポジトリは Wheezy の main, contrib を設定しましたが他のモノを入れ ることも勿論可能。最近の Netatalk は TimeMachine 用の設定が簡単にできるように なっていました。\n","permalink":"https://jedipunkz.github.io/post/2013/11/26/vyatta-timemachine-netatalk/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003e自宅ルータを Vyatta で運用しているのですが、諸電力な筐体に交換した際に HDD ス\nロットが余っていたので HDD を一本さしてみました。もったいないので Netatalk を\nインストールして Mac 用の TimeMachine サーバにするか！そんでもってファイルサー\nバ兼務にしよう！と思い立って作業したら簡単に出来たので共有します。\u003c/p\u003e\n\u003cp\u003eVyatta はご存知の通り Debian Gnu/Linux がベースになっているのでパッケージレポ\nジトリを追加してちょちょいで設定出来ます。\u003c/p\u003e\n\u003ch2 id=\"手順\"\u003e手順\u003c/h2\u003e\n\u003cp\u003e電源を通して Disk を追加します。その後起動。私は 3TB Disk が余っていたのでそれ\nを挿しました。\u003c/p\u003e\n\u003cp\u003edebian wheezy のパッケージレポジトリを Vyatta に追記します。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e% configure\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# set system package repository debian url http://ftp.jp.debian.org/debian\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# set system package repository debian distribution wheezy\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# set system package repository debian components \u0026#34;main contrib\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# commit\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# save\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# exit \u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003enetatalk, avahi をインストールする。その際に libgcrypt11 のバージョン 1.5.0 が\n必要になるのでインストールすること。\u003c/p\u003e","title":"Vyatta で Mac 用 TimeMachine サーバ兼ファイルサーバを構築！"},{"content":"こんにちは。@jedipunkz です。\n毎度お馴染みになった OpenStack の Chef によるデプロイですが、今回は OpenStack Havana 構成を Chef でデプロイする方法についてです。使用するのは今回も rcbops/chef-cookbooks です。ブランチは \u0026lsquo;havana\u0026rsquo; を用います。\n早速ですが構成について。4.1.2 辺りからだと思うのですが構成の前提が物理ネットワー ク4つを前提にし始めました。public, external (VM) を別ける必要が出てきました。 通信の特性も異なるので (public は public API を。external は VM 用) 、別けるの が得策かもしれません。\n構成 +--------------+------------------------------------------------------- external | | +--------------+--(-----------+--(-----------+--------------+---------------------------- public | | | | | | | +------------+ +------------+ +------------+ +------------+ +------------+ +------------+ | controller | | network | | network | | compute | | compute | | workstation| +------------+ +------------+ +------------+ +------------+ +------------+ +------------+ | | | | | | | | | | +--------------+--(-----------+--(-----------+--(-----------+--(-----------+------------- management | | | | +--------------+--------------+--------------+------------------------- guest 上記の構成の特徴 4つの物理ネットワークを前提 public ネットワーク : 外部 API 用ネットワーク external ネットワーク : インスタンス外部接続用ネットワーク guest ネットワーク : インスタンス内部用ネットワーク management ネットワーク : 各コンポーネント接続用ネットワーク public, external のみグローバルネットワーク controller : 2 nics, network : 4 nics, compute : 3nics の構成 controller はシングル構成 network ノードは台数拡張可能, agent 単位でノード間移動可能 compute ノードも台数拡張可能 workstation は chef-repo の所在地, management ネットワークに所属 各ノードの準備 OS インストール後、各ノードのネットワークインターフェースの設定を下記の通り行っ てください。また LVM を使うのであれば cinder ボリュームの設定も必要になってきます。\ncontroller ノード 2 nics を下記の通り設定します。\neth0 を public ネットワークに。gateway をこのインターフェースに設定 eth1 を guest ネットワークに。gateway はなし /etc/network/interfaces の例\nauto eth0 iface eth0 inet static address \u0026lt;ip_addr\u0026gt; netmask \u0026lt;netmask\u0026gt; gateway \u0026lt;gateway\u0026gt; dns-nameservers \u0026lt;dns_cache_server\u0026gt; dns-search \u0026lt;domain\u0026gt; auto eth1 iface eth1 inet static address \u0026lt;ip_addr\u0026gt; netmask \u0026lt;netmask\u0026gt; /dev/sdb を cinder 用ディスクデバイスとします。\n% sudo pvcreate /dev/sdb % sudo vgcreate cinder-volumes /dev/sdb network ノード 4 nics を下記の通り設定します。up route で仮想ネットワークへのルーティングを書 いてあげると、network ノードから直接仮想ネットワーク上のインスタンスへ通信する ことが可能です。最初は書かなくても OK です。\nauto eth0 iface eth0 inet static address \u0026lt;ip_addr\u0026gt; netmask \u0026lt;netmask\u0026gt; dns-nameservers \u0026lt;dns_cache_server\u0026gt; dns-search \u0026lt;domain\u0026gt; auto eth1 iface eth1 inet static address \u0026lt;ip_addr\u0026gt; netmask \u0026lt;netmask\u0026gt; auto eth2 iface eth2 inet static address \u0026lt;ip_addr\u0026gt; netmask \u0026lt;netmask\u0026gt; gateway 10.200.10.1 dns-nameservers 8.8.8.8 8.8.4.4 dns-search cpi.ad.jp auto eth3 iface eth3 inet static address \u0026lt;ip_addr\u0026gt; netmask \u0026lt;netmask\u0026gt; # 仮想ネットワークへのルーティング # up route add -net \u0026lt;virtual_net_cidr\u0026gt; gw \u0026lt;neutron_gw\u0026gt; # up route add -net \u0026lt;virtual_net_cidr\u0026gt; gw \u0026lt;neutron_gw\u0026gt; compute ノード 3 nics を下記の通り設定します。\nauto eth0 iface eth0 inet static address \u0026lt;ip_addr\u0026gt; netmask \u0026lt;netmask\u0026gt; dns-nameservers \u0026lt;dns_cache_server\u0026gt; dns-search \u0026lt;domain\u0026gt; auto eth1 iface eth1 inet static address \u0026lt;ip_addr\u0026gt; netmask \u0026lt;netmask\u0026gt; auto eth2 iface eth2 inet static address \u0026lt;ip_addr\u0026gt; netmask \u0026lt;netmask\u0026gt; gateway \u0026lt;gateway\u0026gt; dns-nameservers \u0026lt;dns_cache_server\u0026gt; dns-search \u0026lt;domain\u0026gt; Cookbooks, Roles, Environments 等の準備 下記の操作は全て workstation ノードからの操作です。 また chef のインストールや chef サーバの構築方法については割愛します。\nrcbops/chef-cookbooks を取得します。\n% git clone https://github.com/rcbops/chef-cookbooks.git % cd chef-cookbooks % git checkout -b havana remotes/origin/havana % # .chef 配下の準備割愛。各 Chef サーバ環境に合わせる % git submodule init % git submodule sync % git submodule update % knife cookbook upload -o cookbooks -a % knife role from file roles/*.rb 今回の構成用の environment を生成します。それぞれの環境に合わせて作成してくだ さい。生成のコツは各 Cookbooks の attributes を見ながら設定することです。 生成した json ファイルを environments ディレクトリ配下に配置します。\n{ \u0026#34;name\u0026#34;: \u0026#34;havana-neutron\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;cookbook_versions\u0026#34;: { }, \u0026#34;json_class\u0026#34;: \u0026#34;Chef::Environment\u0026#34;, \u0026#34;chef_type\u0026#34;: \u0026#34;environment\u0026#34;, \u0026#34;default_attributes\u0026#34;: { }, \u0026#34;override_attributes\u0026#34;: { \u0026#34;package_component\u0026#34;: \u0026#34;havana\u0026#34;, \u0026#34;osops_networks\u0026#34;: { \u0026#34;management\u0026#34;: \u0026#34;10.200.10.0/24\u0026#34;, \u0026#34;public\u0026#34;: \u0026#34;10.200.9.0/24\u0026#34;, \u0026#34;nova\u0026#34;: \u0026#34;10.200.10.0/24\u0026#34; }, \u0026#34;nova\u0026#34;: { \u0026#34;config\u0026#34;: { \u0026#34;use_single_default_gateway\u0026#34;: false, \u0026#34;ram_allocation_ratio\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;cpu_allocation_ratio\u0026#34;: \u0026#34;16\u0026#34; }, \u0026#34;network\u0026#34;: { \u0026#34;provider\u0026#34;: \u0026#34;neutron\u0026#34;, \u0026#34;network_type\u0026#34;: \u0026#34;vlan\u0026#34; }, \u0026#34;apply_patches\u0026#34;: true, \u0026#34;libvirt\u0026#34;: { \u0026#34;vncserver_listen\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;virt_type\u0026#34;: \u0026#34;kvm\u0026#34; }, \u0026#34;db\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;nova\u0026#34; }, \u0026#34;services\u0026#34;: { \u0026#34;novnc-proxy\u0026#34;: { \u0026#34;network\u0026#34;: \u0026#34;public\u0026#34; }, \u0026#34;ec2-admin\u0026#34;: { \u0026#34;network\u0026#34;: \u0026#34;management\u0026#34; } } }, \u0026#34;cinder\u0026#34;: { \u0026#34;db\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;cinder\u0026#34; } }, \u0026#34;keystone\u0026#34;: { \u0026#34;admin_user\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;tenants\u0026#34;: [ \u0026#34;admin\u0026#34;, \u0026#34;service\u0026#34; ], \u0026#34;users\u0026#34;: { \u0026#34;admin\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;secrete\u0026#34;, \u0026#34;roles\u0026#34;: { \u0026#34;admin\u0026#34;: [ \u0026#34;admin\u0026#34; ] } }, \u0026#34;demo\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;demo\u0026#34;, \u0026#34;default_tenant\u0026#34; : \u0026#34;service\u0026#34;, \u0026#34;roles\u0026#34;: { \u0026#34;service\u0026#34;: [ \u0026#34;service\u0026#34; ] } } }, \u0026#34;db\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;keystone\u0026#34; } }, \u0026#34;horizon\u0026#34;: { \u0026#34;theme\u0026#34;: \u0026#34;Rackspace\u0026#34;, \u0026#34;db\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;horizon\u0026#34; }, \u0026#34;endpoint_type\u0026#34; : \u0026#34;publicURL\u0026#34;, \u0026#34;endpoint_scheme\u0026#34; : \u0026#34;http\u0026#34; }, \u0026#34;mysql\u0026#34;: { \u0026#34;root_network_acl\u0026#34;: \u0026#34;%\u0026#34;, \u0026#34;allow_remote_root\u0026#34;: true, \u0026#34;server_root_password\u0026#34;: \u0026#34;secrete\u0026#34;, \u0026#34;server_repl_password\u0026#34;: \u0026#34;secrete\u0026#34;, \u0026#34;server_debian_password\u0026#34;: \u0026#34;secrete\u0026#34; }, \u0026#34;monitoring\u0026#34;: { \u0026#34;procmon_provider\u0026#34;: \u0026#34;monit\u0026#34;, \u0026#34;metric_provider\u0026#34;: \u0026#34;collectd\u0026#34; }, \u0026#34;glance\u0026#34;: { \u0026#34;images\u0026#34;: [ \u0026#34;precise\u0026#34;, \u0026#34;cirros\u0026#34; ], \u0026#34;image\u0026#34;: { }, \u0026#34;image_upload\u0026#34;: false, \u0026#34;db\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;glance\u0026#34; } }, \u0026#34;neutron\u0026#34;: { \u0026#34;service_pass\u0026#34;: \u0026#34;neutron\u0026#34;, \u0026#34;db\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;neutron\u0026#34; } }, \u0026#34;developer_mode\u0026#34;: false } } environment ファイルの chef サーバへのアップロード。\n% knife environment from file environments/havana-neutron.json knife bootstrap によるデプロイ 下記の通り knife bootstrap することで各ノードをデプロイします。\ncontroller ノードのデプロイ。\n% knife bootstrap \u0026lt;controller_ipaddr\u0026gt; -N \u0026lt;controller_name\u0026gt; \\ -r \u0026#39;role[single-controller]\u0026#39;,\u0026#39;role[cinder-volume]\u0026#39; \\ -E havana-neutron -x \u0026lt;username\u0026gt; --sudo network ノードのデプロイ。台数分デプロイしてください。\n% knife bootstrap \u0026lt;network_ipaddr\u0026gt; -N \u0026lt;network_name\u0026gt; \\ -r \u0026#39;role[single-network-node]\u0026#39;,\u0026#39;recipe[nova-network::neutron-l3-agent]\u0026#39; \\ -E neutron-havana -x \u0026lt;username\u0026gt; --sudo compute ノードのデプロイ。台数分デプロイしてください。\n% knife bootstrap \u0026lt;compute_ipaddr\u0026gt; -N \u0026lt;compute_name\u0026gt; \\ -r \u0026#39;role[single-compute]\u0026#39; \\ -E havana-neutron -x \u0026lt;username\u0026gt; --sudo openvswitch の物理 NIC とブリッジインタフェースマッピング作業 各ノードで下記の通り物理 NIC とブリッジのマッピング作業を行ってください。操作 は必ず management ネットワークを介して行うようにしましょう。その他のネットワー クから操作すると通信が途絶える可能性があります。\nnetwork ノード\n% sudo ovs-vsctl add-port br-eth1 eth1 % sudo ovs-vsctl add-port br-ex eth3 compute ノード\n% sudo ovs-vsctl add-port br-eth1 eth1 まとめと考察 havana の roles にはコアプロジェクトのコンポーネントが入っています。つまり ceilometer と heat も上記のデプロイで入ってきます。ceilometer に関して公式のド キュメントではデータ格納用 DB として mongodb が記されていますが、ここでは mysqld 上に格納する構成になっていました。これは個人的には非常に助かります。 また、compute ノードから controller ノードの public api を叩く必要が出てきて、 compute ノードにも public ネットワーク用の NIC を足しましたが、こうするべきな のかどうかは今後考えてみます。理想は management ネットワークを介することですが attributes の調整でけではうまくいきませんでした。\u0026lsquo;havana\u0026rsquo; ブランチはまだ開発が 進んでいるので改善されるかもしれません。コントリビュートするのも手だと思います。\n","permalink":"https://jedipunkz.github.io/post/2013/11/17/openstack-havana-chef-deploy/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003e毎度お馴染みになった OpenStack の Chef によるデプロイですが、今回は OpenStack\nHavana 構成を Chef でデプロイする方法についてです。使用するのは今回も\nrcbops/chef-cookbooks です。ブランチは \u0026lsquo;havana\u0026rsquo; を用います。\u003c/p\u003e\n\u003cp\u003e早速ですが構成について。4.1.2 辺りからだと思うのですが構成の前提が物理ネットワー\nク4つを前提にし始めました。public, external (VM) を別ける必要が出てきました。\n通信の特性も異なるので (public は public API を。external は VM 用) 、別けるの\nが得策かもしれません。\u003c/p\u003e\n\u003ch2 id=\"構成\"\u003e構成\u003c/h2\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e                  +--------------+------------------------------------------------------- external\n                  |              |\n+--------------+--(-----------+--(-----------+--------------+---------------------------- public\n|              |  |           |  |           |              |\n+------------+ +------------+ +------------+ +------------+ +------------+ +------------+\n| controller | |  network   | |  network   | |  compute   | |  compute   | | workstation|\n+------------+ +------------+ +------------+ +------------+ +------------+ +------------+\n|              |  |           |  |           |  |           |  |           |\n+--------------+--(-----------+--(-----------+--(-----------+--(-----------+------------- management\n                  |              |              |              |\n                  +--------------+--------------+--------------+------------------------- guest\n\u003c/code\u003e\u003c/pre\u003e\u003ch4 id=\"上記の構成の特徴\"\u003e上記の構成の特徴\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003e4つの物理ネットワークを前提\u003c/li\u003e\n\u003cli\u003epublic ネットワーク : 外部 API 用ネットワーク\u003c/li\u003e\n\u003cli\u003eexternal ネットワーク : インスタンス外部接続用ネットワーク\u003c/li\u003e\n\u003cli\u003eguest ネットワーク : インスタンス内部用ネットワーク\u003c/li\u003e\n\u003cli\u003emanagement ネットワーク : 各コンポーネント接続用ネットワーク\u003c/li\u003e\n\u003cli\u003epublic, external のみグローバルネットワーク\u003c/li\u003e\n\u003cli\u003econtroller : 2 nics, network : 4 nics, compute : 3nics の構成\u003c/li\u003e\n\u003cli\u003econtroller はシングル構成\u003c/li\u003e\n\u003cli\u003enetwork ノードは台数拡張可能, agent 単位でノード間移動可能\u003c/li\u003e\n\u003cli\u003ecompute ノードも台数拡張可能\u003c/li\u003e\n\u003cli\u003eworkstation は chef-repo の所在地, management ネットワークに所属\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"各ノードの準備\"\u003e各ノードの準備\u003c/h2\u003e\n\u003cp\u003eOS インストール後、各ノードのネットワークインターフェースの設定を下記の通り行っ\nてください。また LVM を使うのであれば cinder ボリュームの設定も必要になってきます。\u003c/p\u003e","title":"OpenStack Havana を Chef でデプロイ"},{"content":"こんにちは。@jedipunkz です。\n第2回 Elasticsearch 勉強会に参加してきました。箇条書きですが参加レポートを記し ておきます。\n開催日 : 2013/11/12 場所 : 東京駅 グラントウキョウサウスタワー リクルートテクノロジーズさま URL : http://elasticsearch.doorkeeper.jp/events/6532 Routing 周りの話 株式会社シーマーク　大谷純さん (@johtani) Index 構成 cluster の中に index -\u0026gt; type が作成される index は shard という部分的な index の集まり shard 数は生成時のみ指定可能 node ごとに replica, primary を別ける replica 数は後に変えられる doc -\u0026gt; hash 値を shard 数で割って replica, primary に登録 doc の id の ハッシュ値を利用 type も含める場合はかの設定を true に クライアントはどのノードに対してクエリを投げても OK routing id の代わりに routing (URL パラメータ) で登録 url リクエストパラメータとして登録時にルーティングパラメータを登録 id の代わりにパラメータで指定された値のハッシュ値を計算して利用 検索時 routing 指定で関係のある shard のみを指定出来る スケールアウト sharding によるスケールアウト数 = インデックス作成時に指定 shard によるインデックスの分割以外にインデックス自体を複数持つことによるスケール 複数のドキュメントをエイリアス書けることが可能 所感 個人的には非常に興味のあるところでした。mongodb のような sharding をイメージし てよいのか？そうでないのか？すら理解出来ていなかったので。sharding を理解する 前提知識の話もあって非常に参考になりました。\nElasticSearchを使ったBaaS基盤の開発 株式会社富士通ソフトウェアテクノロジーズ 滝田聖己さん（@pisatoshi） 運用の話。これは貴重\u0026hellip;。\nshard 数 : 10 , replica : 1 で運用している データは業務データ , トラッキングデータ マルチテナント, 更新の即時反映, ルーティングによる性能向上 が要件 登録更新 -\u0026gt; 検索結果反映までタイムラグがある -\u0026gt; replica 完了まで待つので高コスト replica 数を減らすことで性能向上 routing id を指定することで\u0026hellip; doc id の has から shard 自動選択がデフォルト -\u0026gt; hash key を指定して格納シャードを制御出来る -\u0026gt; doc 登録性能向上 -\u0026gt; 検索対象の shard を絞りこみえる -\u0026gt; 不可を軽減 バックアップ/リストア mysql にもデータ格納 -\u0026gt; backup elasticsearch の index はバックアップ取らず -\u0026gt; bug を踏みたくないのが理由 dynamic mapping の問題 入力データから型を推測 -\u0026gt; 自動マッピング登録 -\u0026gt; マッピング定義が肥大化 -\u0026gt; データ型のコンフリクト mapping の肥大化 mapping 定義は type, field の数に応じてサイズが増加 -\u0026gt; index 性能低下 -\u0026gt; mapping を伴う doc 登録 3sec (mapping size 80MB) -\u0026gt; 各 node への mapping 定義の同期 大量のデータを一気に登録するときは 1 node が速い -\u0026gt; その後シャード再配置したことがある =\u0026gt; dynamic mapping を使うのをやめた -\u0026gt; app で指定\nよく利用するツール bigdesk elasticsearch-head sense (chrome extention) unit test unit test はどうする？\nnodeClient テスト開始時に起動 テストデータを配置して起動, 終了時に削除 memoryIndex で高速に実行 elasticsearch-test が便利 所感 運用の話はどの技術でも重要!!! 実際に困った話、トラブル等聞けて貴重な時間でした。 よく使うツールの話も意外と参考になるので聞けてよかった。\nKibana Cookpad 水戸祐介さん (@y_310) 作った dashboard は elasticsearch に保存される db 要らず , web サーバのみ, クライアントサイドの技術のみで実装されている term, trends, map, table, column\u0026hellip; それぞれ図形式がある title = \u0026ldquo;test\u0026rdquo;, -title 1つの index に異なるスキーマを持つデータを入れられる 1つの index に入れることでグラフを重ねて比較出来る やはり dynamic mapping は使わないほうが良い m1.large x 1 : 1日のインデックスサイズが10GB 超えるあたりで ES が詰まる 下記が当日の資料です。\n所感 Cookpad さんで実際に利用されているとか。運用の話と同じくやはり dynamic routing でのトラブルの話があった。また Cookpad さんでのトラブル例の話もあってよかった。\nElasticSearch＋Kibana v3を利用する際の運用ノウハウ 株式会社リブセンス Y.Kentaro さん (@yoshi_ken) 下記が発表資料。\nElasticSearch+Kibanaでログデータの検索と視覚化を実 現するテクニックと運用ノウハウ from Kentaro Yoshida nginx の basic 認証等でアクセス制限を掛ける例 等など\u0026hellip; ごめんなさい！メモが取れてなかった！\u0026hellip;汗\nFluentd as a Kibana @repeatedly さん fluentd-plugin-kibana-server というkibana をダウンロードしてくるのがめんどい ので fluentd の中で kibana を動かすプラグインを作った話。下記が発表資料。\nhttps://gist.github.com/repeatedly/7427856\nfluentd input plugin として実装 logstash 版もあるらしい 所感 さすが\u0026hellip;。確かにサーバをいちいち作るのめんどいです。fluentd plugin の中で kibana を動かすって発想が\u0026hellip;。\nauth プラグインでアクセスコントロール 株式会社エヌツーエスエム 菅谷信介さん (@shinsuke_sugaya) elasticsearch のアクセス制御をしたい rest api をアクセス制御する ユーザ管理, REST API のアクセス管理, ログイン・ログアウト・トーケン ユーザ管理は elasticsearch 内に格納 path, http method, role の組み合わせで制御 インストールは下記の通り elasticsearch plugin コマンドで。\n% ./bin/plugin --install org.codelibs/elasticsearch-auth/1.0.0 ユーザ作成は POST で可能 所感とまとめ 箇条書きのざっくりしたまとめでしたが\u0026hellip;。個人的には sharding の話はとても聞き たかったので貴重な時間でした。また、運用の話もとてもおもしろかったし plugin 作っ てみた話も「すごい..」って感じでした。僕はこれからアプリも書くけどインフラエン ジニアなのでアーキテクチャをまず知りたいかなぁという印象。と言うかスケールさせ るときにも横に並べて shard するだけの様な話に聞こえましたが、まだぼんやり感が あります。特に shard 周り。mongo の sharding と同じ様なものを想像していますが。 またモチベーション上がったので週末やってみますー！\nkibana3 は一回使いました。javascript, html で実装されているのでクライアントか ら elasticsearch まで直接アクセス出来なければならないとなって、構成的にどうな のかな？と思っていたら菅谷さんの \u0026lsquo;auth プラグイン\u0026rsquo; の話があったりしたので、良 かったです。使ってみます！\n主催の方々、発表者の方々、当日はありがとうございましたー。また参加させてもらい ます。\n","permalink":"https://jedipunkz.github.io/post/2013/11/13/elasticsearch-second-study-report/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003e第2回 Elasticsearch 勉強会に参加してきました。箇条書きですが参加レポートを記し\nておきます。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e開催日 : 2013/11/12\n場所 : 東京駅 グラントウキョウサウスタワー リクルートテクノロジーズさま\nURL : http://elasticsearch.doorkeeper.jp/events/6532\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id=\"routing-周りの話\"\u003eRouting 周りの話\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003e株式会社シーマーク　大谷純さん (@johtani)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4 id=\"index-構成\"\u003eIndex 構成\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003ecluster の中に index -\u0026gt; type が作成される\u003c/li\u003e\n\u003cli\u003eindex は shard という部分的な index の集まり\u003c/li\u003e\n\u003cli\u003eshard 数は生成時のみ指定可能\u003c/li\u003e\n\u003cli\u003enode ごとに replica, primary を別ける\u003c/li\u003e\n\u003cli\u003ereplica 数は後に変えられる\u003c/li\u003e\n\u003cli\u003edoc -\u0026gt; hash 値を shard 数で割って replica, primary に登録\u003c/li\u003e\n\u003cli\u003edoc の id の ハッシュ値を利用\u003c/li\u003e\n\u003cli\u003etype も含める場合はかの設定を true に\u003c/li\u003e\n\u003cli\u003eクライアントはどのノードに対してクエリを投げても OK\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"routing\"\u003erouting\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eid の代わりに routing (URL パラメータ) で登録\u003c/li\u003e\n\u003cli\u003eurl リクエストパラメータとして登録時にルーティングパラメータを登録\u003c/li\u003e\n\u003cli\u003eid の代わりにパラメータで指定された値のハッシュ値を計算して利用\u003c/li\u003e\n\u003cli\u003e検索時 routing 指定で関係のある shard のみを指定出来る\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"スケールアウト\"\u003eスケールアウト\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003esharding によるスケールアウト数 = インデックス作成時に指定\u003c/li\u003e\n\u003cli\u003eshard によるインデックスの分割以外にインデックス自体を複数持つことによるスケール\u003c/li\u003e\n\u003cli\u003e複数のドキュメントをエイリアス書けることが可能\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"所感\"\u003e所感\u003c/h4\u003e\n\u003cp\u003e個人的には非常に興味のあるところでした。mongodb のような sharding をイメージし\nてよいのか？そうでないのか？すら理解出来ていなかったので。sharding を理解する\n前提知識の話もあって非常に参考になりました。\u003c/p\u003e","title":"第2回 Elasticsearch 勉強会参加レポート"},{"content":"こんにちは。@jedipunkz です。\n僕は Chef 使いなのですが、Chef はオーケストレーションまで踏み込んだツールでは ないように思います。せいぜいインテグレーションが出来る程度なのかなぁと。 しかもインテグレーションするにも Cookbooks の工夫が必要です。以前聞いたことの ある話ですが Opscode 社のエンジニア曰く「オーケストレーション等へのアプローチ はそれぞれ好きにやってね」だそうです。\n個人的にオーケストレーションをテーマに色々調べようかと考えているのですが、 Serf という面白いツールが出てきました。\u0026lsquo;Serf\u0026rsquo; はオーケストレーションを手助けし てくれるシンプルなツールになっています。\nもう既にいろんな方が Serf について調べていますが、どのような動きをするのかを自 分なりに理解した点を記しておこうと思います。\n参考にしたサイト 公式サイト http://www.serfdom.io/ クラスメソッド開発者ブログ http://dev.classmethod.jp/cloud/aws/serf_on_ec2/ Glidenote さん http://blog.glidenote.com/blog/2013/10/30/serf-haproxy/ Serf とは Serf は gossip protocol をクラスタにブロードキャストする。gossip protocol は SWIM : Scalable Weakly-consistent Infecton-style process Group Membership Protocol” をベースとして形成されている。\nSWIM Protocol 概略 serf は新しいクラスタとして稼働するか、既存のクラスタに ‘join’ する形で稼働 するかのどちらかで起動する。\n新しいメンバは TCP で状態を \u0026lsquo;full state sync\u0026rsquo; され既存のクラスタ内にて ‘gossipin (噂)される。この ’gosiping’ は UDP で通信されこれはネットワーク使 用量はノード数に比例することになる。\nランダムなノードとの \u0026lsquo;full state sync\u0026rsquo; は TCP で行われるけどこれは ‘gossiping’ に比べて少い\nある一定期間、あるノードが fails 状態の場合、迂回経路を使ってそのノードに対し てチェックを行う。両方の経路にて fails な場合ノードが ‘suspiciou\u0026rsquo; (容疑者) 状態になる。もし迂回経路のチェックが fails しなかった場合ネットワーク障害として 判断される。\nSerf の SWIM Protocol からの改修点 Serf は SWIM Protocol をベースにしていると記しましたが、どのような点を改修した のかまとめました。\n\u0026lsquo;full state sync\u0026rsquo; の TCP 化 serf は ‘full state sync’ を TCP にて行う。これにより serf はネットワーセグ メントが分離されている状態でも通信出来る。\ngossip レイヤの分離 serf は failure detection protocol から完全に gossip レイヤを分離。これでより 高速にデータ増殖が行える。\ndead node の扱い serf は デッドノードに対して回数を記憶している。これにより full state sync が 実施された場合、逆に死んだノードの情報を受取る。SWIM では full state sync を行 わないので死んだノードにを削除してしまう。これはクラスタに対する情報一点集中化 に役立つ。\n使ってみる 早速使ってみます。下記の URL にある demo 用スクリプトを少し改修して使ってみま した。\nhttps://github.com/hashicorp/serf/tree/master/demo/web-load-balancer\n2つノードを立ち上げる AWS でも OpenStack でもベアメタルでも何でも良いのでノードを2台用意する。\n下記のスクリプトを置いて両ノードで実行する。 #!/bin/sh set -e export SERF_ROLE=role01 sudo apt-get install -y unzip # Download and install Serf cd /tmp until wget -O serf.zip https://dl.bintray.com/mitchellh/serf/0.2.1_linux_amd64.zip; do sleep 1 done unzip serf.zip sudo mv serf /usr/local/bin/serf # The member join script is invoked when a member joins the Serf cluster. # Our join script simply adds the node to the load balancer. cat \u0026lt;\u0026lt;EOF \u0026gt;/tmp/join.sh #!/bin/sh echo \u0026#39;member joined\u0026#39; \u0026gt;\u0026gt; /tmp/serf_join.log EOF sudo mv /tmp/join.sh /usr/local/bin/serf_member_join.sh chmod +x /usr/local/bin/serf_member_join.sh # Configure the agent cat \u0026lt;\u0026lt;EOF \u0026gt;/tmp/agent.conf description \u0026#34;Serf agent\u0026#34; start on runlevel [2345] stop on runlevel [!2345] exec /usr/local/bin/serf agent \\\\ -event-handler \u0026#34;member-join=/usr/local/bin/serf_member_join.sh\u0026#34; \\\\ -role=${SERF_ROLE} \u0026gt;\u0026gt;/var/log/serf.log 2\u0026gt;\u0026amp;1 EOF sudo mv /tmp/agent.conf /etc/init/serf.conf # Start the agent! sudo start serf 実行すると \u0026lsquo;role01\u0026rsquo; という Role 名で serf agent が稼働しているはず。またスクリ プトを見てわかると思うが \u0026ndash;event-handler \u0026ldquo;member-join=\u0026lt;スクリプト\u0026gt;\u0026rdquo; としている。 これでクラスタに新しいメンバが join すると /tmp/serf_join.log に \u0026lsquo;member joined\u0026rsquo; というメッセージが出力されるはずだ。実際に実行してみる。\n% sudo serf join \u0026lt;もう片系ノードの IP\u0026gt; % sudo serf members vm01 10.0.2.1 alive role01 vm02 10.0.2.3 alive role01 % tail /tmp/serf_join.log ‘member joined ‘member joined イベントハンドラにはユーザ指定のものも扱える。\nuser:deploy=foo.sh この場合のハンドラは下記のコマンドで発生出来る。\n% serf event deploy コンフィギュレーションファイル 今回は init 起動スクリプト内でイベントハンドラ発生時のスクリプト指定等を行った が、json 形式のコンフィギュレーションファイルにて記述することも可能。\n{ \u0026#34;role\u0026#34;: \u0026#34;load-balancer\u0026#34;, \u0026#34;event_handlers\u0026#34;: [ \u0026#34;member_join.sh\u0026#34;, \u0026#34;user:deploy=foo.sh\u0026#34; ] } 上記ファイルを serf.conf とした場合、このコンフィグの指定は \u0026ndash;config-file=serf.conf で行える。\nロードマップ 最後に Serf の今後のロードマップについて記してあったので、まとめてみた。\nコンフィギュレーションファイル よりカスタマイズ可能なコンフィギュレーションファイルを扱えるようにする。\nSIGHUP SIGHUP 信号でリロード出来ないので、これに対応する。\nイベントハンドラライブラリ イベントハンドラを自作・シェアを容易に行えるようプラグイン化を進める。\nまとめと考察 冒頭にオーケストレーションについて触れましたが、このツールを使うだけでは自分の 考えているオーケストレーションにはならないと思いました。当たり前ですね。。複数 ノードを束ねて構成を形成する、またそれをトリガするのが人間であればそれはインテ グレーションの範囲かなぁと。その先にオーケストレーションがあるとすればトリガ自 身もソフトウェアにさせる必要があるのでそのアルゴリズムを人間が書く必要があるの かなぁと。もちろんそのためにこのSerf は貴重なパーツとなってくれそう。\nあと、ロードマップにも記しましたが開発が盛んにされているようなので今後が楽しみ です。\n","permalink":"https://jedipunkz.github.io/post/2013/11/10/serf/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003e僕は Chef 使いなのですが、Chef はオーケストレーションまで踏み込んだツールでは\nないように思います。せいぜいインテグレーションが出来る程度なのかなぁと。\nしかもインテグレーションするにも Cookbooks の工夫が必要です。以前聞いたことの\nある話ですが Opscode 社のエンジニア曰く「オーケストレーション等へのアプローチ\nはそれぞれ好きにやってね」だそうです。\u003c/p\u003e\n\u003cp\u003e個人的にオーケストレーションをテーマに色々調べようかと考えているのですが、\nSerf という面白いツールが出てきました。\u0026lsquo;Serf\u0026rsquo; はオーケストレーションを手助けし\nてくれるシンプルなツールになっています。\u003c/p\u003e\n\u003cp\u003eもう既にいろんな方が Serf について調べていますが、どのような動きをするのかを自\n分なりに理解した点を記しておこうと思います。\u003c/p\u003e\n\u003ch2 id=\"参考にしたサイト\"\u003e参考にしたサイト\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e公式サイト \u003ca href=\"http://www.serfdom.io/\"\u003ehttp://www.serfdom.io/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eクラスメソッド開発者ブログ \u003ca href=\"http://dev.classmethod.jp/cloud/aws/serf_on_ec2/\"\u003ehttp://dev.classmethod.jp/cloud/aws/serf_on_ec2/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eGlidenote さん \u003ca href=\"http://blog.glidenote.com/blog/2013/10/30/serf-haproxy/\"\u003ehttp://blog.glidenote.com/blog/2013/10/30/serf-haproxy/\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"serf-とは\"\u003eSerf とは\u003c/h2\u003e\n\u003cp\u003eSerf は gossip protocol をクラスタにブロードキャストする。gossip protocol は\nSWIM : Scalable Weakly-consistent Infecton-style process Group Membership\nProtocol” をベースとして形成されている。\u003c/p\u003e\n\u003ch2 id=\"swim-protocol-概略\"\u003eSWIM Protocol 概略\u003c/h2\u003e\n\u003cp\u003eserf は新しいクラスタとして稼働するか、既存のクラスタに ‘join’ する形で稼働\nするかのどちらかで起動する。\u003c/p\u003e\n\u003cp\u003e新しいメンバは TCP で状態を \u0026lsquo;full state sync\u0026rsquo; され既存のクラスタ内にて\n‘gossipin (噂)される。この ’gosiping’ は UDP で通信されこれはネットワーク使\n用量はノード数に比例することになる。\u003c/p\u003e\n\u003cp\u003eランダムなノードとの \u0026lsquo;full state sync\u0026rsquo; は TCP で行われるけどこれは\n‘gossiping’ に比べて少い\u003c/p\u003e","title":"Serf を使ってみた"},{"content":"こんにちは。@jedipunkz です。\n以前、\u0026ldquo;Swift HA 構成を Chef でデプロイ\u0026rdquo; というタイトルで記事を書きました。\nhttp://jedipunkz.github.io/blog/2013/07/26/swift-ha-chef-deploy/\nこちらですが、Swift-Proxy, MySQL, Keystone をそれぞれ haproxy, keepalived で HA 組みました。ですが、これは実用的なのかどうか自分でずっと考えていました。\nMySQL と KeepAlived はできればシングル構成にしたいのと、Swift-Proxy は HA で組 みたい。MySQL は Master/Master レプリケーション構成になり、どちらかのノードが 障害を起こし万が一復旧が難しくなった時、構築し直しがしんどくなります。かと言っ て Swift-Proxy をシングル構成にすると今度はノード追加・削除の作業時にサービス 断が発生します。Swift-Proxy を再起動書ける必要があるからです。なので Swift-Proxy は引き続き HA 構成にしたい。\nもう一点、見直したいと思っていました。\n日経コンピュータから出版されている \u0026ldquo;仮想化大全 2014\u0026rdquo; の記事を読んでいて 気がついたのですが。Swift には下記の通りそれぞれのサーバがあります。\nswift-proxy-server swift-account-server swift-container-server swift-object-server Swift には下記のような特徴がある事がわかりました。\nswift-object swift-object は swift-accout, swift-container とは物理リソースの扱いに全く異な る特性を持っています。swift-account, swift-container はクライアントからのリクエ ストに対して \u0026ldquo;アカウントの存在を確認\u0026rdquo;, \u0026ldquo;ACL 情報の確認\u0026rdquo; 等を行うサーバであるの に対して swift-object はストレージ上のオブジェクトをクライアントに提供、または 逆に格納するサーバです。よって、Disk I/O の利用特性として swift-account, container は SSD 等、高スループットの Disk を利用するケースが推奨されるのに対 して swift-object はオブジェクトの実体を格納する必要があるため Disk 容量の大き なストレージを要する。\nよって今回は下記の構成変更をしてより実用的な構成を Chef でデプロイする方法を記 そうと思います。\nswift-object を swift-account, swift-container とノードを分離する MySQL/Keystone の HA を行わず Swift-Proxy のみ HA 化する 参考資料 http://www.rackspace.com/knowledge_center/article/managing-openstack-object-storage-with-chef 仮想化大全 構成 +-----------------+ | load balancer | +-----------------+ | +-------------------+-------------------+-------------------+-------------------+---------------------- proxy network | | | | | +-----------------+ +-----------------+ +-----------------+ +-----------------+ +-----------------+ | chef server | | chef workstation| | swift-mange | | swift-proxy01 | | swift-proxy02 | +-----------------+ +-----------------+ +-----------------+ +-----------------+ +-----------------+ ...\u0026gt; scaling | | | | | +-------------------+-------------------+-------------------+-------------------+-------------------+-- storage network | | | | | | +-----------------+ +-----------------+ +-----------------+ +-----------------+ +-----------------+ +-----------------+ | swift-storage01 | | swift-storage02 | | swift-storage03 | | swift-account01 | | swift-account02 | | swift-account03 | +-----------------+ +-----------------+ +-----------------+ +-----------------+ +-----------------+ +-----------------+ ..\u0026gt; scaling 特徴 load balancer は一般的なハードウェア、ソフトウェアで用意 (今回は割愛) swift-proxy は同等の機能のモノを並列に swift-storage と \u0026lsquo;swift-account, container\u0026rsquo; はそれぞれの zone 毎に並べるため同数 swift-account は swift-account, swift-container が稼働 swift-manage は mysql, keystone, git server を搭載 chef server は外部に配置しても構わないが到達出来る箇所に chef workstation は全ての操作を行う端末。全てのノードに到達出来る箇所に それぞれのノードでの disk の準備 object, account, container 共に OS 領域とは別の disk を必要とするため /dev/sdb 等のディスクを追加し下記の通り gdisk でパーティションを切る。\n% sudo apt-get update; sudo apt-get -y install gdisk % sudo gdisk /dev/sdb # デバイス名は例 対象ノード : swift-object0[1-3], swift-account0[1-3]\nデプロイ準備 下記は全て chef-workstation での操作\nrcpops 管理の chef-repo を取得する。\n% git clone https://github.com/rcbops/chef-cookbooks.git % cd chef-cookbooks 現時点で v4.1.2 がリリースされているので tag を利用して checkout する。\n% git tag -l 1.0.0 release-2011.3-d5 release-v2.0 v2.9.0 v2.9.1 v2.9.2 v2.9.3 v2.9.4 v2.9.5 v2.9.6 v2.9.7 v2.9.8 v3.0.0 v3.0.1 v3.1.0 v4.1.0 v4.1.1 v4.1.2 % git checkout -b v4.1.2 refs/tags/v4.1.2 git submodule 化されている cookbooks を取得する。\n% git submodule init % git submodule sync % git submodule upate cookbooks, roles の chef-server へのアップロードを行う。\n% knife cookbook upload -o cookbook -a % knife role from file role/*.rb 今回の構成一式を収容する chef environment を作成する。\n{ \u0026#34;name\u0026#34;: \u0026#34;swift\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;cookbook_versions\u0026#34;: { }, \u0026#34;json_class\u0026#34;: \u0026#34;Chef::Environment\u0026#34;, \u0026#34;chef_type\u0026#34;: \u0026#34;environment\u0026#34;, \u0026#34;default_attributes\u0026#34;: { }, \u0026#34;override_attributes\u0026#34;: { \u0026#34;package_component\u0026#34;: \u0026#34;grizzly\u0026#34;, \u0026#34;osops_networks\u0026#34;: { \u0026#34;management\u0026#34;: \u0026#34;10.200.9.0/24\u0026#34;, \u0026#34;public\u0026#34;: \u0026#34;10.200.9.0/24\u0026#34;, \u0026#34;nova\u0026#34;: \u0026#34;10.200.9.0/24\u0026#34;, \u0026#34;swift\u0026#34;: \u0026#34;10.200.9.0/24\u0026#34; }, \u0026#34;keystone\u0026#34;: { \u0026#34;admin_user\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;tenants\u0026#34;: [ \u0026#34;admin\u0026#34;, \u0026#34;service\u0026#34; ], \u0026#34;users\u0026#34;: { \u0026#34;admin\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;secrete\u0026#34;, \u0026#34;roles\u0026#34;: { \u0026#34;admin\u0026#34;: [ \u0026#34;admin\u0026#34; ] } }, \u0026#34;demo\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;demo\u0026#34;, \u0026#34;default_tenant\u0026#34; : \u0026#34;service\u0026#34;, \u0026#34;roles\u0026#34;: { \u0026#34;admin\u0026#34;: [ \u0026#34;admin\u0026#34; ] } } }, \u0026#34;db\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;keystone\u0026#34; } }, \u0026#34;mysql\u0026#34;: { \u0026#34;root_network_acl\u0026#34;: \u0026#34;%\u0026#34;, \u0026#34;allow_remote_root\u0026#34;: true, \u0026#34;server_root_password\u0026#34;: \u0026#34;secrete\u0026#34;, \u0026#34;server_repl_password\u0026#34;: \u0026#34;secrete\u0026#34;, \u0026#34;server_debian_password\u0026#34;: \u0026#34;secrete\u0026#34; }, \u0026#34;monitoring\u0026#34;: { \u0026#34;procmon_provider\u0026#34;: \u0026#34;monit\u0026#34;, \u0026#34;metric_provider\u0026#34;: \u0026#34;collectd\u0026#34; }, \u0026#34;vips\u0026#34;: { \u0026#34;keystone-admin-api\u0026#34;: \u0026#34;10.200.9.112\u0026#34;, \u0026#34;keystone-service-api\u0026#34;: \u0026#34;10.200.9.112\u0026#34;, \u0026#34;keystone-internal-api\u0026#34;: \u0026#34;10.200.9.112\u0026#34;, \u0026#34;swift-proxy\u0026#34;: \u0026#34;10.200.9.112\u0026#34;, \u0026#34;config\u0026#34;: { \u0026#34;10.200.9.112\u0026#34;: { \u0026#34;vrid\u0026#34;: 12, \u0026#34;network\u0026#34;: \u0026#34;management\u0026#34; } } }, \u0026#34;developer_mode\u0026#34;: false, \u0026#34;swift\u0026#34;: { \u0026#34;swift_hash\u0026#34;: \u0026#34;107c0568ea84\u0026#34;, \u0026#34;authmode\u0026#34;: \u0026#34;keystone\u0026#34;, \u0026#34;authkey\u0026#34;: \u0026#34;3f281b71-ce89-4b27-a2ad-ad873d3f2760\u0026#34; } } } 作成した environment ファイル environments/swift.json を chef-server へアップ ロードする。\n% knife environment from file environments/swift.json デプロイ かきのとおり knife bootstrap する。\n% knife bootstrap \u0026lt;manage_ip_addr\u0026gt; -N swift-manage -r \u0026#39;role[base]\u0026#39;,\u0026#39;role[mysql-master]\u0026#39;,\u0026#39;role[keystone]\u0026#39;,\u0026#39;role[swift-management-server]\u0026#39; -E swift --sudo -x thirai % knife bootstrap \u0026lt;proxy01_ip_addr\u0026gt; -N swift-proxy01 -r \u0026#34;role[base]\u0026#34;,\u0026#34;role[swift-proxy-server]\u0026#34;,\u0026#39;role[swift-setup]\u0026#39;,\u0026#39;role[openstack-ha]\u0026#39; -E swift --sudo -x thirai % knife bootstrap \u0026lt;proxy02_ip_addr\u0026gt; -N swift-proxy02 -r \u0026#34;role[base]\u0026#34;,\u0026#34;role[swift-proxy-server]\u0026#34;,\u0026#39;role[openstack-ha]\u0026#39; -E swift --sudo -x thirai % knife bootstrap \u0026lt;storage01_ip_addr\u0026gt; -N swift-storage01 -r \u0026#39;role[base]\u0026#39;,\u0026#39;role[swift-object-server]\u0026#39; -E swift --sudo -x thirai % knife bootstrap \u0026lt;storage02_ip_addr\u0026gt; -N swift-storage02 -r \u0026#39;role[base]\u0026#39;,\u0026#39;role[swift-object-server]\u0026#39; -E swift --sudo -x thirai % knife bootstrap \u0026lt;storage03_ip_addr\u0026gt; -N swift-storage03 -r \u0026#39;role[base]\u0026#39;,\u0026#39;role[swift-object-server]\u0026#39; -E swift --sudo -x thirai % knife bootstrap \u0026lt;account01_ip_addr\u0026gt; -N swift-account01 -r \u0026#39;role[base]\u0026#39;,\u0026#39;role[swift-account-server]\u0026#39;,\u0026#39;role[swift-container-server]\u0026#39; -E swift --sudo -x thirai % knife bootstrap \u0026lt;account02_ip_addr\u0026gt; -N swift-account02 -r \u0026#39;role[base]\u0026#39;,\u0026#39;role[swift-account-server]\u0026#39;,\u0026#39;role[swift-container-server]\u0026#39; -E swift --sudo -x thirai % knife bootstrap \u0026lt;account03_ip_addr\u0026gt; -N swift-account03 -r \u0026#39;role[base]\u0026#39;,\u0026#39;role[swift-account-server]\u0026#39;,\u0026#39;role[swift-container-server]\u0026#39; -E swift --sudo -x thirai zone 番号を付与する。\n% knife exec -E \u0026#34;nodes.find(:name =\u0026gt; \u0026#39;swift-storage01\u0026#39;) {|n| n.set[\u0026#39;swift\u0026#39;][\u0026#39;zone\u0026#39;] = \u0026#39;1\u0026#39;; n.save }\u0026#34; % knife exec -E \u0026#34;nodes.find(:name =\u0026gt; \u0026#39;swift-account01\u0026#39;) {|n| n.set[\u0026#39;swift\u0026#39;][\u0026#39;zone\u0026#39;] = \u0026#39;1\u0026#39;; n.save }\u0026#34; % knife exec -E \u0026#34;nodes.find(:name =\u0026gt; \u0026#39;swift-storage02\u0026#39;) {|n| n.set[\u0026#39;swift\u0026#39;][\u0026#39;zone\u0026#39;] = \u0026#39;2\u0026#39;; n.save }\u0026#34; % knife exec -E \u0026#34;nodes.find(:name =\u0026gt; \u0026#39;swift-account02\u0026#39;) {|n| n.set[\u0026#39;swift\u0026#39;][\u0026#39;zone\u0026#39;] = \u0026#39;2\u0026#39;; n.save }\u0026#34; % knife exec -E \u0026#34;nodes.find(:name =\u0026gt; \u0026#39;swift-storage03\u0026#39;) {|n| n.set[\u0026#39;swift\u0026#39;][\u0026#39;zone\u0026#39;] = \u0026#39;3\u0026#39;; n.save }\u0026#34; % knife exec -E \u0026#34;nodes.find(:name =\u0026gt; \u0026#39;swift-account03\u0026#39;) {|n| n.set[\u0026#39;swift\u0026#39;][\u0026#39;zone\u0026#39;] = \u0026#39;3\u0026#39;; n.save }\u0026#34; zone 番号が付与されたことを確認する。\naccount-server の確認\n% knife exec -E \u0026#39;search(:node,\u0026#34;role:swift-account-server\u0026#34;) \\ { |n| z=n[:swift][:zone]||\u0026#34;not defined\u0026#34;; puts \u0026#34;#{n.name} has the role \\ [swift-account-server] and is in swift zone #{z}\u0026#34;; }\u0026#39; swift-account01 has the role [swift-account-server] and is in swift zone 1 swift-account02 has the role [swift-account-server] and is in swift zone 2 swift-account03 has the role [swift-account-server] and is in swift zone 3 container-server の確認\n% knife exec -E \u0026#39;search(:node,\u0026#34;role:swift-container-server\u0026#34;) \\ { |n| z=n[:swift][:zone]||\u0026#34;not defined\u0026#34;; puts \u0026#34;#{n.name} has the role \\ [swift-container-server] and is in swift zone #{z}\u0026#34;; }\u0026#39; swift-account01 has the role [swift-container-server] and is in swift zone 1 swift-account02 has the role [swift-container-server] and is in swift zone 2 swift-account03 has the role [swift-container-server] and is in swift zone 3 object-server の確認\n% knife exec -E \u0026#39;search(:node,\u0026#34;role:swift-object-server\u0026#34;) \\ { |n| z=n[:swift][:zone]||\u0026#34;not defined\u0026#34;; puts \u0026#34;#{n.name} has the role \\ [swift-object-server] and is in swift zone #{z}\u0026#34;; }\u0026#39; swift-storage01 has the role [swift-object-server] and is in swift zone 1 swift-storage02 has the role [swift-object-server] and is in swift zone 2 swift-storage03 has the role [swift-object-server] and is in swift zone 3 Chef が各々のノードに搭載された Disk を検知出来るか否かを確認する。\n% knife exec -E \\ \u0026#39;search(:node,\u0026#34;role:swift-object-server OR \\ role:swift-account-server \\ OR role:swift-container-server\u0026#34;) \\ { |n| puts \u0026#34;#{n.name}\u0026#34;; \\ begin; n[:swift][:state][:devs].each do |d| \\ puts \u0026#34;\\tdevice #{d[1][\u0026#34;device\u0026#34;]}\u0026#34;; \\ end; rescue; puts \\ \u0026#34;no candidate drives found\u0026#34;; end; }\u0026#39; swift-storage02 device sdb1 swift-storage03 device sdb1 swift-account01 device sdb1 swift-account02 device sdb1 swift-account03 device sdb1 swift-storage01 device sdb1 swift-manage ノードにて chef-client を実行し /etc/swift/ring-workspace/generate-rings.sh を更新する。\nswift-manage% sudo chef-client generate-rings.sh の \u0026rsquo;exit 0\u0026rsquo; 行をコメントアウトし実行\nswift-manage% sudo ${EDITOR} /etc/swift/ring-workspace/generage-rings.sh swift-manage% sudo /etc/swift/ring-workspace/generate-rings.sh この操作で /etc/swift/ring-workspace/rings 配下に account, container, object 用の Rings ファイル群が生成されたことを確認出来るはずである。これらを swift-manage 上で既に稼働している git サーバに push し管理する。\nswift-manage# cd /etc/swift/ring-workspace/rings swift-manage# git add account.builder container.builder object.builder swift-manage# git add account.ring.gz container.ring.gz object.ring.gz swift-manage# git commit -m \u0026#34;initial commit\u0026#34; swift-manage# git push 各々のノードにて chef-client を実行することで git サーバ上の Rings ファイル群 を取得し、swift プロセスを稼働させる。\nswift-proxy01# chef-client swift-proxy02# chef-client swift-storage01# chef-client swift-storage02# chef-client swift-storage03# chef-client swift-account01# chef-client swift-account02# chef-client swift-account03# chef-client 3台のノードが登録されたかどうかを下記の通り確認行う。\nswift-proxy01% sudo swift-recon --md5 [sudo] password for thirai: =============================================================================== --\u0026gt; Starting reconnaissance on 3 hosts =============================================================================== [2013-10-18 11:14:43] Checking ring md5sums 3/3 hosts matched, 0 error[s] while checking hosts. =============================================================================== 動作確認 swift-storage01# source swift-openrc swift-storage01# swift post container01 swift-storage01# echo \u0026#34;test\u0026#34; \u0026gt; test swift-storage01# swift upload container01 test swift-storage01# swift list swift-storage01# swift list container01 ノードの追加方法 次にノードの追加方法を記します。\ngdisk にて /dev/sdb1 等のパーティションを作成する。\n下記の通り knife bootstrap する。\n% knife bootstrap \u0026lt;storage04_ip_addr\u0026gt; -N swift-storage04 -r \u0026#39;role[base]\u0026#39;,\u0026#39;role[swift-object-server]\u0026#39; -E swift --sudo -x thirai % knife bootstrap \u0026lt;account04_ip_addr\u0026gt; -N swift-account04 -r \u0026#39;role[base]\u0026#39;,\u0026#39;role[swift-account-server]\u0026#39;,\u0026#39;role[swift-container-server]\u0026#39; -E swift --sudo -x thirai zone 番号を付与する。\n% knife exec -E \u0026#34;nodes.find(:name =\u0026gt; \u0026#39;swift-storage04\u0026#39;) {|n| n.set[\u0026#39;swift\u0026#39;][\u0026#39;zone\u0026#39;] = \u0026#39;4\u0026#39;; n.save }\u0026#34; % knife exec -E \u0026#34;nodes.find(:name =\u0026gt; \u0026#39;swift-account04\u0026#39;) {|n| n.set[\u0026#39;swift\u0026#39;][\u0026#39;zone\u0026#39;] = \u0026#39;4\u0026#39;; n.save }\u0026#34; zone 番号が付与されたことを確認する。\naccount-server の確認\n% knife exec -E \u0026#39;search(:node,\u0026#34;role:swift-account-server\u0026#34;) \\ { |n| z=n[:swift][:zone]||\u0026#34;not defined\u0026#34;; puts \u0026#34;#{n.name} has the role \\ [swift-account-server] and is in swift zone #{z}\u0026#34;; }\u0026#39; swift-account01 has the role [swift-account-server] and is in swift zone 1 swift-account02 has the role [swift-account-server] and is in swift zone 2 swift-account03 has the role [swift-account-server] and is in swift zone 3 swift-account04 has the role [swift-account-server] and is in swift zone 4 container-server の確認\n% knife exec -E \u0026#39;search(:node,\u0026#34;role:swift-container-server\u0026#34;) \\ { |n| z=n[:swift][:zone]||\u0026#34;not defined\u0026#34;; puts \u0026#34;#{n.name} has the role \\ [swift-container-server] and is in swift zone #{z}\u0026#34;; }\u0026#39; swift-account01 has the role [swift-container-server] and is in swift zone 1 swift-account02 has the role [swift-container-server] and is in swift zone 2 swift-account03 has the role [swift-container-server] and is in swift zone 3 swift-account04 has the role [swift-container-server] and is in swift zone 4 object-server の確認\n% knife exec -E \u0026#39;search(:node,\u0026#34;role:swift-object-server\u0026#34;) \\ { |n| z=n[:swift][:zone]||\u0026#34;not defined\u0026#34;; puts \u0026#34;#{n.name} has the role \\ [swift-object-server] and is in swift zone #{z}\u0026#34;; }\u0026#39; swift-storage01 has the role [swift-object-server] and is in swift zone 1 swift-storage02 has the role [swift-object-server] and is in swift zone 2 swift-storage03 has the role [swift-object-server] and is in swift zone 3 swift-storage04 has the role [swift-object-server] and is in swift zone 4 swift-manage ノードにて chef-client を実行し /etc/swift/ring-workspace/generate-rings.sh を更新する。\nswift-manage% sudo chef-client generate-rings.sh の \u0026rsquo;exit 0\u0026rsquo; 行をコメントアウトし実行\nswift-manage% sudo ${EDITOR} /etc/swift/ring-workspace/generage-rings.sh swift-manage% sudo /etc/swift/ring-workspace/generate-rings.sh この操作で /etc/swift/ring-workspace/rings 配下に account, container, object 用の Rings ファイル群が生成されたことを確認出来るはずである。これらを swift-manage 上で既に稼働している git サーバに push し管理する。\nswift-manage# cd /etc/swift/ring-workspace/rings swift-manage# git add account.builder container.builder object.builder swift-manage# git add account.ring.gz container.ring.gz object.ring.gz swift-manage# git commit -m \u0026#34;added zone 4 nodes\u0026#34; swift-manage# git push 各々のノードにて chef-client を実行することで git サーバ上の Rings ファイル群 を取得し、swift プロセスを稼働させる。\nswift-proxy01# chef-client swift-proxy02# chef-client swift-storage01# chef-client swift-storage02# chef-client swift-storage03# chef-client swift-storage04# chef-client swift-account01# chef-client swift-account02# chef-client swift-account03# chef-client swift-account04# chef-client 4台のノードが登録されたかどうかを下記の通り確認行う。\nswift-proxy01% sudo swift-recon --md5 [sudo] password for thirai: =============================================================================== --\u0026gt; Starting reconnaissance on 4 hosts =============================================================================== [2013-10-18 11:14:43] Checking ring md5sums 4/4 hosts matched, 0 error[s] while checking hosts. =============================================================================== proxy ノードの swift プロセスを停止・起動する\nswift-proxy01# swift-init all stop swift-proxy01# swift-init all start swift-proxy02# swift-init all stop swift-proxy02# swift-init all start ノード削除方法 次にノードの削除方法を記します。\ndisk 障害等のzone4 のノード swift-storage04, swift-account04 を削除する前提で記す。\nswift-manage# swift-ring-builder /etc/swift/ring-workspace/rings/account.builder remove 10.200.9.109 swift-manage# swift-ring-builder /etc/swift/ring-workspace/rings/container.builder remove 10.200.9.109 swift-manage# swift-ring-builder /etc/swift/ring-workspace/rings/object.builder remove 10.200.9.110 swift-manage 上で既に稼働している git サーバに push し管理する。\nswift-manage# cd /etc/swift/ring-workspace/rings swift-manage# git add account.builder container.builder object.builder swift-manage# git add account.ring.gz container.ring.gz object.ring.gz swift-manage# git commit -m \u0026#34;added zone 4 nodes\u0026#34; swift-manage# git push 全てのノードで chef-client を実行する。この操作で rings ファイルの配布が行える。\nswift-proxy01# chef-client swift-proxy02# chef-client swift-storage01# chef-client swift-storage02# chef-client swift-storage03# chef-client swift-storage04# chef-client swift-account01# chef-client swift-account02# chef-client swift-account03# chef-client swift-account04# chef-client proxy ノードの swift プロセスを停止・起動する\nswift-proxy01# swift-init all stop swift-proxy01# swift-init all start swift-proxy02# swift-init all stop swift-proxy02# swift-init all start まとめと考察 MySQL のリカバリは単純にダンプからのリストアが良かったのでシングル構成に出来て よかった。また、通常のシングルスター・マルチスレーブの構成に変えても OK かと思 いますが、Cookbooks を修正する必要がありそう。まぁ Keystone のデータのみを管理 している MySQL なので負荷は無いですしシングル構成が良いと個人的には思います。 また Swift-Proxy は 2 台構成の HA になります。VRRP プロトコルの仕組み、また haproxy の構成上 3 台以上でも OK でありますが、Cookbooks 的に NG でした。これ は修正する価値が大いにありそう。\nまたこの構成を Chef で管理し始めると大量のノードで Chef-Client を実行すること になるので Gnu Prallel や pssh を使った方が良さそう\u0026hellip;。\n","permalink":"https://jedipunkz.github.io/post/2013/10/27/swift-chef/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003e以前、\u0026ldquo;Swift HA 構成を Chef でデプロイ\u0026rdquo; というタイトルで記事を書きました。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://jedipunkz.github.io/blog/2013/07/26/swift-ha-chef-deploy/\"\u003ehttp://jedipunkz.github.io/blog/2013/07/26/swift-ha-chef-deploy/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eこちらですが、Swift-Proxy, MySQL, Keystone をそれぞれ haproxy, keepalived で\nHA 組みました。ですが、これは実用的なのかどうか自分でずっと考えていました。\u003c/p\u003e\n\u003cp\u003eMySQL と KeepAlived はできればシングル構成にしたいのと、Swift-Proxy は HA で組\nみたい。MySQL は Master/Master レプリケーション構成になり、どちらかのノードが\n障害を起こし万が一復旧が難しくなった時、構築し直しがしんどくなります。かと言っ\nて Swift-Proxy をシングル構成にすると今度はノード追加・削除の作業時にサービス\n断が発生します。Swift-Proxy を再起動書ける必要があるからです。なので\nSwift-Proxy は引き続き HA 構成にしたい。\u003c/p\u003e\n\u003cp\u003eもう一点、見直したいと思っていました。\u003c/p\u003e\n\u003cp\u003e日経コンピュータから出版されている \u0026ldquo;仮想化大全 2014\u0026rdquo; の記事を読んでいて\n気がついたのですが。Swift には下記の通りそれぞれのサーバがあります。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eswift-proxy-server\u003c/li\u003e\n\u003cli\u003eswift-account-server\u003c/li\u003e\n\u003cli\u003eswift-container-server\u003c/li\u003e\n\u003cli\u003eswift-object-server\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eSwift には下記のような特徴がある事がわかりました。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eswift-object\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eswift-object は swift-accout, swift-container とは物理リソースの扱いに全く異な\nる特性を持っています。swift-account, swift-container はクライアントからのリクエ\nストに対して \u0026ldquo;アカウントの存在を確認\u0026rdquo;, \u0026ldquo;ACL 情報の確認\u0026rdquo; 等を行うサーバであるの\nに対して swift-object はストレージ上のオブジェクトをクライアントに提供、または\n逆に格納するサーバです。よって、Disk I/O の利用特性として swift-account,\ncontainer は SSD 等、高スループットの Disk を利用するケースが推奨されるのに対\nして swift-object はオブジェクトの実体を格納する必要があるため Disk 容量の大き\nなストレージを要する。\u003c/p\u003e","title":"実用的な Swift 構成を Chef でデプロイ"},{"content":"こんにちは。@jedipunkz です。\n前回、OpenStack と test-kitchen を使った環境構築方法を書きました。下記の記事で す。\nhttp://jedipunkz.github.io/blog/2013/10/13/test-kitchn-openstack-chef-cookbooks-test/\n今回は実際にテストを書く方法を記していたい思います。\n今回使用するテストツールは下記の2つです。\nrspec と serverspec busser-bats 参考資料 Creationline lab さんの資料を参考にさせて頂きました。\nhttp://www.creationline.com/lab/2933\n用意するモノ達 OpenStack にアクセスするためのユーザ・パスワード Keystone の AUTH_URL テストに用いる OS イメージの Image ID テナント ID nova 管理のキーペアの作成 これらは OpenStack を普段から利用されている方なら馴染みのモノかと思います。\n.kitchen.yml ファイルの作成 下記の通り .kitchen.yml ファイルを test-kitchen のルートディレクトリで作成しま す。今後の操作は全てこのディレクトリで作業行います。\n\u0026ldquo;\u0026lt;\u0026gt;\u0026rdquo; で括った箇所が環境に合わせた設定になります。\nまた、ここでは前回同様に \u0026rsquo;ntp\u0026rsquo; の Cookbook をテストする前提で記します。\n+++ driver_plugin: openstack suites: - name: default run_list: - recipe[ntp::default] attributes: {} platforms: - name: ubuntu-12.04 driver_config: openstack_username: \u0026lt;openstack_username\u0026gt; openstack_api_key: \u0026lt;openstack_password\u0026gt; openstack_auth_url: http://\u0026lt;openstack_ip_addr\u0026gt;:5000/v2.0/tokens image_ref: \u0026lt;image_id\u0026gt; flavor_ref: 1 key_name: \u0026lt;key_name\u0026gt; openstack_tenant: \u0026lt;tenant_name\u0026gt; username: \u0026lt;ssh_username\u0026gt; private_key_path: \u0026lt;path_to_secretkey\u0026gt; - name: centos-64 driver_config: openstack_username: \u0026lt;openstack_username\u0026gt; openstack_api_key: \u0026lt;openstack_password\u0026gt; openstack_auth_url: http://\u0026lt;openstack_ip_addr\u0026gt;:5000/v2.0/tokens image_ref: \u0026lt;image_id\u0026gt; flavor_ref: 1 key_name: \u0026lt;key_name\u0026gt; openstack_tenant: \u0026lt;tenant_name\u0026gt; username: \u0026lt;ssh_username\u0026gt; private_key_path: \u0026lt;path_to_secretkey\u0026gt; busser-bats テスト +++\nまずはじめに busser-bats のテストを記します。\nディレクトリ作成 kitchen init を行うことでもこの操作は可能なのですが kitchen-openstack を利用すること を想定しない形で成形されてしまうため、下記の通り実行する。\n% cd $TEST-KITCHEN-ROOT/ % mkdir -p test/integration/default/bats ディレクトリ構成 ディレクトリ構成は\ntest/integration/${SUITE_NAME}/${BUSSER_NAME}/${TEST_NAME} となっています。\nテスト作成 test/integration/default/bats/test.bats として下記のファイルを作成します。\n@test \u0026#34;ntp must be installed\u0026#34; { which ntpd } @test \u0026#34;ntp.conf must be exist\u0026#34; { cat /etc/ntp.conf | grep \u0026#34;server 0.pool.ntp.org iburst\u0026#34; } 一項目と二項目の説明を書いておきます。\n一項目の @test ntpd コマンドが存在するか否かで package \u0026rsquo;ntp\u0026rsquo; がインストール されていることを確認。\n二項目の @test 特定の文字列が /etc/ntp.conf に記述あるか否かでファイルの存在を確認。\nrspec serverspec によるテスト実施 次に rspec + serverspec のテストの記述方法を記します。\nディレクトリ作成 ディレクトリを作成します。\n% mkdir test/integration/default/rspec Gemfile 追記 下記の通り test/integration/default/rspec/Gemfile を作成します。\nsource \u0026#39;https://rubygems.org\u0026#39; gem \u0026#39;serverspec\u0026#39; serverspec-init の実行 serverspec-init コマンドにより初期化を行います。\n% cd test/integration/default/rspec % serverspec-init Select OS type: 1) UN*X 2) Windows Select number: 1 Select a backend type: 1) SSH 2) Exec (local) Select number: 2 + spec/ + spec/localhost/ + spec/localhost/httpd_spec.rb + spec/spec_helper.rb + Rakefile 上記の通り spec ディレクトリ・ファイルが作成されることを確認します。\nntp_spec.rb を作成 下記の通り test/integration/default/rspec/spec/localhost/ntp_spec.rb を作成し ます。\nrequire \u0026#39;spec_helper\u0026#39; describe package(\u0026#39;ntp\u0026#39;) do it { should be_installed } end describe service(\u0026#39;ntp\u0026#39;) do it { should be_enabled } it { should be_running } end describe port(123) do it { should be_listening } end describe file(\u0026#39;/etc/ntp.conf\u0026#39;) do it { should be_file } it { should contain \u0026#34;server 0.pool.ntp.org iburst\u0026#34; } end テスト内容の解説は\u0026hellip;\n\u0026rsquo;ntp\u0026rsquo; パッケージがインストールされていることを確認 \u0026rsquo;ntp\u0026rsquo; サービスが再起動時有効になっていること・起動していることを確認 123 番ポートで Listen していることを確認 /etc/ntp.conf に特定の文字列が存在することを確認 テスト実行 下記の通りテストを実行する。\n% cd $TEST-KITCHEN-ROOT % bundle exec kitchen create # \u0026lt;---- OpenStack 上にインスタンス作成 % bundle exec kitchen setup # \u0026lt;---- Chef Cookbooks の実行 % bundle exec kitchen verify # \u0026lt;---- テストの実施 またこれらの操作は下記の一つのコマンドで実施出来る。\n% bundle exec kitchen test テスト結果 busser-bats 下記の通り2つのテストが実施され \u0026lsquo;ok\u0026rsquo; ステータスが帰って来たことを確認する。\n-----\u0026gt; Running bats test suite 1..2 ok 1 ntp must be installed ok 2 ntp.conf must be exist serverspec 下記の通り 6 個のテスト全てが通り failure 0 個であることを確認する。\n-----\u0026gt; Running rspec test suite [2013-10-17T02:27:19+00:00] INFO: Run List is [] [2013-10-17T02:27:19+00:00] INFO: Run List expands to [] Recipe: (chef-apply cookbook)::(chef-apply recipe) * execute[bundle install --local || bundle install] action run [2013-10-17T02:27:19+00:00] INFO: Processing execute[bundle install --local || bundle install] action run ((chef-apply cookbook)::(chef-apply recipe) line 42) Resolving dependencies... Using diff-lcs (1.2.4) Using highline (1.6.20) Using net-ssh (2.7.0) Using rspec-core (2.14.6) Using rspec-expectations (2.14.3) Using rspec-mocks (2.14.4) Using rspec (2.14.1) Using serverspec (0.10.6) Using bundler (1.3.5) Your bundle is complete! Use `bundle show [gemname]` to see where a bundled gem is installed. [2013-10-17T02:27:19+00:00] INFO: execute[bundle install --local || bundle install] ran successfully - execute bundle install --local || bundle install \u0026hellip;\u0026hellip;\nFinished in 0.0567 seconds 6 examples, 0 failures Finished verifying (0m7.44s).\nまとめ ----- busser-bats はまだ地道なテストの記述が必要らしい。それに比べて serverspec は既 に実用的と言えるかもしれない。どちらのテストツールも Cookbook でデプロイされた 環境の *状態* をテストするモノであって Cookbook, Recipe のテストとは違う。これ はある意味都合が良い。Cookbooks のテストは ChefSpec 等のテストツールでテストを 行い、完成された Cookbooks を実際に複数の OS 上にデプロイしてテストするのが今 回紹介したモノとなる。 ","permalink":"https://jedipunkz.github.io/post/2013/10/20/test-kitchen-openstack-chef-cookbooks-test-2/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003e前回、OpenStack と test-kitchen を使った環境構築方法を書きました。下記の記事で\nす。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://jedipunkz.github.io/blog/2013/10/13/test-kitchn-openstack-chef-cookbooks-test/\"\u003ehttp://jedipunkz.github.io/blog/2013/10/13/test-kitchn-openstack-chef-cookbooks-test/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e今回は実際にテストを書く方法を記していたい思います。\u003c/p\u003e\n\u003cp\u003e今回使用するテストツールは下記の2つです。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003erspec と serverspec\u003c/li\u003e\n\u003cli\u003ebusser-bats\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"参考資料\"\u003e参考資料\u003c/h2\u003e\n\u003cp\u003eCreationline lab さんの資料を参考にさせて頂きました。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://www.creationline.com/lab/2933\"\u003ehttp://www.creationline.com/lab/2933\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"用意するモノ達\"\u003e用意するモノ達\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eOpenStack にアクセスするためのユーザ・パスワード\u003c/li\u003e\n\u003cli\u003eKeystone の AUTH_URL\u003c/li\u003e\n\u003cli\u003eテストに用いる OS イメージの Image ID\u003c/li\u003e\n\u003cli\u003eテナント ID\u003c/li\u003e\n\u003cli\u003enova 管理のキーペアの作成\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eこれらは OpenStack を普段から利用されている方なら馴染みのモノかと思います。\u003c/p\u003e\n\u003ch2 id=\"kitchenyml-ファイルの作成\"\u003e.kitchen.yml ファイルの作成\u003c/h2\u003e\n\u003cp\u003e下記の通り .kitchen.yml ファイルを test-kitchen のルートディレクトリで作成しま\nす。今後の操作は全てこのディレクトリで作業行います。\u003c/p\u003e\n\u003cp\u003e\u0026ldquo;\u0026lt;\u0026gt;\u0026rdquo; で括った箇所が環境に合わせた設定になります。\u003c/p\u003e\n\u003cp\u003eまた、ここでは前回同様に \u0026rsquo;ntp\u0026rsquo; の Cookbook をテストする前提で記します。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-yaml\" data-lang=\"yaml\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e+++\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003edriver_plugin\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003eopenstack\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003esuites\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e- \u003cspan style=\"color:#f92672\"\u003ename\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003edefault\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#f92672\"\u003erun_list\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  - \u003cspan style=\"color:#ae81ff\"\u003erecipe[ntp::default]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#f92672\"\u003eattributes\u003c/span\u003e: {}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eplatforms\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e- \u003cspan style=\"color:#f92672\"\u003ename\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003eubuntu-12.04\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#f92672\"\u003edriver_config\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003eopenstack_username\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003e\u0026lt;openstack_username\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003eopenstack_api_key\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003e\u0026lt;openstack_password\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003eopenstack_auth_url\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003ehttp://\u0026lt;openstack_ip_addr\u0026gt;:5000/v2.0/tokens\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003eimage_ref\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003e\u0026lt;image_id\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003eflavor_ref\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003ekey_name\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003e\u0026lt;key_name\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003eopenstack_tenant\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003e\u0026lt;tenant_name\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003eusername\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003e\u0026lt;ssh_username\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003eprivate_key_path\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003e\u0026lt;path_to_secretkey\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e- \u003cspan style=\"color:#f92672\"\u003ename\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003ecentos-64\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#f92672\"\u003edriver_config\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003eopenstack_username\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003e\u0026lt;openstack_username\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003eopenstack_api_key\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003e\u0026lt;openstack_password\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003eopenstack_auth_url\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003ehttp://\u0026lt;openstack_ip_addr\u0026gt;:5000/v2.0/tokens\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003eimage_ref\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003e\u0026lt;image_id\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003eflavor_ref\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003ekey_name\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003e\u0026lt;key_name\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003eopenstack_tenant\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003e\u0026lt;tenant_name\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003eusername\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003e\u0026lt;ssh_username\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003eprivate_key_path\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003e\u0026lt;path_to_secretkey\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003ebusser-bats テスト\n+++\u003c/p\u003e","title":"test-kitchen と OpenStack で Chef Cookbooks テスト (後篇)"},{"content":"こんにちは。@jedipunkz です。\ntest-kitchen + Vagrant を利用して複数環境で Chef Cookbooks のテストを行う方法は 結構皆さん利用されていると思うのですが Vagrant だと手元のマシンに仮想マシンが バシバシ立ち上げるので僕はあまり好きではないです。そこで、OpenStack のインスタ ンスをその代替で使えればいいなぁと結構前から思っていたのですが、今回うまくいっ たのでその方法を記します。\n用意するモノ OpenStack 環境一式 Chef がインストールされた OS イメージとその ID test-kitchen を実行するワークステーション (お手持ちの Macbook 等) OS イメージの作成ですが Veewee などで自動構築できますし、インスタンス上で Chef のインストールを行った後にスナップショットを作成してそれを利用しても構いません。\ntest-kitchen のインストール test-kitchen をインストールします。versoin 1.0.0 はまだリリースされていないの で github から master ブランチを取得してビルドします。直近で OpenStack に関連 する不具合の修正等が入っているのでこの方法を取ります。\n% git clone https://github.com/opscode/test-kitchen.git % cd test-kitchen % bundle install % rake build # \u0026lt;--- gem をビルド % gen install ./pkg/test-kitchen-1.0.0.dev.gem 現時点 (2013/10/13) で berkshelf の利用しているソフトウェアと衝突を起こす問題 があるので bundle で解決します。下記のように Gemfile に gem \u0026lsquo;kitchen-openstack\u0026rsquo; と記述します。\nsource \u0026#39;https://rubygems.org\u0026#39; gemspec gem \u0026#39;kitchen-openstack\u0026#39; # \u0026lt;--- 追記 group :guard do gem \u0026#39;rb-inotify\u0026#39;, :require =\u0026gt; false gem \u0026#39;rb-fsevent\u0026#39;, :require =\u0026gt; false gem \u0026#39;rb-fchange\u0026#39;, :require =\u0026gt; false gem \u0026#39;guard-minitest\u0026#39;, \u0026#39;~\u0026gt; 1.3\u0026#39; gem \u0026#39;guard-cucumber\u0026#39;, \u0026#39;~\u0026gt; 1.4\u0026#39; end kitchen-openstack のインストール kitchen-openstack をインストールします。こちらも gem をビルドしてインストール します。\n% git clone https://github.com/RoboticCheese/kitchen-openstack.git % cd kitchen-openstack % bundle insatll #\u0026lt;---- 関連ソフトウェアインストール % rake build #\u0026lt;---- gem をビルド % gem install ./pkg/kitchen-openstack-0.5.1.gem .kitchen.yml の作成 .kitchen.yml を用意します。test-kitchen のディレクトリに移動し .kitchen.yml を 下記の例に従って作成します。今回は Ubuntu OS, CentOS にてテストを実行します。\n% cd /path/to/test-kitchen % ${EDITOR} .kitchen.yml +++ driver_plugin: openstack suites: - name: default run_list: - recipe[ntp::default] attributes: {} platforms: - name: ubuntu-12.04 driver_config: openstack_username: \u0026lt;openstac_username\u0026gt; openstack_api_key: \u0026lt;openstack_password\u0026gt; openstack_auth_url: http://\u0026lt;openstack_ip_addr\u0026gt;:5000/v2.0/tokens image_ref: \u0026lt;image_id\u0026gt; flavor_ref: 1 key_name: \u0026lt;key_name\u0026gt; openstack_tenant: service username: ubuntu private_key_path: \u0026lt;path_to_secretkey\u0026gt; - name: centos-64 driver_config: openstack_username: \u0026lt;openstac_username\u0026gt; openstack_api_key: \u0026lt;openstack_password\u0026gt; openstack_auth_url: http://\u0026lt;openstack_ip_addr\u0026gt;:5000/v2.0/tokens image_ref: \u0026lt;image_id\u0026gt; flavor_ref: 1 key_name: \u0026lt;key_name\u0026gt; openstack_tenant: service username: root private_key_path: \u0026lt;path_to_secretkey\u0026gt; ファイルの内容について解説します。\nsuites: 実行したい Chef Cookbooks のレシピ名を指定します。attriutes などをここで上書き することも出来ます。\nplatforms: テストに用いたい OS を列挙していけます。ここでは例として Ubuntu, CentOS を記し ました。\nopenstack_username, openstack_api_key OpenStack にログインするためのユーザ名とパスワードです。keystone で作成します。\nopenstack_auth_url Keystone の URL です。最後に /tokens と付けるのを忘れずに。\nimage_ref それぞれの OS イメージの ID を記します。前述したとおりインスタンスでオペレーショ ン後にスナップショットを作成しそれを記すことも可能です。\nflavor_ref Flavor ID を記します。ここでは一番小さい Flavor である m1.tiny を記しました。\nkey_name インスタンス作成時に選択する Nova のキーペア名です。OpenStack コマンドラインで 言う \u0026ndash;key_name です。\nopenstack_tenant どのテナントにインスタンスを作成するか？を記します。\nusername インスタンスにログインする際のユーザ名を記します。\nprivate_key_path インスタンスにログインするための SSH 秘密鍵のパスを記します。ここではノンパス ワードでログイン出来るよう鍵を生成してあげる必要があります。\nCookbooks の配置 カレントディレクトリ配下に \u0026lsquo;cookbooks\u0026rsquo; という名前のディレクトリを用意し テストで用いたい Cookbooks を配置します。Berkshelf を用いれば簡単です。が、 現時点で Berkshelf の用いているソフトウェア test-kitchen のそれが衝突を起こす のでテスト実行前には Berkshelf ファイルを退避してください。\n% ${EDITOR} Berksfile site :opscode cookbook \u0026#39;ntp; % berks install --path=./cookbooks % mv Berksfile Berksfile.old テスト実行 いよいよテストを実行します。上記の例では Ubuntu OS, Debian OS に対して ntp の Chef Cookbooks を実際にデプロイしテストを行います。\n% bundle exec kitchen test \u0026rsquo;test\u0026rsquo; を引数で渡すと\nインスタンス作成 Chef のインストール Cookbooks のアップロード chef-solo の実行 Cookbooks 中に \u0026rsquo;test\u0026rsquo; ディレクトリがある場合はテスト実行 を行ってくれます。それぞれ別々に実行したい場合は\n% bundle exec kitchen create # \u0026lt;---- インスタンスの作成 % bundle exec kitchen setup # \u0026lt;---- chef のインストールと初回の converge % bundle exec kitchen converge # \u0026lt;---- chef-solo を再度実行 % bundle exec kitchen verify # \u0026lt;---- \u0026#39;test\u0026#39; ディレクトリに従いテスト実行 % bundle exec kitchen destroy # \u0026lt;---- インスタンスの削除 と行えば良いです。converge, verify は何度でも繰り返し実行が可能。\nまとめ 前述したとおり Vagrant を使うと手元のマシンのリソースを大量に消費してしまうの で OpenStack を利用する価値は結構あるのかなぁと思っています。バージョン 1.0.0 がリリースされる時期も近いと思うので今のうちに知っておくと良いかと思います。\nまた、テストと言っても test-kitchen の場合2つの意味があると思います。実際に Cookbooks をインスタンスにインストールするテスト、と Cookbooks 自体のテストと いう意味です。後者についてはまた後ほど記したいと思います。\n","permalink":"https://jedipunkz.github.io/post/2013/10/13/test-kitchn-openstack-chef-cookbooks-test/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003etest-kitchen + Vagrant を利用して複数環境で Chef Cookbooks のテストを行う方法は\n結構皆さん利用されていると思うのですが Vagrant だと手元のマシンに仮想マシンが\nバシバシ立ち上げるので僕はあまり好きではないです。そこで、OpenStack のインスタ\nンスをその代替で使えればいいなぁと結構前から思っていたのですが、今回うまくいっ\nたのでその方法を記します。\u003c/p\u003e\n\u003ch2 id=\"用意するモノ\"\u003e用意するモノ\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eOpenStack 環境一式\u003c/li\u003e\n\u003cli\u003eChef がインストールされた OS イメージとその ID\u003c/li\u003e\n\u003cli\u003etest-kitchen を実行するワークステーション (お手持ちの Macbook 等)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eOS イメージの作成ですが Veewee などで自動構築できますし、インスタンス上で Chef\nのインストールを行った後にスナップショットを作成してそれを利用しても構いません。\u003c/p\u003e\n\u003ch2 id=\"test-kitchen-のインストール\"\u003etest-kitchen のインストール\u003c/h2\u003e\n\u003cp\u003etest-kitchen をインストールします。versoin 1.0.0 はまだリリースされていないの\nで github から master ブランチを取得してビルドします。直近で OpenStack に関連\nする不具合の修正等が入っているのでこの方法を取ります。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e% git clone https://github.com/opscode/test-kitchen.git\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e% cd test-kitchen\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e% bundle install\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e% rake build \u003cspan style=\"color:#75715e\"\u003e# \u0026lt;--- gem をビルド\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e% gen install ./pkg/test-kitchen-1.0.0.dev.gem\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e現時点 (2013/10/13) で berkshelf の利用しているソフトウェアと衝突を起こす問題\nがあるので bundle で解決します。下記のように Gemfile に gem\n\u0026lsquo;kitchen-openstack\u0026rsquo; と記述します。\u003c/p\u003e","title":"test-kitchen と OpenStack で Chef Cookbooks テスト(前篇)"},{"content":"こんにちは。@jedipunkz です。\nGlusterFS をちょっと前に調べてました。何故かと言うと OpenStack Havana がもうす ぐリリースされるのですが、Havana から GlusterFS がサポートされる予定だからです。\nこの辺りに色々情報が載っています。\nhttp://www.gluster.org/category/openstack/\nその前に GlusterFS を構築出来ないといけないので、今回はその方法を書いていきま す。各クラスタタイプ毎に特徴や構築方法が異なるのでその辺りを重点的に。\n環境 Ubuntu Server 12.04.3 LTS PPA レポジトリ利用 /dev/sdb を OS 領域とは別の disk としてサーバに追加する 用いる PPA レポジトリ Ubuntu 12.04.3 LTS の GlusterFS バージョンは 3.2 です。3.4 系が今回使いたかっ たので下記の PPA レポジトリを利用させてもらいます。ちゃんと構築するなら自分で パッケージを作成することをオススメします。\nhttps://launchpad.net/~semiosis/+archive/ubuntu-glusterfs-3.4\n準備 ここからの手順は全てのサーバで操作します。\nレポジトリの利用方法 % sudo aptitude install python-software-properties % sudo add-apt-repository ppa:semiosis/ubuntu-glusterfs-3.4 % sudo apt-get update GlusterFS3.4 のインストール % sudo apt-get install glusterfs-server gluserfs-client xfsprogs のインストール glusterfs は xfs を扱うため xfsprogs をインストールする。\n% sudo apt-get install xfsprogs ディスクの準備 % sudo mkfs.xfs -i size=512 /dev/sdb % sudo mkdir -p /export/brick1 % sudo vim /etc/fstab /dev/sdb /export/brick1 xfs defaults 1 2 # \u0026lt;- 追記 % sudo mount -a % mount 各クラスタタイプでのマウント方法 ここからの手順はどこか一台のノードで行えば OK です。\ndistributed タイプ まずはデフォルトとなる distributed のマウント方法。\n% sudo gluster peer probe \u0026lt;other_node_ip_addr\u0026gt; ... % sudo gluster volume create gv0 \u0026lt;mine_ip_addr\u0026gt;:/export/brick1/sdb \u0026lt;another_node_ip_addr\u0026gt;:/export/brick1/sdb % sudo gluster volume start gv0 % sudo gluster volume info Volume Name: gv0 Type: Distribute Volume ID: 803fdd46-4735-444a-99c8-83d1cee172e6 Status: Started Number of Bricks: 2 Transport-type: tcp Bricks: Brick1: \u0026lt;mine_ip_addr\u0026gt;:/export/brick1/sdb Brick2: \u0026lt;other_ip_addr\u0026gt;:/export/brick1/sdb マウント行う。どちらのノードに対して行うことが出来る。\n% sudo mount -t glusterfs \u0026lt;mine_ip_addr\u0026gt;:/gv0 /mnt その他のクラスタタイプのマウント方法 replicated の場合\u0026hellip;\n% sudo gluster volume create gv0 replica 2 \u0026lt;mine_ip_addr\u0026gt;:/export/brick1/sdb \\ \u0026lt;other_node_ip_addr\u0026gt;:/export/brick1/sdb striped の場合\u0026hellip;\n% sudo gluster volume create gv0 stripe 2 \u0026lt;mine_ip_addr\u0026gt;:/export/brick1/sdb \\ \u0026lt;other_node_ip_addr\u0026gt;:/export/brick1/sdb distributed replicated の場合\u0026hellip;\n% sudo gluster volume create dist-repl replica 2 \\ \u0026lt;mine_ip_addr\u0026gt;:/export/brick1/sdb \\ \u0026lt;other_ip_addr\u0026gt;:/export/brick1/sdb \\ \u0026lt;other_ip_addr\u0026gt;:/export/brick1/sdb \\ \u0026lt;other_ip_addr\u0026gt;:/export/brick1/sdb striped replicated の場合\u0026hellip;\n% sudo gluster volume create strip-repl stripe 2 replica 2 \\ \u0026lt;mine_ip_addr\u0026gt;:/export/brick1/sdb \\ \u0026lt;other_ip_addr\u0026gt;:/export/brick1/sdb \\ \u0026lt;other_ip_addr\u0026gt;:/export/brick1/sdb \\ \u0026lt;other_ip_addr\u0026gt;:/export/brick1/sdb distributed striped replicated の場合\u0026hellip;\n% sudo gluster volume create dist-strip-repl stripe 2 replica 2 \\ \u0026lt;mine_ip_addr\u0026gt;:/export/brick1/sdb \\ \u0026lt;other_ip_addr\u0026gt;:/export/brick1/sdb \\ \u0026lt;other_ip_addr\u0026gt;:/export/brick1/sdb \\ \u0026lt;other_ip_addr\u0026gt;:/export/brick1/sdb \u0026lt;other_ip_addr\u0026gt;:/export/brick1/sdb \\ \u0026lt;other_ip_addr\u0026gt;:/export/brick1/sdb \\ \u0026lt;other_ip_addr\u0026gt;:/export/brick1/sdb \\ \u0026lt;other_ip_addr\u0026gt;:/export/brick1/sdb 各クラスタタイプの特徴 distributed 1つのオブジェクトを GlusterFS 上に保管するとオブジェクト単位でどれかのノードに 対して保管する。よって \u0026lsquo;ノード上の disk x ノード数\u0026rsquo; が合計容量として扱うことが 出来る。ノード障害の場合にはその該当ノード上の disk に保管されているオブジェク トの読み書きは NG 。その他のノード上 disk に保管されているオブジェクトに対して は読み書きが正常に行える。\nstripe 1つのオブジェクトをブロック単位で分割し各ノードに保管する。扱える合計容量は distributed と同じく \u0026lsquo;ノード上の disk x ノード数\u0026rsquo; となる。障害がどこかのノード で発生した場合、全てのオブジェクトの読み書きが行えなくなる。\nreplicated 1つのオブジェクトを 2 台のノードに対してミラーリングを行い保管する。障害系の対 応が可能になる。その分、扱える合計容量は distrubuted/stripe に比べ半分となる。\ndistributed replicated 構成イメージ図\u0026hellip;\n+--------+ | client | +--------+ | +---------------------+ | | +----------+ +----------+ | | | | +--------+ +--------+ +--------+ +--------+ | node1 | | node2 | | node3 | | node4 | +--------+ +--------+ +--------+ +--------+ node1, 2 と 3, 4 にて replicated しそれぞれに対して distributed を組む。扱える disk 容量は node の持っている disk 容量 x ノード数 / 2 となる。distributed は 実質、障害系の対応が良くないので distributed を扱うのであれば、この volume type が推奨されるものと思われる。\nstriped replicated distributed replicated と同等の構成だがブロック単位でのオブジェクトの保管とな る。\nまとめ GlusterFS には一貫性の問題 (disk 容量の一貫性を保つ必要がある) と思っていたが、 昔の話しらしい。容量のことなる disk をノードに追加しても、それをうまく合計容量 に合算されるのを確認した。また分散ファイルシステムの美味しいところと冗長性を兼 ねた構成を組むのが良いと思うので distributed replicated もしくは striped replicated を選択するのがオススメ。今回は TCP で扱ったがバージョン 3.4 からは Infiniband と組み合わせて RDMA を扱うことが可能。下記の URL が参考になる。\nhttp://gluster.org/community/documentation/index.php/Gluster_3.2:_Configuring_GlusterFS_to_work_over_InfiniBand\n","permalink":"https://jedipunkz.github.io/post/2013/10/12/glusterfs-install/","summary":"\u003cp\u003eこんにちは。@jedipunkz です。\u003c/p\u003e\n\u003cp\u003eGlusterFS をちょっと前に調べてました。何故かと言うと OpenStack Havana がもうす\nぐリリースされるのですが、Havana から GlusterFS がサポートされる予定だからです。\u003c/p\u003e\n\u003cp\u003eこの辺りに色々情報が載っています。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://www.gluster.org/category/openstack/\"\u003ehttp://www.gluster.org/category/openstack/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eその前に GlusterFS を構築出来ないといけないので、今回はその方法を書いていきま\nす。各クラスタタイプ毎に特徴や構築方法が異なるのでその辺りを重点的に。\u003c/p\u003e\n\u003ch2 id=\"環境\"\u003e環境\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eUbuntu Server 12.04.3 LTS\u003c/li\u003e\n\u003cli\u003ePPA レポジトリ利用\u003c/li\u003e\n\u003cli\u003e/dev/sdb を OS 領域とは別の disk としてサーバに追加する\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"用いる-ppa-レポジトリ\"\u003e用いる PPA レポジトリ\u003c/h2\u003e\n\u003cp\u003eUbuntu 12.04.3 LTS の GlusterFS バージョンは 3.2 です。3.4 系が今回使いたかっ\nたので下記の PPA レポジトリを利用させてもらいます。ちゃんと構築するなら自分で\nパッケージを作成することをオススメします。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://launchpad.net/~semiosis/\u0026#43;archive/ubuntu-glusterfs-3.4\"\u003ehttps://launchpad.net/~semiosis/+archive/ubuntu-glusterfs-3.4\u003c/a\u003e\u003c/p\u003e\n\u003ch1 id=\"準備\"\u003e準備\u003c/h1\u003e\n\u003cp\u003eここからの手順は全てのサーバで操作します。\u003c/p\u003e\n\u003ch2 id=\"レポジトリの利用方法\"\u003eレポジトリの利用方法\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e% sudo aptitude install python-software-properties\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e% sudo add-apt-repository ppa:semiosis/ubuntu-glusterfs-3.4\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e% sudo apt-get update\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"glusterfs34-のインストール\"\u003eGlusterFS3.4 のインストール\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e% sudo apt-get install glusterfs-server gluserfs-client\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"xfsprogs-のインストール\"\u003exfsprogs のインストール\u003c/h2\u003e\n\u003cp\u003eglusterfs は xfs を扱うため xfsprogs をインストールする。\u003c/p\u003e","title":"GlusterFS の各クラスタタイプ構築"},{"content":"こんにちは。@jedipunkz です。\nMesos アーキテクチャについて2つめの記事です。\nhttp://jedipunkz.github.io/blog/2013/09/28/mesos-architecture-number-1/\n上記の前回の記事で Mesos 自体のアーキテクチャについて触れましたが、今回は Mesos + Marathon + Docker の構成について理解したことを書いていこうと思います。\nmesos クラスタは 幾つかの mesos masters と沢山の mesos slaves から成っており、 mesos slaves の上では docker を操作する executor が稼働している。marathon は mesos master の上で稼働する mesos framework である。init や upstart の様な存在 であることが言え、REST API を持ち container の動作を制御する。marathon には ruby の client 等も存在する。下記がそれ。\nhttps://github.com/mesosphere/marathon_client\n構成 +-----------------+ | docker registry | index.docker.io (もしくは local registry) +-----------------+ | +----------------+ | | +--------------+ +--------------+ | mesos master | | mesos master | +--------------+ +--------------+ | | |----------------+-----------------------------------| +--------------+ +--------------+ +--------------+ | mesos slave | | mesos slave | ... | mesos slave | +--------------+ +--------------+ +--------------+ | | | +--------------+ +--------------+ +--------------+ | mesos slave | | mesos slave | ... | mesos slave | +--------------+ +--------------+ +--------------+ . . . . . . . . . +--------------+ +--------------+ +--------------+ | mesos slave | | mesos slave | ... | mesos slave | +--------------+ +--------------+ +--------------+ オファから docker が稼働するまでの流れ 上記の構成の図を見ながら理解していきましょう。\nHTTP API もしくは web UI で marathon がリクエストを受ける marathon はリソースリクエストを作成しオファが受け付けられるのを待つ オファが受け付けられた後、mesos master は slave に task の仕様を送信する slave では docker コマンドラインツールを実行する mesos docker を mesos slave デーモンが呼び出す docker コマンドラインツールはローカルの docker デーモンと image cache, lxc ツールにより通信する もし image cache が存在すればそれを、無ければ docker registry から pull する。 その時、index.docker.io の代わりにローカルの docker registry を稼働させることも可能 docker デーモンが container を稼働させる marathon のクラスタとしての動作 marathon は init や upstart のようなモノだと上記で説明しましたが、図を交えて説明して いきましょう。\nmarathon が \u0026lsquo;serarch service\u0026rsquo; と \u0026lsquo;docker\u0026rsquo; を稼働させている状態だとする。\n+----------+ +----------+ +----------+ | | | | | | | |search| | | |docker| | | |docker| | +----------+ +----------+ +----------+ +----------+ +----------+ +----------+ | | | | | | | |search| | | |docker| | | |docker| | +----------+ +----------+ +----------+ +----------+ +----------+ +----------+ | | | | | | | |search| | | |docker| | | |docker| | +----------+ +----------+ +----------+ サービスの状況によりオファが立て込んでくると下記のように docker をスケールアウ トが発生する。\n+----------+ +----------+ +----------+ | |docker| | | |docker| | | | | |search| | | |docker| | | |docker| | +----------+ +----------+ +----------+ +----------+ +----------+ +----------+ | | | |docker | | |docker| | | |search| | | |docker| | | |docker| | +----------+ +----------+ +----------+ +----------+ +----------+ +----------+ | |docker| | | |docker| | | |docker| | | |search| | | |docker| | | |docker| | +----------+ +----------+ +----------+ システムに異常がありノードが落ちた場合、下記のように marathon は serach service と docker をノード間で移動させる処置を行う。\n+----------+ +----------+ | |docker| | | |search| | | |docker| | | |docker| | +----------+ +----------+ +----------+ +----------+ +----------+ | |docker| | |docker| | | |docker| | | |search| | | |docker| | | |docker| | +----------+ +----------+ +----------+ +----------+ +----------+ +----------+ | |docker| | | |docker| | | |docker| | | |search| | | |docker| | | |docker| | +----------+ +----------+ +----------+ まとめ mesos と docker, marathon の関係について記していきました。今度、実際にこの構成 を組んでみて障害系のテストしてみたいです。あとは framework について理解してい く必要がありそう。あとは chronos についても。chronos については下記の URL が公 式らしい。これは cron 代替な仕組みらしいです。\nhttps://github.com/airbnb/chronos\nまだまだ理解できていないことだらけだ\u0026hellip;。\n","permalink":"https://jedipunkz.github.io/post/2013/10/01/methos-architecture-number-2-docker-on-mesos/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003eMesos アーキテクチャについて2つめの記事です。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://jedipunkz.github.io/blog/2013/09/28/mesos-architecture-number-1/\"\u003ehttp://jedipunkz.github.io/blog/2013/09/28/mesos-architecture-number-1/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e上記の前回の記事で Mesos 自体のアーキテクチャについて触れましたが、今回は\nMesos + Marathon + Docker の構成について理解したことを書いていこうと思います。\u003c/p\u003e\n\u003cp\u003emesos クラスタは 幾つかの mesos masters と沢山の mesos slaves から成っており、\nmesos slaves の上では docker を操作する executor が稼働している。marathon は\nmesos master の上で稼働する mesos framework である。init や upstart の様な存在\nであることが言え、REST API を持ち container の動作を制御する。marathon には\nruby の client 等も存在する。下記がそれ。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/mesosphere/marathon_client\"\u003ehttps://github.com/mesosphere/marathon_client\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"構成\"\u003e構成\u003c/h2\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e+-----------------+\n| docker registry | index.docker.io (もしくは local registry)\n+-----------------+\n|\n+----------------+\n|                |\n+--------------+ +--------------+\n| mesos master | | mesos master |\n+--------------+ +--------------+\n|                |\n|----------------+-----------------------------------|\n\n+--------------+ +--------------+     +--------------+\n| mesos slave  | | mesos slave  | ... | mesos slave  | \n+--------------+ +--------------+     +--------------+\n|                |                    |\n+--------------+ +--------------+     +--------------+\n| mesos slave  | | mesos slave  | ... | mesos slave  | \n+--------------+ +--------------+     +--------------+\n.                .                    .\n.                .                    .\n.                .                    .\n+--------------+ +--------------+     +--------------+\n| mesos slave  | | mesos slave  | ... | mesos slave  |\n+--------------+ +--------------+     +--------------+\n\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"オファから-docker-が稼働するまでの流れ\"\u003eオファから docker が稼働するまでの流れ\u003c/h2\u003e\n\u003cp\u003e上記の構成の図を見ながら理解していきましょう。\u003c/p\u003e","title":"Methos アーキテクチャ #2 (Docker on Mesos)"},{"content":"こんにちは。@jedipunkz です。\nDevOps Day Tokyo 2013 に参加してきました。たくさんの刺激を受けたのでレポート書 いておきます。\n開催日 : 2013年09月28日 場所 : 東京六本木ミッドタウン Yahoo! Japan さま URL : http://www.devopsdays.org/events/2013-tokyo/ Making Operation Visible Nick Galbreath (@ngalbreath) さん DevOps の拠点 Etsy に努めた経緯のある DevOps リーダ Galbreath さん。DevOps の 概略から何が必要でありどう行動に起こせばよいか説明してくださいました。\nMaking operations visible - devopsdays tokyo 2013 from Nick Galbreath こちら、Galbreath さんの当日の資料。\nDevOps が実行出来ない理由 Tool が足りない 社風の影響 見えないモノが価値がないと事業から考えられている 出来る事は、価値があるモノの社内への説明と、Tool を使った可視化。データの可視 化が重要。Ops の人は結構「データをどこそこの部署に見せても理解してもらえない」 だとか「データを閲覧させると万が一の時にシステムが破損する」等と考えがち。が、 ビジネス寄りの人にとって重要なグラフが含まれていたり、アカウント担当の人に役立 つものも含まれている。ましてシステムが破損することなど決して無い。\n重要なのは \u0026ldquo;運用のメトリクスを公開する\u0026rdquo; こと！\nGraphite グラフ描画ツール まず完成度が高いわけではない 同類のソフトウェアでは行えないクエリが発行出来る REST API Flexible Input \u0026amp; Output Simple UI \u0026amp; Dashboard 3rd Party Custom Client Side Dashboard あり URL 型なので Dashboard 開発が楽ちん 稼働させるための物理インフラリソースは結構必要 apt-get install graphite できるよ statd UDP 使ってる Event Data を Application から statd へ 下記は例。ログイン情報を送るためのコードはこれだけ。\nStatD::increment('logins') 国別にも出来る。\nStatD::increment('logins'); $kuni = geoip2country($ipv4); StatD::increment('logins.$kuni') 後に DataDog の方から詳しく説明がある。\nSensu Sean Porter さん メモとりにくいプレゼンだったので思い出しながら\u0026hellip;.\nSensu は Nagios のプラグイン等がそのまま再利用できる監視ツール。API を介してア ラートの具合やクライアントのリスト・監視項目のリスト等が取得できます。Nagios を使った場合、ターゲットノードの追加のたびにコンフィギュレーションを生成しなく てはならなかったり不便だったのが Sensu を開発しようとした動機だとか。Chef との 親和性が高く、Chef Cookbook の公開もしている。私も実際に使っていますが、\nSensu サーバの構築 監視項目の追加 ターゲットノードへのアージェントの追加 等はすべて Chef の Knife コマンドで出来ます。特に Chef からの影響だと思われる が Omnibus 形式のパッケージを採用していることもあり、エージェントのインストー ルは簡単です。パッケージの中に動作に必要な Ruby 一式も含まれています。\nドキュメントは下記のところにあります。\nhttp://docs.sensuapp.org/0.10/index.html\npuppet max martin for next level +++\nPuppet 3.x Hiera integration PuppetDB Mcollective 2.x Geppetto : IDE Puppet Forge : web site Puppet が提供するソフトウェアやサービス達。DevOps の次のステップへの必要な技術 要素。\npuppet 3.x pupput2 に比較して speedup x 3 agents/server : 100% up これらは理論値ではなく実効値！ Puppet2 と比較してかなりの性能アップらしい。処理速度3倍は実効値だそうです。\npuppet3.0 = hiera functions + data bindings hiera : hierarchical key value store with pluggable backend (json, yaml..) keep site specific data out of puppet code parameter values are now automatically looked up in hiera data bind でコードがシンプルに : include ntp -\u0026gt; yaml, json.. json や yaml でパラメータを記述出来るのでコードがシンプルで綺麗になるよ、との こと。Chef でいう Attributes だと思われる。\npuppetdb puppet データすべてを格納 (facts, catalogs, reports,..) replaces existing library is much faster and more faster postgreSQL, Clojure, JVM -\u0026gt; JAR ファイルで展開 クエリシンタックスを自ら定義可能 API がシンプルなので dashboard などの開発も簡単 PostgreSQL が最近あちらの国ではイケてるっていう話はよく聞いていたけど Puppet3 は PostgreSQL, Clojur, JVM の組み合わせで構成されているらしい。Chef も Erlang に置き換わったあたりを見ての反応かなぁ？と想像。\nMCollective orchestration engine ruby を使って独自のエージェント開発可能 mco rpc service restart service=httpd \u0026ndash;nodes=hosts.txt # query a file mco rpc service restart service=httpd -W country=uk -dm=puppetdb # discover mco rpc rpcutil ping -I example.com # direct addressing ruby のライブラリとして利用可能 query a file のサンプル\nc = rpcclient(\u0026quot;service\u0026quot;) c.discover :nodes =\u0026gt; File.readline(\u0026quot;host.txt\u0026quot;).map {|i| i.chomp} printrpc c.restart(:service =\u0026gt; \u0026quot;httpd\u0026quot;) これはオーケストレーションツール。ホスト名の記述されたファイルから、それらのホ ストに対して一斉に指示を送ったり検索を行ったりコマンドの実行が出来る。これは Ruby ライブラリとして再利用出来るらしい。これは便利。\nGeppetto IDE for developing puppet modules and code git \u0026amp; svn Linux, Mac OSX, Windows Linux, Mac, Windows 用の IDE まで提供してるんですね。驚き。Puppet ものすごい勢 い。\nPuppet Forge module repository 1,500+ modules puppet module install foo/foo module のためのチームが内部にある コミュニティの作ったモジュールを公開しているレポジトリ WEB サービス。1,500 以 上のモジュールが公開されているらしい。このためのチームも Puppet 社内にいるとか。\n迷ったら健全な方を Cookpad Naruta Issei さん 印象的なプレゼンでした。\n現実に起こった問題を切り口に Devops について語っていました。リリース日当日 Ops が「今日リリースだと初めて知る」という事件。繰り返さないためにどうするか？\nリリース日の決定に Ops の承認が必要なルールにする？ Ops 「ソースコードが fix してからリリース日まで3営業日必要？」 このようなルールを作りがちだけど、これでは Ops が権威になってしまう。Ops の立 場からしてこの「権威」になりやすいという特徴があるが、Ops はそう振る舞ってはい けない！という話。DevOps に必要なコミュニケーションで回避出来ると。DevOps であ る限り仕事は楽しくなくてはならない。承認を取るテクニック・政治が発生するのもこ れまた問題。\nCookpad はこれからもコミュニケーションでこういった問題をクリアしていく！と宣言 がありました。最後は若干 Ops 避難のようなプレゼンを Ops の前でするかのような状 況に Issei さんも声を震わせながらのプレゼンでしたが、逆にそれが僕には響きまし た。ぼくも Ops 出身だし、こういった問題は嫌になる程見てきているので、この訴え かけはガツンときた。プレゼン最高でした！\nDataDog Alexis Lê-Quôc さん 監視ツールにとっての重要な要素\nTimely Correct Comprehensive (包括的) これらを満たせなければ全く無駄。\n前提条件を元にメトリクスを決めている -\u0026gt; 不完全なメトリクスが多い -\u0026gt; 最低限の条件でメトリクスに要素を追加できることが重要だと考える statd types gauges counters histgrams timers sets 参考資料 statd と graphite の連携\nhttps://github.com/etsy/statsd/blob/master/docs/graphite.md\nサポートされているバックエンド一覧\nhttps://github.com/etsy/statsd/blob/master/docs/backend.md\nまとめ とても有意義だった。丸一日、最後まで聞いているのも辛かったけど、運営されている 側の方々はもっと大変だったでしょう。ありがとうございました。日本ではなかなか話 を聞けないめちゃホットな方々の話を直に聞けるなんて無いのでホント貴重でした。ま たディスカッションには参加出来なかったけど、久しぶりに会えた方がいてめちゃ良かっ たです。同じ方向向いている方々がこれだけ社外には居るんだという気付きもあったり。\n皆さん、ありがとうございました！\n","permalink":"https://jedipunkz.github.io/post/2013/09/29/devops-day-tokyo-2013-report/","summary":"\u003cp\u003eこんにちは。@jedipunkz です。\u003c/p\u003e\n\u003cp\u003eDevOps Day Tokyo 2013 に参加してきました。たくさんの刺激を受けたのでレポート書\nいておきます。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e開催日 : 2013年09月28日\n場所   : 東京六本木ミッドタウン Yahoo! Japan さま\nURL    : http://www.devopsdays.org/events/2013-tokyo/\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1 id=\"making-operation-visible\"\u003eMaking Operation Visible\u003c/h1\u003e\n\u003cpre\u003e\u003ccode\u003eNick Galbreath (@ngalbreath) さん\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eDevOps の拠点 Etsy に努めた経緯のある DevOps リーダ Galbreath さん。DevOps の\n概略から何が必要でありどう行動に起こせばよいか説明してくださいました。\u003c/p\u003e\n\u003ciframe src=\"http://www.slideshare.net/slideshow/embed_code/26632342\"\nwidth=\"427\" height=\"356\" frameborder=\"0\" marginwidth=\"0\" marginheight=\"0\"\nscrolling=\"no\" style=\"border:1px solid #CCC;border-width:1px 1px\n0;margin-bottom:5px\" allowfullscreen\u003e \u003c/iframe\u003e \u003cdiv\nstyle=\"margin-bottom:5px\"\u003e \u003cstrong\u003e \u003ca\nhref=\"https://www.slideshare.net/nickgsuperstar/making-operations-visible-dev-opsdays-tokyo-2013key\"\ntitle=\"Making operations visible - devopsdays tokyo 2013\"\ntarget=\"_blank\"\u003eMaking operations visible - devopsdays tokyo 2013\u003c/a\u003e\n\u003c/strong\u003e from \u003cstrong\u003e\u003ca href=\"http://www.slideshare.net/nickgsuperstar\"\ntarget=\"_blank\"\u003eNick Galbreath\u003c/a\u003e\u003c/strong\u003e \u003c/div\u003e\n\u003cp\u003eこちら、Galbreath さんの当日の資料。\u003c/p\u003e\n\u003ch2 id=\"devops-が実行出来ない理由\"\u003eDevOps が実行出来ない理由\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eTool が足りない\u003c/li\u003e\n\u003cli\u003e社風の影響\u003c/li\u003e\n\u003cli\u003e見えないモノが価値がないと事業から考えられている\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e出来る事は、価値があるモノの社内への説明と、Tool を使った可視化。データの可視\n化が重要。Ops の人は結構「データをどこそこの部署に見せても理解してもらえない」\nだとか「データを閲覧させると万が一の時にシステムが破損する」等と考えがち。が、\nビジネス寄りの人にとって重要なグラフが含まれていたり、アカウント担当の人に役立\nつものも含まれている。ましてシステムが破損することなど決して無い。\u003c/p\u003e","title":"DevOps Day Tokyo 2013 参加レポート"},{"content":"こんにちは。@jedipunkz です。\n今回はクラスタマネージャである Mesos について書こうと思います。\nMesos は Apache Software Foundation によって管理されるソフトウェアで分散アプリ ケーションをクラスタ化することが出来るマネージャです。Twitter が採用しているこ とで有名だそうで、開発にも積極的に参加しているそうです。\nhttp://mesos.apache.org/\n@riywo さんが既に Mesos + Marathon + Zookeper + Docker な構成を組む手順をブロ グで紹介されていますので是非試してみると面白いと思います。\nhttp://tech.riywo.com/blog/2013/09/27/mesos-introduction-1/\n私は理解した Mesos のアーキテクチャについて少し書いていきたいと思います。\n全体の構造 +-----------+ | zookeeper | | quorum | +-----------+ | +----------------+----------------+ | | | +--------------+ +--------------+ +--------------+ | mesos-master | | mesos-master | | mesos-master | | active | | hot standby | | hot standby | +--------------+ +--------------+ +--------------+ ... | +----------------+----------------+ | | | +--------------+ +--------------+ +--------------+ | mesos-slave | | mesos-slave | | mesos-slave | | executor | | executor | | executor | | +----++----+ | | +----++----+ | | +----++----+ | | |task||task| | | |task||task| | | |task||task| | | +----++----+ | | +----++----+ | | +----++----+ | +--------------+ +--------------+ +--------------+ ... 基本的に few masters + many slaves の構成です。task は slaves の上で走ります。 master はオファーによりアプリケーション(フレームワーク)に対して CPU, メモリの リソースをシェア出来ます。リソースは slave ID, resource1: amount1, resource2, amount2, \u0026hellip; といった配列を含みます。master はポリシに従いそれぞれ のフレームワークに対してリソースをどれだけ提供するか決定します。プラグイン形式 で様々なポリシを取り込む仕組みになっています。\nmesos は zookeeper により HA を組むことが出来ます。1つの active master と複数 (もしくは1つの) stand-by master(s) の構成です。active を投票するため zookeeper のアルゴリズムが用いられます。\nmesos-master を起動する際に下記のパラメータを渡すとmulti masters の構成が組め ます。\n--zk=zk://host1:port1/path,host2:port2/path,... それに対して multi slaves に対しては下記のパラータを渡します。\n--master=zk://host1:port1/path,host2:port2/path,... ネットワークの分断 ネットワークの分断により様々な事象が発生します。幾つかのパターンによる動きにつ いて見ていきます。\nslave が master と分断された場合 ヘルスチェックが失敗。master は slave のことを \u0026lsquo;deactivated\u0026rsquo; と判断しその slave に分配した task を LOST とします。\nmaster に再接続出来ない場合 \u0026lsquo;deactivated\u0026rsquo; となった slave が master に再接続出来ない場合、シャットダウンの 指示が送られる\nslave が zookeeper と分断された場合 master がいない状態となり、再び master の投票が行われ active master が現れるま で master からの指示を拒否する\nリソースのオファ +-------------------+ | framework | | +----+----+ | | |job1|job2| | | +----+----+ | | scheduler | +-------------------+ ~(2) |(3) | ~ +-------------------+ | mesos-master | | allocation module | +-------------------+ ~(1) |(4) | ~ +-------------------+ | mesos-slave | | executor | | +----+----+ | | |task|task| | | +----+----+ | +-------------------+ リソースのオファの流れについて。\n(1) mesos-slave が空きの CPU, Mem の状況(4cpus,4g mem)を mesos-mater に伝える。 (2) mesos-master はそのリソース空き状況を framework に対して伝える (3) framework の scheduler は mesos-mater に対して mesos-slave の上で動かすべ き2個のタスクについて伝える。1個めは 2cpus, 1g mem。2個めは 1cpus, 2g memと。 (4) master は task を slave に対して伝える。結果 1cpu, 1gb mem がアロケートさ れない状況だが、それについては別の framework に対してアロケートすることとな る。 まとめ このクラスタ自体が1つの OS かのような動きを取ることが見て取れます。Mesos の上 で Hadoop, Spark などを稼働させることが出来るそうですが、次回、私は docker を動 かしてみたいと思っています。\nhttp://mesosphere.io/2013/09/26/docker-on-mesos/\nここが参考になります。\n駆け足でしたが、以上です。\n","permalink":"https://jedipunkz.github.io/post/2013/09/28/mesos-architecture-number-1/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003e今回はクラスタマネージャである Mesos について書こうと思います。\u003c/p\u003e\n\u003cp\u003eMesos は Apache Software Foundation によって管理されるソフトウェアで分散アプリ\nケーションをクラスタ化することが出来るマネージャです。Twitter が採用しているこ\nとで有名だそうで、開発にも積極的に参加しているそうです。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://mesos.apache.org/\"\u003ehttp://mesos.apache.org/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e@riywo さんが既に Mesos + Marathon + Zookeper + Docker な構成を組む手順をブロ\nグで紹介されていますので是非試してみると面白いと思います。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://tech.riywo.com/blog/2013/09/27/mesos-introduction-1/\"\u003ehttp://tech.riywo.com/blog/2013/09/27/mesos-introduction-1/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e私は理解した Mesos のアーキテクチャについて少し書いていきたいと思います。\u003c/p\u003e\n\u003ch2 id=\"全体の構造\"\u003e全体の構造\u003c/h2\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e+-----------+\n| zookeeper |\n|  quorum   |\n+-----------+\n|\n+----------------+----------------+\n|                |                |\n+--------------+ +--------------+ +--------------+\n| mesos-master | | mesos-master | | mesos-master |\n|    active    | |  hot standby | |  hot standby |\n+--------------+ +--------------+ +--------------+ ...\n|\n+----------------+----------------+\n|                |                |\n+--------------+ +--------------+ +--------------+\n|  mesos-slave | |  mesos-slave | |  mesos-slave |\n|   executor   | |   executor   | |   executor   |\n| +----++----+ | | +----++----+ | | +----++----+ |\n| |task||task| | | |task||task| | | |task||task| |\n| +----++----+ | | +----++----+ | | +----++----+ |\n+--------------+ +--------------+ +--------------+ ...\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e基本的に few masters + many slaves の構成です。task は slaves の上で走ります。\nmaster はオファーによりアプリケーション(フレームワーク)に対して CPU, メモリの\nリソースをシェア出来ます。リソースは slave ID, resource1: amount1,\nresource2, amount2, \u0026hellip; といった配列を含みます。master はポリシに従いそれぞれ\nのフレームワークに対してリソースをどれだけ提供するか決定します。プラグイン形式\nで様々なポリシを取り込む仕組みになっています。\u003c/p\u003e","title":"Mesos アーキテクチャ #1"},{"content":"こんにちは。@jedipunkz です。\n前回 kibana + elasticsearch + fluentd を構築する方法を載せたのだけど手動で構築 したので格好悪いなぁと思っていました。いうことで！ Chef でデプロイする方法を調 べてみました。\n意外と簡単に出来たし、スッキリした形でデプロイ出来たのでオススメです。\n前提の環境は\u0026hellip; Ubuntu 12.04 LTS precise Chef サーバ構成 入力するログは nginx (例) オールインワン構成 Cookbook が他の OS に対応しているか確認していないので Ubuntu を前提にしていま す。Chef サーバのデプロイや knife の設定は済んでいるものとして説明していきます。 例で nginx のログを入力します。なので nginx も Chef でデプロイします。ここは他 のものに置き換えてもらっても大丈夫です。手順を省略化するためオールインワン構成 で説明します。nginx, fluentd は複数のノードに配置することも手順を読み替えれば もちろん可能です。\nCookbook の準備 お決まり。Cookbooks の取得に Berkshelf を用いる。\n% cd chef-repo % gem install berkshelf % ${EDITOR} Berksfile cookbook \u0026#39;elasticsearch\u0026#39;, git: \u0026#39;https://github.com/elasticsearch/cookbook-elasticsearch.git\u0026#39; cookbook \u0026#39;td-agent\u0026#39;, git: \u0026#39;https://github.com/treasure-data/chef-td-agent.git\u0026#39; cookbook \u0026#39;kibana\u0026#39;, git: \u0026#39;https://github.com/realityforge/chef-kibana.git\u0026#39; cookbook \u0026#39;nginx\u0026#39;, git: \u0026#39;https://github.com/opscode-cookbooks/nginx.git\u0026#39; Cookbooks を取得します。\n% berks install --path=./cookbooks elasticsearch デプロイ準備 elasticsearch の Role を作成する。java, elasticsearch レシピを指定。また Java に openjdk7 を override_attributes にて指定する。openjdk6 だと elasticsearch が起動し なかった。尚、デフォルトは 6 です。Environment で override_attributes を指定し てもらっても構わないです。\n% ${EDITOR} roles/elasticsearch.rb name \u0026#34;elasticsearch\u0026#34; description \u0026#34;Base role applied to elasticsearch nodes.\u0026#34; run_list( \u0026#34;recipe[java]\u0026#34;, \u0026#34;recipe[elasticsearch]\u0026#34; ) override_attributes \u0026#34;java\u0026#34; =\u0026gt; { \u0026#34;install_flavor\u0026#34; =\u0026gt; \u0026#34;openjdk\u0026#34;, \u0026#34;jdk_version\u0026#34; =\u0026gt; \u0026#34;7\u0026#34; } fluentd デプロイの準備 td-agent (fluentd) の Role を作成する。override_attributes にて td-agent のプ ラグインを指定。配列で複数指定可能。こちらも同じく Role で override_attributes を指定したが Environments で指定しても構わないです。あと公式の README には attributes に直接 plugins を指定するよう書いてありましたが、構成毎に plugins は変えたいと思うので、この様に Roles, Environments で指定するのが良いと思います。\n% ${EDITOR} roles/td-agent.rb name \u0026#34;td-agent\u0026#34; description \u0026#34;Base role applied to td-agent nodes.\u0026#34; run_list( \u0026#34;recipe[td-agent]\u0026#34; ) override_attributes \u0026#34;td_agent\u0026#34; =\u0026gt; { \u0026#34;plugins\u0026#34; =\u0026gt; [ \u0026#34;elasticsearch\u0026#34; ] } td-agent.conf の template を作成。今回は nginx の /var/log/nginx/localhost.access.log を入力する。これは例なので好きなログを指定 可能。\n% ${EDITOR} cookbooks/td-agent/templates/default/td-agent.conf.erb \u0026lt;source\u0026gt; type tail format nginx path /var/log/nginx/localhost.access.log tag foo01.nginx.access \u0026lt;/source\u0026gt; \u0026lt;match *.nginx.*\u0026gt; index_name adminpack type_name nginx type elasticsearch include_tag_key true tag_key @log_name host localhost port 9200 logstash_format true flush_interval 10s \u0026lt;/match\u0026gt; デプロイ Cookbooks, Roles を Chef サーバにアップロード。\n% knife cookbook upload -o cookbooks -a % knife role from file roles/*.rb knife bootstrap しデプロイ！\n% knife bootstrap \u0026lt;ip_addr\u0026gt; -N \u0026lt;hostname\u0026gt; -r \\ \u0026#39;role[td-agent]\u0026#39;,\u0026#39;role[elasticsearch]\u0026#39;,\u0026#39;recipe[kibana]\u0026#39;,\u0026#39;recipe[nginx]\u0026#39; \\ --sudo -x \u0026lt;username\u0026gt; 下記の URL にブラウザでアクセスし nginx のログを生成。\nhttp://\u0026lt;ip_addr\u0026gt;/ 下記の URL にて Kibana の UI 越しにログ解析結果を確認出来る。\nhttp://\u0026lt;ip_addr\u0026gt;:5601 まとめ fluentd plugins は複数指定可能。環境毎に td-agent.conf を変更したいという要望 があると思うけど fluentd の Recipe を修正する必要がありそう。それとも td-agent.conf 自体でうまくさばけるのかな？調べてない。オールインワン構成で説明 しましたが、elasticsearch + kibana のノードに対して、複数の nginx(なんでも良い)\ntd-agent ノードの構成が一般的だと思います。手順を読み替えてやってみてくださ い。その場合 kibana の Cookbook の attributes 中に elasticsearch の IP アドレ ス・ポートを記す箇所があるので Role, Environments で上書きしてあげてください。 default[\u0026#39;kibana\u0026#39;][\u0026#39;elasticsearch\u0026#39;][\u0026#39;hosts\u0026#39;] = [\u0026#39;127.0.0.1\u0026#39;] default[\u0026#39;kibana\u0026#39;][\u0026#39;elasticsearch\u0026#39;][\u0026#39;port\u0026#39;] = 9200 あと td-agent/templates/default/td-agent.conf.erb の中に記した host, port の指 定も elasticsearch のホスト情報に書き換えてあげてください。\nhost localhost port 9200 以上です。意外と簡単に出来ました。スッキリ。\n","permalink":"https://jedipunkz.github.io/post/2013/09/13/chef-kibana-elasticsearch-fluentd/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003e前回 kibana + elasticsearch + fluentd を構築する方法を載せたのだけど手動で構築\nしたので格好悪いなぁと思っていました。いうことで！ Chef でデプロイする方法を調\nべてみました。\u003c/p\u003e\n\u003cp\u003e意外と簡単に出来たし、スッキリした形でデプロイ出来たのでオススメです。\u003c/p\u003e\n\u003ch2 id=\"前提の環境は\"\u003e前提の環境は\u0026hellip;\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eUbuntu 12.04 LTS precise\u003c/li\u003e\n\u003cli\u003eChef サーバ構成\u003c/li\u003e\n\u003cli\u003e入力するログは nginx (例)\u003c/li\u003e\n\u003cli\u003eオールインワン構成\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eCookbook が他の OS に対応しているか確認していないので Ubuntu を前提にしていま\nす。Chef サーバのデプロイや knife の設定は済んでいるものとして説明していきます。\n例で nginx のログを入力します。なので nginx も Chef でデプロイします。ここは他\nのものに置き換えてもらっても大丈夫です。手順を省略化するためオールインワン構成\nで説明します。nginx, fluentd は複数のノードに配置することも手順を読み替えれば\nもちろん可能です。\u003c/p\u003e\n\u003ch2 id=\"cookbook-の準備\"\u003eCookbook の準備\u003c/h2\u003e\n\u003cp\u003eお決まり。Cookbooks の取得に Berkshelf を用いる。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e% cd chef-repo\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e% gem install berkshelf\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e% \u003cspan style=\"color:#e6db74\"\u003e${\u003c/span\u003eEDITOR\u003cspan style=\"color:#e6db74\"\u003e}\u003c/span\u003e Berksfile\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ecookbook \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;elasticsearch\u0026#39;\u003c/span\u003e, git: \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;https://github.com/elasticsearch/cookbook-elasticsearch.git\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ecookbook \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;td-agent\u0026#39;\u003c/span\u003e, git: \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;https://github.com/treasure-data/chef-td-agent.git\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ecookbook \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;kibana\u0026#39;\u003c/span\u003e, git: \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;https://github.com/realityforge/chef-kibana.git\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ecookbook \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;nginx\u0026#39;\u003c/span\u003e, git: \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;https://github.com/opscode-cookbooks/nginx.git\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eCookbooks を取得します。\u003c/p\u003e","title":"Chef で kibana + elasticsearch + fluentd デプロイしてみた"},{"content":"こんにちは。@jedipunkz です。\nOpenStack 第14回勉強会 ハッカソンに参加してきました。その時の自分の作業ログを 記しておきます。自分の作業内容は \u0026lsquo;OpenStack + Docker 構築\u0026rsquo; です。\n場所 : CreationLine さま 日時 : 2013年9月8日(土) 当日の atnd。\nhttp://atnd.org/events/42891\n当日発表のあった内容\nAnsible で OpenStack を実際に皆の前でデプロイ！ Yoshiyama さん開発 LogCas お披露目 Havana の機能改善・機能追加内容確認 その他 Horizon の機能についてだったり openstack.jp の運用についてなど 自分が話を聞きながら黙々とやったことは\nOpenStack + Docker 構築 結果\u0026hellip; NG 動かず。時間切れ。公式の wiki の手順がだいぶ変なので手順を修正しながら進めました。\n公式の wiki はこちらにあります。\nhttps://wiki.openstack.org/wiki/Docker\nその修正しながらメモった手順を下記に貼り付けておきます。\n作業環境 ホスト : Ubuntu 12.04.3 Precise OpenStack バージョン : devstack (2013/09/08 master ブランチ) 構成 : オールインワン (with heat, ceilometer, neutron) 普通に動かすとエラーが出力される これは devstack (2013/09/08 時点) での不具合なので直ちに修正されるかも。\nデフォルトのエンコーディングが \u0026lsquo;ascii\u0026rsquo; になっているのが原因らしい.\n% python Python 2.7.3 (default, Apr 10 2013, 06:20:15) [GCC 4.6.3] on linux2 Type \u0026#34;help\u0026#34;, \u0026#34;copyright\u0026#34;, \u0026#34;credits\u0026#34; or \u0026#34;license\u0026#34; for more information. \u0026gt;\u0026gt;\u0026gt; import sys \u0026gt;\u0026gt;\u0026gt; sys.getdefaultencoding() \u0026#39;ascii\u0026#39; \u0026gt;\u0026gt;\u0026gt; エラーの内容は… この状態だと… 下記のようなエラーが nova から出力される。むむむ。\n% nova boot --image \u0026#34;ubuntu:latest\u0026#34; --flavor 1 vm01 ERROR: UnicodeError: \u0026#39;ascii\u0026#39; codec can\u0026#39;t decode byte 0xe5 in position 2: ordinal not in range(128) (HTTP 400) (Request-ID: req-40ebfb9b-d76a-40b8-8f75-facb4dd73db4) とりあえず暫定処置として、下記のように setdefaultencoding(\u0026lsquo;utf-8\u0026rsquo;) を追記。ち なみに devstack デプロイ前にこの作業を済ませました。デプロイ後だと色々めんどい。 当日、何度もめんどい場面に遭遇しました\u0026hellip;\n% ${EDITOR} /usr/lib/python2.7/sitecustomize.py # install the apport exception handler if available try: import apport_python_hook except ImportError: pass else: apport_python_hook.install() # 下記の2行を追記 import sys sys.setdefaultencoding(\u0026#39;utf-8\u0026#39;) Ubuntu12.04 の場合 Linux Kernel 3.8 にアップデート wiki には載っていないが docker が Linux Kernel 3.8 以上を推奨しているため raring から 3.8 を持ってくる。\n% sudo apt-get update % sudo apt-get install linux-image-generic-lts-raring linux-headers-generic-lts-raring % sudo reboot socat インストール 後に実行する install_docker.sh スクリプトが必要とする。実行前に入れないとと痛 い目見る。当日何度も痛い目見ました\u0026hellip;\n% sudo apt-get install socat localrc 追記 localrc に下記を追記。その他のパラメータは各自のモノで良い。\nVIRT_DRIVER=docker install_docker.sh 実行 一般ユーザ権限で OK 。中で sudo している。が、極稀に sudo のタイムアウトが来る ので、色々しんどい。ちなみに僕はここで何度もやりなおしました。\u0026hellip;\n% ./tools/docker/install_docker.sh devstack インストール devstack をデプロイする。\n% ./stack.sh index.docker.io から \u0026lsquo;ubuntu\u0026rsquo; イメージをダウンロード index.docker.io のトップレベルから \u0026lsquo;ubuntu\u0026rsquo; イメージを取得。どのイメージでも良い。\n% sudo docker pull ubuntu Pulling repository ubuntu 8dbd9e392a96: Download complete b750fe79269d: Download complete 27cf78414709: Download complete docker-registry (プライベートレポジトリ) に対して push する -\u0026gt; Glance に登録 この状態で docker-registry.sh というプロセスが起動しているはず。これは docker\nのプライベートレポジトリに相当。5042 tcp で待ち受けているので下記のように tag を打った後、プライベートレポジトリに アップロード。\n% sudo docker tag ubuntu 10.200.9.25:5042/ubuntu % sudo docker push 10.200.9.25:5042/ubuntu The push refers to a repository [10.200.9.25:5042/ubuntu] (len: 1) Sending image list Pushing repository 10.200.9.25:5042/ubuntu (1 tags) Pushing 8dbd9e392a964056420e5d58ca5cc376ef18e2de93b5cc90e868a1bbc8318c1c Pushing tags for rev [8dbd9e392a964056420e5d58ca5cc376ef18e2de93b5cc90e868a1bbc8318c1c] on {http://10.200.9.25:5042/v1/repositories/ubuntu/tags/latest} docker のレポジトリについてはここが参考になる。\nhttp://docs.docker.io/en/latest/use/workingwithrepository/\nglance に登録されていることを確認 \u0026lsquo;ubuntu:latest\u0026rsquo; が表示されるはず。やった。\n% glance image-list +--------------------------------------+---------------------------------+-------------+------------------+----------+--------+ | ID\t| Name\t| Disk Format | Container Format | Size\t| Status | +--------------------------------------+---------------------------------+-------------+------------------+----------+--------+ | f5845be4-1ac0-42c7-9280-a8c316be6beb | cirros-0.3.1-x86_64-uec\t| ami\t| ami\t| 25165824 | active | | aa8bcdff-6eb9-402b-9e27-675648dbe311 | cirros-0.3.1-x86_64-uec-kernel | aki\t| aki\t| 4955792 | active | | f8857736-5613-401a-b28c-02c286271af4 | cirros-0.3.1-x86_64-uec-ramdisk | ari\t| ari\t| 3714968 | active | | ae56cacb-7eb6-413e-bd75-46f3cd63123b | docker-busybox:latest\t| raw\t| docker\t| 2271796 | active | | 613b05c3-30f6-4c53-aa94-103ea516373e | ubuntu:latest\t| raw\t| docker\t| 71497587 | active | +--------------------------------------+---------------------------------+-------------+------------------+----------+--------+ nova boot ぐったり\u0026hellip; nova-scheduler がエラー吐いてるぽいけど、原因つかめず。ハッカソン時間切れ。\n% nova boot --image \u0026#34;ubuntu:latest\u0026#34; --flavor m1.tiny vm01 % nova list +--------------------------------------+------+--------+------------+-------------+----------+ | ID | Name | Status | Task State | Power State | Networks | +--------------------------------------+------+--------+------------+-------------+----------+ | e1563b55-bb5d-43a6-a1fc-c3bc63600ac7 | vm01 | ERROR | None | NOSTATE | | +--------------------------------------+------+--------+------------+-------------+----------+ エラー内容.. 下記のエラーが出力されていた。なんだか OpenStack のメッセージじゃないような。 調べてたら素の Python のメッセージらしく。うーん。devstack なので仕方ない。\nSep 7 14:53:08 devstack01 2013-09-07 14:53:08.490 ERROR nova.compute.manager [req-d4ff6a45-88bc-4d6e-8f9d-43de79e601c6 demo demo] [instance: fdd0fdfb-aaa2-4e91-98cc-cdb347e9ae09] Instance failed to spawn#0122013-09-07 14:53:08.490 31835 TRACE nova.compute.manager [instance: fdd0fdfb-aaa2-4e91-98cc-cdb347e9ae09] Traceback (most recent call last):#0122013-09-07 14:53:08.490 31835 TRACE nova.compute.manager [instance: fdd0fdfb-aaa2-4e91-98cc-cdb347e9ae09] File \u0026#34;/opt/stack/nova/nova/compute/manager.py\u0026#34;, line 1416, in _spawn#0122013-09-07 14:53:08.490 31835 TRACE nova.compute.manager [instance: fdd0fdfb-aaa2-4e91-98cc-cdb347e9ae09] block_device_info)#0122013-09-07 14:53:08.490 31835 TRACE nova.compute.manager [instance: fdd0fdfb-aaa2-4e91-98cc-cdb347e9ae09] File \u0026#34;/opt/stack/nova/nova/virt/docker/driver.py\u0026#34;, line 305, in spawn#0122013-09-07 14:53:08.490 31835 TRACE nova.compute.manager [instance: fdd0fdfb-aaa2-4e91-98cc-cdb347e9ae09] raise exception.InstanceDeployFailure(msg.format(e),#0122013-09-07 14:53:08.490 31835 TRACE nova.compute.manager [instance: fdd0fdfb-aaa2-4e91-98cc-cdb347e9ae09] File \u0026#34;/opt/stack/nova/nova/openstack/common/gettextutils.py\u0026#34;, line 255, in __getattribute__#0122013-09-07 14:53:08.490 31835 TRACE nova.compute.manager [instance: fdd0fdfb-aaa2-4e91-98cc-cdb347e9ae09] return UserString.UserString.__getattribute__(self, name)#0122013-09-07 14:53:08.490 31835 TRACE nova.compute.manager [instance: fdd0fdfb-aaa2-4e91-98cc-cdb347e9ae09] AttributeError: \u0026#39;Message\u0026#39; object has no attribute \u0026#39;format\u0026#39;#0122013-09-07 14:53:08.490 31835 TRACE nova.compute.manager [instance: fdd0fdfb-aaa2-4e91-98cc-cdb347e9ae09] 所感とまとめ 前半の1,2時間、みなさんの話に聞き入ってしまったので時間が…。発表の内容も皆さ んの会話も楽しかった。また参加したい。Netron 周りで聞きたい事とか色々あったの だけどコアデベロッパの方に聞けなかったのが後悔。今度教えてもらおう。あとやっぱ りみなさん詳しい。その情報どこから？ということまでよく知っているし知識が深い。 理解度がまだまだちゃうなぁと実感。次回も参加させていただこうと思います。\n","permalink":"https://jedipunkz.github.io/post/2013/09/09/14th-openstack-study-hackathon/","summary":"\u003cp\u003eこんにちは。@jedipunkz です。\u003c/p\u003e\n\u003cp\u003eOpenStack 第14回勉強会 ハッカソンに参加してきました。その時の自分の作業ログを\n記しておきます。自分の作業内容は \u0026lsquo;OpenStack + Docker 構築\u0026rsquo; です。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e場所 : CreationLine さま\n日時 : 2013年9月8日(土)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e当日の atnd。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://atnd.org/events/42891\"\u003ehttp://atnd.org/events/42891\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e当日発表のあった内容\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAnsible で OpenStack を実際に皆の前でデプロイ！\u003c/li\u003e\n\u003cli\u003eYoshiyama さん開発 LogCas お披露目\u003c/li\u003e\n\u003cli\u003eHavana の機能改善・機能追加内容確認\u003c/li\u003e\n\u003cli\u003eその他 Horizon の機能についてだったり openstack.jp の運用についてなど\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e自分が話を聞きながら黙々とやったことは\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eOpenStack + Docker 構築\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e結果\u0026hellip; NG 動かず。時間切れ。公式の wiki の手順がだいぶ変なので手順を修正しながら進めました。\u003c/p\u003e\n\u003cp\u003e公式の wiki はこちらにあります。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://wiki.openstack.org/wiki/Docker\"\u003ehttps://wiki.openstack.org/wiki/Docker\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eその修正しながらメモった手順を下記に貼り付けておきます。\u003c/p\u003e\n\u003ch2 id=\"作業環境\"\u003e作業環境\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eホスト : Ubuntu 12.04.3 Precise\u003c/li\u003e\n\u003cli\u003eOpenStack バージョン : devstack (2013/09/08 master ブランチ)\u003c/li\u003e\n\u003cli\u003e構成 : オールインワン (with heat, ceilometer, neutron)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"普通に動かすとエラーが出力される\"\u003e普通に動かすとエラーが出力される\u003c/h2\u003e\n\u003cp\u003eこれは devstack (2013/09/08 時点) での不具合なので直ちに修正されるかも。\u003c/p\u003e","title":"第14回 OpenStack 勉強会参加ログ"},{"content":"こんにちは。@jedipunkz です。\n{% img /pix/kibana3.png %}\n前回の記事で Kibana + elasticsearch + fluentd を試しましたが、ツイッターで @nora96o さんに \u0026ldquo;Kibana3 使うと、幸せになれますよ！\u0026rdquo; と教えてもらいました。早 速試してみましたので、メモしておきます。\n前回の記事。\nhttp://jedipunkz.github.io/blog/2013/09/07/kibana-plus-elasticsearch-plus-fluentd/\n前半の手順は前回と同様ですが、念のため書いておきます。\n前提の環境 OS : Ubuntu 12.04 Precise (同じ方法で 13.04 Raring でも出来ました) 必要なパッケージのインストール 下記のパッケージを事前にインストールします。\n% sudo apt-get install git-core build-essential ruby1.9.3 openjdk-7-jdk 手順を省くために Ruby はパッケージで入れました。また Java は他の物を利用しても 構いません。Ruby は fluentd が、Java は elasticsearch が必要とします。\nelasticsearch のインストール 下記のサイトより elasticsearch をダウンロードします。\nhttp://www.elasticsearch.org/download/\n現時点 (2013/09/08) で最新のバージョンは 0.90.3 でした。\n% wget https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-0.90.3.deb % sudo dpkg -i elasticsearch-0.90.3.deb % sudo service elasticsearch start fluentd のインストール fluentd を下記の通りインストールします。\n% sudo -i % curl -L http://toolbelt.treasure-data.com/sh/install-ubuntu-precise.sh | sh /etc/td-agent/td-agent.conf を下記の通りに修正します。サンプルで Apache のログ を elasticsearch に飛ばす設定にします。\n\u0026lt;source\u0026gt; type tail format apache path /var/log/apache2/access.log tag kibana01.apache.access \u0026lt;/source\u0026gt; \u0026lt;match *.apache.*\u0026gt; index_name adminpack type_name apache type elasticsearch include_tag_key true tag_key @log_name host 10.0.1.8 port 9200 logstash_format true flush_interval 3s \u0026lt;/match\u0026gt; host パラメータは環境に合わせて設定します。\nfluentd を起動します。\n% sudo service td-agent start Apache のインストール システムに必要なわけではありませんが、サンプルで Apache のログを fluentd 経由 で飛ばして Kibana で確認したいのでインストールします。\n% sudo apt-get install apache2 % sudo chown -R td-agent /var/log/apache2 ファイルのオーナを td-agent にする必要がありました。\nKibana3 のインストール 下記のサイトから Kibana3 をダウンロードします。\nhttp://three.kibana.org/intro.html\n% wget https://github.com/elasticsearch/kibana/archive/master.tar.gz % tar zxvf master.tar.gz % sudo mv kibana-master /var/www/kibana3\nconfig.js を修正します。elasticsearch の起動している URL を入力します。ここで 注意が必要なのが、ブラウザからアクセス出来る elasticsearch のアドレスにする必 要があるという点です。従来の Kibana の場合は Ruby 製だったため Kibana から elasticsearch に到達できれば良かったのですが、Kibana3 は JavaScript 製ですので ブラウザから elasticsearch の 9200 版ポートに到達出来る必要があります。\n% sudo ${EDITOR} /var/www/kibana3/config.js ...snip... elasticsearch: \u0026#34;http://\u0026lt;ElasticSearch IP addr\u0026gt;:9200\u0026#34;, ...snip... これで完了です。\nブラウザでアクセス あとはブラウザでアクセスするだけです。\nhttp://\u0026lt;kibana IP addr\u0026gt;/kibana3/ まとめ ElasticSearch を外部に公開する必要があるのだけど、その辺りどう制限掛けるかは別 途調べないといけないなぁと。あと ElasticSearch, fluentd を Cookbook でデプロイ するのも出来そう。もちろん Apache も。次回以降の記事に載せます。またブラウザで アクセスした後に様々な形式のグラフ等を追加していけました。もちろん Kibana3 を 使わないでも elasticseach だけで API によるログの検索が出来たりして、僕らエン ジニアにとってはめっちゃくちゃメリットあります。あぁなんでもっと早く使ってなかっ たんだろう。elasticsearch\u0026hellip;。\n","permalink":"https://jedipunkz.github.io/post/2013/09/08/kibana3-plus-elasticsearch-plus-fluentd/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003e{% img /pix/kibana3.png %}\u003c/p\u003e\n\u003cp\u003e前回の記事で Kibana + elasticsearch + fluentd を試しましたが、ツイッターで\n@nora96o さんに \u0026ldquo;Kibana3 使うと、幸せになれますよ！\u0026rdquo; と教えてもらいました。早\n速試してみましたので、メモしておきます。\u003c/p\u003e\n\u003cp\u003e前回の記事。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://jedipunkz.github.io/blog/2013/09/07/kibana-plus-elasticsearch-plus-fluentd/\"\u003ehttp://jedipunkz.github.io/blog/2013/09/07/kibana-plus-elasticsearch-plus-fluentd/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e前半の手順は前回と同様ですが、念のため書いておきます。\u003c/p\u003e\n\u003ch2 id=\"前提の環境\"\u003e前提の環境\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eOS : Ubuntu 12.04 Precise (同じ方法で 13.04 Raring でも出来ました)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"必要なパッケージのインストール\"\u003e必要なパッケージのインストール\u003c/h2\u003e\n\u003cp\u003e下記のパッケージを事前にインストールします。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e% sudo apt-get install git-core build-essential ruby1.9.3 openjdk-7-jdk\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e手順を省くために Ruby はパッケージで入れました。また Java は他の物を利用しても\n構いません。Ruby は fluentd が、Java は elasticsearch が必要とします。\u003c/p\u003e\n\u003ch2 id=\"elasticsearch-のインストール\"\u003eelasticsearch のインストール\u003c/h2\u003e\n\u003cp\u003e下記のサイトより elasticsearch をダウンロードします。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://www.elasticsearch.org/download/\"\u003ehttp://www.elasticsearch.org/download/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e現時点 (2013/09/08) で最新のバージョンは 0.90.3 でした。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e% wget https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-0.90.3.deb\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e% sudo dpkg -i elasticsearch-0.90.3.deb\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e% sudo service elasticsearch start\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"fluentd-のインストール\"\u003efluentd のインストール\u003c/h2\u003e\n\u003cp\u003efluentd を下記の通りインストールします。\u003c/p\u003e","title":"Kibana3 + elasticsearch + fluentd を試した"},{"content":"こんにちは。@jedipunkz です。\n自動化の流れを検討する中でログ解析も忘れてはいけないということで ElasticSearch を使いたいなぁとぼんやり考えていて Logstash とか Kibana とかいうキーワードも目 に止まるようになってきました。\nElasticSaerch は API で情報を検索出来たりするので自動化にもってこい。バックエ ンドに Logstash を使って\u0026hellip; と思ってたのですが最近よく聞くようになった fluentd をそろそろ真面目に使いたい！ということで、今回は Kibana + ElasticSearch + fluentd の組み合わせでログ解析システムを組む方法をメモしておきます。\n参考にさせて頂いた URL http://memocra.blogspot.jp/2013/04/kibanakibanaelasticsearchfluentd.html\n前提の環境 OS : Ubuntu 12.04 Precise (同じ方法で 13.04 Raring でも出来ました) 必要なパッケージインストール 下記のパッケージを事前にインストールします。\n% sudo apt-get install git-core build-essential ruby1.9.3 openjdk-7-jdk 手順を省くために Ruby はパッケージで入れました。また Java は他の物を利用しても 構いません。Ruby は Kibana, fluentd が、Java は ElasticSearch が必要とします。\nElasticSearch のインストール 下記のサイトより ElasticSearch をダウンロードします。\nhttp://www.elasticsearch.org/download/\n現時点 (2013/09/07) で最新のバージョンは 0.90.3 でした。\n% wget https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-0.90.3.deb % sudo dpkg -i elasticsearch-0.90.3.deb % sudo service elasticsearch start Kibana のインストール Kibana をダウンロードして Gemfile にしたがって必要なモノをインストールします。\n% git clone --branch=kibana-ruby https://github.com/rashidkpc/Kibana.git % sudo gem install bundler % cd Kibana % sudo bundle install % sudo gem install tzinfo-data KibanaConfig.rb の KibanaHost はリスンアドレスになるので 0.0.0.0 に修正sます。\nKibanaHost = \u0026#39;0.0.0.0\u0026#39; Kibana を起動します。\n% ruby kibana.rb == Sinatra/1.4.3 has taken the stage on 5601 for development with backup from Thin \u0026gt;\u0026gt; Thin web server (v1.5.1 codename Straight Razor) \u0026gt;\u0026gt; Maximum connections set to 1024 \u0026gt;\u0026gt; Listening on 0.0.0.0:5601, CTRL+C to stop fluentd のインストール fluentd を下記の通りインストールします。\n% sudo -i % curl -L http://toolbelt.treasure-data.com/sh/install-ubuntu-precise.sh | sh /etc/td-agent/td-agent.conf を下記の通りに修正します。サンプルで Apache のログ を ElasticSearch に飛ばす設定にします。\n\u0026lt;source\u0026gt; type tail format apache path /var/log/apache2/access.log tag kibana01.apache.access \u0026lt;/source\u0026gt; \u0026lt;match *.apache.*\u0026gt; index_name adminpack type_name apache type elasticsearch include_tag_key true tag_key @log_name host 10.0.1.8 port 9200 logstash_format true flush_interval 3s \u0026lt;/match\u0026gt; host パラメータは環境に合わせて設定します。\nfluentd を起動します。\n% sudo service td-agent start Apache のインストール システムに必要なわけではありませんが、サンプルで Apache のログを fluentd 経由 で飛ばして Kibana で確認したいのでインストールします。\n% sudo apt-get install apache2 % sudo chown -R td-agent /var/log/apache2 ファイルのオーナを td-agent にする必要がありました。\nブラウザでアクセス Apache2 のポート 80 にブラウザでアクセスし、その後 ポート 5601 にアクセスする と Kibana の画面が表示されます。ポート 80 のアクセスに応じて結果が出力されます。\n{% img /pix/kibana-cap.png %}\nまとめ LogStash でも今度試してみたい。 あと API をどう叩くかは..\nhttp://www.elasticsearch.org/guide/reference/api/\nにリファレンスがあるので、時間を見つけてやってみる。Apache 以外のログ転送につ いては fluentd を詳細に知る必要があるので、そちらも時間を見つけてやってみる。 kibana.rb を起動するもっと良い方法がないかも調べないと。\n2013.09.08追記\n@nora96o さんに「Kibana3 使うと幸せになれますよ！」と教えて頂いて早速 Kibana3 も試してみました。\nhttp://jedipunkz.github.io/blog/2013/09/08/kibana3-plus-elasticsearch-plus-fluentd/\n","permalink":"https://jedipunkz.github.io/post/2013/09/07/kibana-plus-elasticsearch-plus-fluentd/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003e自動化の流れを検討する中でログ解析も忘れてはいけないということで ElasticSearch\nを使いたいなぁとぼんやり考えていて Logstash とか Kibana とかいうキーワードも目\nに止まるようになってきました。\u003c/p\u003e\n\u003cp\u003eElasticSaerch は API で情報を検索出来たりするので自動化にもってこい。バックエ\nンドに Logstash を使って\u0026hellip; と思ってたのですが最近よく聞くようになった fluentd\nをそろそろ真面目に使いたい！ということで、今回は Kibana + ElasticSearch +\nfluentd の組み合わせでログ解析システムを組む方法をメモしておきます。\u003c/p\u003e\n\u003ch2 id=\"参考にさせて頂いた-url\"\u003e参考にさせて頂いた URL\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"http://memocra.blogspot.jp/2013/04/kibanakibanaelasticsearchfluentd.html\"\u003ehttp://memocra.blogspot.jp/2013/04/kibanakibanaelasticsearchfluentd.html\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"前提の環境\"\u003e前提の環境\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eOS : Ubuntu 12.04 Precise (同じ方法で 13.04 Raring でも出来ました)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"必要なパッケージインストール\"\u003e必要なパッケージインストール\u003c/h2\u003e\n\u003cp\u003e下記のパッケージを事前にインストールします。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e% sudo apt-get install git-core build-essential ruby1.9.3 openjdk-7-jdk\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e手順を省くために Ruby はパッケージで入れました。また Java は他の物を利用しても\n構いません。Ruby は Kibana, fluentd が、Java は ElasticSearch が必要とします。\u003c/p\u003e\n\u003ch2 id=\"elasticsearch-のインストール\"\u003eElasticSearch のインストール\u003c/h2\u003e\n\u003cp\u003e下記のサイトより ElasticSearch をダウンロードします。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://www.elasticsearch.org/download/\"\u003ehttp://www.elasticsearch.org/download/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e現時点 (2013/09/07) で最新のバージョンは 0.90.3 でした。\u003c/p\u003e","title":"Kibana + ElasticSearch + fluentd を試してみた"},{"content":"こんにちは。@jedipunkz です。\n自宅の IPv6 化、したいなぁとぼんやり考えていたのですが、Hurricane Electric Internet Services を見つけました。IPv4 の固定グローバル IP を持っていれば誰で も IPv6 のトンネルサービスを無料で受けられるサービスです。\n1つのユーザで5アカウントまで取得でき (5 エンドポイント)、1アカウントで /64 の アドレスがもらえます。また申請さえすれば (クリックするだけ) /48 も1アカウント 毎にもらえます。つまり /48 x 5 + /64 x 5 \u0026hellip; でか！\n私の宅内は Vyatta で PPPOE してるのですが、各種 OS (Debian, NetBSD\u0026hellip;) や機器 (Cisco, JunOS..)のコンフィギュレーションを自動生成してくれるので、接続するだけ であればそれをターミナルに貼り付けるだけ！です。\nサービス URL Hurricane Electric は下記の URL です。アカウントもここで作成出来ます。\nhttp://tunnelbroker.net\nIPv6 接続性を確保する方法 Vyatta が IPv6 のアドレスを持ち接続性を確保するだけであれば、上に記したように コピペで出来ます。上記の URL でアカウントを作成しログインします。左メニューの \u0026ldquo;Create Regular Tunnel\u0026rdquo; を押して、自分の情報 (IPv4 のエンドポイントアドレス等) を入力します。その後、取得した IPv6 のレンジのリンクをクリックし上記メニュー \u0026ldquo;Example Configuration\u0026rdquo; を選択します。プルダウンメニューが現れるので、自宅の OS や機器に合った名前を選択します。\nすると下記のようなコマンドがテキストエリアに出力されるのでこれをコピペする\u0026hellip;だ けです。\nconfigure edit interfaces tunnel tun0 set encapsulation sit set local-ip xxx.xxx.xxx.xxx set remote-ip xxx.xxx.xxx.xxx set address 2001:470:xx:xxx:2/64 set description \u0026#34;HE.NET IPv6 Tunnel\u0026#34; exit set protocols static interface-route6 ::/0 next-hop-interface tun0 commit コピペしたら接続できたか確認しましょう。\n% ping6 ipv6.google.com -c 3 PING ipv6.google.com(2404:6800:4004:803::1010) 56 data bytes 64 bytes from 2404:6800:4004:803::1010: icmp_seq=1 ttl=59 time=32.6 ms 64 bytes from 2404:6800:4004:803::1010: icmp_seq=2 ttl=59 time=27.4 ms 64 bytes from 2404:6800:4004:803::1010: icmp_seq=3 ttl=59 time=20.5 ms --- ipv6.google.com ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 2002ms rtt min/avg/max/mdev = 20.542/26.901/32.677/4.971 ms Vyatta 配下の宅内 LAN の端末を IPv6 化する ここまでの操作だと Vyatta は IPv6 トンネル接続出来ましたが、その配下の機器はま だ接続できていません。そこで IPv6 RA (Router Advertise) します。\nconfigure edit interfaces ethernet eth0 set ipv6 router-advert send-advert true set ipv6 router-advert cur-hop-limit 64 set ipv6 router-advert max-interval 10 set ipv6 router-advert other-config-flag true set ipv6 router-advert default-preference high set ipv6 router-advert managed-flag true set ipv6 router-advert prefix 2001:470:xx:xxx::/64 set ipv6 router-advert prefix 2001:470:xx:xxx::/64 autonomous-flag true commit ここで RA のみの起動をしていますが、LAN 内に Windows 端末がある場合、IPv6 ネー ムサーバの情報が取得できないため DHCPv6 サーバを別途起動する必要があります。が、 私の LAN 内には Mac, Linux, *BSD しかありませんでしたので起動しませんでした。 必要に応じて起動してください。\nIPv6 ファイアーウォール設定 これで端末も IPv6 トンネル接続できるようになった！のですが、外部から端末に直接 接続が出来てしまいます。ファイアーウォールを設定しましょう。\nconfigure set firewall ipv6-name tun-in description \u0026#34;IPv6 Traffice to Internal\u0026#34; set firewall ipv6-name tun-in default-action drop set firewall ipv6-name tun-in rule 10 action accept set firewall ipv6-name tun-in rule 10 description \u0026#34;Accept Established-Related\u0026#34; set firewall ipv6-name tun-in rule 10 state established enable set firewall ipv6-name tun-in rule 10 state related enable set firewall ipv6-name tun-local default-action drop set firewall ipv6-name tun-local description \u0026#34;IPv6 Traffic to Router\u0026#34; set firewall ipv6-name tun-local rule 10 action accept set firewall ipv6-name tun-local rule 10 description \u0026#34;Accept Established-Related\u0026#34; set firewall ipv6-name tun-local rule 10 state established enable set firewall ipv6-name tun-local rule 10 state related enable set interface tunnel tun0 firewall in ipv6-name tun-in set interface tunnel tun0 firewall local ipv6-name tun-local commit save まとめ ルータ配下のサービスを IPv6 で外部に公開したい場合は tun-in のファイアーウォールに ルールを追加すれば OK です。\nまた自動生成出来るサンプルコンフィギュレーションに対応したOS, 機器の種類が豊富です。 pfSense や Solaris, Windows, ScreenOS, Fortigate 等など、多岐にわたっているの で、大抵の方は問題ないのではないでしょうか。\n","permalink":"https://jedipunkz.github.io/post/2013/09/01/hurricane-electric-vyatta-ipv6/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003e自宅の IPv6 化、したいなぁとぼんやり考えていたのですが、Hurricane Electric\nInternet Services を見つけました。IPv4 の固定グローバル IP を持っていれば誰で\nも IPv6 のトンネルサービスを無料で受けられるサービスです。\u003c/p\u003e\n\u003cp\u003e1つのユーザで5アカウントまで取得でき (5 エンドポイント)、1アカウントで /64 の\nアドレスがもらえます。また申請さえすれば (クリックするだけ) /48 も1アカウント\n毎にもらえます。つまり /48 x 5 + /64 x 5 \u0026hellip; でか！\u003c/p\u003e\n\u003cp\u003e私の宅内は Vyatta で PPPOE してるのですが、各種 OS (Debian, NetBSD\u0026hellip;) や機器\n(Cisco, JunOS..)のコンフィギュレーションを自動生成してくれるので、接続するだけ\nであればそれをターミナルに貼り付けるだけ！です。\u003c/p\u003e\n\u003ch2 id=\"サービス-url\"\u003eサービス URL\u003c/h2\u003e\n\u003cp\u003eHurricane Electric は下記の URL です。アカウントもここで作成出来ます。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://tunnelbroker.net\"\u003ehttp://tunnelbroker.net\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"ipv6-接続性を確保する方法\"\u003eIPv6 接続性を確保する方法\u003c/h2\u003e\n\u003cp\u003eVyatta が IPv6 のアドレスを持ち接続性を確保するだけであれば、上に記したように\nコピペで出来ます。上記の URL でアカウントを作成しログインします。左メニューの\n\u0026ldquo;Create Regular Tunnel\u0026rdquo; を押して、自分の情報 (IPv4 のエンドポイントアドレス等)\nを入力します。その後、取得した IPv6 のレンジのリンクをクリックし上記メニュー\n\u0026ldquo;Example Configuration\u0026rdquo; を選択します。プルダウンメニューが現れるので、自宅の\nOS や機器に合った名前を選択します。\u003c/p\u003e","title":"Hurricane Electric + Vyatta で宅内 IPv6 化"},{"content":"こんにちは。@jedipunkz です。\n以前、こんな記事をブログに記しました。2012/06 の記事です。\nhttp://jedipunkz.github.io/blog/2012/06/13/vyatta-vpn/\nその後、PPTP で保護されたネットワークの VPN パスワードを奪取出来るツールが公開 されました。2012/07 のことです。よって今では VPN に PPTP を用いることが推奨さ れていません。\nということで L2TP over IPsec による VPN 構築を Vyatta で行う方法を記します。\nfig.1 : home lan と vyatta のアドレス +--------+ +-----+ home lan ---| vyatta | --- the internet --- | CPE | +--------+ +-----+ X.X.X.X/X(NAT) pppoe0 Y.Y.Y.Y この様に X.X.X.X/X と Y.Y.Y.Y/Y が関係しているとします。CPE は VPN により X.X.X.X/X に接続することが出来ます。\n手順 : IPsec 下記の操作で IPsec を待ち受けるインターフェースの設定します。\n% configure # edit vpn ipsec # set ipsec-interface interface pppoe0 インターフェース名は環境に合わせて設定してください。私の環境では pppoe0 です。\nIPsec パケットが NAT を超えるように設定します。\n# set nat-traversal enable CPE がどの環境にいるか、IPsec 接続を許可するネットワークアドレスを入力します。 渡しの場合はどこからでも接続できるよう 0.0.0.0/0 を入力しました。\n# set nat-networks allowed-network 0.0.0.0/0 # exit 手順 : L2TP 入力を省くために edit します。\n# edit vpn l2tp remote-access IPsec 認証モードに pre-shared-secret を選択し pre-shared-secret (パスワード) を設定します。このパスワードは接続するユーザ全員が知るものです。また認証モード を local に設定します。\n# set ipsec-settings authentication mode pre-shared-secret # set ipsec-settings authentication pre-shared-secret \u0026lt;パスワード\u0026gt; # set authentication mode local 接続ユーザを作成します。パスワードはユーザに合わせたパスワードを入力します。先 ほど設定した pre-shared-secret とは別のパスワードを設定するべきです。\n# set authentication local-users username \u0026lt;ユーザ名\u0026gt; password \u0026lt;パスワード\u0026gt; 接続するネットワーク情報を記します。pool のアドレス範囲は環境に合わせて設定し てください。渡しの場合は第4オクテット 100 - 120 を設定しました。\n# set outside-address Y.Y.Y.Y # set client-ip-pool start X.X.X.100 # set client-ip-pool stop X.X.X.120 最後に commit \u0026amp; save を忘れずに。\n# commit # save まとめ iPhone で接続を確認しました。iOS の場合は L2TP, PPTP, IPsec と3つのタブがあり、 どれかを選択するようになっていますが、L2TP over IPsec の場合には L2TP タブの項 目に情報を入力すれば OK です。\n","permalink":"https://jedipunkz.github.io/post/2013/08/24/vyatta-l2tp-ipsec-vpn/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003e以前、こんな記事をブログに記しました。2012/06 の記事です。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://jedipunkz.github.io/blog/2012/06/13/vyatta-vpn/\"\u003ehttp://jedipunkz.github.io/blog/2012/06/13/vyatta-vpn/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eその後、PPTP で保護されたネットワークの VPN パスワードを奪取出来るツールが公開\nされました。2012/07 のことです。よって今では VPN に PPTP を用いることが推奨さ\nれていません。\u003c/p\u003e\n\u003cp\u003eということで L2TP over IPsec による VPN 構築を Vyatta で行う方法を記します。\u003c/p\u003e\n\u003ch2 id=\"fig1--home-lan-と-vyatta-のアドレス\"\u003efig.1 : home lan と vyatta のアドレス\u003c/h2\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e                +--------+                      +-----+\n    home lan ---| vyatta | --- the internet --- | CPE |\n                +--------+                      +-----+\n    X.X.X.X/X(NAT)     pppoe0\n                       Y.Y.Y.Y\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eこの様に X.X.X.X/X と Y.Y.Y.Y/Y が関係しているとします。CPE は VPN により\nX.X.X.X/X に接続することが出来ます。\u003c/p\u003e\n\u003ch2 id=\"手順--ipsec\"\u003e手順 : IPsec\u003c/h2\u003e\n\u003cp\u003e下記の操作で IPsec を待ち受けるインターフェースの設定します。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e% configure\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# edit vpn ipsec\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# set ipsec-interface interface pppoe0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eインターフェース名は環境に合わせて設定してください。私の環境では pppoe0 です。\u003c/p\u003e","title":"Vyatta で L2TP over IPsec による VPN 構築"},{"content":"こんにちは。@jedipunkz です\n今更なのかもしれませんが、OpenStack の nova-network を IPv6 対応する方法を調べ てみました。何故 nova-network なのか? 自宅の構成が nova-network だからです..。 以前は Quantum (現 Neutron) 構成で使っていましたが、ノードをコントローラとコン ピュートに別けた時に NIC が足らなくなり\u0026hellip;。\nさて本題です。下記のサイトを参考にしました。ほぼそのままの手順ですが、自分のた めにもメモです。\n参考 URL http://docs.openstack.org/grizzly/openstack-compute/admin/content/configuring-compute-to-use-ipv6-addresses.html\n前提 OpenStack の構成は予め構築されている nova-network を用いている 構成はオールインワンでも複数台構成でも可能 手順 nova がインストールされているすべてのノードで python-netaddr をインストールし ます。私の場合は rcbops の chef cookbooks で構築したのですが、既にインストール されていました。\n% sudo apt-get install python-netaddr nova-network が稼働しているノードで radvd をインストールします。これは IPv6 を Advertise しているルータ等が予め備わっている環境であっても、インストー ルする必要があります。また /etc/radvd.conf が初め無いので radvd 単体では稼働し ませんが、問題ありません。OpenStack の場合 /var/lib/nova 配下のコンフィギュレー ションファイルを読み込んでくれます。\n% sudo apt-get install radvd /etc/sysctl.conf に下記の記述を追記します。RA の受信とフォワーディングを許可し ています。\n% ${EDITOR} /etc/sysctl.conf net.ipv6.conf.default.forwarding=1 net.ipv6.conf.all.forwarding = 1 net.ipv6.conf.all.accept_ra = 1 % sudo sysctl -p nova がインストールされている全てのノードで /etc/nova.conf に下記の行を追記し ます。\n% sudo ${EDITOR} /etc/nova/nova.conf use_ipv6=true # \u0026lt;- 追記 nova プロセスを再起動します。\n% cd /etc/init.d/; for i in $( ls nova-* ); do sudo service $i restart; done 次に IPv6 のレンジの仮想ネットワークを nova-manage コマンドで生成します。環境にあったレンジを 追加してください。\n% sudo nova-manage network create public --fixed_range_v4 x.x.x.x/24 \\ --fixed_range_v6 xxxx:xxx:xx:xxx::/64 --bridge=br300 --bridge_interface=eth0 \\ --dns1=8.8.8.8 --dns2=8.8.4.4 --multi_host=T ここで IPv4 のレンジも追加しなくてはならないようです。IPv6 オンリーで生成したところ、 nova-network が下記のエラーを吐いてハングアップしました。\nTRACE nova Stderr: \u0026#39;Error: an inet prefix is expected rather than \u0026#34;None\u0026#34;.\\n\u0026#39; また仮想ネットワーク生成時に指定した物理ネットワークインターフェース名は IPv6 が通信出来る セグメントのものを利用してください。(環境に合わせてください)\nあとは nova boot で仮想マシンを生成すると IPv4, IPv6 のデュアルスタックで起動してきます。\nまとめ OpenStack 外の構成についても IPv6 にもちろん対応している必要があります。上記の例だと OpenStack の eth0 側の IPv6 ネットワークを適切にルーティングしてくれるルータが必要です。 自分の場合は自宅で Vyatta を使って行いました。(今度その方法も記そうと思います) また floating ip は IPv6 には対応していないそうです。はじめこの方法を取ろうと思ったのですがダメでした。 上記のように public ネットワーク側のネットワークインターフェースをブリッジインターフェースにして public ネットワークを生成する方法で対応出来ますので問題ないかと。\nまた、Neutron 構成でも今度 IPv6 対応してみないと。:D\n","permalink":"https://jedipunkz.github.io/post/2013/08/18/openstack-nova-network-ipv6/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e です\u003c/p\u003e\n\u003cp\u003e今更なのかもしれませんが、OpenStack の nova-network を IPv6 対応する方法を調べ\nてみました。何故 nova-network なのか? 自宅の構成が nova-network だからです..。\n以前は Quantum (現 Neutron) 構成で使っていましたが、ノードをコントローラとコン\nピュートに別けた時に NIC が足らなくなり\u0026hellip;。\u003c/p\u003e\n\u003cp\u003eさて本題です。下記のサイトを参考にしました。ほぼそのままの手順ですが、自分のた\nめにもメモです。\u003c/p\u003e\n\u003ch2 id=\"参考-url\"\u003e参考 URL\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"http://docs.openstack.org/grizzly/openstack-compute/admin/content/configuring-compute-to-use-ipv6-addresses.html\"\u003ehttp://docs.openstack.org/grizzly/openstack-compute/admin/content/configuring-compute-to-use-ipv6-addresses.html\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"前提\"\u003e前提\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eOpenStack の構成は予め構築されている\u003c/li\u003e\n\u003cli\u003enova-network を用いている\u003c/li\u003e\n\u003cli\u003e構成はオールインワンでも複数台構成でも可能\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"手順\"\u003e手順\u003c/h2\u003e\n\u003cp\u003enova がインストールされているすべてのノードで python-netaddr をインストールし\nます。私の場合は rcbops の chef cookbooks で構築したのですが、既にインストール\nされていました。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e% sudo apt-get install python-netaddr\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003enova-network が稼働しているノードで radvd をインストールします。これは\nIPv6 を Advertise しているルータ等が予め備わっている環境であっても、インストー\nルする必要があります。また /etc/radvd.conf が初め無いので radvd 単体では稼働し\nませんが、問題ありません。OpenStack の場合 /var/lib/nova 配下のコンフィギュレー\nションファイルを読み込んでくれます。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e% sudo apt-get install radvd\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e/etc/sysctl.conf に下記の記述を追記します。RA の受信とフォワーディングを許可し\nています。\u003c/p\u003e","title":"OpenStack nova-network IPv6 対応"},{"content":"こんにちは。@jedipunkz です。\nrcbops Cookbooks で Neutron 構成の OpenStack をデプロイする方法を書きたいと思 います。先日紹介した openstack-chef-repo にも Neutron のレシピが含まれているの ですが、まだまだ未完成で手作業をおりまぜながらのデプロイになっていまうので、今 現在のところ Neutron 構成を組みたいのであればこの rcbops の Cookbooks を用いる しかないと思います。\n今回は VLAN モードの構築を紹介します。GRE モードも少し手順を修正すれば構成可能 です。最後のまとめに GRE モードの構築について少し触れています。\n構成 public network +----------------+----------------+----------------+----------------+---------------- | | | +--------------+ +--------------+ +--------------+ +--------------+ +--------------+ | controller01 | | network01 | | network02 | | compute01 | | compute02 | +--------------+ +-------+------+ +-------+------+ +-------+------+ +-------+------+ | | | | | | | | | management network +----------------+-------o--------+-------o--------+-------o--------+-------o-------- | | | | vm network +----------------+----------------+----------------+-------- 特徴は\u0026hellip;\nnetwork ノード 2台 (何台でも可) compute ノード 2台 (何台でも可) api ネットワークは management ネットワーク上に展開 (別けても可) controller ノードは1台 (ha な cookbooks を使うので2台構成も可) 図には表現されていませんが management ネットワーク上に Chef ワークステーションと Chef サーバが必要 物理 NIC のマッピング情報 controller : management(eth0), public(eth1) network : management(eth0), vm(eth1), public(eth2) compute : management(eth0), vm(eth1) 前提のネットワークセグメント情報 ここでは仮に下記のネットワークセグメントを前提として手順を記します。\npublic ネットワーク : 10.0.0.0/24 vm ネットワーク : 10.0.1.0/24 management(api) ネットワーク : 10.0.2.0/24 デプロイの準備 Chef 的なワークステーション端末から操作を行います。皆さんの使っているノート PC 等で OK です。\n% git clone https://github.com/rcbops/chef-cookbooks.git ~/chef-repo % cd ~/chef-repo v4.0.0 (安定した最新版) を利用します。\n% git checkout v4.0.0 % git submodule init % git submodule sync % git submodule update Chef サーバに Cookbooks をアップロードします。\n% knife cookbook upload -o cookbooks -a Chef サーバに Roles をアップロードします。\n% knife role from file roles/*rb environment を json 形式で生成します。これは各 Cookbooks の attributes を上書きし、一つの構成 を作るために用いられます。\n% ${EDITOR} environments/env-neutron.json { \u0026#34;name\u0026#34;: \u0026#34;env-neutron\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;cookbook_versions\u0026#34;: { }, \u0026#34;json_class\u0026#34;: \u0026#34;Chef::Environment\u0026#34;, \u0026#34;chef_type\u0026#34;: \u0026#34;environment\u0026#34;, \u0026#34;default_attributes\u0026#34;: { }, \u0026#34;override_attributes\u0026#34;: { \u0026#34;package_component\u0026#34;: \u0026#34;grizzly\u0026#34;, \u0026#34;osops_networks\u0026#34;: { \u0026#34;management\u0026#34;: \u0026#34;10.0.2.0/24\u0026#34;, \u0026#34;public\u0026#34;: \u0026#34;10.0.0.0/24\u0026#34;, \u0026#34;nova\u0026#34;: \u0026#34;10.0.2.0/24\u0026#34; }, \u0026#34;nova\u0026#34;: { \u0026#34;config\u0026#34;: { \u0026#34;use_single_default_gateway\u0026#34;: false, \u0026#34;ram_allocation_ratio\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;cpu_allocation_ratio\u0026#34;: \u0026#34;16\u0026#34; }, \u0026#34;network\u0026#34;: { \u0026#34;provider\u0026#34;: \u0026#34;quantum\u0026#34;, \u0026#34;network_type\u0026#34;: \u0026#34;vlan\u0026#34; }, \u0026#34;apply_patches\u0026#34;: true, \u0026#34;libvirt\u0026#34;: { \u0026#34;vncserver_listen\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;virt_type\u0026#34;: \u0026#34;kvm\u0026#34; }, \u0026#34;db\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;nova\u0026#34; }, \u0026#34;services\u0026#34;: { \u0026#34;novnc-proxy\u0026#34;: { \u0026#34;network\u0026#34;: \u0026#34;public\u0026#34; } } }, \u0026#34;cinder\u0026#34;: { \u0026#34;db\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;cinder\u0026#34; } }, \u0026#34;keystone\u0026#34;: { \u0026#34;admin_user\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;tenants\u0026#34;: [ \u0026#34;admin\u0026#34;, \u0026#34;service\u0026#34; ], \u0026#34;users\u0026#34;: { \u0026#34;admin\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;secrete\u0026#34;, \u0026#34;roles\u0026#34;: { \u0026#34;admin\u0026#34;: [ \u0026#34;admin\u0026#34; ] } }, \u0026#34;demo\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;demo\u0026#34;, \u0026#34;default_tenant\u0026#34; : \u0026#34;service\u0026#34;, \u0026#34;roles\u0026#34;: { \u0026#34;service\u0026#34;: [ \u0026#34;service\u0026#34; ] } } }, \u0026#34;db\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;keystone\u0026#34; } }, \u0026#34;horizon\u0026#34;: { \u0026#34;theme\u0026#34;: \u0026#34;Default\u0026#34;, \u0026#34;db\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;horizon\u0026#34; } }, \u0026#34;mysql\u0026#34;: { \u0026#34;root_network_acl\u0026#34;: \u0026#34;%\u0026#34;, \u0026#34;allow_remote_root\u0026#34;: true, \u0026#34;server_root_password\u0026#34;: \u0026#34;secrete\u0026#34;, \u0026#34;server_repl_password\u0026#34;: \u0026#34;secrete\u0026#34;, \u0026#34;server_debian_password\u0026#34;: \u0026#34;secrete\u0026#34; }, \u0026#34;monitoring\u0026#34;: { \u0026#34;procmon_provider\u0026#34;: \u0026#34;monit\u0026#34;, \u0026#34;metric_provider\u0026#34;: \u0026#34;collectd\u0026#34; }, \u0026#34;glance\u0026#34;: { \u0026#34;images\u0026#34;: [ \u0026#34;precise\u0026#34;, \u0026#34;cirros\u0026#34; ], \u0026#34;image\u0026#34;: { }, \u0026#34;image_upload\u0026#34;: false, \u0026#34;db\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;glance\u0026#34; } }, \u0026#34;quantum\u0026#34;: { \u0026#34;service_pass\u0026#34;: \u0026#34;quantum\u0026#34;, \u0026#34;db\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;quantum\u0026#34; } }, \u0026#34;developer_mode\u0026#34;: false } } 生成した environment ファイルを Chef サーバにアップロードします。\n% knife environment from file environments/env-neutron.json デプロイ 各ノード、下記の通りデプロイします。\ncontroller ノード % knife bootstrap \u0026lt;ip_of_controller01\u0026gt; -N controller01 -r \\ \u0026#39;role[ha-controller1]\u0026#39;,\u0026#39;role[cinder-volume]\u0026#39; -E env-neutron --sudo -x \u0026lt;your_accout\u0026gt; cinder-volumes 用のディスクがある場合は下記の手順で Cinder が利用できるように なります。\n% sudo pvcreate /dev/sdb #デバイス名は仮 % sudo vgcreate cinder-volumes /dev/sdb % sudo service cinder-volume restart network ノード % knife bootstrap \u0026lt;ip_of_network01\u0026gt; -N network01 -r \\ \u0026#39;role[single-network-node]\u0026#39;,\u0026#39;recipe[nova-network::quantum-l3-agent]\u0026#39; \\ -E env-neutron --sudo -x \u0026lt;your_account\u0026gt; % knife bootstrap \u0026lt;ip_of_network02\u0026gt; -N network02 -r \\ \u0026#39;role[single-network-node]\u0026#39;,\u0026#39;recipe[nova-network::quantum-l3-agent]\u0026#39; \\ -E env-neutron --sudo -x \u0026lt;your_account\u0026gt; compute ノード % knife bootstrap \u0026lt;ip_of_compute01\u0026gt; -N compute01 -r \u0026#39;role[single-compute]\u0026#39; \\ -E env-neutron --sudo -x \u0026lt;your_account\u0026gt; % knife bootstrap \u0026lt;ip_of_compute02\u0026gt; -N compute02 -r \u0026#39;role[single-compute]\u0026#39; \\ -E env-neutron --sudo -x \u0026lt;your_account\u0026gt; 物理 NIC のブリッジインターフェースへのマッピング操作 Open vSwitch が管理しているブリッジインターフェースと物理インターフェースをマッ ピングしてあげます。\nnetwork ノードの2台にて下記の操作を行います。\n% sudo ovs-vsctl add-port br-eth1 eth1 % sudo ovs-vsctl add-port br-ex eth2 compute ノードの2台にて下記の操作を行います。\n% sudo ovs-vsctl add-port br-eth1 eth1 仮想ネットワークと仮想マシンの生成 環境によって仮想ネットワークの構成が異なりますので手順は割愛しますが、Horizon かコマンドライン・API を使って仮想ネットワークを生成し仮想マシンを接続すれば完 了です。\nまとめ network ノードを2台構成にしましたが何が嬉しいかと言うと、l3-agent, dhcp-agent を冗長取ることが可能になります。障害時、手作業になりますが切り替えが可能です。 l3-agent は仮想ルータ毎に、dhcp-agent は仮想ネットワーク毎に切り替えが可能です。 詳しくは私の以前の記事を参考にしてください。\nhttp://jedipunkz.github.io/blog/2013/04/26/quantum-network-distributing/\nまた、compute ノードを更に増やせば仮想マシンの数を増やすことが出来ます。\n今回は controller ノードを1台構成にしましたが、更に下記の手順で controller02 ノードを増やし controller ノードの HA 化を行うことも出来ます。\n% knife bootstrap \u0026lt;ip_of_controller02\u0026gt; -N controller02 -r \\ \u0026#39;role[ha-controller1]\u0026#39; -E env-neutron --sudo -x \u0026lt;your_accout\u0026gt; HA 構成を各々のノードに構成させるため各ノードで chef-client を1回ずつ実行しま す。\ncontroller01% sudo chef-client controller02% sudo chef-client HA 構成は haproxy, keepalived で形成されています。\nまた、environment ファイルを下記のように修正すると VLAN の代わりに GRE トンネ ルが扱えます。\n\u0026#34;nova\u0026#34;: { \u0026#34;network\u0026#34;: { \u0026#34;provider\u0026#34;: \u0026#34;quantum\u0026#34;, \u0026#34;network_type\u0026#34;: \u0026#34;gre\u0026#34; } }, \u0026lt;snip\u0026gt;... \u0026#34;quantum\u0026#34;: { \u0026#34;ovs\u0026#34;: { \u0026#34;network_type\u0026#34; : \u0026#34;gre\u0026#34;, \u0026#34;tunnnel_range\u0026#34; : \u0026#34;1:1000\u0026#34; } }, 以上です。roles ディレクトリ配下に様々な Roles があるので、各コンポーネント毎 にノードを別けるなどの構成も可能そうです。皆さん試してみてください。\n","permalink":"https://jedipunkz.github.io/post/2013/08/16/rcbops-cookbooks-neutron-openstack/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003ercbops Cookbooks で Neutron 構成の OpenStack をデプロイする方法を書きたいと思\nいます。先日紹介した openstack-chef-repo にも Neutron のレシピが含まれているの\nですが、まだまだ未完成で手作業をおりまぜながらのデプロイになっていまうので、今\n現在のところ Neutron 構成を組みたいのであればこの rcbops の Cookbooks を用いる\nしかないと思います。\u003c/p\u003e\n\u003cp\u003e今回は VLAN モードの構築を紹介します。GRE モードも少し手順を修正すれば構成可能\nです。最後のまとめに GRE モードの構築について少し触れています。\u003c/p\u003e\n\u003ch2 id=\"構成\"\u003e構成\u003c/h2\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e                                                                                 public network\n+----------------+----------------+----------------+----------------+----------------\n|                |                |                                  \n+--------------+ +--------------+ +--------------+ +--------------+ +--------------+\n| controller01 | |  network01   | |  network02   | |  compute01   | |  compute02   |\n+--------------+ +-------+------+ +-------+------+ +-------+------+ +-------+------+\n|                |       |        |       |        |       |        |       |    management network\n+----------------+-------o--------+-------o--------+-------o--------+-------o--------\n                         |                |                |                |    vm network\n                         +----------------+----------------+----------------+--------\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e特徴は\u0026hellip;\u003c/p\u003e","title":"rcbops Cookbooks で Neutron 構成 OpenStack"},{"content":"こんにちは。@jedipunkz です。\n今日も軽めの話題を。\nGmail を Emacs + Mew で読み書きする方法を何故かいつも忘れてしまうので自分のた めにもメモしておきます。Gmail はブラウザで読み書き出来るのに！と思われるかもし れませんが、Emacs で文章が書けるのは重要なことです。:D\n対象 OS 比較的新しい\u0026hellip;\nDebian Gnu/Linux Ubuntu を使います。\n手順 Emacs, Mew, stunnel4 をインストールします。Emacs は好きな物を入れてください。\n% sudo apt-get install emacs24-nox stunnel4 mew mew-bin ca-certificates openssl コマンドで mail.pem を生成します。生成したものを /etc/stunnel 配下に設 置します。\n% openssl req -new -out mail.pem -keyout mail.pem -nodes -x509 -days 365 % sudo cp mail.pem /etc/stunnel/ stunnel はインストール直後、起動してくれないので ENABLE=1 に修正します。\n% sudo ${EDITOR} /etc/default/stunnel4 ENABLE=1 # 0 -\u0026gt; 1 へ変更 stunenl.conf のサンプルを /etc/stunnel 配下に設置します。\n% sudo cp /usr/share/doc/stunnel4/examples/stunnel.conf-sample /etc/stunnel/stunnel.conf $HOME/.mew.el ファイルを生成します。自分のアカウント情報などを入力します。\n; Stunnel (setq mew-prog-ssl \u0026#34;/usr/bin/stunnel4\u0026#34;) ; IMAP for Gmail (setq mew-proto \u0026#34;%\u0026#34;) (setq mew-imap-server \u0026#34;imap.gmail.com\u0026#34;) (setq mew-imap-user \u0026#34;example@gmail.com\u0026#34;) (setq mew-imap-auth t) (setq mew-imap-ssl t) (setq mew-imap-ssl-port \u0026#34;993\u0026#34;) (setq mew-smtp-auth t) (setq mew-smtp-ssl t) (setq mew-smtp-ssl-port \u0026#34;465\u0026#34;) (setq mew-smtp-user \u0026#34;example@gmail.com\u0026#34;) (setq mew-smtp-server \u0026#34;smtp.gmail.com\u0026#34;) (setq mew-fcc \u0026#34;%Sent\u0026#34;) ; 送信メイルを保存する (setq mew-imap-trash-folder \u0026#34;%[Gmail]/ゴミ箱\u0026#34;) (setq mew-use-cached-passwd t) (setq mew-ssl-verify-level 0) $HOME/.emacs.d/init.el に Mew の記述を追記します。\n(autoload \u0026#39;mew \u0026#34;mew\u0026#34; nil t) (autoload \u0026#39;mew-send \u0026#34;mew\u0026#34; nil t) (setq mew-fcc \u0026#34;+outbox\u0026#34;) ; 送信メールを保存 (setq exec-path (cons \u0026#34;/usr/bin\u0026#34; exec-path)) Emacs + Mew を起動します。\n% emacs -e mew まとめ 以上です。他の distro だと ca-certificate とか無いので、大変だなぁといつも思っ てしまいます。\n","permalink":"https://jedipunkz.github.io/post/2013/08/12/emacs-mew-gmail/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003e今日も軽めの話題を。\u003c/p\u003e\n\u003cp\u003eGmail を Emacs + Mew で読み書きする方法を何故かいつも忘れてしまうので自分のた\nめにもメモしておきます。Gmail はブラウザで読み書き出来るのに！と思われるかもし\nれませんが、Emacs で文章が書けるのは重要なことです。:D\u003c/p\u003e\n\u003ch2 id=\"対象-os\"\u003e対象 OS\u003c/h2\u003e\n\u003cp\u003e比較的新しい\u0026hellip;\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDebian Gnu/Linux\u003c/li\u003e\n\u003cli\u003eUbuntu\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eを使います。\u003c/p\u003e\n\u003ch2 id=\"手順\"\u003e手順\u003c/h2\u003e\n\u003cp\u003eEmacs, Mew, stunnel4 をインストールします。Emacs は好きな物を入れてください。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e% sudo apt-get install emacs24-nox stunnel4 mew mew-bin ca-certificates\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eopenssl コマンドで mail.pem を生成します。生成したものを /etc/stunnel 配下に設\n置します。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e% openssl req -new -out mail.pem -keyout mail.pem -nodes -x509 -days \u003cspan style=\"color:#ae81ff\"\u003e365\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e% sudo cp mail.pem /etc/stunnel/\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003estunnel はインストール直後、起動してくれないので ENABLE=1 に修正します。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e% sudo \u003cspan style=\"color:#e6db74\"\u003e${\u003c/span\u003eEDITOR\u003cspan style=\"color:#e6db74\"\u003e}\u003c/span\u003e /etc/default/stunnel4\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eENABLE\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e \u003cspan style=\"color:#75715e\"\u003e# 0 -\u0026gt; 1 へ変更\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003estunenl.conf のサンプルを /etc/stunnel 配下に設置します。\u003c/p\u003e","title":"Emacs + Mew で Gmail を読み書きする"},{"content":"こんにちは。@jedipunkz です。\nLinux のウィンドウマネージャは使い続けて長いのですが、既に1周半しました。twm -\u0026gt; gnome -\u0026gt; enlightenment -\u0026gt; OpenBox -\u0026gt; .. 忘れた .. -\u0026gt; twm -\u0026gt; vtwm -\u0026gt; awesome -\u0026gt; kde -\u0026gt; gnome -\u0026gt; enligtenment \u0026hellip;\n巷では Linux のデスクトップ環境は死んだとか言われているらしいですが、stumpwm というウィンドウマネージャは結構いいなと思いました。タイル型のウィンドウマネー ジャで Emacs 好きの人が開発したらしいです。設定は lisp で書けます。\n見た目は派手では無いのですが、\nグルーピング機能 すべての操作がキーボードで出来る タイル型であるので煩わしいマウスでのウィンドウ操作が不要 という点に惹かれました。\nLinux を使う時、私の場合 Debian Gnu/Linux Unstalble をいつも使うのですが、 Unstable だと apt-get install stumpwm したバイナリがコケる\u0026hellip;ということでビル ドしてあげました。普段慣れないビルド方法だったので、その時の手順を自分のために もメモしておきます。\n前提環境 Debian Gnu/Linux unstable 利用 X の環境は揃っている ビルド手順 clisp をインストール clisp をインストールします。\n% sudo apt-get install clisp-dev lisp.run というファイルを stumpwm が見つけられないので symlink 張ってあげます。 ちょっと泥臭い。\n% sudo ln -s /usr/lib/clisp-2.49/base /usr/lib/clisp-2.49/full sbcl をインストール sbcl をインストールします。\n% sudo apt-get install sbcl quicklisp をインストール quicklisp をインストールします。また lisp init file に諸々追加します。\n% wget http://beta.quicklisp.org/quicklisp.lisp % sbcl --load quicklisp.lisp * (quicklisp-quickstart:install) * (ql:system-apropos \u0026#34;vecto\u0026#34;) * (ql:quickload \u0026#34;vecto\u0026#34;) * (ql:add-to-init-file) * (ql:quickload \u0026#34;clx\u0026#34;) * (ql:quickload \u0026#34;cl-ppcre\u0026#34;) * quit stumpwm のビルド stumpwm をビルドします。\n% git clone https://github.com/sabetts/stumpwm.git % cd stumpwm % ./configure % make % sudo make install まとめ $HOME/.stumpwmrc を配置して、いろいろカスタマイズ出来ます。ウェブを検索すると 皆さんの rc ファイルが見つかるので参考にすると良いかも。prefix キーは C-t なの ですが、tmux と被りますよね\u0026hellip;。tmux 側は変えたくないので stumpwm 側を C-z 等 に変更して使っています。\n(set-prefix-key (kbd \u0026#34;C-z\u0026#34;)) ","permalink":"https://jedipunkz.github.io/post/2013/08/09/debian-unstable-stumpwm/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003eLinux のウィンドウマネージャは使い続けて長いのですが、既に1周半しました。twm -\u0026gt; gnome -\u0026gt;\nenlightenment -\u0026gt; OpenBox -\u0026gt;  .. 忘れた .. -\u0026gt; twm -\u0026gt; vtwm -\u0026gt; awesome -\u0026gt; kde -\u0026gt;\ngnome -\u0026gt; enligtenment \u0026hellip;\u003c/p\u003e\n\u003cp\u003e巷では Linux のデスクトップ環境は死んだとか言われているらしいですが、stumpwm\nというウィンドウマネージャは結構いいなと思いました。タイル型のウィンドウマネー\nジャで Emacs 好きの人が開発したらしいです。設定は lisp で書けます。\u003c/p\u003e\n\u003cp\u003e見た目は派手では無いのですが、\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eグルーピング機能\u003c/li\u003e\n\u003cli\u003eすべての操作がキーボードで出来る\u003c/li\u003e\n\u003cli\u003eタイル型であるので煩わしいマウスでのウィンドウ操作が不要\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eという点に惹かれました。\u003c/p\u003e\n\u003cp\u003eLinux を使う時、私の場合 Debian Gnu/Linux Unstalble をいつも使うのですが、\nUnstable だと apt-get install stumpwm したバイナリがコケる\u0026hellip;ということでビル\nドしてあげました。普段慣れないビルド方法だったので、その時の手順を自分のために\nもメモしておきます。\u003c/p\u003e\n\u003ch2 id=\"前提環境\"\u003e前提環境\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eDebian Gnu/Linux unstable 利用\u003c/li\u003e\n\u003cli\u003eX の環境は揃っている\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"ビルド手順\"\u003eビルド手順\u003c/h2\u003e\n\u003ch4 id=\"clisp-をインストール\"\u003eclisp をインストール\u003c/h4\u003e\n\u003cp\u003eclisp をインストールします。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e% sudo apt-get install clisp-dev\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003elisp.run というファイルを stumpwm が見つけられないので symlink 張ってあげます。\nちょっと泥臭い。\u003c/p\u003e","title":"Debian Unstable で stumpwm"},{"content":"こんにちは。@jedipunkz です。\n前回、github.com/rcbops の Cookbooks を利用した OpenStack デプロイ方法を紹介し ました。これは RackSpace 社の Private Cloud Service で使われている Cookbooks で Apache ライセンスの元、誰でも利用できるようになっているものです。HA 構成を 組めたり Swift の操作 (Rings 情報管理など) も Chef で出来る優れた Cookbooks な わけですが、運用するにあたり幾つか考えなくてはならないこともありそうです。\nchef-client の実行順番と実行回数が密接に関わっていること HA 構成の手動切替等、運用上必要な操作について考慮する必要性 ※ 後者については OpenStack ユーザ会の方々に意見もらいました。\n特に前項は Chef を利用する一番の意義である \u0026ldquo;冪等性\u0026rdquo; をある程度 (全くという意味 ではありませんが) 犠牲にしていると言えます。また chef-client の実行回数、タイ ミング等 Cookbooks を完全に理解していないと運用は難しいでしょう。自ら管理して いる Cookbooks なら問題ないですが、rcbops が管理しているので常に更新状況を追っ ていく必要もありそうです。\n一方、Opscode, RackSpace, AT\u0026amp;T 等のエンジニアが管理している Cookbooks がありま す。これは以前、日本の OpenStack 勉強会で私が話した \u0026lsquo;openstack-chef-repo\u0026rsquo; を利 用したモノです。github.com/stackforge の元に管理されています。 openstack-chef-repo は Berksfile, Roles, Environments のみの構成で各 Cookbooks は Berksfile を元に取得する形になります。取得先は同じく github.com/stackforge 上で管理されています。\nこちらの Cookbooks を利用するメリットとしては\n冪等性が保たれる 複数ベンダ管理なのでリスクがある程度軽減される などです。逆にデメリットは\u0026hellip;\n容易に HA 構成を組むことが出来ない node, environment, role による search で自律的に構成してくれない です。一長一短ですが、運用し切れない HA 構成を組むのであれば、冪等性が保たれる こちらの Cookbooks を使う、というのも一つの考えだと思います。\n今回は nova-network を用いた複数台構成 (controller + compute x n) を この Stackforge Cookbooks を使ってデプロイする方法を紹介しようと思います！\n前提条件 Chef サーバの構築は済ませている knife をワークショテーションで扱えるまでの準備は出来ている デプロイする先のノード (controller01 , compute0[12]) は sshd が起動している 構成 +----------------+----------------+------------------------------------------------- public | eth0 | eth0 | eth0 network +--------------+ +--------------+ +--------------+ +--------------+ +--------------+ | controller01 | | compute01 | | compute02 | | chef server | | workstation | +--------------+ +--------------+ +--------------+ +--------------+ +--------------+ | eth1 | eth2 | | eth2 | | eth0 | eth0 +----------------+-------o--------+-------o--------+----------------+--------------- api/management | eth1 | eth1 network +----------------+----------------------------------------- vm network 特徴は\u0026hellip;\ncontroller01 ノードは OpenStack コントローラノード compute0[12] ノードは OpenStack コンピュートノード Chef サーバ、ワークステーションは api/management ネットワーク上に配置 compute0[12] は 3 本目の NIC (eth1) を出し vm ネットワークをマッピング (bridge デバイス) API endpoint は controller01 の eth1 で受ける 仮想マシンは compute ノードの eth1 にブリッジ接続し eth0 を介してインターネットへ IP アドレス情報\u0026hellip;\ncontroller01, eth0 : 10.0.0.10, eth1 : 10.0.1.10 compute01, eth0 : 10.0.0.11, eth1 : 172.24.18.11, eth2 : 10.0.1.11 compute02, eth0 : 10.0.0.12, eth1 : 172.24.18.12, eth2 : 10.0.1.12 chef server, eth0 : 10.0.1.13 workstation, eth0 : 10.0.1.14 デプロイ手順 ここからの操作は全てワークステーション上から行います。\nまず、openstakc-chef-repo を取得します。私のアカウント上のものを利用します。stackforge のモノは 管理が追いついていないため、動作しません\u0026hellip;。私のモノは fork して Roles を主に修正しています。\n% git clone git://github.com/jedipunkz/openstack-chef-repo.git ~/openstack-chef-repo % cd ~/openstack-chef-repo Cookbooks を取得します。Berksfile に従って取得しますが予め berkshelf を gem で インストールしましょう。\n% gem install berkshelf --no-ri --no-rdoc % rbenv rehash # rbenv を利用している場合 % berks install --path=./coobooks 上記の環境に合わせ environment を作成します。この environment は各 Cookbooks の attributes を上書きし 1つの environment で 1つの構成 (controller, compute..) を形成します。knife bootstrap, chef-client を 実行するとこの environment をサーチし、それぞれが連携し合ってくれます。\n% ${EDITOR} environments/openstack-cluster01.rb name \u0026#34;separated\u0026#34; description \u0026#34;separated nodes environment\u0026#34; override_attributes( \u0026#34;release\u0026#34; =\u0026gt; \u0026#34;grizzly\u0026#34;, \u0026#34;osops_networks\u0026#34; =\u0026gt; { \u0026#34;management\u0026#34; =\u0026gt; \u0026#34;10.0.1.0/24\u0026#34;, \u0026#34;public\u0026#34; =\u0026gt; \u0026#34;10.0.1.0/24\u0026#34;, \u0026#34;nova\u0026#34; =\u0026gt; \u0026#34;10.0.1.0/24\u0026#34; }, \u0026#34;mysql\u0026#34; =\u0026gt; { \u0026#34;bind_address\u0026#34; =\u0026gt; \u0026#34;0.0.0.0\u0026#34;, \u0026#34;root_network_acl\u0026#34; =\u0026gt; \u0026#34;%\u0026#34;, \u0026#34;allow_remote_root\u0026#34; =\u0026gt; true, \u0026#34;server_root_password\u0026#34; =\u0026gt; \u0026#34;secrete\u0026#34;, \u0026#34;server_repl_password\u0026#34; =\u0026gt; \u0026#34;secrete\u0026#34;, \u0026#34;server_debian_password\u0026#34; =\u0026gt; \u0026#34;secrete\u0026#34; }, \u0026#34;nova\u0026#34; =\u0026gt; { \u0026#34;network\u0026#34; =\u0026gt; { \u0026#34;fixed_range\u0026#34; =\u0026gt; \u0026#34;172.18.0.0/24\u0026#34;, \u0026#34;public_interface\u0026#34; =\u0026gt; \u0026#34;eth0\u0026#34; } }, \u0026#34;openstack\u0026#34; =\u0026gt; { \u0026#34;developer_mode\u0026#34; =\u0026gt; true, \u0026#34;compute\u0026#34; =\u0026gt; { \u0026#34;rabbit\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.0.1.10\u0026#34; }, \u0026#34;novnc_proxy\u0026#34; =\u0026gt; { \u0026#34;bind_interface\u0026#34; =\u0026gt; \u0026#34;eth1\u0026#34; }, \u0026#34;libvirt\u0026#34; =\u0026gt; { \u0026#34;bind_interface\u0026#34; =\u0026gt; \u0026#34;eth1\u0026#34; }, \u0026#34;network\u0026#34; =\u0026gt; { \u0026#34;fixed_range\u0026#34; =\u0026gt; \u0026#34;172.18.0.0/24\u0026#34; }, \u0026#34;networks\u0026#34; =\u0026gt; [ { \u0026#34;label\u0026#34; =\u0026gt; \u0026#34;private\u0026#34;, \u0026#34;ipv4_cidr\u0026#34; =\u0026gt; \u0026#34;172.18.0.0/24\u0026#34;, \u0026#34;num_networks\u0026#34; =\u0026gt; \u0026#34;1\u0026#34;, \u0026#34;network_size\u0026#34; =\u0026gt; \u0026#34;255\u0026#34;, \u0026#34;bridge\u0026#34; =\u0026gt; \u0026#34;br200\u0026#34;, \u0026#34;bridge_dev\u0026#34; =\u0026gt; \u0026#34;eth1\u0026#34;, \u0026#34;dns1\u0026#34; =\u0026gt; \u0026#34;8.8.8.8\u0026#34;, \u0026#34;dns2\u0026#34; =\u0026gt; \u0026#34;8.8.4.4\u0026#34;, \u0026#34;multi_host\u0026#34; =\u0026gt; \u0026#34;T\u0026#34; } ] }, \u0026#34;identity\u0026#34; =\u0026gt; { \u0026#34;bind_interface\u0026#34; =\u0026gt; \u0026#34;eth1\u0026#34;, \u0026#34;users\u0026#34; =\u0026gt; { \u0026#34;demo\u0026#34; =\u0026gt; { \u0026#34;password\u0026#34; =\u0026gt; \u0026#34;demo\u0026#34;, \u0026#34;default_tenant\u0026#34; =\u0026gt; \u0026#34;service\u0026#34;, \u0026#34;roles\u0026#34; =\u0026gt; { \u0026#34;Member\u0026#34; =\u0026gt; [ \u0026#34;Member\u0026#34; ] } } } }, \u0026#34;image\u0026#34; =\u0026gt; { \u0026#34;bind_interface\u0026#34; =\u0026gt; \u0026#34;eth1\u0026#34;, }, \u0026#34;network\u0026#34; =\u0026gt; { \u0026#34;api\u0026#34; =\u0026gt; { \u0026#34;bind_interface\u0026#34; =\u0026gt; \u0026#34;eth1\u0026#34;, } }, \u0026#34;db\u0026#34; =\u0026gt; { \u0026#34;bind_interface\u0026#34; =\u0026gt; \u0026#34;eth1\u0026#34;, \u0026#34;compute\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.0.1.10\u0026#34; }, \u0026#34;identity\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.0.1.10\u0026#34; }, \u0026#34;image\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.0.1.10\u0026#34; }, \u0026#34;network\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.0.1.10\u0026#34; }, \u0026#34;volume\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.0.1.10\u0026#34; }, \u0026#34;dashboard\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.0.1.10\u0026#34; } }, \u0026#34;mq\u0026#34; =\u0026gt; { \u0026#34;bind_interface\u0026#34; =\u0026gt; \u0026#34;eth1\u0026#34; }, \u0026#34;endpoints\u0026#34; =\u0026gt; { \u0026#34;identity-api\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.0.1.10\u0026#34; }, \u0026#34;identity-admin\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.0.1.10\u0026#34; }, \u0026#34;compute-api\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.0.1.10\u0026#34; }, \u0026#34;compute-ec2-api\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.0.1.10\u0026#34; }, \u0026#34;compute-ec2-admin\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.0.1.10\u0026#34; }, \u0026#34;compute-novnc\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.0.1.10\u0026#34;, }, \u0026#34;network-api\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.0.1.10\u0026#34;, }, \u0026#34;image-api\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.0.1.10\u0026#34;, }, \u0026#34;image-registry\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.0.1.10\u0026#34;, }, \u0026#34;volume-api\u0026#34; =\u0026gt; { \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;10.0.1.10\u0026#34;, } } } ) spiceweasel を gem でインストールします。これは Cookbooks, Roles, Environments 等を Chef サーバへアップロードしたり、knife bootstrap を一気に行なってくれるツー ルです。knife bootstrap コマンドの書式も書く必要がありません。\n% gem install spiceweasel --no-ri --no-rdoc % rbenv rehash # rbenv を使っている場合 infrastracture.yml を修正します。作成した environments 名の修正と node: を修正 します。\n...\u0026lt;snip\u0026gt;... environments: - separated: ...\u0026lt;snip\u0026gt;... nodes: - 10.0.0.10: run_list: role[os-compute-single-controller] options: -N controller01 -E separated --sudo -x \u0026lt;your_accout_name\u0026gt; - 10.0.0.11: run_list: role[os-compute-worker] options: -N compute01 -E separated --sudo -x \u0026lt;your_accout_name\u0026gt; - 10.0.0.12: run_list: role[os-compute-worker] options: -N compute02 -E separated --sudo -x \u0026lt;your_accout_name\u0026gt; spiceweasel を実行します。すると knife コマンドの一覧が出力されますので実行さ れる内容を確認します。ここではまだ実行が行われません。\n% spiceweasel infrastructure.yml -e オプションをつけて実行すると先ほど出力された knife コマンドが順に実行されま す。この操作で3台のデプロイが完了します。\n% spiceweasel -e infrastructure.yml まとめと考察 以上の操作で OpenStack 複数台構成が組める。Neutron 構成についても \u0026lsquo;openstack-network\u0026rsquo; Cookbook に実装が入っている様子なので以後試してみたいです。 今回は public ネットワーク・api/management ネットワーク・vm ネットワークの3つ のネットワーク構成で組みました。運用するのであれば public と api/management が 別れている必要があると思いますので。自宅で作ってみたい！でもネットワークが\u0026hellip;と いう場合、environment ファイルの内容を修正すれば pubcic/api/management ネット ワーク・vm ネットワークの2つのネットワーク構成でも組めるので試してみてください。 もしくは bridge_dev の記述を environment から削除すると仮想ネットワークの物理 ネットワークへのマッピングがされないので、1つのネットワークでも構成出来ます。\nまた、オールインワン構成も組めます。オールインワン構成の場合\n% knife bootstrap \u0026lt;ip_addr\u0026gt; -N allinone01 -E allinone \\ -r \u0026#39;role[allinone-compute]\u0026#39; --sudo --x \u0026lt;your_account\u0026gt; でデプロイ出来ます。こちらもネットワーク構成は1つでも2つでも3つでも組めます。もう少し 詳しい手順をオールインワン構成については別途ブログにまとめるかもしれませんー。\n","permalink":"https://jedipunkz.github.io/post/2013/08/06/opscode-cookbooks-openstack-deploy/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003e前回、github.com/rcbops の Cookbooks を利用した OpenStack デプロイ方法を紹介し\nました。これは RackSpace 社の Private Cloud Service で使われている Cookbooks\nで Apache ライセンスの元、誰でも利用できるようになっているものです。HA 構成を\n組めたり Swift の操作 (Rings 情報管理など) も Chef で出来る優れた Cookbooks な\nわけですが、運用するにあたり幾つか考えなくてはならないこともありそうです。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003echef-client の実行順番と実行回数が密接に関わっていること\u003c/li\u003e\n\u003cli\u003eHA 構成の手動切替等、運用上必要な操作について考慮する必要性\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e※ 後者については OpenStack ユーザ会の方々に意見もらいました。\u003c/p\u003e\n\u003cp\u003e特に前項は Chef を利用する一番の意義である \u0026ldquo;冪等性\u0026rdquo; をある程度 (全くという意味\nではありませんが) 犠牲にしていると言えます。また chef-client の実行回数、タイ\nミング等 Cookbooks を完全に理解していないと運用は難しいでしょう。自ら管理して\nいる Cookbooks なら問題ないですが、rcbops が管理しているので常に更新状況を追っ\nていく必要もありそうです。\u003c/p\u003e\n\u003cp\u003e一方、Opscode, RackSpace, AT\u0026amp;T 等のエンジニアが管理している Cookbooks がありま\nす。これは以前、日本の OpenStack 勉強会で私が話した \u0026lsquo;openstack-chef-repo\u0026rsquo; を利\n用したモノです。github.com/stackforge の元に管理されています。\nopenstack-chef-repo は Berksfile, Roles, Environments のみの構成で各 Cookbooks\nは Berksfile を元に取得する形になります。取得先は同じく github.com/stackforge\n上で管理されています。\u003c/p\u003e","title":"openstack-chef-repo で OpenStack 複数台構成をデプロイ"},{"content":"こんにちは。@jedipunkz です。\n最近、自動化は正義だと最近思うのですが、その手助けをしてくれるツール Cobbler を試してみました。Cobbler と複数 OS, ディストリビューションを CLI, GUI で管理出 来るツールです。PXE, TFTP, DHCPを組分せれば OS の自動構築が出来るのは古くから ありますが、TFTP サーバに配置するカーネルイメージやマックアドレスの管理を一元 して管理してくれるのがこの Cobbler です。\n今回は Cobbler の構築方法をお伝えします。本当は Chef Cookbooks で構築したかっ たのですが Opscode Community にある Cookbooks はイマイチだったので、今回は手動 で。\n前提環境 OS は CentOSを。Ubuntu を利用すると DHCP のコンフィギュレーションを自動で出 来ません 利用するネットワークの DHCP はオフにします 構築手順 SELINUX を無効にします。石◯さん、ごめんなさい。\n# ${EDITOR} /etc/sysconfig/selinux SELINUX=disabled # setenforce 0 EPEL のレポジトリを追加します。\n# rpm -Uvh http://ftp.iij.ad.jp/pub/linux/fedora/epel/6/x86_64/epel-release-6-8.noarch.rpm cobbler をインストールします。またその他必要なパッケージもここでインストールし ます。\n# yum install cobbler debmirror pykickstart 自分の設定したいパスワードを生成して /etc/cobbler/settings 内の default_password_crypted: に設定します。パスワードの生成は下記のように openssl コマンドを利用します。\n% openssl passwd -1 /etc/cobbler/settings 内の下記のパラメータについて設定します。manage_dhcp: は 1 にしておくと、Cobbler が自動で ISC DHCP の設定も書き換えてくれるので、ほぼ必 須の設定です。\nserver: \u0026lt;自ホストの IP\u0026gt; next_server: \u0026lt;自ホストの IP\u0026gt; manage_dhcp: 1 /etc/cobbler/dhcp.template を設定します。自分のネットワーク環境に合わせて設定 します。\nsubnet 192.168.1.0 netmask 255.255.255.0 { option routers 192.168.1.254; option domain-name-servers 8.8.8.8,8.8.4.4; option subnet-mask 255.255.255.0; range dynamic-bootp 192.168.1.120 192.168.1.150; default-lease-time 21600; max-lease-time 43200; next-server $next_server; class \u0026#34;pxeclients\u0026#34; { match if substring (option vendor-class-identifier, 0, 9) = \u0026#34;PXEClient\u0026#34;; if option pxe-system-type = 00:02 { filename \u0026#34;ia64/elilo.efi\u0026#34;; } else if option pxe-system-type = 00:06 { filename \u0026#34;grub/grub-x86.efi\u0026#34;; } else if option pxe-system-type = 00:07 { filename \u0026#34;grub/grub-x86_64.efi\u0026#34;; } else { filename \u0026#34;pxelinux.0\u0026#34;; } } } /etc/xinetd.d/rsync の disable = yes を no に設定します。\n# ${EDITOR} /etc/xinetd.d/rsync \u0026lt;snip\u0026gt; disable = no \u0026lt;snip\u0026gt; サービスを起動します。\n# service cobbler start # service dhcpd start # service httpd start # service xinetd start OS 起動時に各サービスが起動するよう設定します。\n# chkconfig cobbler on # chkconfig dhcpd on # chkconfig httpd on # chkconfig xinetd on iptables をオフにします。\n# service iptables stop # chkconfig iptables off 構築は完了です。\nディストリビューションのインポート ここではテストで Ubuntu Server 12.04 amd64 をインポートしてみます。\nダウンロードしてきたインストール ISO をマウントします。\n# mount -t iso9660 -o loop,ro /path/to/Image.iso /mnt インポートします。\n# cobbler import --name=ubuntu1204 --arch=x86_64 --path=/mnt Debian 系の Ubuntu は x86_64 ではなく通常 amd64 と記しますが、Cobbler が CentOS, RHEL を前提に開発されているので x86_64 と記します。注意してください。\nインポートされたか確認します。\n# cobbler distro list # cobbler profile list ここでは自分の作成した preseed.cfg を使いたいのでその設定を行います。この操作 は行わなくても構いませんが、preseed.cfg 内で自分の環境に合わせて色々したいと思 うので、行なっておくと便利です。予め preseed.cfg は作成する必要ありますが。\n# cp /path/to/preseed.cfg /var/lib/cobbler/kickstarts/ubuntu1204-preseed.cfg # cobbler profile edit --name=ubuntu1204-x86_64 \\ --kickstart=/var/lib/cobbler/kickstarts/ubuntu1204-preseed.cfg \\ --kopts=\u0026#34;priority=critical locale=en_US\u0026#34; インストールターゲットマシンの登録 インストールターゲットのマシンを登録します。予め NIC の Mac アドレスを用意して ください。\n# cobbler system add --name=foo --profile=ubuntu1204-x86_64 # cobbler system edit --name=foo --interface=eth0 \\ --mac=00:1c:25:72:1f:79 --ip-address=192.168.1.120 --netmask \\ 255.255.255.0 --static=0 # cobbler system edit --name=foo -gateway=192.168.1.254 --hostname=foo こちらもネットワーク環境に合わせて gateway や netmask の情報を記してください。\nターゲットマシンのインストール PXE ブートするだけです。\nまとめ \u0026lsquo;cobbler-web\u0026rsquo; の設定は今回は説明しませんでした、慣れている人なら CLI のほうがい いと思うので \u0026lsquo;cobbler\u0026rsquo; パッケージだけでも十分だと思います。\nCobbler は CentOS, RHEL を中心に考えられたツールなので Debian 系への対応がイマ イチでした。あと、import する時にコケることがあります。コケる理由がエラーメッ セージとして表示されないものもあるので、ちょっと苦労します。\nただ管理する・新しいターゲット・ディストリビューションの登録を容易にしてくれる ツールとしてはとても有用です。今、この辺りの操作であれば Chef で出来ないかな？ と考えている最中です。と言うか既に Cookbooks を用意し始めています。Chef でシン プルな PXE インストール環境を作った方が多くのディストリビューションに対応出来 そうですし、やる意味あるかな？と思っています。\n","permalink":"https://jedipunkz.github.io/post/2013/07/28/cobbler-os-automation-install/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003e最近、自動化は正義だと最近思うのですが、その手助けをしてくれるツール Cobbler\nを試してみました。Cobbler と複数 OS, ディストリビューションを CLI, GUI で管理出\n来るツールです。PXE, TFTP, DHCPを組分せれば OS の自動構築が出来るのは古くから\nありますが、TFTP サーバに配置するカーネルイメージやマックアドレスの管理を一元\nして管理してくれるのがこの Cobbler です。\u003c/p\u003e\n\u003cp\u003e今回は Cobbler の構築方法をお伝えします。本当は Chef Cookbooks で構築したかっ\nたのですが Opscode Community にある Cookbooks はイマイチだったので、今回は手動\nで。\u003c/p\u003e\n\u003ch2 id=\"前提環境\"\u003e前提環境\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eOS は CentOSを。Ubuntu を利用すると DHCP のコンフィギュレーションを自動で出\n来ません\u003c/li\u003e\n\u003cli\u003e利用するネットワークの DHCP はオフにします\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"構築手順\"\u003e構築手順\u003c/h2\u003e\n\u003cp\u003eSELINUX を無効にします。石◯さん、ごめんなさい。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# ${EDITOR} /etc/sysconfig/selinux\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eSELINUX\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003edisabled\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# setenforce 0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eEPEL のレポジトリを追加します。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# rpm -Uvh http://ftp.iij.ad.jp/pub/linux/fedora/epel/6/x86_64/epel-release-6-8.noarch.rpm\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003ecobbler をインストールします。またその他必要なパッケージもここでインストールし\nます。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# yum install cobbler debmirror pykickstart\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e自分の設定したいパスワードを生成して /etc/cobbler/settings 内の\ndefault_password_crypted: に設定します。パスワードの生成は下記のように openssl\nコマンドを利用します。\u003c/p\u003e","title":"Cobbler で OS 自動インストール"},{"content":"こんにちは。@jedipunkz です。\n最近 Chef で OpenStack をデプロイすることばかりに興味持っちゃって、他のことも やらんとなぁと思っているのですが、せっかくなので Swift HA 構成を Chef でデプロ イする方法を書きます。\nSwift って分散ストレージなのに HA ってなんよ！と思われるかもしれませんが、ご存 知の様に Swift はストレージノード (accout, object, container) とプロキシノード に別れます。今回紹介する方法だとプロキシノードを Keepalived と Haproxy で HA、 また MySQL も KeepAlived で HA の構成に出来ました。いつものように RackSpace 管 理の Cookbooks を使っています。\n参考資料 http://www.rackspace.com/knowledge_center/article/openstack-object-storage-configuration\n構成 構成は簡単に記すと下記のようになります。特徴としては\u0026hellip;\nswift-proxy01, swift-proxy02 で HA。VRRP + LB な構成。 swift-proxy01 で git サーバ稼働。Rings 情報を管理。 swift-storageNN がストレージノード OS は Ubuntu server 12.04 です。\n|--------- VRRP + Load Balancer ------| +-----------------+ +-----------------+ +-----------------+ +-----------------+ +-----------------+ | swift-proxy01 | | swift-proxy02 | | swift-storage01 | | swift-storage02 | | swift-storage03 | +-----------------+ +-----------------+ +-----------------+ +-----------------+ +-----------------+ | | | | | +-------------------+-------------------+-------------------+-------------------+------------------ | | +-----------------+ +-----------------+ | chef workstation| | chef server | +-----------------+ +-----------------+ 絵は書く意味なかったか\u0026hellip;。\n手順 chef server 構築 例によって Omnibus パッケージを使います。\n% wget https://opscode-omnibus-packages.s3.amazonaws.com/ubuntu/12.04/x86_64/chef-server_11.0.8-1.ubuntu.12.04_amd64.deb % sudo dpkg -i chef-server_11.0.8-1.ubuntu.12.04_amd64.deb % sudo chef-server-ctl reconfigure % knife configure -i \u0026lt; 適当に答える \u0026gt; \u0026lsquo;knife configure -i\u0026rsquo; で自分用の秘密鍵が生成出来ます。\nworkstation ノードでの準備 ほぼ全ての操作を workstation ノードで行います。knife.rb や秘密鍵の設置等につい ては方法を割愛します。このへんはモヤモヤと説明しますが、皆さんならご存知かと思 いますので..。\ngithub より rackspace 管理の Chef Cookbooks を取得する。\n% git clone https://github.com/rcbops/chef-cookbooks.git ~/openstack-chef-repo % cd ~/openstack-chef-repo v.4.0.0 とい現在 (2013/07/25) 最新リリース版をチェックアウトする。\n% git checkout v4.0.0 % git submodule init % git submodule sync % git submodule update \u0026lsquo;chef server\u0026rsquo; ノードへ Cookbooks をアップロードする\n% knife cookbook upload -o cookbooks -a \u0026lsquo;chef-server\u0026rsquo; ノードへ Roles をアップロードする\n% knife role from file roles/*rb 今回の構成用の environment \u0026lsquo;swift-ha\u0026rsquo; を用意します。ここでは各 Cookbooks の Attributes を上書きし、一つの構成を組みます。Cookbooks 内でこの environment 名 をキーに検索し自ノードと同環境のノードを見つけ出し、関連付けがされます。\n{ \u0026#34;name\u0026#34;: \u0026#34;swift-ha\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;cookbook_versions\u0026#34;: { }, \u0026#34;json_class\u0026#34;: \u0026#34;Chef::Environment\u0026#34;, \u0026#34;chef_type\u0026#34;: \u0026#34;environment\u0026#34;, \u0026#34;default_attributes\u0026#34;: { }, \u0026#34;override_attributes\u0026#34;: { \u0026#34;package_component\u0026#34;: \u0026#34;grizzly\u0026#34;, \u0026#34;osops_networks\u0026#34;: { \u0026#34;management\u0026#34;: \u0026#34;10.0.0.0/24\u0026#34;, \u0026#34;public\u0026#34;: \u0026#34;10.0.0.0/24\u0026#34;, \u0026#34;nova\u0026#34;: \u0026#34;10.0.0.0/24\u0026#34;, \u0026#34;swift\u0026#34;: \u0026#34;10.0.0.0/24\u0026#34; }, \u0026#34;keystone\u0026#34;: { \u0026#34;admin_user\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;tenants\u0026#34;: [ \u0026#34;admin\u0026#34;, \u0026#34;service\u0026#34; ], \u0026#34;users\u0026#34;: { \u0026#34;admin\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;secrete\u0026#34;, \u0026#34;roles\u0026#34;: { \u0026#34;admin\u0026#34;: [ \u0026#34;admin\u0026#34; ] } }, \u0026#34;demo\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;demo\u0026#34;, \u0026#34;default_tenant\u0026#34; : \u0026#34;service\u0026#34;, \u0026#34;roles\u0026#34;: { \u0026#34;admin\u0026#34;: [ \u0026#34;admin\u0026#34; ] } } }, \u0026#34;db\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;keystone\u0026#34; } }, \u0026#34;mysql\u0026#34;: { \u0026#34;root_network_acl\u0026#34;: \u0026#34;%\u0026#34;, \u0026#34;allow_remote_root\u0026#34;: true, \u0026#34;server_root_password\u0026#34;: \u0026#34;secrete\u0026#34;, \u0026#34;server_repl_password\u0026#34;: \u0026#34;secrete\u0026#34;, \u0026#34;server_debian_password\u0026#34;: \u0026#34;secrete\u0026#34; }, \u0026#34;monitoring\u0026#34;: { \u0026#34;procmon_provider\u0026#34;: \u0026#34;monit\u0026#34;, \u0026#34;metric_provider\u0026#34;: \u0026#34;collectd\u0026#34; }, \u0026#34;vips\u0026#34;: { \u0026#34;keystone-admin-api\u0026#34;: \u0026#34;10.0.0.11\u0026#34;, \u0026#34;keystone-service-api\u0026#34;: \u0026#34;10.0.0.11\u0026#34;, \u0026#34;keystone-internal-api\u0026#34;: \u0026#34;10.0.0.11\u0026#34;, \u0026#34;mysql-db\u0026#34;: \u0026#34;10.0.0.12\u0026#34;, \u0026#34;swift-proxy\u0026#34;: \u0026#34;10.0.0.11\u0026#34; }, \u0026#34;developer_mode\u0026#34;: false, \u0026#34;swift\u0026#34;: { \u0026#34;swift_hash\u0026#34;: \u0026#34;127005c8ea84\u0026#34;, \u0026#34;authmode\u0026#34;: \u0026#34;keystone\u0026#34;, \u0026#34;authkey\u0026#34;: \u0026#34;1f281c71-cf89-5b27-a2ad-ad873d3f2760\u0026#34; } } } 生成した environment ファイル \u0026rsquo;environments/swift-ha.json\u0026rsquo; を Chef サーバへアッ プロードします。\n% knife environment from file environments/swift-ha.json disk デバイスの用意 swift-storageNN で オブジェクト格納用の Disk がある場合はパーティションを作成 します。追加の Disk が無くても構わないと思います。後に Chef が使用可能な Disk デバイスを検知してくれます。ここでは例として /dev/sdb として認識されていること を前提に記します。追加の disk が無い場合は /dev/sda6 等、OS インストール時にオ ブジェクト格納用のパーティションを用意してもらえば大丈夫です。また GPT なパー ティションを切る必要があると思いますので (大きいから) fdisk ではなく gdisk を 用いましょう。\n% swift-storageNN% sudo apt-get update; apt-get -y install gdisk % swift-storageNN% sudo gdisk /dev/sdb 各ノードをブートストラップ いよいよデプロイします。\n% knife bootstrap \u0026lt;ip_swift-proxy01\u0026gt; -N swift-proxy01 -r \u0026#39;role[ha-swift-controller1]\u0026#39; -E swift-ha --sudo -x jedipunkz % knife bootstrap \u0026lt;ip_swift-proxy02\u0026gt; -N swift-proxy02 -r \u0026#39;role[ha-swift-controller2]\u0026#39; -E swift-ha --sudo -x jedipunkz % knife bootstrap \u0026lt;ip_swift-storage01\u0026gt; -N swift-storage01 -r \\ \u0026#39;role[swift-object-server]\u0026#39;,\u0026#39;role[swift-container-server]\u0026#39;,\u0026#39;role[swift-account-server]\u0026#39; -E swift-ha --sudo -x jedipunkz % knife bootstrap \u0026lt;ip_swift-sotrage02\u0026gt; -N swift-storage02 -r \\ \u0026#39;role[swift-object-server]\u0026#39;,\u0026#39;role[swift-container-server]\u0026#39;,\u0026#39;role[swift-account-server]\u0026#39; -E swift-ha --sudo -x jedipunkz % knife bootstrap \u0026lt;ip_swift-storage03\u0026gt; -N swift-storage03 -r \\ \u0026#39;role[swift-object-server]\u0026#39;,\u0026#39;role[swift-container-server]\u0026#39;,\u0026#39;role[swift-account-server]\u0026#39; -E swift-ha --sudo -x jedipunkz Chef サーバにノード情報が登録されているので各ストレージノードの zone 情報にシー ケンシャル番号を付与します。これにより各ノードは自分の zone 情報を Chef サーバ から知ることが出来ます。\n% knife exec -E \u0026#34;nodes.find(:name =\u0026gt; \u0026#39;swift-storage01\u0026#39;) {|n| n.set[\u0026#39;swift\u0026#39;][\u0026#39;zone\u0026#39;] = \u0026#39;1\u0026#39;; n.save }\u0026#34; % knife exec -E \u0026#34;nodes.find(:name =\u0026gt; \u0026#39;swift-storage02\u0026#39;) {|n| n.set[\u0026#39;swift\u0026#39;][\u0026#39;zone\u0026#39;] = \u0026#39;2\u0026#39;; n.save }\u0026#34; % knife exec -E \u0026#34;nodes.find(:name =\u0026gt; \u0026#39;swift-storage03\u0026#39;) {|n| n.set[\u0026#39;swift\u0026#39;][\u0026#39;zone\u0026#39;] = \u0026#39;3\u0026#39;; n.save }\u0026#34; Account, Container, Object に対して適切に割り当てられたか確認を行います。\n% knife exec -E \u0026#39;search(:node,\u0026#34;role:swift-account-server\u0026#34;) \\ { |n| z=n[:swift][:zone]||\u0026#34;not defined\u0026#34;; puts \u0026#34;#{n.name} has the role \\ role [swift-account-server] and is in swift zone #{z}\u0026#34;; }\u0026#39; % knife exec -E \u0026#39;search(:node,\u0026#34;role:swift-container-server\u0026#34;) \\ { |n| z=n[:swift][:zone]||\u0026#34;not defined\u0026#34;; puts \u0026#34;#{n.name} has the role \\ [swift-container-server] and is in swift zone #{z}\u0026#34;; }\u0026#39; % knife exec -E \u0026#39;search(:node,\u0026#34;role:swift-object-server\u0026#34;) \\ { |n| z=n[:swift][:zone]||\u0026#34;not defined\u0026#34;; puts \u0026#34;#{n.name} has the role \\ [swift-object-server] and is in swift zone #{z}\u0026#34;; }\u0026#39; また、先ほど用意した Swift 用ディスクデバイスを Chef がディスカバ出来るか確認 を行います。\n% knife exec -E \\ \u0026#39;search(:node,\u0026#34;role:swift-object-server OR \\ role:swift-account-server \\ OR role:swift-container-server\u0026#34;) \\ { |n| puts \u0026#34;#{n.name}\u0026#34;; \\ begin; n[:swift][:state][:devs].each do |d| \\ puts \u0026#34;\\tdevice #{d[1][\u0026#34;device\u0026#34;]}\u0026#34;; \\ end; rescue; puts \\ \u0026#34;no candidate drives found\u0026#34;; end; }\u0026#39; またディスカバされたディスクデバイス /dev/sdb1 が既にマウントされているかどう か確認します。\nswift-strageNN# df\n\u0026lsquo;swift-proxy01\u0026rsquo; ノードにて再度 chef-client を実行し /etc/swift/ring-workspace/generate-rings.sh を 更新する。上記 zone 情報に従って内容が更新される。\nswift-proxy01# chef-client 生成された generate-rails.sh を実行。\u0026rsquo;exit 0\u0026rsquo; の行をコメントアウトして実行すること。\nswift-proxy01# cd /etc/swift/ring-workspace swift-proxy01# ./generate-rings.sh /etc/swift/ring-workstation/rings 配下に Rings ファイルが生成されたはずです。確 認してみてください。この Rings ファイルを git サーバにプッシュします。git サー バは既に \u0026lsquo;swift-proxy01\u0026rsquo; ノード上に構築されています。\nswift-proxy01# cd /etc/swift/ring-workspace/rings swift-proxy01# git add account.builder container.builder object.builder swift-proxy01# git add account.ring.gz container.ring.gz object.ring.gz swift-proxy01# git commit -m \u0026#34;initial commit\u0026#34; swift-proxy01# git push git サーバにプッシュされた Rings ファイルを各ノードに配布します。配布の方法は 各ノードで chef-client を実行することです。これは knife ssh 等を用いても構いま せん。\nswift-proxy01# chef-client swift-proxy02# chef-client swift-storage01# chef-client swift-storage02# chef-client swift-storage03# chef-client swift-storageNN の計3台が登録されたかどうかを確認します。\nswift-proxy# swift-recon --md5 動作確認 動作確認をしましょう。\nswift-storage01# source swift-openrc swift-storage01# swift post container01 swift-storage01# echo \u0026#34;test\u0026#34; \u0026gt; test swift-storage01# swift upload container01 test swift-storage01# swift list swift-storage01# swift list container01 また、これらの操作が swift-proxy01, swift-proxy02 の片系を落とした状態でも可能 かどうかも確認しましょう。\nまとめ swift-proxy 片系の障害時にも読み込み系・書き込み系の操作が可能なことを確認出来 ました。復旧に関しても MySQL 的な Slave 系 (今回だと swift-proxy02(後からブー トストラップしたノード)) に関しては knife bootstrap で出来ます。Master 系の復 旧は簡単にはいきません。この事態に備えるには Rings 情報のバックアップ (Git レ ポジトリバックアップ) と MySQL のダンプの定期的取得が必要になるでしょう。\nまた、今回 VRRP で HA しているので2台構成になります。Swift プロキシは本来、ノー ドを増やすことでより多くのリクエストを受けられる (何がボトルネックになるかで状 況は変わりますが) 利点があるため、この2台構成がベストかどうかは状況に合わせて 考えた方が良いかもしれません。この構成の場合、ロードバランサが必要ない点がメリッ トだったりします。\n","permalink":"https://jedipunkz.github.io/post/2013/07/26/swift-ha-chef-deploy/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003e最近 Chef で OpenStack をデプロイすることばかりに興味持っちゃって、他のことも\nやらんとなぁと思っているのですが、せっかくなので Swift HA 構成を Chef でデプロ\nイする方法を書きます。\u003c/p\u003e\n\u003cp\u003eSwift って分散ストレージなのに HA ってなんよ！と思われるかもしれませんが、ご存\n知の様に Swift はストレージノード (accout, object, container) とプロキシノード\nに別れます。今回紹介する方法だとプロキシノードを Keepalived と Haproxy で HA、\nまた MySQL も KeepAlived で HA の構成に出来ました。いつものように RackSpace 管\n理の Cookbooks を使っています。\u003c/p\u003e\n\u003ch2 id=\"参考資料\"\u003e参考資料\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"http://www.rackspace.com/knowledge_center/article/openstack-object-storage-configuration\"\u003ehttp://www.rackspace.com/knowledge_center/article/openstack-object-storage-configuration\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"構成\"\u003e構成\u003c/h2\u003e\n\u003cp\u003e構成は簡単に記すと下記のようになります。特徴としては\u0026hellip;\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eswift-proxy01, swift-proxy02 で HA。VRRP + LB な構成。\u003c/li\u003e\n\u003cli\u003eswift-proxy01 で git サーバ稼働。Rings 情報を管理。\u003c/li\u003e\n\u003cli\u003eswift-storageNN がストレージノード\u003c/li\u003e\n\u003cli\u003eOS は Ubuntu server 12.04\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eです。\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e|--------- VRRP + Load Balancer ------|\n\n+-----------------+ +-----------------+ +-----------------+ +-----------------+ +-----------------+\n|  swift-proxy01  | |  swift-proxy02  | | swift-storage01 | | swift-storage02 | | swift-storage03 |\n+-----------------+ +-----------------+ +-----------------+ +-----------------+ +-----------------+\n|                   |                   |                   |                   |\n+-------------------+-------------------+-------------------+-------------------+------------------\n|                   |\n+-----------------+ +-----------------+\n| chef workstation| |   chef server   |\n+-----------------+ +-----------------+\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e絵は書く意味なかったか\u0026hellip;。\u003c/p\u003e","title":"Swift HA 構成を Chef でデプロイ"},{"content":"こんにちは。@jedipunkz です。\nOpenStack を運用する中でコントローラは重要です。コントローラノードが落ちると、 仮想マシンの操作等が利用出来ません。コントローラの冗長構成を取るポイントは公式 wiki サイトに記述あるのですが PaceMaker を使った構成でしんどいです。何より運用 する人が混乱する仕組みは避けたいです。\nRackSpace 社の管理している Chef Cookbooks の Roles に \u0026lsquo;ha-controller1\u0026rsquo;, \u0026lsquo;ha-controller2\u0026rsquo; というモノがあります。今回はこれを使った HA 構成の構築方法に ついて書いていこうかと思います。\n構成 最小構成を作りたいと思います。HA のためのコントローラノード2台, コンピュートノー ド1台, Chef ワークショテーション1台, Chef サーバノード1台。\n+----------------+----------------+----------------+----------------+--------------- public network | | | eth0 | | 10.0.0.0/24 +--------------+ +--------------+ +--------------+ +--------------+ +--------------+ | controller01 | | controller01 | | compute01 | | chef server | | workstation | +--------------+ +--------------+ +--------------+ +--------------+ +--------------+ | eth1 +------------------------------------------------- fixed range network 172.16.0.0/24 特徴 \u0026lsquo;controller01\u0026rsquo;, \u0026lsquo;controller02\u0026rsquo; で HA 化コントローラ \u0026lsquo;compute01\u0026rsquo; はコンピュートノード \u0026lsquo;chef server\u0026rsquo; は Chef サーバ、Chef11 推奨 ほとんどの作業を行う \u0026lsquo;workstaion\u0026rsquo; は Chef ワークステーション nova-network 構成 Chef サーバの構築 Chef サーバの構築方法は本題から外れるので割愛します。今は Omnibus インストーラで一発です。\nwokstation ノードでの準備 workstation で knife を使えるまでの準備についても割愛します。 プロンプトに何もホスト名が記されていないモノは全て workstation ノード上での操作です。 Rackspace 管理の Cookbook を取得します。\n% git clone https://github.com/rcbops/chef-cookbooks.git ~/openstack-chef-repo % cd ~/openstack-chef-repo v4.0.0 というタグ (現在 2013/07/16 最新リリース版) をチェックアウトします。また submodue update で関連 する Cookbooks を全て取得する事ができます。\n% git checkout v4.0.0 % git submodule init % git submodule sync % git submodule update 取得した Cookbooks を Chef サーバにアップロードします。\n% knife cookbook upload -o cookbooks -a 同様に Roles も Chef サーバにアップロードします。\n% knife role from file roles/*rb environment ファイルを生成します。ここで指定するパラメターは各々の Cookbooks の Attributes を上書き出来ます。 また、Recipe 内でノードサーチする際に同じ environment 名が指定されているノードをクラスタ構成の一部と判断出来る といった仕組みです。\n% ${EDITOR} environments/Cluster01.json { \u0026#34;name\u0026#34;: \u0026#34;Cluster01\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;cookbook_versions\u0026#34;: { }, \u0026#34;json_class\u0026#34;: \u0026#34;Chef::Environment\u0026#34;, \u0026#34;chef_type\u0026#34;: \u0026#34;environment\u0026#34;, \u0026#34;default_attributes\u0026#34;: { }, \u0026#34;override_attributes\u0026#34;: { \u0026#34;package_component\u0026#34;: \u0026#34;grizzly\u0026#34;, \u0026#34;osops_networks\u0026#34;: { \u0026#34;management\u0026#34;: \u0026#34;10.0.0.0/24\u0026#34;, \u0026#34;public\u0026#34;: \u0026#34;10.0.0.0/24\u0026#34;, \u0026#34;nova\u0026#34;: \u0026#34;10.0.0.0/24\u0026#34; }, \u0026#34;nova\u0026#34;: { \u0026#34;networks\u0026#34;: [ { \u0026#34;bridge\u0026#34;: \u0026#34;br100\u0026#34;, \u0026#34;num_networks\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;dns2\u0026#34;: \u0026#34;8.8.4.4\u0026#34;, \u0026#34;dns1\u0026#34;: \u0026#34;8.8.8.8\u0026#34;, \u0026#34;ipv4_cidr\u0026#34;: \u0026#34;172.16.0.0/24\u0026#34;, \u0026#34;network_size\u0026#34;: \u0026#34;256\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;private\u0026#34;, \u0026#34;bridge_dev\u0026#34;: \u0026#34;eth1\u0026#34; } ], \u0026#34;config\u0026#34;: { \u0026#34;use_single_default_gateway\u0026#34;: false, \u0026#34;ram_allocation_ratio\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;cpu_allocation_ratio\u0026#34;: \u0026#34;16\u0026#34; }, \u0026#34;network\u0026#34;: { \u0026#34;dmz_cidr\u0026#34;: \u0026#34;172.18.0.0/24\u0026#34;, \u0026#34;public_interface\u0026#34;: \u0026#34;eth0\u0026#34;, \u0026#34;multi_host\u0026#34;: true, \u0026#34;fixed_range\u0026#34;: \u0026#34;172.16.0.0/24\u0026#34; }, \u0026#34;apply_patches\u0026#34;: true, \u0026#34;libvirt\u0026#34;: { \u0026#34;vncserver_listen\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;virt_type\u0026#34;: \u0026#34;kvm\u0026#34; }, \u0026#34;db\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;nova\u0026#34; } }, \u0026#34;cinder\u0026#34;: { \u0026#34;db\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;cinder\u0026#34; } }, \u0026#34;keystone\u0026#34;: { \u0026#34;admin_user\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;tenants\u0026#34;: [ \u0026#34;admin\u0026#34;, \u0026#34;service\u0026#34; ], \u0026#34;users\u0026#34;: { \u0026#34;admin\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;secrete\u0026#34;, \u0026#34;roles\u0026#34;: { \u0026#34;admin\u0026#34;: [ \u0026#34;admin\u0026#34; ] } }, \u0026#34;demo\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;demo\u0026#34;, \u0026#34;default_tenant\u0026#34; : \u0026#34;service\u0026#34;, \u0026#34;roles\u0026#34;: { \u0026#34;service\u0026#34;: [ \u0026#34;service\u0026#34; ] } } }, \u0026#34;db\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;keystone\u0026#34; } }, \u0026#34;horizon\u0026#34;: { \u0026#34;theme\u0026#34;: \u0026#34;Rackspace\u0026#34;, \u0026#34;db\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;horizon\u0026#34; } }, \u0026#34;mysql\u0026#34;: { \u0026#34;root_network_acl\u0026#34;: \u0026#34;%\u0026#34;, \u0026#34;allow_remote_root\u0026#34;: true }, \u0026#34;monitoring\u0026#34;: { \u0026#34;procmon_provider\u0026#34;: \u0026#34;monit\u0026#34;, \u0026#34;metric_provider\u0026#34;: \u0026#34;collectd\u0026#34; }, \u0026#34;glance\u0026#34;: { \u0026#34;images\u0026#34;: [ \u0026#34;precise\u0026#34;, \u0026#34;cirros\u0026#34; ], \u0026#34;image\u0026#34;: { }, \u0026#34;image_upload\u0026#34;: true, \u0026#34;db\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;glance\u0026#34; } }, \u0026#34;vips\u0026#34;: { \u0026#34;cinder-api\u0026#34;: \u0026#34;10.0.0.22\u0026#34;, \u0026#34;glance-api\u0026#34;: \u0026#34;10.0.0.22\u0026#34;, \u0026#34;glance-registry\u0026#34;: \u0026#34;10.0.0.22\u0026#34;, \u0026#34;horizon-dash\u0026#34;: \u0026#34;10.0.0.22\u0026#34;, \u0026#34;horizon-dash_ssl\u0026#34;: \u0026#34;10.0.0.22\u0026#34;, \u0026#34;keystone-admin-api\u0026#34;: \u0026#34;10.0.0.22\u0026#34;, \u0026#34;keystone-service-api\u0026#34;: \u0026#34;10.0.0.22\u0026#34;, \u0026#34;keystone-internal-api\u0026#34;: \u0026#34;10.0.0.22\u0026#34;, \u0026#34;mysql-db\u0026#34;: \u0026#34;10.0.0.21\u0026#34;, \u0026#34;nova-api\u0026#34;: \u0026#34;10.0.0.22\u0026#34;, \u0026#34;nova-ec2-public\u0026#34;: \u0026#34;10.0.0.22\u0026#34;, \u0026#34;nova-novnc-proxy\u0026#34;: \u0026#34;10.0.0.22\u0026#34;, \u0026#34;nova-xvpvnc-proxy\u0026#34;: \u0026#34;10.0.0.22\u0026#34;, \u0026#34;rabbitmq-queue\u0026#34;: \u0026#34;10.0.0.23\u0026#34; }, \u0026#34;developer_mode\u0026#34;: false } } 作成した environment を Chef サーバにアップロードします。\n% knife environment from file environments/Cluster01.json HA コントローラデプロイ controller01, controller02 ノードを HA 化するわけですが、この操作を全て Chef で行います。\n% knife bootstrap \u0026lt;ip_controller01\u0026gt; -N controller01 -r \u0026#39;role[ha-controller1]\u0026#39; -E Cluster01 --sudo -x jedipunkz % knife bootstrap \u0026lt;ip_controller02\u0026gt; -N controller02 -r \u0026#39;role[ha-controller2]\u0026#39; -E Cluster01 --sudo -x jedipunkz chef サーバに node の情報が登録されました。ここから environment で指定した VIP を利用した HA 構成を組むため再度 chef-client を実行します。chef-client をデーモン化しておいた場合は 時間と共に自動で構築されるはずです。\ncontroller01# chef-client controller02# chef-client Cinder の利用に関しては片系のコントローラノードに寄せる必要があります。\u0026lsquo;cinder-volumes\u0026rsquo; という名前の Volume Group を controller01 ノードに作成し下記の操作を行います。\n% knife node run_list add controller01 \u0026#39;role[cinder-volume]\u0026#39; controller01# chef-client controller01# service cinder-volume restart Compute ノードのデプロイ compute ノードのデプロイも同様に Chef を用います。\n% knife bootstrap \u0026lt;ip_compute01\u0026gt; -N compute01 -r \u0026#39;role[single-compute]\u0026#39; -E Cluster01 --sudo -x jedipunkz compute ノードの追加に関しても同様に行なっていくことで仮想マシンの数を拡張出来ます。\nデプロイされた HA 構成の内部 PaceMaker は今回使われていないようです。デプロイされた構成の内部を覗いてみると下記のような特徴がありました。\n各 OpenStack APIs Keepalived で VRRP 構成。active/passive 構成で明示的な Master は存在しない。その都度、評価の高いノードが VIP を引き継ぎサービスリクエストに応じる。また、リクエストを受けたコントローラノードは haproxy により HA の2台に対して それぞれリクエストを分散。ラウンドロビンの冗長が取られている。\nRabbitMQ Keepalived で VRRP 構成。APIs と同様に active/passive 構成。その後は HA 内片系のコントローラの RabbitMQ が全てのリクエストに 応え処理を行う。\nMySQL Keepalived で VRRP 構成。APIs と同様に active/passive 構成。MySQL はミドルウェアベースで master/master, active/passive レプリケーションが組まれている。\n障害系の対処 ここまで来ると、HA 構成が壊れた際に Chef で復旧を自動化してみたくなります。試したところ HA 構成の MySQL 的な passive 側 が壊れた際には knife bootstrap で直ちに復旧することが出来ました。ただし active (master) 側が壊れた際にはバックアップした データからのリストア作業が必要でした。つまり Chef での自動復旧はできませんでした。\nまとめ keepalived と haproxy によるシンプルな HA 構成のため運用する人間が理解・対処する事も容易になると考えられます。 コントローラノード障害といっても、大抵は disk 交換や筐体交換で復旧出来るわけですから、HA さえ組まれていればかなり 安心して運用が出来そうです。今回は nova-network を試してみましたが、neutron 構成も組める Cookbooks になっているので 時間を見つけてやってみようかなと考えています。\n","permalink":"https://jedipunkz.github.io/post/2013/07/17/openstach-ha-chef-deploy/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003eOpenStack を運用する中でコントローラは重要です。コントローラノードが落ちると、\n仮想マシンの操作等が利用出来ません。コントローラの冗長構成を取るポイントは公式\nwiki サイトに記述あるのですが PaceMaker を使った構成でしんどいです。何より運用\nする人が混乱する仕組みは避けたいです。\u003c/p\u003e\n\u003cp\u003eRackSpace 社の管理している Chef Cookbooks の Roles に \u0026lsquo;ha-controller1\u0026rsquo;,\n\u0026lsquo;ha-controller2\u0026rsquo; というモノがあります。今回はこれを使った HA 構成の構築方法に\nついて書いていこうかと思います。\u003c/p\u003e\n\u003ch2 id=\"構成\"\u003e構成\u003c/h2\u003e\n\u003cp\u003e最小構成を作りたいと思います。HA のためのコントローラノード2台, コンピュートノー\nド1台, Chef ワークショテーション1台, Chef サーバノード1台。\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e+----------------+----------------+----------------+----------------+--------------- public network\n|                |                | eth0           |                |                10.0.0.0/24\n+--------------+ +--------------+ +--------------+ +--------------+ +--------------+\n| controller01 | | controller01 | |  compute01   | | chef server  | | workstation  |\n+--------------+ +--------------+ +--------------+ +--------------+ +--------------+\n                                  | eth1\n                                  +------------------------------------------------- fixed range network\n                                                                                     172.16.0.0/24\n\u003c/code\u003e\u003c/pre\u003e\u003ch4 id=\"特徴\"\u003e特徴\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003e\u0026lsquo;controller01\u0026rsquo;, \u0026lsquo;controller02\u0026rsquo; で HA 化コントローラ\u003c/li\u003e\n\u003cli\u003e\u0026lsquo;compute01\u0026rsquo; はコンピュートノード\u003c/li\u003e\n\u003cli\u003e\u0026lsquo;chef server\u0026rsquo; は Chef サーバ、Chef11 推奨\u003c/li\u003e\n\u003cli\u003eほとんどの作業を行う \u0026lsquo;workstaion\u0026rsquo; は Chef ワークステーション\u003c/li\u003e\n\u003cli\u003enova-network 構成\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"chef-サーバの構築\"\u003eChef サーバの構築\u003c/h2\u003e\n\u003cp\u003eChef サーバの構築方法は本題から外れるので割愛します。今は Omnibus インストーラで一発です。\u003c/p\u003e","title":"OpenStack HA 構成を Chef でデプロイ"},{"content":"こんにちは。@jedipunkz です。\n前回の記事で OpenCenter を使った OpenStack デプロイを行いましたが、デプロイの 仕組みの実体は Opscode Chef です。慣れている人であれば Chef を単独で使った方が よさそうです。僕もこの方法を今後取ろうと思っています。\n幾つかの構成を試している最中ですが、今回 nova-network を使ったオールインワン構 成を作ってみたいと思います。NIC の数は1つです。ノート PC や VPS サービス上にも 構築できると思いますので試してみてください。\n今回は Chef サーバの構築や Knife の環境構築に関しては割愛します。\nまた全ての操作は workstation ノードで行います。皆さんお手持ちの Macbook 等です。 デプロイする先は OpenStack をデプロイするサーバです。\n手順 Chef Cookbook を取得 RackSpace 社のエンジニアがメンテナンスしている Chef Cookbook を使います。各 Cookbook が git submodule 化されているので \u0026ndash;recursive オプションを付けます。\n% git clone https://github.com/rcbops/chef-cookbooks.git ~/openstack-chef-repo % cd openstack-chef-repo \u0026lsquo;v4.0.0\u0026rsquo; ブランチをチェックアウト master ブランチは今現在 (2013/07/08) folsom ベースの構成になっているので \u0026lsquo;grizzly\u0026rsquo; のためのブランチ \u0026lsquo;v4.0.0\u0026rsquo; をローカルにチェックアウトします。\n% git checkout v4.0.0 submodule である Cookbooks を初期化 -\u0026gt; 更新を行います。\n% git submodule init % git submodule sync % git submodule update Environment の作成 Chef の中に Environment という情報があります。これは環境に合わせ各 Cookbooks 内の Attributes 等を上書きすることが可能です。この Environment を作成すること でそれぞれ独立している Cookbooks を用いて一つの環境を作ることが可能になっています。 Environement は clone してきた環境の environments ディレクトリ配下に格納します。 下記の情報をコピペして environments/AllinOne.json として保存してください。\n{ \u0026#34;name\u0026#34;: \u0026#34;AllinOne\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;cookbook_versions\u0026#34;: { }, \u0026#34;json_class\u0026#34;: \u0026#34;Chef::Environment\u0026#34;, \u0026#34;chef_type\u0026#34;: \u0026#34;environment\u0026#34;, \u0026#34;default_attributes\u0026#34;: { }, \u0026#34;override_attributes\u0026#34;: { \u0026#34;package_component\u0026#34;: \u0026#34;grizzly\u0026#34;, \u0026#34;osops_networks\u0026#34;: { \u0026#34;management\u0026#34;: \u0026#34;10.0.0.0/24\u0026#34;, \u0026#34;public\u0026#34;: \u0026#34;10.0.0.0/24\u0026#34;, \u0026#34;nova\u0026#34;: \u0026#34;10.0.0.0/24\u0026#34; }, \u0026#34;nova\u0026#34;: { \u0026#34;networks\u0026#34;: [ { \u0026#34;bridge\u0026#34;: \u0026#34;br100\u0026#34;, \u0026#34;num_networks\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;dns2\u0026#34;: \u0026#34;8.8.4.4\u0026#34;, \u0026#34;dns1\u0026#34;: \u0026#34;8.8.8.8\u0026#34;, \u0026#34;ipv4_cidr\u0026#34;: \u0026#34;172.16.0.0/24\u0026#34;, \u0026#34;network_size\u0026#34;: \u0026#34;256\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;private\u0026#34; } ], \u0026#34;config\u0026#34;: { \u0026#34;use_single_default_gateway\u0026#34;: false, \u0026#34;ram_allocation_ratio\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;cpu_allocation_ratio\u0026#34;: \u0026#34;16\u0026#34; }, \u0026#34;network\u0026#34;: { \u0026#34;dmz_cidr\u0026#34;: \u0026#34;172.18.0.0/24\u0026#34;, \u0026#34;public_interface\u0026#34;: \u0026#34;eth0\u0026#34;, \u0026#34;multi_host\u0026#34;: true, \u0026#34;fixed_range\u0026#34;: \u0026#34;172.16.0.0/24\u0026#34; }, \u0026#34;apply_patches\u0026#34;: true, \u0026#34;libvirt\u0026#34;: { \u0026#34;vncserver_listen\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;virt_type\u0026#34;: \u0026#34;qemu\u0026#34; } }, \u0026#34;keystone\u0026#34;: { \u0026#34;admin_user\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;tenants\u0026#34;: [ \u0026#34;admin\u0026#34;, \u0026#34;service\u0026#34; ], \u0026#34;users\u0026#34;: { \u0026#34;admin\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;secrete\u0026#34;, \u0026#34;roles\u0026#34;: { \u0026#34;admin\u0026#34;: [ \u0026#34;admin\u0026#34; ] } }, \u0026#34;demo\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;demo\u0026#34;, \u0026#34;default_tenant\u0026#34; : \u0026#34;service\u0026#34;, \u0026#34;roles\u0026#34;: { \u0026#34;service\u0026#34;: [ \u0026#34;service\u0026#34; ] } } } }, \u0026#34;horizon\u0026#34;: { \u0026#34;theme\u0026#34;: \u0026#34;Rackspace\u0026#34; }, \u0026#34;mysql\u0026#34;: { \u0026#34;root_network_acl\u0026#34;: \u0026#34;%\u0026#34;, \u0026#34;allow_remote_root\u0026#34;: true }, \u0026#34;monitoring\u0026#34;: { \u0026#34;procmon_provider\u0026#34;: \u0026#34;monit\u0026#34;, \u0026#34;metric_provider\u0026#34;: \u0026#34;collectd\u0026#34; }, \u0026#34;glance\u0026#34;: { \u0026#34;images\u0026#34;: [ \u0026#34;precise\u0026#34;, \u0026#34;cirros\u0026#34; ], \u0026#34;image\u0026#34;: { }, \u0026#34;image_upload\u0026#34;: true }, \u0026#34;developer_mode\u0026#34;: false } } 環境に合わせ修正する必要があるのは\u0026hellip;\n\u0026#34;osops_networks\u0026#34;: { \u0026#34;management\u0026#34;: \u0026#34;10.0.0.0/24\u0026#34;, \u0026#34;public\u0026#34;: \u0026#34;10.0.0.0/24\u0026#34;, \u0026#34;nova\u0026#34;: \u0026#34;10.0.0.0/24\u0026#34; }, だけです。私の環境は 10.0.0.0/24 にあるので上記のように記しましたが、これはみ なさんのネットワーク環境に合わせ修正してください。\nRoles の修正 デフォルトの \u0026lsquo;allinone\u0026rsquo; Role では Cinder が使えない状態です。これは別ストレー ジを扱うことを前提にしているためと思われます。今回はオールインワンでコミコミの 構成を組みたいので roles/allinone.json を下記のように追記します。\nrun_list( \u0026#34;role[single-controller]\u0026#34;, \u0026#34;role[single-compute]\u0026#34;, \u0026#34;role[cinder-volume]\u0026#34; # \u0026lt;- 追加 ) Cookbooks, Roles, Environments を Chef サーバへアップロード チェックアウトした Cookbooks, Roles, あと今作成した Environements を Chef サー バへアップロードします。\n% knife cookbook upload -o cookbooks -a % knife role from file roles/*.rb % knife environment from file environments/AllinOne.json OpenStack をデプロイ いよいよ、次のコマンドでデプロイを行います。\n% knife bootstrap \u0026lt;ip_address\u0026gt; -N \u0026lt;hostname\u0026gt; -r \u0026#39;role[allinone]\u0026#39; -E AllinOne --sudo -x \u0026lt;username\u0026gt; 数分経つと OpenStack デプロイが完了します。ブラウザで https://\u0026lt;ip_address\u0026gt; へ アクセスして確認してみてください。ログインアカウントを上記の environments 内に 記してあります。(user : demo, pass : demo, もしくは user : admin, pass : secrete)\nCinder の利用 この状態で VM を作成してアクセスすることは出来るのですが、Block Storage as a Service (Cinder) は利用できない状態です。下記の操作で使えるようになります。\n% sudo dd if=/dev/zero of=/var/lib/cinder/volumes-disk bs=2 count=0 seek=7G % sudo modprobe loop % sudo losetup /dev/loop3 /var/lib/cinder/volumes-disk % sudo pvcreate /dev/loop3 % sudo vgcreate cinder-volumes /dev/loop3 % sudo service cinder-volume restart まとめと考察 操作して気がついたと思いますが、Roles が数多く存在します。これは色んな構成が組 めることを明示していると言えます。また、Environments を OpenStack 上の一つの構 成に見立てる (例 : controller + compute x n ) あたりが、とても Chef との親和性 が高いと言えます。今まで構築のために bash スクリプトを書いてきましたが、もうそ の必要が無くなりました。また Git のブランチに \u0026lsquo;folsom\u0026rsquo; 等がある通り、ブランチ をチェックアウトし直すことで folsom ベースの構成も組むことが出来ます。私は今、 コントローラノードの HA 化をこの Cookbooks でデプロイしてみています。とても完 成度が高いなぁと感じるのは environments である json ファイルに vips という項目 を追記するだけで HA 構成が組める点です。これにより MySQL, Rabbitmq, API が VIP を持ち、コントローラノードの片系に障害が発生しても OpenStack 全体の構成 (今回 で言う一つの Envorinments ですね) がサービス継続できる！ということです。次回、 機会がありましたら、その構成のデプロイ方法についても紹介したいと思います。\nまた、今回は VPS サービス上等に構築出来るように\n\u0026#34;virt_type\u0026#34;: \u0026#34;qemu\u0026#34; と environments に記しましたが、KVM リソースが扱える環境であればここを \u0026lsquo;kvm\u0026rsquo; とすることでパフォーマンスは若干上がると思います。\n","permalink":"https://jedipunkz.github.io/post/2013/07/08/chef-openstack-deploy/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003e前回の記事で OpenCenter を使った OpenStack デプロイを行いましたが、デプロイの\n仕組みの実体は Opscode Chef です。慣れている人であれば Chef を単独で使った方が\nよさそうです。僕もこの方法を今後取ろうと思っています。\u003c/p\u003e\n\u003cp\u003e幾つかの構成を試している最中ですが、今回 nova-network を使ったオールインワン構\n成を作ってみたいと思います。NIC の数は1つです。ノート PC や VPS サービス上にも\n構築できると思いますので試してみてください。\u003c/p\u003e\n\u003cp\u003e今回は Chef サーバの構築や Knife の環境構築に関しては割愛します。\u003c/p\u003e\n\u003cp\u003eまた全ての操作は workstation ノードで行います。皆さんお手持ちの Macbook 等です。\nデプロイする先は OpenStack をデプロイするサーバです。\u003c/p\u003e\n\u003ch2 id=\"手順\"\u003e手順\u003c/h2\u003e\n\u003ch4 id=\"chef-cookbook-を取得\"\u003eChef Cookbook を取得\u003c/h4\u003e\n\u003cp\u003eRackSpace 社のエンジニアがメンテナンスしている Chef Cookbook を使います。各\nCookbook が git submodule 化されているので \u0026ndash;recursive オプションを付けます。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e% git clone https://github.com/rcbops/chef-cookbooks.git ~/openstack-chef-repo\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e% cd openstack-chef-repo\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch4 id=\"v400-ブランチをチェックアウト\"\u003e\u0026lsquo;v4.0.0\u0026rsquo; ブランチをチェックアウト\u003c/h4\u003e\n\u003cp\u003emaster ブランチは今現在 (2013/07/08) folsom ベースの構成になっているので\n\u0026lsquo;grizzly\u0026rsquo; のためのブランチ \u0026lsquo;v4.0.0\u0026rsquo; をローカルにチェックアウトします。\u003c/p\u003e","title":"Chef で OpenStack デプロイ"},{"content":"こんにちは。@jedipunkz です。\n第13回 OpenStack 勉強会に参加してきました。内容の濃い収穫のある勉強会でした。 参加してよかった。特にえぐちさんの OpenCenter に関するプレゼン (下記のスライド 参照のこと) には驚きました。ちょうど当日 RackSpace のエンジニアが管理している github 上の Chef Cookbooks を使って OpenStack 構築できたぁ！と感動していたのに、 その晩のうちに Chef を使って GUI で！ OpenStack が自動構築出来るだなんて\u0026hellip;。\nえぐちさんのスライド資料はこちら。\nhttp://www.slideshare.net/guchi_hiro/open-centeropenstack\n早速、私も手元で OpenCenter 使ってみました。えぐちさん、情報ありがとうございましたー。\n実は私 としては Chef 単体で OpenStack を構築したいのですが、OpenCenter がどう Cookbook や Roles, Environment を割り当てているのか知りたかったので、 OpenCenter を使って構築してみました。今回はその準備。今日は OpenCenter で皆さ んも OpenStack を構築できるようキャプチャ付きで方法を紹介しますが、次の機会に Chef 単体での OpenStack の構築方法を紹介出来ればいいなぁと思っています。\n構成 +---------------+---------------+---------------+---------------+-------------- public network |eth0 |eth0 |eth1 |eth1 | eth1 |10.200.10.11 |10.200.10.12 |10.200.10.13 |10.200.10.14 |10.200.10.15 +-------------+ +-------------+ +-------------+ +-------------+ +-------------+ | opencenter | | oc-chef | |oc-controller| | oc-compute01| | oc-compute02| +-------------+ +-------------+ +-------------+ +-------------+ +-------------+ |10.200.9.14 |10.200.9.15 | eth0 |eth0 +---------------+-------------- vm network 5台使ってみた 全てのノードでインターネットへの経路を public network に向ける VM が接続する br100 ブリッジは eth0 にバインドした VM からインターネットへのアクセスは oc-computeNN が NAT する oc-chef は Chef Server ! OpenCenter の構築 構築は\u0026hellip;\n% curl -s -L http://sh.opencenter.rackspace.com/install.sh | sudo bash -s - --role=server % curl -s -L http://sh.opencenter.rackspace.com/install.sh | \\ sudo bash -s - --role=dashboard --ip=10.200.10.11 これだけ！\nOpenCenter で管理するノードへエージェントをインストール oc-chef, oc-controller, oc-computeNN は全て OpenCenter で管理する必要があるの で OpenCeter エージェントをインストールします。\n% curl -s -L http://sh.opencenter.rackspace.com/install.sh | \\ sudo bash -s - --role=agent --ip=10.200.10.11 GUI へアクセス https://10.200.10.11 ブラウザで上記の URL にアクセスするとデフォルトアカウント情報 (ユーザ名 : admin, パスワード : password) でログイン出来ます。\n{% img /pix/oc01.png 600 %}\nこんな感じ。\nChef Server のインストール oc-chef をクリック -\u0026gt; \u0026lsquo;Install Chef Server\u0026rsquo; を選択\n暫くするとタスクがグリーンになり完了。\n{% img /pix/oc02.png 600 %}\nこうなります。この Chef Server 上の Cookbooks, Roles 等を利用して knife bootstrap しコントローラ・コンピュートノードを構築する仕組みです。\nNovaCluster の作成 NovaCluster という controller x n, compute x n のセットを意味するモノを作りま す。入力フォームが表示されるので下記の画像のように入力します。\n{% img /pix/oc03.png 600 %}\nNAT Exclusion CIDRs が DMZ, VM Network CIDR が Fixed Range だと理解すれば、他 は問題ないように思います。\n{% img /pix/oc04.png 600 %}\n結果、こうなります。\nコントローラノード構築 available nodes にあるノード \u0026lsquo;oc-controller\u0026rsquo; を \u0026lsquo;Infrastructure\u0026rsquo; へドラッグア ンドドロップします。\n{% img /pix/oc05.png 600 %}\nコンピュートノード構築 available nodes にあるノード \u0026lsquo;oc-computeNN\u0026rsquo; を \u0026lsquo;AZ Nova\u0026rsquo; へドラッグアンドドロッ プします。\n{% img /pix/oc07.png 600 %}\n2台ともドラッグアンドドロップすると、こうなります。\nHorizon へアクセスし操作 あとはいつものように OpenStack を操作するだけです。Horizon へは下記の URL でア クセス出来ます。admin ユーザのパスワードは先ほどの NovaCluster 作成の際に入力 したモノです。\nhttps://10.200.10.13 まとめ 完成された OpenStack を覗いたのですが monit が利用されていたり、Horizon が HTTPS 化されていたり、完成度が高かったです。さすが RackSpace 製という感じ。も う OpenStack 構築スクリプトを作成する必要がなくなったなぁと。Chef で構築するべ きです。\n内部で使われている Chef Cookbooks は下記の URL に同じものがありました。\nhttps://github.com/rcbops/chef-cookbooks\nこの Chef Cookbooks を単体で使って構築してみましたが同様に1コマンドで構築が出 来ました。(オール・イン・ワン構成しか試していません) 僕らとしては GUI は仕事で は操作しにくいのでこのCookbooks を利用した形で改修し使っていきたいなぁと感じて います。GUI であってもドラッグアンドドロップで元には戻せませんし、後々 Cookbook に手を入れ構成を変更するといったことも面倒になりそうです。\nとは言っても、ドラッグアンドドロップで構築できるなんて\u0026hellip; 。誰でも構築できる時 期にあるんですね。運用するには Cookbook を理解している必要がありそうですが、 RackSpace としては Private Cloud Service に運用もセットで付加して売っているの で問題ないのでしょう。\nちなみに Chef の情報は os-chef ノードの root ユーザホームディレクトリ配下で探 れます。\noc-chef# cd ~ oc-chef# knife cookbook list oc-chef# knife role list oc-chef# knife environment list oc-chef# knife environment show NovaCluster01 ","permalink":"https://jedipunkz.github.io/post/2013/07/02/chef-opencenter-openstack/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003e第13回 OpenStack 勉強会に参加してきました。内容の濃い収穫のある勉強会でした。\n参加してよかった。特にえぐちさんの OpenCenter に関するプレゼン (下記のスライド\n参照のこと) には驚きました。ちょうど当日 RackSpace のエンジニアが管理している\ngithub 上の Chef Cookbooks を使って OpenStack 構築できたぁ！と感動していたのに、\nその晩のうちに Chef を使って GUI で！ OpenStack が自動構築出来るだなんて\u0026hellip;。\u003c/p\u003e\n\u003cp\u003eえぐちさんのスライド資料はこちら。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://www.slideshare.net/guchi_hiro/open-centeropenstack\"\u003ehttp://www.slideshare.net/guchi_hiro/open-centeropenstack\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e早速、私も手元で OpenCenter 使ってみました。えぐちさん、情報ありがとうございましたー。\u003c/p\u003e\n\u003cp\u003e実は私 としては Chef 単体で OpenStack を構築したいのですが、OpenCenter がどう\nCookbook や Roles, Environment を割り当てているのか知りたかったので、\nOpenCenter を使って構築してみました。今回はその準備。今日は OpenCenter で皆さ\nんも OpenStack を構築できるようキャプチャ付きで方法を紹介しますが、次の機会に\nChef 単体での OpenStack の構築方法を紹介出来ればいいなぁと思っています。\u003c/p\u003e\n\u003ch2 id=\"構成\"\u003e構成\u003c/h2\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e+---------------+---------------+---------------+---------------+-------------- public network\n|eth0           |eth0           |eth1           |eth1           | eth1\n|10.200.10.11   |10.200.10.12   |10.200.10.13   |10.200.10.14   |10.200.10.15\n+-------------+ +-------------+ +-------------+ +-------------+ +-------------+\n| opencenter  | |   oc-chef   | |oc-controller| | oc-compute01| | oc-compute02|\n+-------------+ +-------------+ +-------------+ +-------------+ +-------------+\n                                                |10.200.9.14    |10.200.9.15\n                                                | eth0          |eth0\n                                                +---------------+-------------- vm network\n\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\n\u003cli\u003e5台使ってみた\u003c/li\u003e\n\u003cli\u003e全てのノードでインターネットへの経路を public network に向ける\u003c/li\u003e\n\u003cli\u003eVM が接続する br100 ブリッジは eth0 にバインドした\u003c/li\u003e\n\u003cli\u003eVM からインターネットへのアクセスは oc-computeNN が NAT する\u003c/li\u003e\n\u003cli\u003eoc-chef は Chef Server !\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"opencenter-の構築\"\u003eOpenCenter の構築\u003c/h2\u003e\n\u003cp\u003e構築は\u0026hellip;\u003c/p\u003e","title":"内部で Chef を使っている OpenCenter で OpenStack 構築"},{"content":"こんにちは。@jedipunkz です。\n自動化の基盤を導入するために色々調べているのですが、監視も自動化しなくちゃ！と いうことで Sensu を調べてたのですが Chef との相性バッチリな感じで、自分的にイ ケてるなと思いました。\n公式サイト http://www.sonian.com/cloud-monitoring-sensu/ ドキュメント http://docs.sensuapp.org/0.9/index.html 開発元が予め Chef の Cookbook (正確にはラッパー Cookbook 開発のための Cookbook で Include して使う) を用意してくれていたり、インストールを容易にする ための Omnibus 形式のパッケージの提供だったり。Omnibus なのでインストールと共 に Sensu が推奨する Ruby 一式も一緒にインストールされます。Chef と同じですね。\n今回紹介したいのは、Chef で Sensu を構築・制御する方法です。\n+--------------+ +--------------+ | chef-server | | workstation | +--------------+ +--------------+ | | +----------------+ | +--------------+ | sensu-server | +--------------+ | +----------------+----------------+----------------+ | | | | +--------------+ +--------------+ +--------------+ +--------------+ | sensu-client | | sensu-client | | sensu-client | | sensu-client | ..\u0026gt; +--------------+ +--------------+ +--------------+ +--------------+ | service node | | service node | | service node | | service node | +--------------+ +--------------+ +--------------+ +--------------+ この構成の処理の流れとしては\u0026hellip;\nsensu-server, sensu-client の構築の流れ workstation から cookbook, role, data_bag を chef-server へアップ workstation から sensu-server を bootstrap で構築 workstation から sensu-client を boostrrap で構築 監視項目の追加・無効化の流れ workstation から監視項目 data bag の chef-server へのアップ sensu-server 上の chef-client が chef-server から data bag の取得 sensu-server に新たな(削除された)監視項目が追加 sensu-client が新たな(削除された)監視項目を検知し、監視開始 つまり\u0026hellip; 運用で必要な操作 (監視対象の追加・監視項目の追加・無効化)を workstation から knife を使って全て行えるっていうことです。しかも Chef も Sensu も API を持っているので自動化を形成するプログラムの開発も容易です。実際に API を叩くちょっとしたダッシュボードを Ruby on Rails で作ってみましたが簡単に出来 ました。\n今回はこの構成の構築と操作方法について書いていきます。予め私のほうで作っておい た sensu のための chef-repo を使って環境を作っていきます。\nhttps://github.com/jedipunkz/sensu-chef-repo\nこの chef-repo には\nBerksfile サンプルの監視項目 data bag SSL 鍵生成の仕組み (後に data bag に収める) のみが入っています。公式の sensu-chef レポジトリ内にあったサンプルを利用して作っ ています。また、Berksfile 内で、これまた\nchef-redis (redis の cookbook) sensu-chef (公式の sensu cookbook, ラッパー cookbook 内で用いる) chef-monitor (ラッパー cookbook の例) を取得するようにしています。それぞれ fork して私のレポジトリに置いています。動 く状態を保ちたかったためです。redis に関しては結構手を加えました。そのままでは 全く構築出来ない状態でしたので。chef-monitor は内部で sensu-chef (公式 cookbook) を Include しているラッパー cookbook です。公式 cookbook 内で例とし て挙げられていたモノです。こちらは手を加えていません。\nでは手順を\u0026hellip;\n(chef 環境の構築方法は割愛します)\nsensu-server のデプロイ sensu-chef-repo の取得を行います。ここからの操作は全て上図の workstation 上で の操作になります。\n% git clone https://github.com/jedipunkz/sensu-chef-repo SSL 鍵ペアを生成し data bag に投入します。\n% cd ~/sensu-chef-repo/data_bags/ssl % ./ssl_certs.sh generate % knife data bag create sensu % knife data bag from file sensu ./ssl.json サンプル監視項目 proc_cron.json を data bags に投入します。\n% cd ~/sensu-chef-repo/ % knife data bag create sensu_checks % knife data bag from file sensu_checks data_bags/sensu_checks/proc_cron.json proc_cron.json の内容は下記の通り\n{ \u0026#34;id\u0026#34;: \u0026#34;proc_cron\u0026#34;, \u0026#34;command\u0026#34;: \u0026#34;check-procs.rb -p cron -C 1\u0026#34;, \u0026#34;subscribers\u0026#34;: [ \u0026#34;sensu-client\u0026#34; ], \u0026#34;interval\u0026#34;: 10 } json の構成を説明すると..\nid : 監視項目名 command : agent が実行するコマンド subscribers : 監視対象のグループ名, chef role 名が自動で監視対象に割り当てられる interval : 監視間隔 (秒) となります。\nBerkshelf を使って cookbook の取得を行います。\n% gem install berksfile --no-ri --no-rdoc % berks install --path ./cookbooks/ roles を chef server へアップロードします。\n% knife role from file roles/sensu-client.rb # 後の sensu-client デプロイのためついでに準備します % knife role from file roles/sensu-server.rb \u0026ldquo;master_address\u0026rdquo; を sensu-server の IP アドレスに書き換えます。書き換える箇所 は \u0026lsquo;monitor\u0026rsquo; cookbook の attributes です。\n% ${EDITOR} cookbooks/monitor/attributes/default.rb default[\u0026#34;monitor\u0026#34;][\u0026#34;master_address\u0026#34;] = \u0026#34;XXX.XXX.XXX.XXX\u0026#34; cookbooks を chef server へアップロードします。\nworkstation% knife cookbook upload -a sensu-server を knife を用いてブートストラップします。\n% knife bootstrap \u0026lt;server-ip\u0026gt; -N \u0026lt;server-name\u0026gt; -r \u0026#39;role[sensu-server]\u0026#39; -x root -i \u0026lt;secret-key\u0026gt; sensu ダッシュボード URL : http://:8080 にアクセスし動作確認, アカ ウント情報は下記の attributes に記載してあります。\ncookbooks/sensu/attributes/default.rb sensu-client デプロイ方法 knife を用いて sensu-client をデプロイします。\n% knife bootstrap \u0026lt;client-ip\u0026gt; -N \u0026lt;client-name\u0026gt; -r \u0026#39;role[sensu-client]\u0026#39; -x root -i \u0026lt;secret-key\u0026gt; この状態で先ほどの \u0026lsquo;proc_cron\u0026rsquo; が監視開始されます。\n監視項目の追加方法 下記のような json ファイルを生成します。ここでは例として nginx のプロセス監視 のための項目を追加してみます。\n% ${EDITOR} data_bags/sensu_checks/proc_nginx.json { \u0026#34;id\u0026#34;: \u0026#34;proc_nginx\u0026#34;, \u0026#34;command\u0026#34;: \u0026#34;check-procs.rb -p nginx -w 5 -c 10\u0026#34;, \u0026#34;subscribers\u0026#34;: [ \u0026#34;sensu-client\u0026#34; ], \u0026#34;interval\u0026#34;: 10 } 先ほども書きましたが subscribers は chef 的な role 名と一致しています。なので sensu の監視をグルーピングしたいときは role 名を変えて knife bootstrap すると 良いと思います。ここでは例として \u0026lsquo;sensu-client\u0026rsquo; という先ほど利用した role 名を 用います。\ndata bags に監視項目情報を投入します。\n% knife data bag from file sensu_checks data_bags/sensu_checks/proc_nginx.json 暫くすると下記のプロセスを経て監視が開始される\nsensu-server 上の chef-client が interval 間隔後実行 sensu-server に proc_nginx.json が配置 sensu-server が chef により再起動 sensu-client 上の sensu agent が自らの subscribers が属している proc_nginx.json を検知 sensu-client が nginx のプロセス監視開始 まとめ 監視対象追加 (agent 仕込み), 監視項目追加を workstation 上から knife を使って 操作出来ました。監視項目の削除についてはダミーの json (command 項に \u0026rsquo;echo ok\u0026rsquo; など設定) を投入することで、私は対処していますが、本来は data bag が存在しない ことを検知して sensu-server 上から監視項目を削除する cookbook に仕上げなければ いけないと思います。\n","permalink":"https://jedipunkz.github.io/post/2013/06/20/sensu-chef-controll/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003e自動化の基盤を導入するために色々調べているのですが、監視も自動化しなくちゃ！と\nいうことで Sensu を調べてたのですが Chef との相性バッチリな感じで、自分的にイ\nケてるなと思いました。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e公式サイト \u003ca href=\"http://www.sonian.com/cloud-monitoring-sensu/\"\u003ehttp://www.sonian.com/cloud-monitoring-sensu/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eドキュメント \u003ca href=\"http://docs.sensuapp.org/0.9/index.html\"\u003ehttp://docs.sensuapp.org/0.9/index.html\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e開発元が予め Chef の Cookbook (正確にはラッパー Cookbook 開発のための\nCookbook で Include して使う) を用意してくれていたり、インストールを容易にする\nための Omnibus 形式のパッケージの提供だったり。Omnibus なのでインストールと共\nに Sensu が推奨する Ruby 一式も一緒にインストールされます。Chef と同じですね。\u003c/p\u003e\n\u003cp\u003e今回紹介したいのは、Chef で Sensu を構築・制御する方法です。\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e+--------------+ +--------------+\n| chef-server  | | workstation  |\n+--------------+ +--------------+\n       |                |\n       +----------------+ \n       |\n+--------------+\n| sensu-server |\n+--------------+\n       |\n       +----------------+----------------+----------------+\n       |                |                |                |\n+--------------+ +--------------+ +--------------+ +--------------+\n| sensu-client | | sensu-client | | sensu-client | | sensu-client | ..\u0026gt;\n+--------------+ +--------------+ +--------------+ +--------------+\n| service node | | service node | | service node | | service node |\n+--------------+ +--------------+ +--------------+ +--------------+\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eこの構成の処理の流れとしては\u0026hellip;\u003c/p\u003e","title":"Sensu 監視システムを Chef で制御"},{"content":"こんにちは。@jedipunkz です。\nrequire \u0026lsquo;chef\u0026rsquo; して Ruby コードの中で chef を利用したいと思って色々調べていた のですが、そもそもリファレンスが無くサンプルコードもごくわずかしかネット上に見 つけられない状態でした。結局ソースコードを読んで理解していく世界なわけですが、 サンプルコードが幾つかあると他の人に役立つかなぁと思い、ブログに載せていこうか なぁと。\nまず Chef サーバへアクセスするためには下記の情報が必要です。\nユーザ名 ユーザ用のクライアント鍵 Chef サーバの URL これらは Chef::Config で記していきます。\nでは早速サンプルコードです。まずは data bags 内データの一覧を取得するコードで す。data bags 内のデータを全で取得し配列で表示します。\n#!/usr/bin/env ruby require \u0026#39;rubygems\u0026#39; require \u0026#39;chef/rest\u0026#39; require \u0026#39;chef/search/query\u0026#39; Chef::Config[:node_name]=\u0026#39;user01\u0026#39; Chef::Config[:client_key]=\u0026#39;/home/user01/user01.pem\u0026#39; Chef::Config[:chef_server_url]=\u0026#34;https://10.200.9.22\u0026#34; Chef::DataBag::list.each do |bag_name, url| Chef::DataBag::load(bag_name).each do |item_name, url| item = Chef::DataBagItem.load(bag_name, item_name).to_hash puts item end end 次は data bags にデータを入力するコードです。json_data という JSON 形式のデー タを test_data という data bag に放り込んでいます。\n#!/usr/bin/env ruby require \u0026#39;rubygems\u0026#39; require \u0026#39;chef/rest\u0026#39; require \u0026#39;chef/search/query\u0026#39; Chef::Config[:node_name]=\u0026#39;user01\u0026#39; Chef::Config[:client_key]=\u0026#39;/home/user01/user01.pem\u0026#39; Chef::Config[:chef_server_url]=\u0026#34;https://10.0.0.10\u0026#34; json_data = { \u0026#34;id\u0026#34; =\u0026gt; \u0026#34;test\u0026#34;, \u0026#34;command\u0026#34; =\u0026gt; \u0026#34;echo test\u0026#34; } databag_item = Chef::DataBagItem.new databag_item.data_bag(\u0026#39;test_data\u0026#39;) databag_item.raw_data = proc_nginx databag_item.save 次は nodes 一覧の取得です。\n#!/usr/bin/env ruby require \u0026#39;rubygems\u0026#39; require \u0026#39;chef/rest\u0026#39; require \u0026#39;chef/search/query\u0026#39; Chef::Config[:node_name]=\u0026#39;user01\u0026#39; Chef::Config[:client_key]=\u0026#39;/home/user01/user01.pem\u0026#39; Chef::Config[:chef_server_url]=\u0026#34;https://10.0.0.10\u0026#34; Chef::Node.list.each do |node| puts node end 次は bootstrap するコード。\n#!/usr/bin/env ruby require \u0026#39;rubygems\u0026#39; require \u0026#34;chef\u0026#34; require \u0026#34;chef/knife/core/bootstrap_context\u0026#34; require \u0026#39;chef/knife\u0026#39; require \u0026#39;chef/knife/ssh\u0026#39; require \u0026#39;net/ssh\u0026#39; require \u0026#39;net/ssh/multi\u0026#39; require \u0026#39;chef/knife/bootstrap\u0026#39; Chef::Config[:node_name]=\u0026#39;user01\u0026#39; Chef::Config[:client_key]=\u0026#39;/home/user01/user01.pem\u0026#39; Chef::Config[:validation_key]=\u0026#39;/home/user01/chef-validator.pem\u0026#39; Chef::Config[:chef_server_url]=\u0026#34;https://10.0.0.10\u0026#34; kb = Chef::Knife::Bootstrap.new kb.name_args = [\u0026#34;sensu-client04.deathstar.jp\u0026#34;, \u0026#34;10.0.0.20\u0026#34;] kb.config[:ssh_user] = \u0026#34;root\u0026#34; kb.config[:identity_file] = \u0026#34;~/novakey01\u0026#34; kb.config[:ssh_port] = \u0026#34;22\u0026#34; kb.config[:run_list ] = \u0026#34;role[sensu-client]\u0026#34; kb.config[:template_file] = \u0026#34;/home/thirai/chef-full.erb\u0026#34; kb.run 以上です。他のサンプルもこれから探していこうかと思ってます。knife のソース見る のが一番はやいかなぁと。もしくは Chef のテストコード見るか。皆さんもご存知であ れば共有してくださーい。\n","permalink":"https://jedipunkz.github.io/post/2013/06/12/chef-ruby-code/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003erequire \u0026lsquo;chef\u0026rsquo; して Ruby コードの中で chef を利用したいと思って色々調べていた\nのですが、そもそもリファレンスが無くサンプルコードもごくわずかしかネット上に見\nつけられない状態でした。結局ソースコードを読んで理解していく世界なわけですが、\nサンプルコードが幾つかあると他の人に役立つかなぁと思い、ブログに載せていこうか\nなぁと。\u003c/p\u003e\n\u003cp\u003eまず Chef サーバへアクセスするためには下記の情報が必要です。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eユーザ名\u003c/li\u003e\n\u003cli\u003eユーザ用のクライアント鍵\u003c/li\u003e\n\u003cli\u003eChef サーバの URL\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eこれらは Chef::Config で記していきます。\u003c/p\u003e\n\u003cp\u003eでは早速サンプルコードです。まずは data bags 内データの一覧を取得するコードで\nす。data bags 内のデータを全で取得し配列で表示します。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-ruby\" data-lang=\"ruby\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#!/usr/bin/env ruby\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003erequire \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;rubygems\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003erequire \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;chef/rest\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003erequire \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;chef/search/query\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eChef\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eConfig\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e[\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e:node_name\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e]=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;user01\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eChef\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eConfig\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e[\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e:client_key\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e]=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;/home/user01/user01.pem\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eChef\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eConfig\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e[\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e:chef_server_url\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e]=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;https://10.200.9.22\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eChef\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eDataBag\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003elist\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eeach \u003cspan style=\"color:#66d9ef\"\u003edo\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e|\u003c/span\u003ebag_name, url\u003cspan style=\"color:#f92672\"\u003e|\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#66d9ef\"\u003eChef\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eDataBag\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003eload(bag_name)\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eeach \u003cspan style=\"color:#66d9ef\"\u003edo\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e|\u003c/span\u003eitem_name, url\u003cspan style=\"color:#f92672\"\u003e|\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    item \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eChef\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eDataBagItem\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eload(bag_name, item_name)\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eto_hash\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    puts item\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#66d9ef\"\u003eend\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eend\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e次は data bags にデータを入力するコードです。json_data という JSON 形式のデー\nタを test_data という data bag に放り込んでいます。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-ruby\" data-lang=\"ruby\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#!/usr/bin/env ruby\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003erequire \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;rubygems\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003erequire \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;chef/rest\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003erequire \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;chef/search/query\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eChef\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eConfig\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e[\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e:node_name\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e]=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;user01\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eChef\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eConfig\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e[\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e:client_key\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e]=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;/home/user01/user01.pem\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eChef\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eConfig\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e[\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e:chef_server_url\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e]=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;https://10.0.0.10\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ejson_data \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;id\u0026#34;\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u0026gt;\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;test\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;command\u0026#34;\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u0026gt;\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;echo test\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edatabag_item \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eChef\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eDataBagItem\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003enew\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edatabag_item\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edata_bag(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;test_data\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edatabag_item\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eraw_data \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e proc_nginx\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edatabag_item\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003esave\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e次は nodes 一覧の取得です。\u003c/p\u003e","title":"Chef を Ruby コード内で利用する"},{"content":"こんにちは。@jedipunkz です。\nCeph を運用する上で考慮しなければいけないのがトラフィックの負荷です。特に OSD 同士のレプリケーション・ハートビートには相当トラフィックの負荷が掛かることが想 像出来ます。\nこのため MDS, MON の通信に影響を与えないよう、OSD レプリケーション・ハートビー トのためのネットワークを別に設けるのがベストプラクティスな構成の様です。このネッ トワークのことをクラスターネットワークと Ceph 的に言うそうです。\nこんな接続になります。\n+------+ |Client| +------+ | +-------+-------+-------+-------+------ public network | | | | | +-----+ +-----+ +-----+ +-----+ +-----+ | MON | | MDS | | OSD | | OSD | | OSD | +-----+ +-----+ +-----+ +-----+ +-----+ | | | ----------------+-------+-------+------ cluster network 上図の様に MON, MDS は public ネットワークを介し OSD のレプリケーション・ハー トビートのみ cluster ネットワークを介します。Client と MDS との通信に影響を与 えない構成になります。\n今回はその様な構成を ceph-deploy を使って構築する方法を書いていきます。前提と なるホストとプロセスとネットワークの関係は下記の図の通りです。\n+----------+ | 'client' | +----------+ | +---------------+---------------+------------- public network | | | |10.0.0.11 | 10.0.0.12 | 10.0.0.13 +----------+ +----------+ +----------+ | 'ceph01' | | 'ceph02' | | 'ceph03' | | osd | | osd | | osd | | mon | | mon | | mon | | mds | | mds | | mds | +----------+ +----------+ +----------+ |172.18.0.11 | 172.18.0.12 | 172.18.0.13 | | | +---------------+---------------+------------- cluster network 特徴としては..\nceph01-03 には NIC を2本出します。 ceph01-03 の全てに MDS, MON, OSD を稼働させます。 ceph01-03:/dev/sdb を Ceph 用のディスクとして利用 となります。\nCeph-Deply を利用するまでの準備 今回は ceph-deploy を利用し Ceph を構築する。そのための準備として下記の操作を 行う。\nceph サーバ (ceph01-03) の準備 \u0026lsquo;ceph\u0026rsquo; ユーザの作成を行う。\n% ssh user@ceph-server % sudo useradd -d /home/ceph -m ceph % sudo passwd ceph sudoers の設定を行う。\n% echo \u0026quot;ceph ALL = (root) NOPASSWD:ALL\u0026quot; | sudo tee /etc/sudoers.d/ceph % sudo chmod 0440 /etc/sudoers.d/ceph \u0026lsquo;client\u0026rsquo; の準備 ホスト \u0026lsquo;client\u0026rsquo; で準備をします。この準備によって ceph-deploy をそれぞれのホス トに対して実行できるようになります。\nまず、ノンパスフレーズの SSH 公開鍵・秘密鍵を生成します。\nclient% ssh-keygen Generating public/private key pair. Enter file in which to save the key (/ceph-client/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /ceph-client/.ssh/id_rsa. Your public key has been saved in /ceph-client/.ssh/id_rsa.pub. 公開鍵をターゲットホスト (ceph01-03) に配置します。\nclient% ssh-copy-id ceph@ceph01 client% ssh-copy-id ceph@ceph02 client% ssh-copy-id ceph@ceph03 ceph-deploy を取得します。\nclient% git clone https://github.com/ceph/ceph-deploy.git ~/ceph-deploy \u0026lsquo;python-virtualenv\u0026rsquo; パッケージをインストールする。\nclient% sudo apt-get update ; sudo apt-get -y install python-virtualenv ceph-deploy をブートストラップする\nclient% cd ~/ceph-deploy client% ./bootstrap PATH を通す。下記は例。\nclient% ${EDITOR} ~/.zshrc export PATH=$HOME/ceph-deploy:$PATH ホスト名の解決を行う。\ncephclient% sudo ${EDITOR} /etc/hosts 10.0.0.11 ceph01 10.0.0.12 ceph02 10.0.0.13 ceph03 これで準備は OK です。ceph-deploy が使える状態になりました。\nCeph 構築手順 今回は ceph01-03 の3台構成を構築しますが、すべての操作はホスト \u0026lsquo;client\u0026rsquo; から行 います。先ほど配置した公開鍵によってそれぞれのホストに対して操作が行えます。\nceph サーバ・クライアント間通信のための鍵の生成とコンフィギュレーションの生成 を下記の操作にて行う。\nclient% ceph-deploy new ceph01 ceph02 ceph03 上記の操作で生成されたカレントディレクトリ上の ceph.conf に対して下記の記述を 追記する。\npublic network = 10.0.0.0/24 cluster network = 172.18.0.0/24 [mon.a] host = ceph01 mon addr = 10.0.0.11:6789 [mon.b] host = ceph02 mon addr = 10.0.0.12:6789 [mon.c] host = ceph03 mon addr = 10.0.0.13:6789 [osd.0] public addr = 10.0.0.11 cluster addr = 172.18.0.11 [osd.1 public addr = 10.0.0.12 cluster addr = 172.18.0.12 [osd.2] public addr = 10.0.0.13 cluster addr = 172.18.0.13 [mds.a] host = ceph01 [mds.a] host = ceph02 [mds.a] host = ceph03 次の操作でそれぞれのホストに対して ceph の公開しているレポジトリを参照させ ceph をインストールしていきます。\ncephclient% ceph-deploy install ceph01 ceph02 ceph03 MON daemon のデプロイを行う。\ncephclient% ceph-deploy mon create ceph01 ceph02 ceph03 鍵のデプロイを行う。Ceph サーバ間・クライアント間での共有鍵です。1 Cluster に対して1つの鍵を保有することになります。\ncephclient% ceph-deploy gatherkeys ceph01 ceph02 ceph03 OSD daemon のデプロイを行う。下記の様にパーティションを指定しなければツールが 自動でパーティショニングを行なってくれます。\ncephclient% ceph-deploy osd create create ceph01:/dev/sdb ceph02:/dev/sdb ceph03:/dev/sdb MDS deamon のデプロイを行う。\ncephcleint% ceph-deploy mds create ceph01 ceph02 ceph03 完成です。\n全てのホスト ceph01-03 にて MON, MDS, OSD のプロセスが稼働しているのが確認出来 ると思います。実際にどれかのホストの MDS に対して client から ceph ストレージ をマウントしてみてください。\nまとめ 通常のフラットなネットワーク上にデプロイする方法とほぼ同じ操作で構築出来ます。 異なるところは ceph.conf に対して設定を追加した点です。また、それぞれのホスト への ceph のインストールの時にオプションが渡せます。\u0026ndash;testing と渡せば RC 版の ceph が利用できます。今回の様に何も記さなければ stable 版の利用ということにな ります。\n","permalink":"https://jedipunkz.github.io/post/2013/05/25/ceph-cluster-network/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003eCeph を運用する上で考慮しなければいけないのがトラフィックの負荷です。特に OSD\n同士のレプリケーション・ハートビートには相当トラフィックの負荷が掛かることが想\n像出来ます。\u003c/p\u003e\n\u003cp\u003eこのため MDS, MON の通信に影響を与えないよう、OSD レプリケーション・ハートビー\nトのためのネットワークを別に設けるのがベストプラクティスな構成の様です。このネッ\nトワークのことをクラスターネットワークと Ceph 的に言うそうです。\u003c/p\u003e\n\u003cp\u003eこんな接続になります。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e                +------+\n                |Client|\n                +------+\n                |\n+-------+-------+-------+-------+------ public network\n|       |       |       |       |\n+-----+ +-----+ +-----+ +-----+ +-----+\n| MON | | MDS | | OSD | | OSD | | OSD |\n+-----+ +-----+ +-----+ +-----+ +-----+\n                |       |       |\n----------------+-------+-------+------ cluster network\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e上図の様に MON, MDS は public ネットワークを介し OSD のレプリケーション・ハー\nトビートのみ cluster ネットワークを介します。Client と MDS との通信に影響を与\nえない構成になります。\u003c/p\u003e","title":"Ceph クラスターネットワーク構成"},{"content":"こんにちは。最近 OpenStack の導入に向けて保守性や可用性について調査している @jedipunkz です。\nOpenStack は MySQL のダンプや OS イメージ・スナップショットのバックアップをとっ ておけばコントローラの復旧も出来ますし、Grizzly 版の Quantum では冗長や分散が 取れるので障害時に耐えられます。また Quantum の復旧は手動もで可能です。最後の 悩みだった Cinder の接続先ストレージですが、OpenStack のスタンスとしては高価な ストレージの機能を使ってバックアップ取るか、Ceph, SheepDog のようなオープンソー スを使うか、でした。で、今回は Ceph を OpenStack に連携させようと思いました。\nこの作業により Cinder の接続先ストレージが Ceph になるのと Glance の OS イメー ジ・スナップショットの保管先が Ceph になります。\n下記の参考資料が完成度高く、ほぼ内容はそのままです。若干付け足していますが。\n参考資料 http://ceph.com/docs/master/rbd/rbd-openstack/\n前提の構成 +-------------+-------------+--------------------------------------------- Public/API Network | | | +-----------+ +-----------+ +-----------+ +-----------+ +-----------+ +-----------+ | | | | |vm|vm|.. | | | | | | | | controller| | network | +-----------+ | ceph01 | | ceph01 | | ceph01 | | | | | | compute | | | | | | | | | | | | | | | | | | | +-----------+ +-----------+ +-----------+ +-----------+ +-----------+ +-----------+ | | | | | | | | +-------------+-----)-------+-----)-------+-------------+-------------+-- Management/API Network | | +-------------+-----------------------------------+-- Data Network Ceph は OpenStack の Management Network 上に配置 Ceph は3台構成 (何台でも可) OpenStack も3台構成 (何台でも可) 連携処理するのは controller, compute ノード では早速手順ですが、OpenStack と Ceph の構築手順は割愛します。私の他の記事を参 考にしていただければと思います。\n構築スクリプト ceph-deploy で Ceph 構築 Ceph + OpenStack 連携手順 OpenStack 用に Ceph Pool を作成する ceph01% sudo ceph pool create volumes 128 ceph01% sudo ceph pool create images 128 sudoers の設定 controller, compute ノードにて sudoers の設定\njedipunkz ALL = (root) NOPASSWD:ALL ceph パッケージのインストール controller, compute ノードに ceph をインストールする。\ncontroller% wget -q -O- 'https://ceph.com/git/?p=ceph.git;a=blob_plain;f=keys/release.asc' | sudo apt-key add - controller% echo deb http://ceph.com/debian/ $(lsb_release -sc) main | sudo tee /etc/apt/sources.list.d/ceph.list controller% sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y python-ceph ceph-common /etc/ceph 作成 controller% sudo mkdir /etc/ceph compute % sudo mkdir /etc/ceph ceph コンフィギュレーションのコピー controller, compute ノードに ceph コンフィギュレーションをコピーする。尚、接続 先の OpenStack ノードでの sudoers 設定は予め済ませること。\nceph01% sudo -i ceph01# ssh \u0026lt;controller\u0026gt; sudo tee /etc/ceph/ceph.conf \u0026lt;/etc/ceph/ceph.conf ceph01# ssh \u0026lt;compute\u0026gt; sudo tee /etc/ceph/ceph.conf \u0026lt;/etc/ceph/ceph.conf 認証設定 nova, cinder, glance 用にユーザを作成する。\nceph01% sudo ceph auth get-or-create client.volumes mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool=volumes, allow rx pool=images' ceph01% sudo ceph auth get-or-create client.images mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool=images' キーリングの作成 Ceph キーリングの作成を行う。Glance, Cinder が起動しているホスト controller ノードに キーリングを配置する。\nceph01% sudo ceph auth get-or-create client.images | ssh {your-glance-api-server} sudo tee /etc/ceph/ceph.client.images.keyring ceph01% ssh {your-glance-api-server} sudo chown glance:glance /etc/ceph/ceph.client.images.keyring ceph01% sudo ceph auth get-or-create client.volumes | ssh {your-volume-server} sudo tee /etc/ceph/ceph.client.volumes.keyring ceph01% ssh {your-volume-server} sudo chown cinder:cinder /etc/ceph/ceph.client.volumes.keyring compute ノードにて libvirt に secret key を格納する。ここで登場する uuid は後 に利用するためメモをとっておくこと。\nceph01 % sudo ceph auth get-key client.volumes | ssh 10.200.10.59 tee client.volumes.key compute% cat \u0026gt; secret.xml \u0026lt;\u0026lt;EOF \u0026lt;secret ephemeral='no' private='no'\u0026gt; \u0026lt;usage type='ceph'\u0026gt; \u0026lt;name\u0026gt;client.volumes secret\u0026lt;/name\u0026gt; \u0026lt;/usage\u0026gt; \u0026lt;/secret\u0026gt; EOF comupte% sudo virsh secret-define --file secret.xml \u0026lt;uuid of secret is output here\u0026gt; compute% sudo virsh secret-set-value --secret {uuid of secret} --base64 $(cat client.volumes.key) \u0026amp;\u0026amp; rm client.volumes.key secret.xml OpenStack 連携のための設定 controller:/etc/glance/glance-api.conf に下記を追記。\ndefault_store=rbd rbd_store_user=images rbd_store_pool=images show_image_direct_url=True controller:/etc/cinder/cinder.conf に下記を追記。先ほど登場した uuid を入力す る。\nvolume_driver=cinder.volume.driver.RBDDriver rbd_pool=volumes rbd_user=volumes rbd_secret_uuid={uuid of secret} controller:/etc/init/cinder-volume.conf の冒頭に下記の記述を追記する。\nenv CEPH_ARGS=\u0026quot;--id volumes\u0026quot; OpenStack の各サービスを再起動もしくはホストの再起動を行う。\nsudo service glance-api restart sudo service nova-compute restart sudo service cinder-volume restart 確認 実際にインスタンスを作成して Volume をアタッチしディスクを消費していくと Ceph のディスク使用量が増えていきます。\n% cinder create --display-name test 5 % nova volumeattach \u0026lt;instance_id\u0026gt; \u0026lt;volume_id\u0026gt; auto まとめ Cinder は分散ストレージですので各ファイルのレプリカが全て失われない限りデータ はロストしません。ただし Ceph 自体の完成度は以前に比べ高くはなったものの、運用 に耐えられるかどうかまだ私にも分かりません。先日の OpenStack Day に来日してい たファウンデーションの方が「ベンダロックインするな」と言っていました。僕もオー プンソースでなんとかしたいと思っています。OpenStack を導入するためには今、Ceph は欠かすことが出来ないコンポーネントな気がしています。皆で Ceph も盛り上げて行 きたいです。\nまた、この構成の際のOpenStack 全体の保全について考えると\u0026hellip;\nMySQL のデータさえダンプの取得すれば OK OS イメージ・スナップショットは Ceph 上にあるのでバックアップ不要 Ceph はなんとしても守る。バックアップ取るのは難しい Network ノードは分散・冗長可能, データのバックアップは不要 Compute ノード上のインスタンスデータは Ceph のスナップショットから復旧 といったことが考えられます。つまり MySQL のデータさえダンプしておけば OpenStack 全体が復旧できることになります。実際にやってみましたが可能でした。\n","permalink":"https://jedipunkz.github.io/post/2013/05/19/openstack-ceph/","summary":"\u003cp\u003eこんにちは。最近 OpenStack の導入に向けて保守性や可用性について調査している\n\u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003eOpenStack は MySQL のダンプや OS イメージ・スナップショットのバックアップをとっ\nておけばコントローラの復旧も出来ますし、Grizzly 版の Quantum では冗長や分散が\n取れるので障害時に耐えられます。また Quantum の復旧は手動もで可能です。最後の\n悩みだった Cinder の接続先ストレージですが、OpenStack のスタンスとしては高価な\nストレージの機能を使ってバックアップ取るか、Ceph, SheepDog のようなオープンソー\nスを使うか、でした。で、今回は Ceph を OpenStack に連携させようと思いました。\u003c/p\u003e\n\u003cp\u003eこの作業により Cinder の接続先ストレージが Ceph になるのと Glance の OS イメー\nジ・スナップショットの保管先が Ceph になります。\u003c/p\u003e\n\u003cp\u003e下記の参考資料が完成度高く、ほぼ内容はそのままです。若干付け足していますが。\u003c/p\u003e\n\u003ch2 id=\"参考資料\"\u003e参考資料\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"http://ceph.com/docs/master/rbd/rbd-openstack/\"\u003ehttp://ceph.com/docs/master/rbd/rbd-openstack/\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"前提の構成\"\u003e前提の構成\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003e+-------------+-------------+--------------------------------------------- Public/API Network\n|             |             |             \n+-----------+ +-----------+ +-----------+ +-----------+ +-----------+ +-----------+\n|           | |           | |vm|vm|..   | |           | |           | |           |\n| controller| |  network  | +-----------+ |  ceph01   | |  ceph01   | |  ceph01   |\n|           | |           | |  compute  | |           | |           | |           |\n|           | |           | |           | |           | |           | |           |\n+-----------+ +-----------+ +-----------+ +-----------+ +-----------+ +-----------+\n|             |     |       |     |       |             |             |\n+-------------+-----)-------+-----)-------+-------------+-------------+-- Management/API Network\n                    |             |                       \n                    +-------------+-----------------------------------+-- Data Network\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eCeph は OpenStack の Management Network 上に配置\u003c/li\u003e\n\u003cli\u003eCeph は3台構成 (何台でも可)\u003c/li\u003e\n\u003cli\u003eOpenStack も3台構成 (何台でも可)\u003c/li\u003e\n\u003cli\u003e連携処理するのは controller, compute ノード\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eでは早速手順ですが、OpenStack と Ceph の構築手順は割愛します。私の他の記事を参\n考にしていただければと思います。\u003c/p\u003e","title":"OpenStack + Ceph 連携"},{"content":"こんにちは。@jedipunkz です。 今回は Opscode Chef でユーザ・グループを作成する方法をまとめます。\n\u0026lsquo;users\u0026rsquo; Cookbook を使います。\n% cd ${YOUR_CHEF_REPO} % ${EDITOR} Berksfile cookbook 'users' % berks install --path ./cookbooks data_bag を使ってユーザ・グループの管理をしたいので管理ディレクトリを作成しま す。\n% mkdir -p data_bags/users data_bags/users/jedipunkz.json ファイルを作成します。必要に応じて内容を書き換えてください。\n{ \u0026quot;id\u0026quot;: \u0026quot;jedipunkz\u0026quot;, \u0026quot;ssh_keys\u0026quot;: \u0026quot;ssh-rsa AAAABx92tstses jedipunkz@somewhere\u0026quot;, \u0026quot;groups\u0026quot;: [ \u0026quot;sysadmin\u0026quot;, \u0026quot;sudo\u0026quot; ], \u0026quot;uid\u0026quot;: 2001, \u0026quot;shell\u0026quot;: \u0026quot;\\/usr\\/bin\\/zsh\u0026quot;, \u0026quot;comment\u0026quot;: \u0026quot;jedipunkz sysadmin\u0026quot;, \u0026quot;password\u0026quot;: \u0026quot;$1$s%H8BMHlB$7s3h30y9IB1SklftZXYhvssJ\u0026quot; } json ファイルの説明です。\nid : ユーザ名 ssh_keys : SSH 公開鍵 groups : 所属させるグループ uid : unix id sheell : ログインシェル comment : コメント passwd : ハッシュ化したパスワード 特にハッシュ化したパスワードは下記のコマンドで生成出来ます。\n% openssl passwd -1 'yourPassword' data_bag を作成し json ファイルを読み込みます。\n% knife data bag create users % knife data bag from file users data_bags/users/jedipunkz.json 現在 (2013/05/18 現在) 、\u0026lsquo;users\u0026rsquo; Cookbook に不具合があるらしく groups に記した グループにユーザが所属してくれませんでした。なので下記の対処をします。 sysadmins.rb を今回は利用します。このファイルに下記の行を追記します。僕は sudo グループに所属させたかったので (先ほど groups: に記した) こうしましたが、他の グループが良ければ変更してください。また、Ubuntu Server を扱うことがメインの僕 なので group_id は 27 にしています。適宜変更してください。\n% ${EDITOR} cookbooks/users/recipes/sysadmins.rb # 下記の行を追記 users_manage \u0026quot;sudo\u0026quot; do group_id 27 end cookbook を Chef サーバにアップロードします。\n% knife cookbook upload users 適用したいノードの run_list に Recipe \u0026lsquo;users::sysadmins\u0026rsquo; を追加します。\n% knife node run_list add ${YOUR_NODE_NAME} users::sysadmins chef-client の次回実行時にユーザ \u0026lsquo;jedipunkz\u0026rsquo; が作成されているはずです。SSH で ログインして確認してみてください。待ちきれなかったら knife ssh して chef-client を実行してください。\nこの \u0026lsquo;users\u0026rsquo; cookbook は他の Cookbook からも呼び出して利用することが出来るので 応用が利きますね。\n","permalink":"https://jedipunkz.github.io/post/2013/05/18/chef-cookbook-adding-users/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e です。\n今回は Opscode Chef でユーザ・グループを作成する方法をまとめます。\u003c/p\u003e\n\u003cp\u003e\u0026lsquo;users\u0026rsquo; Cookbook を使います。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e% cd ${YOUR_CHEF_REPO}\n% ${EDITOR} Berksfile\ncookbook 'users'\n% berks install --path ./cookbooks\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003edata_bag を使ってユーザ・グループの管理をしたいので管理ディレクトリを作成しま\nす。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e% mkdir -p data_bags/users\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003edata_bags/users/jedipunkz.json ファイルを作成します。必要に応じて内容を書き換えてください。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e{\n  \u0026quot;id\u0026quot;: \u0026quot;jedipunkz\u0026quot;,\n  \u0026quot;ssh_keys\u0026quot;: \u0026quot;ssh-rsa AAAABx92tstses jedipunkz@somewhere\u0026quot;,\n  \u0026quot;groups\u0026quot;: [ \u0026quot;sysadmin\u0026quot;, \u0026quot;sudo\u0026quot; ],\n  \u0026quot;uid\u0026quot;: 2001,\n  \u0026quot;shell\u0026quot;: \u0026quot;\\/usr\\/bin\\/zsh\u0026quot;,\n  \u0026quot;comment\u0026quot;: \u0026quot;jedipunkz sysadmin\u0026quot;,\n  \u0026quot;password\u0026quot;: \u0026quot;$1$s%H8BMHlB$7s3h30y9IB1SklftZXYhvssJ\u0026quot;\n\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ejson ファイルの説明です。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eid : ユーザ名\u003c/li\u003e\n\u003cli\u003essh_keys : SSH 公開鍵\u003c/li\u003e\n\u003cli\u003egroups : 所属させるグループ\u003c/li\u003e\n\u003cli\u003euid : unix id\u003c/li\u003e\n\u003cli\u003esheell : ログインシェル\u003c/li\u003e\n\u003cli\u003ecomment : コメント\u003c/li\u003e\n\u003cli\u003epasswd : ハッシュ化したパスワード\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e特にハッシュ化したパスワードは下記のコマンドで生成出来ます。\u003c/p\u003e","title":"Chef Cookbook でユーザ・グループ追加"},{"content":"今回は ceph-deploy というツールを使って Ceph ストレージを簡単に構築することが 出来るので紹介します。Ceph は分散ストレージでオブジェクトストレージとしてもブ ロックストレージとしても動作します。今回の構築ではブロックストレージとしてのみ の動作です。\nCeph が公開しているのが ceph-deploy なわけですが、マニュアル操作に代わる構築方 法として公開しているようです。その他にも Chef Cookbook も公開されているようで す。\nそれでは早速。\n今回の構成 +--------+ +--------+ +--------+ | ceph01 | | ceph02 | | ceph03 | | osd | | osd | | osd | | mon | | mon | | mon | | mds | | mds | | mds | +--------+ +--------+ +--------+ | 10.0.0.1 | 10.0.0.2 | 10.0.0.3 | | | +----------+----------+ | | 10.0.0.10 +-------------+ | workstation | +-------------+ 特徴は\nすべてのホストで osd, mon, mds を動作 ceph データ格納用ディスクデバイスを /dev/sdb として利用 workstation は ceph-deploy を実行するホスト です。osd は object store daemon で実際にファイルを格納していくデーモン。mon はモニタリング用デーモン, mds は metadata server で POSIX 互換のファイルシステ ムをクライアントに提供するためのデーモンです。\nceph-deploy を使うまでの準備 ceph-deploy を使うまでのターゲットのホスト ceph01-03 と workstation と共に準備 が必要です。\nceph01-03 の準備 \u0026lsquo;ceph\u0026rsquo; ユーザの作成を行う。\n% ssh user@ceph-server % sudo useradd -d /home/ceph -m ceph % sudo passwd ceph sudoers の設定を行う。\n% echo \u0026quot;ceph ALL = (root) NOPASSWD:ALL\u0026quot; | sudo tee /etc/sudoers.d/ceph % sudo chmod 0440 /etc/sudoers.d/ceph workstation の準備 ノンパスフレーズの SSH 公開鍵・秘密鍵を生成する。\nworkstation% ssh-keygen Generating public/private key pair. Enter file in which to save the key (/ceph-client/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /ceph-client/.ssh/id_rsa. Your public key has been saved in /ceph-client/.ssh/id_rsa.pub. 公開鍵をターゲットホスト (ceph01-03) に配置\nworkstation% ssh-copy-id ceph@ceph01 workstation% ssh-copy-id ceph@ceph02 workstation% ssh-copy-id ceph@ceph03 ceph-deploy の取得を行う。\nworkstation% git clone https://github.com/ceph/ceph-deploy.git ~/ceph-deploy \u0026lsquo;python-virtualenv\u0026rsquo; パッケージをインストールする。\nworkstation% sudo apt-get update ; sudo apt-get -y install python-virtualenv ceph-deploy をブートストラップする\nworkstation% cd ~/ceph-deploy workstation% ./bootstrap PATH を通す。自分の shell に合わせて登録してください。\nworkstation% ${EDITOR} ~/.zshrc export PATH=$HOME/ceph-deploy:$PATH ホスト名の解決を行う。\nworkstation% sudo ${EDITOR} /etc/hosts 10.0.0.1 ceph01 10.0.0.2 ceph02 10.0.0.3 ceph03 これで準備は終わり。\n3台構成構築 3台 (ceph01-03) を新規に構築する方法を書きます。すべて workstaiton 上からの操 作です。\nceph サーバ・クライアント間通信のための鍵の生成とコンフィギュレーションの生成 を下記の操作で行う。\nworkstation% ceph-deploy new ceph01 ceph02 ceph03 下記の操作で ceph パッケージのインストールを各 Ceph サーバにて行う。\u0026ndash;testing 等と引数を渡せば RC 版の利用が行える。何も渡さなければ stable 版。\nworkstation% ceph-deploy install ceph01 ceph02 ceph03 MON daemon のデプロイを行う。\nworkstation% ceph-deploy mon create ceph01 ceph02 ceph03 鍵のデプロイを行う。Ceph サーバ間・クライアント間での共有鍵である。1 Cluster に対して1つの鍵を保有する。\nworkstation% ceph-deploy gatherkeys create ceph01 ceph02 ceph03 OSD daemon のデプロイを行う。下記の様にパーティションを指定しなければツールが 自動でパーティショニングを行なってくれる。\nworkstation% ceph-deploy osd create create ceph01:/dev/sdb ceph02:/dev/sdb ceph03:/dev/sdb MDS deamon のデプロイを行う。\ncephcleint% ceph-deploy mds create ceph01 ceph02 ceph03 これで終わりです。これらの操作が終わるとすべてのホスト ceph01-03 で mon, osd, mds の各デーモンが起動していることが分かると思います。超カンタン！\nマウントしてみよう！ さぁ～、クライアントからマウントしてみましょう。ここでは workstaion ホストを利 用します。Linux 系のマシンで同じネットワークセグメントに属していれば大抵マウン ト出来ると思います。mds が稼働しているホストに対してであればどこにでもマウント 出来ます。\nBlock Device としてマウントする方法 ストレージ上に block device を生成しそれをマウントする。\nworkstation% rbd create foo --size 4096 workstation% sudo modprobe rbd workstation% sudo rbd map foo --pool rbd --name client.admin workstation% sudo mkfs.ext4 -m0 /dev/rbd/rbd/foo workstation% sudo mkdir /mnt/myrbd workstation% sudo mount /dev/rbd/rbd/foo /mnt/myrbd Kernel Driver を用いてマウントする方法 kernel Driver を用いてストレージをマウントする。\nworkstation% sudo mkdir /mnt/mycephfs workstation% sudo mount -t ceph 10.0.0.1:6789:/ /mnt/mycephfs -o \\ name=admin,secret=`sudo ceph-authtool -p /etc/ceph/ceph.keyring` Fuse Driver (ユーザランド) を用いてマウントする方法 ユーザランドソフトウェア FUSE を用いてマウントする。\nworkstation% sudo mkdir /home/\u0026lt;username\u0026gt;/cephfs workstation% sudo ceph-fuse -m 10.0.0.1:6789 /home/\u0026lt;username\u0026gt;/cephfs まとめ もし導入するのであればマニュアルでの構築も一度体験した方が良いかもしれません。 ツールを使うと一体どんな作業がされているのか理解出来ないので。ただ今ではマニュ アル操作で構築している途中に \u0026lsquo;ceph-deploy を使ってください\u0026rsquo; と warning が出る ので、開発元としてもこちらの構築方法を薦めたいのでしょう。あと Ceph はドキュメ ントが非常に充実しています。ドキュメントの全てに大事なことが書いてあるので一度 読むことをオススメします。また Ceph が Chef Cookbook も公開しているようで、そ ちらの方法もドキュメントにチラっと書いてありました。私はまだ試していませんが時 間があればやってみたいです。あとあと！ceph-deploy はまだ未完成な域を脱していま せん。上記の通り新規構築系の操作はひと通り出来るのですが、ホストの削除系の実装 がまだされていませんでした。ホスト追加系の操作に関しても削除系程ではないのです が完成度が上がっていません。手作業で少しカバーしてあげる必要があります。\nOpenStack の Cinder の先のストレージについて最近考えていました。LVM 管理のロー カルディスクでもいいのですが運用のことを考えるとバックアップを取らなくちゃい けないのだけど logcal volume が存在しないのでスナップショットバックアップが出 来なそう。Cinder は比較的高価なストレージも扱えるのでそちらの機能でバックアッ プ取るのもいいけど、ここはオープンソースでなんとかしたい！と思って Ceph を検討 してみました。\nCeph は分散ストレージでオブジェクトストレージとしてもブロックストレージとして も動作が可能。OpenStack と組み合わせると Cinder の先のストレージとしても Glance のイメージ置き場としても利用可能らしい。Cinder の接続先ストレージとして の動作方法はまた別の機会にブログに書きます。\n","permalink":"https://jedipunkz.github.io/post/2013/05/11/ceph-deploy/","summary":"\u003cp\u003e今回は ceph-deploy というツールを使って Ceph ストレージを簡単に構築することが\n出来るので紹介します。Ceph は分散ストレージでオブジェクトストレージとしてもブ\nロックストレージとしても動作します。今回の構築ではブロックストレージとしてのみ\nの動作です。\u003c/p\u003e\n\u003cp\u003eCeph が公開しているのが ceph-deploy なわけですが、マニュアル操作に代わる構築方\n法として公開しているようです。その他にも Chef Cookbook も公開されているようで\nす。\u003c/p\u003e\n\u003cp\u003eそれでは早速。\u003c/p\u003e\n\u003ch2 id=\"今回の構成\"\u003e今回の構成\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003e+--------+ +--------+ +--------+\n| ceph01 | | ceph02 | | ceph03 |\n|  osd   | |  osd   | |  osd   |\n|  mon   | |  mon   | |  mon   |\n|  mds   | |  mds   | |  mds   |\n+--------+ +--------+ +--------+\n| 10.0.0.1 | 10.0.0.2 | 10.0.0.3\n|          |          |          \n+----------+----------+\n|\n| 10.0.0.10\n+-------------+\n| workstation |\n+-------------+\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e特徴は\u003c/p\u003e","title":"Ceph-Deploy で Ceph 分散ストレージ構築"},{"content":"こんにちは。Grizzly がリリースされてから暫く経ちました。今回は Folsom リリース まであった Quantum ノードのボトルネックと単一障害点を解決する新しい機能につい て評価した結果をお伝えします。\nFolsom までは\nQuantum L3-agent が落ちると、その OpenStack 一式の上にある仮想マシン全ての通 信が途絶える Quantum L3-agent に仮想マシンの全てのトラフィックが集まりボトルネックとなる。 という問題がありました。Folsom リリース時代にもし僕が職場で OpenStack を導入す るのであればこれらを理由に nova-network を選択していたかもしれません。 nova-network は compute ノードが落ちればその上の仮想マシンも同時に落ちるが、他 の compute ノード上の仮想マシンの通信には影響を与えないからです。もちろん仮想 ルータ・仮想ネットワークの生成等を API でユーザに提供したいなどの要望があれば Quantum を選択するしかありませんが。これに対して Grizzly リリースの Quantum は 改善に向けて大きな機能を提供してくれています。L3-agent, DHCP-agent の分散・冗 長機能です。\n下記の構成が想定出来ます。ここでは Network ノードを2台用意しました。それ以上の 台数に増やすことも出来ます。\n+-------------+-------------+-------------------------- Public/API Network | | | +-----------+ +-----------+ +-----------+ +-----------+ | | | | | | |vm|vm|.. | | controller| | network | | network | +-----------+ | | | | | | | compute | +-----------+ +-----------+ +-----------+ +-----------+ | | | | | | | +-------------+-----)-------+-----)-------+-----)------ Management/API Network | | | +-------------+-------------+------ Data Network L3-agent の分散は仮想ルータ単位で行います。それに対し DHCP-agent は仮想 ネットワーク単位で行います。\nagent 一覧の取得 上記の構成を構築すると下記のように agent 一覧が取得出来ます。\n% quantum agent-list # 'admin' ユーザでアクセス +--------------------------------------+--------------------+-----------------------+-------+----------------+ | id | agent_type | host | alive | admin_state_up | +--------------------------------------+--------------------+-----------------------+-------+----------------+ | 44795822-2d9f-434e-ba98-748f7411442f | DHCP agent | grizzly03.example.com | :-) | True | | a5150a40-0405-4399-ac1a-be012f55d9f5 | DHCP agent | grizzly02.example.com | :-) | True | | b7bf4e59-06ac-475c-84ab-413d8d29f293 | Open vSwitch agent | grizzly04.example.com | :-) | True | | cc5a6b94-6ddd-4109-8f2f-1b28c6aaf5e6 | L3 agent | grizzly03.example.com | :-) | True | | d39803cf-19d3-47d7-8205-cf9a143dd0ea | Open vSwitch agent | grizzly02.example.com | :-) | True | | d8e59803-9aad-4c62-a47a-519bc788e0fb | Open vSwitch agent | grizzly03.example.com | :-) | True | | f6f747cf-ffb0-446c-a455-2947fd3e87e8 | L3 agent | grizzly02.example.com | :-) | True | +--------------------------------------+--------------------+-----------------------+-------+----------------+ ホスト名は下記。\ncontroller : grizzly01.exmaple.com network01 : grizzly02.exmaple.com network02 : grizzly03.exmaple.com compute : grizzly04.exmaple.com L3-agent の分散方法 (ノード移動) 仮想ルータ (ここでは \u0026lsquo;router-test01\u0026rsquo; とする) がどの L3-agent に属しているか確 認を取る。\n% quantum l3-agent-list-hosting-router router-demo +--------------------------------------+-----------------------+----------------+-------+ | id | host | admin_state_up | alive | +--------------------------------------+-----------------------+----------------+-------+ | f6f747cf-ffb0-446c-a455-2947fd3e87e8 | grizzly02.example.com | True | :-) | +--------------------------------------+-----------------------+----------------+-------+ 1台目の Network ノード (grizzly02.example.com) 上の L3-agent に属していること が確認取れた。次にこの親子関係を削除する。\n% quantum l3-agent-router-remove f6f747cf-ffb0-446c-a455-2947fd3e87e8 router-test01 Removed Router router-demo to L3 agent 最後に仮想ルータ \u0026lsquo;router-test01\u0026rsquo; を2台目の Network ノード上の L3-agent の管理 下に設定する。\n% quantum l3-agent-router-add cc5a6b94-6ddd-4109-8f2f-1b28c6aaf5e6 router-demo Added router router-demo to L3 agent % quantum l3-agent-list-hosting-router router-demo +--------------------------------------+-----------------------+----------------+-------+ | id | host | admin_state_up | alive | +--------------------------------------+-----------------------+----------------+-------+ | cc5a6b94-6ddd-4109-8f2f-1b28c6aaf5e6 | grizzly0404.cpi.ad.jp | True | xxx | +--------------------------------------+-----------------------+----------------+-------+ DHCP-Agent の分散方法 (ノード移動) 仮想マシンが所属しているネットワーク (ここでは \u0026lsquo;int_net\u0026rsquo;) がどの DHCP-agent に所属しているか確認する。\n% quantum dhcp-agent-list-hosting-net int_net +--------------------------------------+-----------------------+----------------+-------+ | id | host | admin_state_up | alive | +--------------------------------------+-----------------------+----------------+-------+ | a5150a40-0405-4399-ac1a-be012f55d9f5 | grizzly02.example.com | True | :-) | +--------------------------------------+-----------------------+----------------+-------+ 1台目のノードに所属しているのが確認できる。次に \u0026lsquo;int_net\u0026rsquo; が所属する DHCP-agent を削除行う。\n% quantum dhcp-agent-network-remove a5150a40-0405-4399-ac1a-be012f55d9f5 int_net Removed network int_net to DHCP agent 2台目のノードの DHCP-agent を仮想ネットワーク \u0026lsquo;int_net\u0026rsquo; に紐付ける。\n% quantum dhcp-agent-network-add 44795822-2d9f-434e-ba98-748f7411442f int_net Added network int_net to DHCP agent % quantum dhcp-agent-list-hosting-net int_net +--------------------------------------+-----------------------+----------------+-------+ | id | host | admin_state_up | alive | +--------------------------------------+-----------------------+----------------+-------+ | 44795822-2d9f-434e-ba98-748f7411442f | grizzly0404.cpi.ad.jp | True | :-) | +--------------------------------------+-----------------------+----------------+-------+ まとめ この様に仮想ルータ, 仮想ネットワーク単位で Network ノードの agent の分散が行え る。上記のように仮想ルータ・ネットワークが1つずつでは分散という意味では無いが 運用の過程で仮想ルータ・ネットワークは増えることが想定出来るのでその際にはトラ フィック・DHCP 機能を分散することが可能になる、と言える。また片系の Network ノー ドに寄せておいてからの障害テスト -\u0026gt; もう片系への移動も行なってみたが作業ととも に仮想マシンの通信が復旧した。このテストを行う前まで \u0026lsquo;agent の移動だけ行えるの であって仮想ルータ自体が移動するわけではないので冗長という意味はない\u0026rsquo; と考えて いたのだが、実際には上記の操作で namespace が移動していることが判り (Quantum の仮想ルータの実体は Linux Namespace) 障害テストの結果、うまくいった。 OpenStack を導入するという意味で、この機能は非常に大きな前進だと僕は思っていま す。\n","permalink":"https://jedipunkz.github.io/post/2013/04/26/quantum-network-distributing/","summary":"\u003cp\u003eこんにちは。Grizzly がリリースされてから暫く経ちました。今回は Folsom リリース\nまであった Quantum ノードのボトルネックと単一障害点を解決する新しい機能につい\nて評価した結果をお伝えします。\u003c/p\u003e\n\u003cp\u003eFolsom までは\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eQuantum L3-agent が落ちると、その OpenStack 一式の上にある仮想マシン全ての通\n信が途絶える\u003c/li\u003e\n\u003cli\u003eQuantum L3-agent に仮想マシンの全てのトラフィックが集まりボトルネックとなる。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eという問題がありました。Folsom リリース時代にもし僕が職場で OpenStack を導入す\nるのであればこれらを理由に nova-network を選択していたかもしれません。\nnova-network は compute ノードが落ちればその上の仮想マシンも同時に落ちるが、他\nの compute ノード上の仮想マシンの通信には影響を与えないからです。もちろん仮想\nルータ・仮想ネットワークの生成等を API でユーザに提供したいなどの要望があれば\nQuantum を選択するしかありませんが。これに対して Grizzly リリースの Quantum は\n改善に向けて大きな機能を提供してくれています。L3-agent, DHCP-agent の分散・冗\n長機能です。\u003c/p\u003e\n\u003cp\u003e下記の構成が想定出来ます。ここでは Network ノードを2台用意しました。それ以上の\n台数に増やすことも出来ます。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e+-------------+-------------+-------------------------- Public/API Network\n|             |             |\n+-----------+ +-----------+ +-----------+ +-----------+\n|           | |           | |           | |vm|vm|..   |\n| controller| |  network  | |  network  | +-----------+\n|           | |           | |           | |  compute  |\n+-----------+ +-----------+ +-----------+ +-----------+\n|             |     |       |     |       |     |\n+-------------+-----)-------+-----)-------+-----)------ Management/API Network\n                    |             |             |\n                    +-------------+-------------+------ Data Network\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eL3-agent の分散は仮想ルータ単位で行います。それに対し DHCP-agent は仮想\nネットワーク単位で行います。\u003c/p\u003e","title":"Quantum Network ノードの分散・冗長"},{"content":"以前にも話題にしたことがある Chef For OpenStack ですが今週新しい情報が入って来 ました。#ChefConf 2013 というイベントがあったのですがここで Opscode の Matt Ray さんらが集まり OpenStack を Chef で構築する \u0026lsquo;Chef for OpenStack\u0026rsquo; について 語られた模様です。その時の資料が SlideShare に上がっていたので見てみました。\nChef for OpenStack: Grizzly Roadmap from Matt Ray 気にあった点を幾つか挙げていきます。\nhttps://github.com/osops で管理される 各コンポーネントの cookbook の名前には \u0026lsquo;-cookbook\u0026rsquo; を最後に付ける quantum, cinder, ceilometer, heat 等、比較的新しいコンポーネントも加わる gerrit でコードレビューされ CI も提供される Chef11 が用いられる Ruby 1.9.x に対応した chef-client が用いられる Foodcritic で可能な限りテストされる chef-solo はサポートされない 5月に \u0026lsquo;2013.1.0\u0026rsquo; がリリースされる (openstack 2013.1 対応と思われる) chef-repo の形で提供される Ubuntu 12.04 が前提 HyperVisor は KVM, LXC がサポートされる 以上です。恐らく chef-repo で提供されるということは spiceweasel を使った構成構 築が出来るような形になるでしょう。楽しみです。またコントリビュートする方法も掲 載されているので興味が有る方は協力してみるのも楽しいかもしれません。\n","permalink":"https://jedipunkz.github.io/post/2013/04/21/chef-for-openstack-grizzly-roadmap/","summary":"\u003cp\u003e以前にも話題にしたことがある Chef For OpenStack ですが今週新しい情報が入って来\nました。#ChefConf 2013 というイベントがあったのですがここで Opscode の Matt\nRay さんらが集まり OpenStack を Chef で構築する \u0026lsquo;Chef for OpenStack\u0026rsquo; について\n語られた模様です。その時の資料が SlideShare に上がっていたので見てみました。\u003c/p\u003e\n\u003ciframe src=\"http://www.slideshare.net/slideshow/embed_code/19197748\"\nwidth=\"427\" height=\"356\" frameborder=\"0\" marginwidth=\"0\" marginheight=\"0\"\nscrolling=\"no\" style=\"border:1px solid #CCC;border-width:1px 1px\n0;margin-bottom:5px\" allowfullscreen webkitallowfullscreen mozallowfullscreen\u003e\n\u003c/iframe\u003e \u003cdiv style=\"margin-bottom:5px\"\u003e \u003cstrong\u003e \u003ca\nhref=\"http://www.slideshare.net/mattray/chef-for-openstack-grizzly-roadmap\"\ntitle=\"Chef for OpenStack: Grizzly Roadmap\" target=\"_blank\"\u003eChef for\nOpenStack: Grizzly Roadmap\u003c/a\u003e \u003c/strong\u003e from \u003cstrong\u003e\u003ca\nhref=\"http://www.slideshare.net/mattray\" target=\"_blank\"\u003eMatt Ray\u003c/a\u003e\u003c/strong\u003e\n\u003c/div\u003e\n\u003cp\u003e気にあった点を幾つか挙げていきます。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/osops\"\u003ehttps://github.com/osops\u003c/a\u003e で管理される\u003c/li\u003e\n\u003cli\u003e各コンポーネントの cookbook の名前には \u0026lsquo;-cookbook\u0026rsquo; を最後に付ける\u003c/li\u003e\n\u003cli\u003equantum, cinder, ceilometer, heat 等、比較的新しいコンポーネントも加わる\u003c/li\u003e\n\u003cli\u003egerrit でコードレビューされ CI も提供される\u003c/li\u003e\n\u003cli\u003eChef11 が用いられる\u003c/li\u003e\n\u003cli\u003eRuby 1.9.x に対応した chef-client が用いられる\u003c/li\u003e\n\u003cli\u003eFoodcritic で可能な限りテストされる\u003c/li\u003e\n\u003cli\u003echef-solo はサポートされない\u003c/li\u003e\n\u003cli\u003e5月に \u0026lsquo;2013.1.0\u0026rsquo; がリリースされる (openstack 2013.1 対応と思われる)\u003c/li\u003e\n\u003cli\u003echef-repo の形で提供される\u003c/li\u003e\n\u003cli\u003eUbuntu 12.04 が前提\u003c/li\u003e\n\u003cli\u003eHyperVisor は KVM, LXC がサポートされる\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e以上です。恐らく chef-repo で提供されるということは spiceweasel を使った構成構\n築が出来るような形になるでしょう。楽しみです。またコントリビュートする方法も掲\n載されているので興味が有る方は協力してみるのも楽しいかもしれません。\u003c/p\u003e","title":"Chef for OpenStack"},{"content":"こんにちは jedipunkz です。\nVirtio に対応していない OS を OpenStack で稼働させることが今まで出来なかったの ですが Grizzly から非 Virtio な OS イメージが扱えるようになった。今まで NetBSD やら古い FreeBSD やら virtio ドライバを OS イメージに入れることに苦労していたの だけど、これで問題無くなった。\n最初、この機能のこと調べるのに「どうせ libvirt が生成する xml を書き換えるのだ から nova 周りの設定なんだろうー」と思っていたら全く方法が見つからず\u0026hellip;。結局 OS イメージを格納している Glance の設定にありました。\nここでは FreeBSD7.4 Release を例に挙げて説明していきます。\n前提とする環境 OpenStack Grizzly が稼働していること ホスト OS に Ubuntu 12.04.2 LTS が稼働していること ゲスト OS に FreeBSD 7.4 Release を用いる とします。OS のバージョンはホスト・ゲスト共に、上記以外でも構いません。Grizzly さえ動いていれば OK です。\nOS イメージ作成 KVM で OS イメージを作成します。もちろん virtio なインターフェースは指定せず\nIDE ディスクインターフェース e1000 (intel) ネットワークインターフェース を指定してあげてください。\n% kvm-img create -f qcow2 \u0026lt;IMAGE_NAME\u0026gt; 5G % sudo kvm -m 1024 --cdrom FreeBSD-7.4-RELEASE-amd64-disc1.iso --drive \\ file=./\u0026lt;IMAGE_NAME\u0026gt; -boot d -net nic,model=e1000 -net user -nographic \\ -vnc :9 VNC クライアントソフトを用いてホスト :9 番に接続し OS をインストールする。\nGlance への登録 OpenStack API に接続する環境変数等を合わせ下記のコマンドを実行します。\n% glance image-create --name=\u0026quot;FreeBSD7.4\u0026quot; --is-public \\ true --container-format bare --disk-format qcow2 \u0026lt; \u0026lt;IMAGE_NAME\u0026gt; % glance image-update --property hw_vif_model=e1000 \u0026quot;FreeBSD7.4\u0026quot; % glance image-update --property hw_disk_bus=ide \u0026quot;FreeBSD7.4\u0026quot; \u0026ndash;property オプションでディスク・ネットワークインターフェースの指定を変更して います。\nVM の稼働 あとは普段通り nova boot コマンドで VM を稼働させるだけです。\n% nova boot --nic net-id=\u0026lt;network_id\u0026gt; --image \u0026lt;image_id\u0026gt; --flavor \u0026lt;flavor_number\u0026gt; \u0026lt;vm_name\u0026gt; ","permalink":"https://jedipunkz.github.io/post/2013/04/21/openstack-non-virtio/","summary":"\u003cp\u003eこんにちは jedipunkz です。\u003c/p\u003e\n\u003cp\u003eVirtio に対応していない OS を OpenStack で稼働させることが今まで出来なかったの\nですが Grizzly から非 Virtio な OS イメージが扱えるようになった。今まで NetBSD\nやら古い FreeBSD やら virtio ドライバを OS イメージに入れることに苦労していたの\nだけど、これで問題無くなった。\u003c/p\u003e\n\u003cp\u003e最初、この機能のこと調べるのに「どうせ libvirt が生成する xml を書き換えるのだ\nから nova 周りの設定なんだろうー」と思っていたら全く方法が見つからず\u0026hellip;。結局\nOS イメージを格納している Glance の設定にありました。\u003c/p\u003e\n\u003cp\u003eここでは FreeBSD7.4 Release を例に挙げて説明していきます。\u003c/p\u003e\n\u003ch2 id=\"前提とする環境\"\u003e前提とする環境\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eOpenStack Grizzly が稼働していること\u003c/li\u003e\n\u003cli\u003eホスト OS に Ubuntu 12.04.2 LTS が稼働していること\u003c/li\u003e\n\u003cli\u003eゲスト OS に FreeBSD 7.4 Release を用いる\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eとします。OS のバージョンはホスト・ゲスト共に、上記以外でも構いません。Grizzly\nさえ動いていれば OK です。\u003c/p\u003e\n\u003ch2 id=\"os-イメージ作成\"\u003eOS イメージ作成\u003c/h2\u003e\n\u003cp\u003eKVM で OS イメージを作成します。もちろん virtio なインターフェースは指定せず\u003c/p\u003e","title":"OpenStack Grizzy で非 Virtio OS 稼働"},{"content":"OpenStack Grizzly がリリースされて2週間ほど経過しました。皆さん動かしてみまし たか？今回、毎度の構築 Bash スクリプトを開発したので公開します。\n下記のサイトで公開しています。\nhttps://github.com/jedipunkz/openstack_grizzly_install\nこのスクリプト、複数台構成とオールインワン構成の両方が構成出来るようなっていま すが、今回は簡単なオールインワン構成の組み方をを簡単に説明したいと思います。\n前提の環境 Ubuntu 12.04 LTS が稼働している Cinder のためのディスクを OS 領域と別に用意 (/dev/sdb1 など) オールインワン構成の場合は 2 NICs 準備 Ubuntu 13.04 の daily build も完成度上がっている時期ですが OVS 側の対応が OpenStack 構成に問題を生じさせるため 12.04 LTS + Ubuntu Cloud Archive の組み合 わせで構築するのが主流になっているようです。また、Cinder 用のディスクは OS 領 域を保持しているディスクとは別 (もしくはパーティションを切ってディスクデバイス を別けても可) が必要です。オールインワン構成の場合は NIC を2つ用意する必要があ ります。通常 OpenStack を複数台構成する場合は\nコントローラノード x 1 台 ネットワークノード x 1 台 コンピュートノード x n 台 で組み、VM はコンピュートノードからネットワークノードを介してインターネットに 接続します。よってそのため更に NIC が必要になるのですが、オールインワン構成の 場合は\nマネージメントネットワーク, API ネットワーク(内部通信用) パブリックネットワーク (VM のためのブリッジインターフェース) の計2つを用意してください。\n実行前の準備 OS のインストール OS のインストール方法は割愛しますが\n\u0026lsquo;openssh-server\u0026rsquo; のみをインストール ディスクが1つしかない場合は cinder 用のパーティションを用意 の条件が満たされていれば OK です。\nCinder 用のディスクデバイスパーティショニング Cinder 用に信頼性のあるディスクを用意している場合は fdisk 等を用いてパーティショ ニングしてください。近々 loopback デバイスでも構築できるようスクリプトの改修を する予定です。ディスクが一つしかない場合は先程述べたとおり、OS インストール時 にパーティショニングしたディスクデバイスを使います。\n% sudo fdisk /dev/sdb ネットワークインターフェースの設定 下記のように2つのネットワークインターフェースを設定してください。\n% sudo ${EDITOR} /etc/network/interfaces auto lo iface lo inet loopback # this NIC will be used for VM traffic to the internet auto eth0 iface eth0 inet static up ifconfig $IFACE 0.0.0.0 up up ip link set $IFACE promisc on down ip link set $IFACE promisc off down ifconfig $IFACE down address 10.200.9.10 netmask 255.255.255.0 dns-nameservers \u0026lt;DNS_RESOLVER1\u0026gt; \u0026lt;DNS_RESOLVER\u0026gt; dns-search example.com # this NIC must be on management network auto eth1 iface eth1 inet static address 10.200.10.10 netmask 255.255.255.0 gateway 10.200.10.1 dns-nameservers \u0026lt;DNS_RESOLVER1\u0026gt; \u0026lt;DNS_RESOLVER\u0026gt; eth0 が VM のためのブリッジインターフェースになります。eth1 はマネージメントネッ トワーク用・内部 API 通信用の兼務です。\nスクリプトの取得とパラメータ設定 スクリプトの取得を行います。\n% git clone git://github.com/jedipunkz/openstack_grizlly_install.git % cd openstack_grizzly_install パラメータを設定するため setup.conf 内の各パラメータを設定変更します。数多くの パラメータがありますが、最低限のパラメータということで\u0026hellip;\nHOST_IP='10.200.10.10' HOST_PUB_IP='10.200.9.10' PUBLIC_NIC='eth0' を設定してください。HOST_IP は eth1 の IP アドレス、HOST_PUB_IP は eth0 の IP アドレス、PUBLIC_NIC は eth0 (HOST_PUB_IP のインターフェース名) を指定します。\nスクリプトの実行 いよいよスクリプトを実行します。\n% sudo ./setup.sh allinone しばらくすると構築が完了します。あとは\nhttp://${HOST_IP}/horizon/ にブラウザでアクセスすると WEB I/F である Horizon のログイン画面が表示されます。 パラメータをいじっていなければユーザ : demo, パスワード : demo でアクセス出来 ます。\n各 API にコマンドでアクセスする API にアクセスするためにコマンドを用いることも出来ます。スクリプトを実行した結 果、下記のファイルが生成されているはずです。\n~/openstackrc-demo # 'demo' ユーザで API にアクセス ~/openstackrc # 'admin' ユーザで API にアクセス \u0026lsquo;demo\u0026rsquo; ユーザでアクセスするためには\n% source ~/openstackrc-demo を実行してください。環境変数が設定され API にアクセス出来るようになります。例 として下記のコマンドを実行してみてください。\n% glnace image-list +--------------------------------------+---------------------+-------------+------------------+------------+--------+ | ID | Name | Disk Format | Container Format | Size | Status | +--------------------------------------+---------------------+-------------+------------------+------------+--------+ | 1a7943a5-8f8f-4c02-9763-5a6d519c31bb | Cirros 0.3.0 x86_64 | qcow2 | bare | 9761280 | active | +--------------------------------------+---------------------+-------------+------------------+------------+--------+ OS イメージ一覧が取得出来ます。スクリプトで予め Glance に登録した Cirros とい う小さな OS イメージが確認出来るはずです。\nまとめ 本格的な構成を組むのであれば上記の URL にも知る指定ある複数台構成を組んでみて ください。同じくスクリプトで構築出来ます。また今回から Quantum に実装された LBaaS も組めるようになっています。構築出来た OpenStack で LB を組んでみてくだ さい。LBaaS の説明については OpenStack ユーザ会の中島さんのブログが参考になり ます。\nhttp://aikotobaha.blogspot.jp/2013/04/use-full-function-of-openstack-grizzly.html\nLBaaS で組める負荷分散方式が \u0026lsquo;ROUND_ROBIN\u0026rsquo; 以外にも選択出来るぽいのでもう少し 調べたら、僕のブログでも紹介しようかと思います。また Grizzly になって数多くの 機能が新たに実装されているので引き続き紹介していこうかと思います。\nOpenStack は多くの機能がありますし構成の仕方も様々。予め理解しなければいけない 技術も多岐にわたるのでブログだけではなかなか説明し切れないところです。 OpenStack のコミュニティが書いた \u0026lsquo;OpenStack Operations Guide\u0026rsquo; なるドキュメント が最近リリースされました。\nhttp://docs.openstack.org/ops/\n日本のユーザ会でもこのドキュメントを翻訳しようという活動がされている最中です。 興味があるかたは一度読むことをオススメしますし、もし更に興味が有る方は翻訳活動 に協力するのはいかがでしょうか。ユーザ会の ML で現在話が進んでいます。\n引き続き、OpenStack ネタはアップしていきますー。\n","permalink":"https://jedipunkz.github.io/post/2013/04/20/openstack-grizzly-installation-script/","summary":"\u003cp\u003eOpenStack Grizzly がリリースされて2週間ほど経過しました。皆さん動かしてみまし\nたか？今回、毎度の構築 Bash スクリプトを開発したので公開します。\u003c/p\u003e\n\u003cp\u003e下記のサイトで公開しています。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/jedipunkz/openstack_grizzly_install\"\u003ehttps://github.com/jedipunkz/openstack_grizzly_install\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eこのスクリプト、複数台構成とオールインワン構成の両方が構成出来るようなっていま\nすが、今回は簡単なオールインワン構成の組み方をを簡単に説明したいと思います。\u003c/p\u003e\n\u003ch2 id=\"前提の環境\"\u003e前提の環境\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eUbuntu 12.04 LTS が稼働している\u003c/li\u003e\n\u003cli\u003eCinder のためのディスクを OS 領域と別に用意 (/dev/sdb1 など)\u003c/li\u003e\n\u003cli\u003eオールインワン構成の場合は 2 NICs 準備\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eUbuntu 13.04 の daily build も完成度上がっている時期ですが OVS 側の対応が\nOpenStack 構成に問題を生じさせるため 12.04 LTS + Ubuntu Cloud Archive の組み合\nわせで構築するのが主流になっているようです。また、Cinder 用のディスクは OS 領\n域を保持しているディスクとは別 (もしくはパーティションを切ってディスクデバイス\nを別けても可) が必要です。オールインワン構成の場合は NIC を2つ用意する必要があ\nります。通常 OpenStack を複数台構成する場合は\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eコントローラノード x 1 台\u003c/li\u003e\n\u003cli\u003eネットワークノード x 1 台\u003c/li\u003e\n\u003cli\u003eコンピュートノード x n 台\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eで組み、VM はコンピュートノードからネットワークノードを介してインターネットに\n接続します。よってそのため更に NIC が必要になるのですが、オールインワン構成の\n場合は\u003c/p\u003e","title":"OpenStack Grizzly 構築スクリプト"},{"content":"chef-solo を使うの？Chef サーバを使うの？という議論は結構前からあるし、答えは 「それぞれの環境次第」だと思うのだが、僕は個人的に Chef サーバを使ってます。ホ ステッド Chef を使いたいけどお金ないし。会社で導入する時はホステッド Chef を契 約してもらうことを企んでます。(・∀・) 何故なら cookbooks を開発することがエン ジニアの仕事の本質であって Chef サーバを運用管理することは本質ではないから。そ れこそクラウド使えという話だと思う。\nでも！Chef に慣れるには無料で使いたいし、継続的に Cookbooks をターゲットノード で実行したい。ということで Chef サーバを構築して使っています。\nChef 10 の時代は Chef サーバの構築方法は下記の通り3つありました。\n手作業！ Bootstrap 構築 Opscode レポジトリの Debian, Ubuntu, CentOS パッケージ構築 それが Chef 11 では\nUbuntu, RHEL のパッケージ (パッケージインストールですべて環境が揃う) http://www.opscode.com/chef/install/\nこの方法1つだけ。でも簡単になりました。\n\u0026lsquo;Chef Server\u0026rsquo; タブを選択するとダウンロード出来る。じっくりは deb ファイルの中 身を見たことがないけど、チラ見した時に chef を deb の中で実行しているように見 えた。徹底してるｗ\nChef 10 時代のパッケージと違って行う操作は下記の2つのコマンドだけ。\n% sudo dpkg -i chef-server_11.0.6-1.ubuntu.12.04_amd64.deb # ダウンロードしたもの % sudo chef-server-ctl reconfigure 簡単。でも\u0026hellip; この状態だと https://\u0026lt;サーバの FQDN\u0026gt; でサーバが Listen している。 IP アドレスでアクセスしてもリダイレクトされる。つまり、ローカルネットワーク上 に構築することが出来ない。安易に hosts で解決も出来ない。何故ならターゲットノー ドは通常まっさらな状態なので bootstrap するたびに hosts を書くなんてアホらしい しやってはいけない。\n今日の本題。ローカルネットワーク上に Chef 11 サーバを構築する方法を調べました。\n修正箇所 /var/opt/chef-server/chef-pedant/etc/pedant_config.rb URL を IP アドレスに変更する。\nchef_server \u0026quot;https://chef.example.com\u0026quot; /var/opt/chef-server/erchef/etc/app.config URL を IP アドレスに変更する。\n{s3_url, \u0026quot;https://chef.example.com\u0026quot;}, /var/opt/chef-server/nginx/etc/nginx.conf URL を IP アドレスに変更する。2箇所あるので注意。鍵ファイルにも FQDN が記され ているがそれらは変更しない。\nserver_name chef.example.com; /etc/chef-server/chef-server-running.json URL を IP アドレスに変更する。鍵ファイルにも FQDN が記されているがそれらは変更 しない。\n\u0026quot;vip\u0026quot;: \u0026quot;chef.example.com\u0026quot;, \u0026quot;api_fqdn\u0026quot;: \u0026quot;chef.example.com\u0026quot;, \u0026quot;web_ui_fqdn\u0026quot;: \u0026quot;chef.example.com\u0026quot;, \u0026quot;server_name\u0026quot;: \u0026quot;chef.example.com\u0026quot;, \u0026quot;url\u0026quot;: \u0026quot;https://chef.example.com\u0026quot;, 最後に Chef サーバを再起動する。\n% sudo chef-server-ctl restart まとめ 会社で、この環境を使って色々試しています。今のところ不具合なく動作している。強 引ワザなのでもっと綺麗な方法を知っている方がいましたら教えて下さい。 m ( _ _ ) m\n","permalink":"https://jedipunkz.github.io/post/2013/04/06/chef-11-private-network/","summary":"\u003cp\u003echef-solo を使うの？Chef サーバを使うの？という議論は結構前からあるし、答えは\n「それぞれの環境次第」だと思うのだが、僕は個人的に Chef サーバを使ってます。ホ\nステッド Chef を使いたいけどお金ないし。会社で導入する時はホステッド Chef を契\n約してもらうことを企んでます。(・∀・) 何故なら cookbooks を開発することがエン\nジニアの仕事の本質であって Chef サーバを運用管理することは本質ではないから。そ\nれこそクラウド使えという話だと思う。\u003c/p\u003e\n\u003cp\u003eでも！Chef に慣れるには無料で使いたいし、継続的に Cookbooks をターゲットノード\nで実行したい。ということで Chef サーバを構築して使っています。\u003c/p\u003e\n\u003cp\u003eChef 10 の時代は Chef サーバの構築方法は下記の通り3つありました。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e手作業！\u003c/li\u003e\n\u003cli\u003eBootstrap 構築\u003c/li\u003e\n\u003cli\u003eOpscode レポジトリの Debian, Ubuntu, CentOS パッケージ構築\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eそれが Chef 11 では\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eUbuntu, RHEL のパッケージ (パッケージインストールですべて環境が揃う)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ca href=\"http://www.opscode.com/chef/install/\"\u003ehttp://www.opscode.com/chef/install/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eこの方法1つだけ。でも簡単になりました。\u003c/p\u003e\n\u003cp\u003e\u0026lsquo;Chef Server\u0026rsquo; タブを選択するとダウンロード出来る。じっくりは deb ファイルの中\n身を見たことがないけど、チラ見した時に chef を deb の中で実行しているように見\nえた。徹底してるｗ\u003c/p\u003e\n\u003cp\u003eChef 10 時代のパッケージと違って行う操作は下記の2つのコマンドだけ。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e% sudo dpkg -i chef-server_11.0.6-1.ubuntu.12.04_amd64.deb # ダウンロードしたもの\n% sudo chef-server-ctl reconfigure\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e簡単。でも\u0026hellip; この状態だと https://\u0026lt;サーバの FQDN\u0026gt; でサーバが Listen している。\nIP アドレスでアクセスしてもリダイレクトされる。つまり、ローカルネットワーク上\nに構築することが出来ない。安易に hosts で解決も出来ない。何故ならターゲットノー\nドは通常まっさらな状態なので bootstrap するたびに hosts を書くなんてアホらしい\nしやってはいけない。\u003c/p\u003e","title":"Chef 11 サーバのローカルネットワーク上構築"},{"content":"クラウドマネジメント勉強会に参加してきた。今が旬なのか定員140名が埋まっていま した。クラウドフェデレーションサービス各種の話が聞ける貴重な勉強会の場でした。\n場所 : スクエアエニックスさん 日程 : 2013年4月5日 19:00 - 少し長くなるので、早速。\nクラウド運用管理研究会 クラウド利用推進機構が運営するクラウド運用管理研究会は下記の3つに分別されるそ うです。今回は一項目の \u0026lsquo;クラウドマネジメントツール研究会\u0026rsquo; にあたるそう。別の研 究会も既に勉強会を実施しているそうです。\nクラウドマネジメントツール研究会 デザインパターン研究会 運用管理・監視研究会 AWS OpsWorks アマゾンデータサービスジャパン AWS 片山さん, 船崎さん OpsWorks は最近話題になった AWS 利用者に無料で提供されるクラウドフェデレーショ ンサービス。Web UI で操作し簡単デプロイを実現するサービスです。\nOpsWorks が自動化するモノ サーバ設定 ミドルウェア構築 特徴 Chef フレームワークを利用 (chef-solo を内部的に利用) 任意の cookbooks を利用可能 LB, AP, DB などをレイヤ化, 任意のレイヤも作成可能 OpsWorks の流れ Stack 作成 レイヤ作成 (LB, AP, DB, 任意) レシピの作成 レイヤにインスタンス作成 下記をレイヤ化で区別する Package インストール OS 設定 アプリデプロイ 所感 AWS OpsWorks の登場で他のクラウドフェデレーションサービスがどうなるの？とさえ 思った。AWS はインターネット・ホスティング業界のあらゆるサービスを押さえようと している感がある。もう隙間がない！ｗ OpsWorks に関してまだ問題は残っているそう だ。VPC, micro 現在未対応など。が解決に向けて作業しているそう。\nAeolus Conductor RedHat 中井さん 概要 複数クラウドに対応したイメージ作成・アプリケーション環境構築の自動化ツール\nアプリのデプロイ機能にフォーカス Red Hat CoudForms が商用版 マルチクラウド (ユーザにクラウドが割り当てられる, Hybrid, EC2, RHEV) 自動化について中井さんの案 (手作り) libvirt キック kickstart 実行 post script にて puppet 実行 manifest は github で管理 これらは単一のサーバのみで実施できて、複数台構成等を前提に出来ない等の問題があ る。それらを解決するのが Aeolus Conductor。\nAeolus Conductor の要素 システムテンプレート用意 (XML) : OS 構成内容が記されている マシンイメージ JEOS アプリケーションブループリント (アプリデプロイ設計書) shell script である。puppet, chef を呼び出しても OK. Config サーバを介して VM 間の構成を管理している : インテグレート！ DB, Web Aeolus Conductor の不便な点 特定のクラウド特有の機能には未対応 複数 VM デプロイ時のワークフロー処理が不十分 所感 画面を見させてもらったが AWS EC2, RHEV (RedHat の仮想化ソフト) とマルチクラウ ドに対応していた。ユーザにどのクラウドを割り当てるか？等の権限委譲が出来るもの ユニーク。\nScalr Scalr ユーザ会 梶川さん (IDC フロンティア) 概要, 特徴 オープンソースのマルチクラウド管理ツール 利用出来るクラウド : AWS, Eucalyptus, RackSpace, nimbula, OpenStack, \u0026hellip; 冗長化・オートスケール可能 モニタリングも自動で開始 DNS 管理, オートスケール時、自動的に修正が行われる スクリプト実行 (任意のタイミングで可能、またタイミングを作成可能) 各サービスのコンフィグプリセット管理 (ミドルウェアのパラメータ？) 所感 Scalr ユーザ会のメンバ募集中だそうだ。個人的にオープンソースの Scalr を試そう と思ったことがあるのだが、手順の wiki が解りづらかった。商用サービスを使わせる ためにわざと解りづらくしているのか？と思うほど。ユーザ拡大のために是非ドキュメ ントの整備をお願いしたい。\nRightScale の利用効果と苦労話 So-net エンタテインメント 成田さん 利用効果 1つのスクリプトを複数台に対して実行可能 手作業が自動化へ ベストプラクティスの利用が可能に モニタリングの自動化へ サーバ台数のスケジューリング化 セキュリティグループはマクロで作成 権限分離による開発者・プロデューサに役割移譲 履歴管理の自動記録 chef recipe が right スクリプトとして走らせられる 苦労話 Alert 設定のミスでメール大量受信 自動化スクリプトのエラー対応 計画メンテナンスの後は要注意 (仕様変更) RightScale 上の表示を過信しない, 詳しくはクラウドサービス側を確認 LANG=ja_JP.UTF-8 するとコケる メンテナンスは金曜日日中 (月に一回) 所感 実際に運用している方の話はとても貴重。特に苦労ネタはなかなか知ることが出来ない ので。自動化のためにスクリプトを書くのがインフラ系エンジニアの仕事になると知ら せてくれた。Right スクリプトには Chef のレシピも走らせることが出来る、というも が魅力。またインフラ系エンジニア以外の職種の人にも権限委譲し UI を操作してもら える辺りは、業務の最適化のために大いに利用できると感じた。\nChef の話 Engine Yard @yando さん Engine Yard とは PaaS AWS + Chef + サポート, 監視 chef-solo をキック chef recipe の管理は Engine Yard が行う Chef へのモチベーション 冪等性 シェルスクリプトだと構築直後の状態しか保証されない chef-solo の話 knife-solo でノードに SSH せずに実行 利用するにあたって直面する課題 レシピの実装 -\u0026gt; github 上のレシピを参照・利用 Vagrant の利用でレシピの複数プラットフォーム上でのテスト レシピの配布方法 -\u0026gt; github, berkshelf, knife solo, nfs, chef-server レシピの反映 -\u0026gt; capistrano, chef-client, cron Engine Yard Local クラウドと同じレシピでローカルに開発環境を構築出来るツール クラウドはコストが掛かるし遅いので出来る事ならローカルで、という発想 所感 Chef 流行ってますね。うんうん、(・∀・)ｲｲ!! 個人的には Chef の Cookbooks 開発 はインフラエンジニアにしてもらいたい。Engine Yard のような PaaS 使うならアレだ けどクラウド使うなら運用は引き続き必要だし、運用を意識した Cookbooks 開発は絶 対に必要になってくるからだ。Chef の関連技術がものすごいスピードで進化している のも魅力。より便利で旬な技術をすぐに利用し貢献する、という良いサイクルをうちの 会社でも実現したい。だって楽しいから。chef-solo 使うの？chef サーバ使うの？と いう話はここでも挙がってた。どこかでブログにしようかな。僕は chef サーバ使わな い理由がないと思ってる。\n","permalink":"https://jedipunkz.github.io/post/2013/04/06/cloudmanagement/","summary":"\u003cp\u003eクラウドマネジメント勉強会に参加してきた。今が旬なのか定員140名が埋まっていま\nした。クラウドフェデレーションサービス各種の話が聞ける貴重な勉強会の場でした。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e場所 : スクエアエニックスさん\n日程 : 2013年4月5日 19:00 -\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e少し長くなるので、早速。\u003c/p\u003e\n\u003ch2 id=\"クラウド運用管理研究会\"\u003eクラウド運用管理研究会\u003c/h2\u003e\n\u003cp\u003eクラウド利用推進機構が運営するクラウド運用管理研究会は下記の3つに分別されるそ\nうです。今回は一項目の \u0026lsquo;クラウドマネジメントツール研究会\u0026rsquo; にあたるそう。別の研\n究会も既に勉強会を実施しているそうです。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eクラウドマネジメントツール研究会\u003c/li\u003e\n\u003cli\u003eデザインパターン研究会\u003c/li\u003e\n\u003cli\u003e運用管理・監視研究会\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"aws-opsworks\"\u003eAWS OpsWorks\u003c/h1\u003e\n\u003cpre\u003e\u003ccode\u003eアマゾンデータサービスジャパン AWS 片山さん, 船崎さん\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOpsWorks は最近話題になった AWS 利用者に無料で提供されるクラウドフェデレーショ\nンサービス。Web UI で操作し簡単デプロイを実現するサービスです。\u003c/p\u003e\n\u003ch2 id=\"opsworks-が自動化するモノ\"\u003eOpsWorks が自動化するモノ\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eサーバ設定\u003c/li\u003e\n\u003cli\u003eミドルウェア構築\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"特徴\"\u003e特徴\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eChef フレームワークを利用 (chef-solo を内部的に利用)\u003c/li\u003e\n\u003cli\u003e任意の cookbooks を利用可能\u003c/li\u003e\n\u003cli\u003eLB, AP, DB などをレイヤ化, 任意のレイヤも作成可能\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"opsworks-の流れ\"\u003eOpsWorks の流れ\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eStack 作成\u003c/li\u003e\n\u003cli\u003eレイヤ作成 (LB, AP, DB, 任意)\u003c/li\u003e\n\u003cli\u003eレシピの作成\u003c/li\u003e\n\u003cli\u003eレイヤにインスタンス作成\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"下記をレイヤ化で区別する\"\u003e下記をレイヤ化で区別する\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003ePackage インストール\u003c/li\u003e\n\u003cli\u003eOS 設定\u003c/li\u003e\n\u003cli\u003eアプリデプロイ\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"所感\"\u003e所感\u003c/h2\u003e\n\u003cp\u003eAWS OpsWorks の登場で他のクラウドフェデレーションサービスがどうなるの？とさえ\n思った。AWS はインターネット・ホスティング業界のあらゆるサービスを押さえようと\nしている感がある。もう隙間がない！ｗ OpsWorks に関してまだ問題は残っているそう\nだ。VPC, micro 現在未対応など。が解決に向けて作業しているそう。\u003c/p\u003e","title":"クラウドマネジメント勉強会レポ"},{"content":"もう数日で OpenStack の次期バージョン版 Grizzly がリリースされるタイミングだが 現行バージョン Folsom の OpenStack の上に NetBSD を載せてみた。完全にお遊び だけど\u0026hellip;。\n結局、ほとんど何も特別な対応取ることなく NetBSD が動いた。もちろんハイパーバイ ザは KVM です。だけど少し条件がある。\nqemu の不具合があり Ubuntu 12.04 LTS + Ubuntu Cloud Archives の組み合わせでは NetBSD が動作しなかった。下記のようなカーネルパニックが発生。\npanic: pci_make_tag: bad request この不具合に相当するんじゃないかと思ってる。\nhttps://bugs.launchpad.net/qemu/+bug/897771\nよって下記の組み合わせで動作を確認した。\nUbuntu 12.10 + OpenStack (Native Packages) qemu, kvm : 1.2.0+noroms-0ubuntu2.12.10.3 NetBSD 6.1 RC2 amd64 前提条件 OpenStack Folsom が動作していること。\nNetBSD OS イメージ作成 nova-compute が動作しているホストの qemu-kvm を利用する。OpenStack 上に何でも 良いので OS を動作させこの kvm プロセスのパラメータを参考に kvm コマンドを実行 し NetBSD をインストールさせた。一番確実な方法。\n% cd ~/ % wget http://ftp.netbsd.org/pub/NetBSD/iso/6.1_RC2/NetBSD-6.1_RC2-amd64.iso % kvm-img create -f qcow2 netbsd.img 5G % /usr/bin/kvm -M pc-1.0 -cpu \\ core2duo,+lahf_lm,+rdtscp,+hypervisor,+avx,+osxsave,+save,+aes,+popcnt,+sse4.2,+sse4.1,+cx16,+vmx,+pclmuldq,+ht,+ss,+ds \\ -enable-kvm -m 512 -smp 1,sockets=1,cores=1,threads=1 -device piix3-usb-uhci,id=usb,bus=pci.0,addr=0x1.0x2 \\ -drive file=~/netbsd.img,if=none,id=drive-virtio-disk0,format=qcow2,cache=none \\ -device virtio-blk-pci,scsi=off,bus=pci.0,addr=0x4,drive=drive-virtio-disk0,id=virtio-disk0,bootindex=1 \\ -net nic,model=virtio -vnc :9 -cdrom ~/NetBSD-6.1_RC2-amd64.iso VNC :9 に接続して NetBSD を普通ににインストールする。\nインストールが終わったら CDROM デバイスを外して起動。\n% core2duo,+lahf_lm,+rdtscp,+hypervisor,+avx,+osxsave,+save,+aes,+popcnt,+sse4.2,+sse4.1,+cx16,+vmx,+pclmuldq,+ht,+ss,+ds \\ -enable-kvm -m 512 -smp 1,sockets=1,cores=1,threads=1 -device piix3-usb-uhci,id=usb,bus=pci.0,addr=0x1.0x2 \\ -drive file=~/netbsd.img,if=none,id=drive-virtio-disk0,format=qcow2,cache=none \\ -device virtio-blk-pci,scsi=off,bus=pci.0,addr=0x4,drive=drive-virtio-disk0,id=virtio-disk0,bootindex=1 \\ -net nic,model=virtio -vnc :9 VNC :9 に接続し下記の操作を行う。\nvioif0 という virtio なネットワークインターフェースが起動するので下記のように 追記する。\n# vi /etc/rc.conf # 下記を追記 ifconfig_vioif0=dhcp sshd=YES OpenStack の metadata server から nova の管理するキーペア鍵を取得し authorized_keys に配置する様、/etc/rc.local に追記する。curl とか便利なツール はもちろん！入っていないので ftp コマンドでなんとかする。\n# vi /etc/rc.local # 下記を追記 if [ ! -d /root/.ssh ]; then mkdir -p /root/.ssh fi echo \u0026gt;\u0026gt; /root/.ssh/authorized_keys cd /tmp ftp http://169.254.169.254/latest/meta-data/public-keys/0/openssh-key cat openssh-key | grep \u0026#39;ssh-rsa\u0026#39; \u0026gt;\u0026gt; /root/.ssh/authorized_keys echo \u0026#34;AUTHORIZED_KEYS:\u0026#34; echo \u0026#34;*********************\u0026#34; cat /root/.ssh/authorized_keys echo \u0026#34;*********************\u0026#34; nova のキーペアでログインしたいので sshd は root ユーザでログイン出来るように 修正行う。\n# vi /etc/ssh/sshd_config PermitRootLogin yes OS をシャットダウンしてイメージ作成は終わり。\nGlance へ登録 netbsd.img を Glance に接続できるホストへ移動しイメージ登録を行う。\n% glance add name=\u0026quot;NetBSD 6.1 RC2 amd64\u0026quot; is_public=true \\ container_format=ovf disk_format=qcow2 \u0026lt; ~/netbsd.img まとめ 何も手を加えていない\u0026hellip;。NetBSD 6.1 は最初から Virtio が有効になっているので何 も考えることなくイメージ作成が出来た。コツは nova-compute が実際に動作している ホストでイメージを作ること。ハイパーバイザの OS が若干古いホストでも作業してみ たのだが、OpenStack に載せた途端カーネルパニックに陥った。Qemu はものすごいス ピードで進化しているのでバージョンの差異は致命的であると共に、日に日に快適な環 境が整ってきているとも言える。\n","permalink":"https://jedipunkz.github.io/post/2013/03/28/netbsd-on-openstack/","summary":"\u003cp\u003eもう数日で OpenStack の次期バージョン版 Grizzly がリリースされるタイミングだが\n現行バージョン Folsom の OpenStack の上に NetBSD を載せてみた。完全にお遊び\nだけど\u0026hellip;。\u003c/p\u003e\n\u003cp\u003e結局、ほとんど何も特別な対応取ることなく NetBSD が動いた。もちろんハイパーバイ\nザは KVM です。だけど少し条件がある。\u003c/p\u003e\n\u003cp\u003eqemu の不具合があり Ubuntu 12.04 LTS + Ubuntu Cloud Archives の組み合わせでは\nNetBSD が動作しなかった。下記のようなカーネルパニックが発生。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003epanic: pci_make_tag: bad request\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eこの不具合に相当するんじゃないかと思ってる。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://bugs.launchpad.net/qemu/\u0026#43;bug/897771\"\u003ehttps://bugs.launchpad.net/qemu/+bug/897771\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eよって下記の組み合わせで動作を確認した。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eUbuntu 12.10 + OpenStack (Native Packages)\u003c/li\u003e\n\u003cli\u003eqemu, kvm : 1.2.0+noroms-0ubuntu2.12.10.3\u003c/li\u003e\n\u003cli\u003eNetBSD 6.1 RC2 amd64\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"前提条件\"\u003e前提条件\u003c/h2\u003e\n\u003cp\u003eOpenStack Folsom が動作していること。\u003c/p\u003e\n\u003ch2 id=\"netbsd-os-イメージ作成\"\u003eNetBSD OS イメージ作成\u003c/h2\u003e\n\u003cp\u003enova-compute が動作しているホストの qemu-kvm を利用する。OpenStack 上に何でも\n良いので OS を動作させこの kvm プロセスのパラメータを参考に kvm コマンドを実行\nし NetBSD をインストールさせた。一番確実な方法。\u003c/p\u003e","title":"NetBSD on OpenStack"},{"content":"こんにちは。@jedipunkz です。\n今日は Chef Cookbook の管理をしてくれる Berkshelf について。\nBerkshelf は Librarian-Chef と同じく Cookbook の管理をしてくれるツールです。依 存関係のクリアもしてくれます。Opscode の中の人 @someara さんにこんなこと言われ て、\n@jedipunkz berkshelf \u0026gt; librarian-chef\n\u0026mdash; somearaさん (@someara) 2013年2月5日 Librarian-chef じゃなくて Berkshelf 使えってことだろうなぁと思ったので僕は Bekshelf を使うようにしてます。先日ブログ記事にした openstack-chef-repo も以前 は Librarian-chef を使っていたのですが最近 Berkshelf に置き換わりました。 openstack-chef-repo は Opscode の中の人の @mattray さん達が管理しています。\nでは早速。\nインストール インストールは簡単。gem install するだけです。\n% gem install berkshelf 使い方 chef-repo 配下で Berksfile を下記のように書きます。\nsite :opscode cookbook 'chef-client' cookbook 'nginx', '= 0.101.2' berks コマンドを実行して Cookbooks をダウンロードします。\n% export BERKSHELF_PATH=/Users/jedipunkz/chef-repo % berks install それぞれ依存関係のある Cookbook 達もダウンロードされます。これでいちいち依存を 解決しつつダウンロードなんてこともしなくて済む。\nここで言えるのは chef-repo とこの Berksfile だけ git 等で管理すれば良いという こと。Cookbooks 達はそれぞれ別のレポジトリに git で管理する形が良いと思う。 プロジェクトで chef を使っていて chef-repo ごと管理しがちだと思うけど、この方 が Cookbook を他のプロジェクトでも使いまわせるメリットがある。\nopscode コミュニティ管理外の Cookbooks 自分で管理している cookbook 等も Berkshelf で管理出来ます。下記のように書くと、\nsite : opscode cookbook 'chef-client' cookbook 'nginx', '= 0.101.2' cookbook \u0026quot;pxe_install_server\u0026quot;, git: 'https://github.com/jedipunkz/pxe_install_server' github から直接 git clone をしてくれます。\nCookbooks のアップデート Cookbook は独立した git レポジトリで管理すると良いと書きましたが、それぞれの Cookbooks は自分が若しくは他人が開発が進むので berks を使って最新の Cookbooks にアップデートすることも出来る。\n% berks update バージョン指定 Cookbook のバージョン指定ももちろん出来る。先の例では\ncookbook 'nginx', '= 0.101.2' では 0.101.2 と言うバージョンを指定した。その他\nEqual to (=) Greater than (\u0026gt;) Greater than equal to (\u0026lt;) Less than (\u0026lt;) Less than equal to (\u0026lt;=) Pessimistic (~\u0026gt;) が指定出来るようだ。\nまとめ Cookbooks の管理は今のところ Berkshelf だけで OK。紹介したもの以外にも幾つか機 能があるので公式のサイトで確認してみて欲しい。個人的には path: 指定が Berksfile で出来ないのが不便。公式のサイトには一応記述あるのだが動かなかった。 そのうち改善されるだろう。また Chef サーバに Cookbooks をアップロードする機能 もあるが、まぁこれは knife でも出来るしいいか。一番欲しかったのは \u0026lsquo;依存関係の クリア\u0026rsquo; だったので。また Berksfile, Berksfile.lock のファイルさえプロジェクト で管理すれば良いと言うのが個人的には綺麗になるなぁという印象。\n昔、Linux の rpm コマンドでパッケージを入れてて依存関係にあるパッケージをコマ ンド打つたびにダウンロードして\u0026hellip;と言う問題を yum が解決してくれて。それに似て るなぁと見ていて思った。\n","permalink":"https://jedipunkz.github.io/post/2013/03/17/berkshelf-chef-cookbook-manage/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003e今日は Chef Cookbook の管理をしてくれる Berkshelf について。\u003c/p\u003e\n\u003cp\u003eBerkshelf は Librarian-Chef と同じく Cookbook の管理をしてくれるツールです。依\n存関係のクリアもしてくれます。Opscode の中の人 @someara さんにこんなこと言われ\nて、\u003c/p\u003e\n\u003cblockquote class=\"twitter-tweet\" lang=\"ja\"\u003e\u003cp\u003e@\u003ca\nhref=\"https://twitter.com/jedipunkz\"\u003ejedipunkz\u003c/a\u003e berkshelf \u0026gt;\nlibrarian-chef\u003c/p\u003e\u0026mdash; somearaさん (@someara) \u003ca\nhref=\"https://twitter.com/someara/status/298664663976120321\"\u003e2013年2月5日\n\u003c/a\u003e\u003c/blockquote\u003e\n\u003cscript async src=\"//platform.twitter.com/widgets.js\" charset=\"utf-8\"\u003e\u003c/script\u003e\n\u003cp\u003eLibrarian-chef じゃなくて Berkshelf 使えってことだろうなぁと思ったので僕は\nBekshelf を使うようにしてます。先日ブログ記事にした openstack-chef-repo も以前\nは Librarian-chef を使っていたのですが最近 Berkshelf に置き換わりました。\nopenstack-chef-repo は Opscode の中の人の @mattray さん達が管理しています。\u003c/p\u003e\n\u003cp\u003eでは早速。\u003c/p\u003e\n\u003ch2 id=\"インストール\"\u003eインストール\u003c/h2\u003e\n\u003cp\u003eインストールは簡単。gem install するだけです。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e% gem install berkshelf\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id=\"使い方\"\u003e使い方\u003c/h2\u003e\n\u003cp\u003echef-repo 配下で Berksfile を下記のように書きます。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esite :opscode\n\ncookbook 'chef-client'\ncookbook 'nginx', '= 0.101.2'\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eberks コマンドを実行して Cookbooks をダウンロードします。\u003c/p\u003e","title":"Berkshelf で Chef Cookbook の管理"},{"content":"こんにちは。@jedipunkz です。\n久々に Chef の話題。\n有名な方々が最近 Chef について記事書いたりで Chef が再び盛り上がってきましたね。 僕のブログにも chef-solo で検索してアクセスしてくる方が増えているようです。\nちょうど今、仕事で試験的なサービスを立ち上げてそこで Chef を使っているのですが Server, Workstation, Target Node(s) の構成で使っていて、僕らは最初から chef-solo と capistrano でってことは考えていませんでした。もちろん chef-solo + capistrano の環境も調査しましたが、今の Server 構成が便利なのでもう戻れない。\n今日は Chef サーバ構成の良さについての記事ではないですが、それについては次回、 時間見つけて書こうかと思ってます。\n今日は \u0026lsquo;chef-client をどうアップデートするか\u0026rsquo; について。せっかく Chef でサーバ構成を継続的にデプロイ出来ても Chef 自身をアップデート出来ないと悲しい ですよね。chef-client が稼働しているインスタンスなんて起動して利用してすぐに破 棄だって時代ですが、なかなかそうもいなかい気がしています。\n「ほら、だから chef-solo 使えばいいんだよ！」って思ってるあなた！違うんですよー。 そのデメリットを上回るメリットが Chef サーバ構成にあるんです。次回書きますｗ\nChef10 から Chef11 と試験してみるにはちょうど良い時期でした。今回の構成は\u0026hellip;\n旧環境 +------------------+ | chef server | | version 10.18 | +------------------+ ^ | +--------------------+ | | | | +------------------+ +------------------+ | chef workstation | | target node | | version 10.24 | | version 10.24 | +------------------+ +------------------+ chef server は apt.opscode.com レポジトリを利用した Chef 10.18 なもの chef workstaion は version 10.24 (2013年3月15日現在 10.x 系最新) target node は chef workstaion から knife bootstrap を行い構成 knife bootstrap の際に下記のオプションを指定します。\n% knife bootstrap -N vmtest01 -r 'role[base]' \\ -i -x root -d chef-full distro は chef-full を選択。chef-full については下記にコードがあります。\nhttps://github.com/opscode/chef/blob/master/lib/chef/knife/bootstrap/chef-full.erb\nコードを抜粋すると、omnibus インストーラをダンロードして実行しているのがわかり ます。つまり Chef11 環境で再度 knife bootstrap すれば新しい Chef がインストー ルされるはず。\ninstall_sh=\u0026#34;http://opscode.com/chef/install.sh\u0026#34; version_string=\u0026#34;-v \u0026lt;%= chef_version %\u0026gt;\u0026#34; if ! exists /usr/bin/chef-client; then if exists wget; then bash \u0026lt;(wget \u0026lt;%= \u0026#34;--proxy=on \u0026#34; if knife_config[:bootstrap_proxy] %\u0026gt; ${install_sh} -O -) ${version_string} elif exists curl; then bash \u0026lt;(curl -L \u0026lt;%= \u0026#34;--proxy \\\u0026#34;#{knife_config[:bootstrap_proxy]}\\\u0026#34; \u0026#34; if knife_config[:bootstrap_proxy] %\u0026gt; ${install_sh}) ${version_string} else echo \u0026#34;Neither wget nor curl found. Please install one and try again.\u0026#34; \u0026gt;\u0026amp;2 exit 1 fi fi このように Chef10 で運用している状態を Chef11 に移行します。新しい環境は\u0026hellip;\n新環境 +------------------+ +------------------+ | chef server | | chef server | | version 10.18 | | version 11.0.6 | +------------------+ +------------------+ ^ | +--------------------+ | | | | +------------------+ +------------------+ | chef workstation | | target node | | version 11.4 | | version 11.4 | +------------------+ +------------------+ chef server は omnibus インストールから構築 chef workstation は version 11.4 (2013年3月15日現在最新) 移行手順 Chef11 サーバを用意する Omnibus インストーラでコマンド3つ打つだけで Chef サーバは構築出来ます。Chef11 からはこの方法が推奨されているようです。bootstrap を使った方法より簡単ですし。\n% wget https://opscode-omnitruck-release.s3.amazonaws.com/ubuntu/12.04/x86_64/chef-server_11.0.6-1.ubuntu.12.04_amd64.deb % sudo dpkg -i chef-server_11.0.6-1.ubuntu.12.04_amd64.deb % sudo chef-server-ctl reconfigure Chef11 Workstation を用意する 詳細は割愛します。chef10 と同じです。手順としては\nchef10 の chef-repo をコピー pem ファイル群を chef11 server からコピー knife configure -i にて knife.rb の生成 cookbooks, roles, data_bags, environments 等を chef11 にアップロード。若干修正が必要な場合がある。 です。\nTarget Node における旧 chef-client の停止と削除 稼働している Chef10 の chef-client を停止し削除、そして pem ファイル達を退避し てあげます。ここでは ssh で消す例を書きますが、これこそ Cookbook を書いて実行 すれば良いと思います。\n% ssh -i \u0026lt;ssh_secret_key\u0026gt; -l root \u0026lt;ip_address\u0026gt; \\ 'service chef-client stop; apt-get -y remove chef; mv /etc/chef /etc/chef.old' 再 knife bootstrap 実行 Chef11 の Workstation から knife bootstrap を実行します。これによって Target Node に Chef11 の環境がインストールされ Chef11 Server と接続出来ます。\n% knife bootstrap \u0026lt;ip_address\u0026gt; -N vmtest01 -r 'role[base]' -i \u0026lt;ssh_secret_key\u0026gt; \\ -x root -d chef-full role[base] 中の run_list に\u0026rsquo;chef-client::service\u0026rsquo; を追加すると chef-client が 常時稼動してくれて定期的に Chef Server と接続、更新してくれます。\nまとめと考察 chef-client の更新は簡単に出来た！\n今回はデフォルトの distro \u0026lsquo;chef-full\u0026rsquo; の例で書いたのだけど、distro には他にも 下記がある。\nhttps://github.com/opscode/chef/tree/master/lib/chef/knife/bootstrap\nLinux のディストリビューション名がついた distro は ruby をパッケージで, Chef を gem でインストールしている。下記は distro \u0026lsquo;ubuntu12.04-gems\u0026rsquo; のコードの抜粋 です。\nif [ ! -f /usr/bin/chef-client ]; then aptitude update aptitude install -y ruby ruby1.8-dev build-essential wget libruby1.8 rubygems fi gem update --no-rdoc --no-ri gem install ohai --no-rdoc --no-ri --verbose gem install chef --no-rdoc --no-ri --verbose \u0026lt;%= bootstrap_version_string %\u0026gt; ruby はディストリビューションが用意しているパッケージを。Chef は bootstrap_version_string を指定し gem でインストールしている。\nつまり \u0026lsquo;ubuntu12.04-gems\u0026rsquo; でも chef-client の更新は出来る。ちなみに試してみま した。chef-client の停止・削除を下記の通り行うと、全く同じ knife bootstrap コ マンドで chef-client のアップデートが行えた。\n% ssh -i \u0026lt;ssh_secret_key\u0026gt; -l root \u0026lt;ip_address\u0026gt; 'service chef-client stop; mv \\ /usr/local/bin/chef-client /usr/local/bin/chef-client.old; mv /etc/chef /etc/chef.old' 以上です。\nなんか簡単なこと書くのに長くなっちゃったけど\u0026hellip;。運用を僕らはまだ出来ていない のだけど、その時のことを考えると今回の試験はしてみたかったし、いい結果が出てよ かった。Chef がメジャーバージョンアップされて Chef Server の構成が大きく代わっ ても対応した cookbook, role, .. があればスムースに移行出来る。\nChef 無しにはインフラエンジニアやってられない時代が来たって感じｗ Chef 周りた のしー！\n追伸: 今回の方法より良いベストプラクティス的な方法があれば教えて下さい。\n","permalink":"https://jedipunkz.github.io/post/2013/03/15/chef-contenuously-deploy/","summary":"\u003cp\u003eこんにちは。\u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003e久々に Chef の話題。\u003c/p\u003e\n\u003cp\u003e有名な方々が最近 Chef について記事書いたりで Chef が再び盛り上がってきましたね。\n僕のブログにも chef-solo で検索してアクセスしてくる方が増えているようです。\u003c/p\u003e\n\u003cp\u003eちょうど今、仕事で試験的なサービスを立ち上げてそこで Chef を使っているのですが Server,\nWorkstation, Target Node(s) の構成で使っていて、僕らは最初から chef-solo と\ncapistrano でってことは考えていませんでした。もちろん chef-solo + capistrano\nの環境も調査しましたが、今の Server 構成が便利なのでもう戻れない。\u003c/p\u003e\n\u003cp\u003e今日は Chef サーバ構成の良さについての記事ではないですが、それについては次回、\n時間見つけて書こうかと思ってます。\u003c/p\u003e\n\u003cp\u003e今日は \u0026lsquo;chef-client をどうアップデートするか\u0026rsquo; について。せっかく Chef\nでサーバ構成を継続的にデプロイ出来ても Chef 自身をアップデート出来ないと悲しい\nですよね。chef-client が稼働しているインスタンスなんて起動して利用してすぐに破\n棄だって時代ですが、なかなかそうもいなかい気がしています。\u003c/p\u003e\n\u003cp\u003e「ほら、だから chef-solo 使えばいいんだよ！」って思ってるあなた！違うんですよー。\nそのデメリットを上回るメリットが Chef サーバ構成にあるんです。次回書きますｗ\u003c/p\u003e\n\u003cp\u003eChef10 から Chef11 と試験してみるにはちょうど良い時期でした。今回の構成は\u0026hellip;\u003c/p\u003e\n\u003ch2 id=\"旧環境\"\u003e旧環境\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003e+------------------+\n|   chef server    |\n|  version 10.18   |\n+------------------+\n^\n|\n+--------------------+\n|                    |\n|                    |\n+------------------+ +------------------+\n| chef workstation | |   target node    |\n|  version 10.24   | |  version 10.24   |\n+------------------+ +------------------+\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003echef server は apt.opscode.com レポジトリを利用した Chef 10.18 なもの\u003c/li\u003e\n\u003cli\u003echef workstaion は version 10.24 (2013年3月15日現在 10.x 系最新)\u003c/li\u003e\n\u003cli\u003etarget node は chef workstaion から knife bootstrap を行い構成\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eknife bootstrap の際に下記のオプションを指定します。\u003c/p\u003e","title":"chef-client の継続的デリバリ"},{"content":"OpenStack をコードで管理するためのフレームワークは幾つか存在するのだけど Ruby で記述出来る Fog が良い！と隣に座ってるアプリエンジニアが言うので僕も最近少し 触ってます。\nFog を使った OpenStack を管理するコードを書くことも大事なのだけど、Fog のコン トリビュートってことで幾つかの機能を付け足して (Quantum Router 周り) ってこと をやってました。まだ取り込まれてないけど。\nその開発の中で pry の存在を教えてもらいその便利さに驚いたので少し説明します。 バリバリ開発系の人は既に知っているだろうけど、インフラ系エンジニアの僕にとって は感激モノでした。\npry は irb 代替な Ruby のインタラクティブシェルです。下記の URL から持ってこれ ます。\nhttps://github.com/pry/pry\nシンタックスハイライトされたり json のレスポンスが綺麗に成形されたり irb 的に 使うだけでも便利なのだけど \u0026lsquo;?\u0026rsquo; や \u0026lsquo;$\u0026rsquo; でコードのシンタックスを確認したりコード 内容を確認したり出来るのがアツい！\nちょうど今回追加した Fog の機能を使って説明していみます。\nFog のコードを require して OpenStack に接続するための情報を設定し OpenStack Quantum に接続します。これで準備完了。\n[38] pry(main)\u0026gt; require \u0026#39;/home/jedipunkz/fog/lib/fog.rb\u0026#39; [49] pry(main)\u0026gt; @connection_hash = { [49] pry(main)* :openstack_username =\u0026gt; \u0026#39;demo\u0026#39;, [49] pry(main)* :openstack_api_key =\u0026gt; \u0026#39;demo\u0026#39;, [49] pry(main)* :openstack_tenant =\u0026gt; \u0026#39;service\u0026#39;, [49] pry(main)* :openstack_auth_url =\u0026gt; \u0026#39;http://172.16.1.11:5000/v2.0/tokens\u0026#39;, [49] pry(main)* :provider =\u0026gt; \u0026#39;OpenStack\u0026#39;, [49] pry(main)* } [50] pry(main)\u0026gt; @quantum = Fog::Network.new(@connection_hash) 試しに Router 一覧を取得します。list_routers メソッドです。\n[54] pry(main)\u0026gt; @quantum.list_routers() =\u0026gt; #\u0026lt;Excon::Response:0x00000003da3560 @body= \u0026#34;{\\\u0026#34;routers\\\u0026#34;: [{\\\u0026#34;status\\\u0026#34;: \\\u0026#34;ACTIVE\\\u0026#34;, \\\u0026#34;external_gateway_info\\\u0026#34;: {\\\u0026#34;network_id\\\u0026#34;: \\\u0026#34;b8ef37a9-9ed1-4b6d-862d-fe9e381a2f2a\\\u0026#34;}, \\\u0026#34;name\\\u0026#34;: \\\u0026#34;router-admin\\\u0026#34;, \\\u0026#34;admin_state_up\\\u0026#34;: true, \\\u0026#34;tenant_id\\\u0026#34;: \\\u0026#34;5e9544d4823a44d59f3591144049f691\\\u0026#34;, \\\u0026#34;id\\\u0026#34;: \\\u0026#34;35c65e2c-5cd8-4eb5-87a8-c370988c101a\\\u0026#34;}]}\u0026#34;, @data= {:body=\u0026gt; {\u0026#34;routers\u0026#34;=\u0026gt; [{\u0026#34;status\u0026#34;=\u0026gt;\u0026#34;ACTIVE\u0026#34;, \u0026#34;external_gateway_info\u0026#34;=\u0026gt; {\u0026#34;network_id\u0026#34;=\u0026gt;\u0026#34;b8ef37a9-9ed1-4b6d-862d-fe9e381a2f2a\u0026#34;}, \u0026#34;name\u0026#34;=\u0026gt;\u0026#34;router-admin\u0026#34;, \u0026#34;admin_state_up\u0026#34;=\u0026gt;true, \u0026#34;tenant_id\u0026#34;=\u0026gt;\u0026#34;5e9544d4823a44d59f3591144049f691\u0026#34;, \u0026#34;id\u0026#34;=\u0026gt;\u0026#34;35c65e2c-5cd8-4eb5-87a8-c370988c101a\u0026#34;}]}, :headers=\u0026gt; {\u0026#34;Content-Type\u0026#34;=\u0026gt;\u0026#34;application/json\u0026#34;, \u0026#34;Content-Length\u0026#34;=\u0026gt;\u0026#34;259\u0026#34;, \u0026#34;Date\u0026#34;=\u0026gt;\u0026#34;Wed, 06 Mar 2013 06:53:22 GMT\u0026#34;}, :status=\u0026gt;200, :remote_ip=\u0026gt;\u0026#34;172.16.1.11\u0026#34;}, @headers= {\u0026#34;Content-Type\u0026#34;=\u0026gt;\u0026#34;application/json\u0026#34;, \u0026#34;Content-Length\u0026#34;=\u0026gt;\u0026#34;259\u0026#34;, \u0026#34;Date\u0026#34;=\u0026gt;\u0026#34;Wed, 06 Mar 2013 06:53:22 GMT\u0026#34;}, @remote_ip=\u0026#34;172.16.1.11\u0026#34;, @status=200\u0026gt; 綺麗に色付けされてレスポンスがあります。\n次に \u0026lsquo;cd @quantum\u0026rsquo; して cd します。そして \u0026lsquo;? メソッド名\u0026rsquo; するとメソッドのシン タックスを確認出来ます。試しに Router を生成する create_router メソッドを見て みます。\n[56] pry(main)\u0026gt; cd @quantum [59] pry(#\u0026lt;Fog::Network::OpenStack::Real\u0026gt;):1\u0026gt; ? create_router From: /home/jedipunkz/fog/lib/fog/openstack/requests/network/create_router.rb @ line 6: Owner: Fog::Network::OpenStack::Real Visibility: public Signature: create_router(name, options=?) Number of lines: 1 そして \u0026lsquo;$ メソッド名\u0026rsquo; するとコードが確認出来ます。\n[64] pry(#\u0026lt;Fog::Network::OpenStack::Real\u0026gt;):1\u0026gt; $ create_router From: /home/jedipunkz/fog/lib/fog/openstack/requests/network/create_router.rb @ line 6: Owner: Fog::Network::OpenStack::Real Visibility: public Number of lines: 27 def create_router(name, options = {}) data = { \u0026#39;router\u0026#39; =\u0026gt; { \u0026#39;name\u0026#39; =\u0026gt; name, } } vanilla_options = [ :admin_state_up, :tenant_id, :network_id, :external_gateway_info, :status, :subnet_id ] vanilla_options.reject{ |o| options[o].nil? }.each do |key| data[\u0026#39;router\u0026#39;][key] = options[key] end request( :body =\u0026gt; Fog::JSON.encode(data), :expects =\u0026gt; [201], :method =\u0026gt; \u0026#39;POST\u0026#39;, :path =\u0026gt; \u0026#39;routers\u0026#39; ) end あとは \u0026lsquo;puts @quantum\u0026rsquo; 等するとオブジェクトの内容が確認出来たり、\u0026rsquo;ls @quantum\u0026rsquo; すると @quantum オブジェクトのメソッド一覧が確認出来たり。\n開発の効率が上がるなぁと感激。\n春なので OpenStack もそろろろ次期リリースの時期。それぞれのコンポーネントの機 能が拡張されているようなので Fog 等のフレームワークにコントリビュートする機会 もますます増えそう。Fog やその他のクラウドフレームワークはなんだかんだ言って AWS のフューチャがメインなので OpenStack の機能追加に追いついていない感がある。 もし興味持っている人が居たら是非一緒に OpenStack 界隈を盛り上げましょう。\n","permalink":"https://jedipunkz.github.io/post/2013/03/06/pry/","summary":"\u003cp\u003eOpenStack をコードで管理するためのフレームワークは幾つか存在するのだけど Ruby\nで記述出来る Fog が良い！と隣に座ってるアプリエンジニアが言うので僕も最近少し\n触ってます。\u003c/p\u003e\n\u003cp\u003eFog を使った OpenStack を管理するコードを書くことも大事なのだけど、Fog のコン\nトリビュートってことで幾つかの機能を付け足して (Quantum Router 周り) ってこと\nをやってました。まだ取り込まれてないけど。\u003c/p\u003e\n\u003cp\u003eその開発の中で pry の存在を教えてもらいその便利さに驚いたので少し説明します。\nバリバリ開発系の人は既に知っているだろうけど、インフラ系エンジニアの僕にとって\nは感激モノでした。\u003c/p\u003e\n\u003cp\u003epry は irb 代替な Ruby のインタラクティブシェルです。下記の URL から持ってこれ\nます。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/pry/pry\"\u003ehttps://github.com/pry/pry\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eシンタックスハイライトされたり json のレスポンスが綺麗に成形されたり irb 的に\n使うだけでも便利なのだけど \u0026lsquo;?\u0026rsquo; や \u0026lsquo;$\u0026rsquo; でコードのシンタックスを確認したりコード\n内容を確認したり出来るのがアツい！\u003c/p\u003e\n\u003cp\u003eちょうど今回追加した Fog の機能を使って説明していみます。\u003c/p\u003e\n\u003cp\u003eFog のコードを require して OpenStack に接続するための情報を設定し OpenStack\nQuantum に接続します。これで準備完了。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-ruby\" data-lang=\"ruby\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003e[\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e38\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e]\u003c/span\u003e pry(main)\u003cspan style=\"color:#f92672\"\u003e\u0026gt;\u003c/span\u003e require \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;/home/jedipunkz/fog/lib/fog.rb\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003e[\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e49\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e]\u003c/span\u003e pry(main)\u003cspan style=\"color:#f92672\"\u003e\u0026gt;\u003c/span\u003e @connection_hash \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003e[\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e49\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e]\u003c/span\u003e pry(main)\u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e   \u003cspan style=\"color:#e6db74\"\u003e:openstack_username\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u0026gt;\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;demo\u0026#39;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003e[\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e49\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e]\u003c/span\u003e pry(main)\u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e   \u003cspan style=\"color:#e6db74\"\u003e:openstack_api_key\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u0026gt;\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;demo\u0026#39;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003e[\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e49\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e]\u003c/span\u003e pry(main)\u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e   \u003cspan style=\"color:#e6db74\"\u003e:openstack_tenant\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u0026gt;\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;service\u0026#39;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003e[\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e49\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e]\u003c/span\u003e pry(main)\u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e   \u003cspan style=\"color:#e6db74\"\u003e:openstack_auth_url\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;http://172.16.1.11:5000/v2.0/tokens\u0026#39;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003e[\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e49\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e]\u003c/span\u003e pry(main)\u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e   \u003cspan style=\"color:#e6db74\"\u003e:provider\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u0026gt;\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;OpenStack\u0026#39;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003e[\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e49\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e]\u003c/span\u003e pry(main)\u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e }\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003e[\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e50\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e]\u003c/span\u003e pry(main)\u003cspan style=\"color:#f92672\"\u003e\u0026gt;\u003c/span\u003e @quantum \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eFog\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eNetwork\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003enew(@connection_hash)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e試しに Router 一覧を取得します。list_routers メソッドです。\u003c/p\u003e","title":"pry のススメ"},{"content":"2013年2月9日に行われた OpenStack 勉強会第11回で話してきました。\nopenstack-chef-repo と言う、Opscode Chef で OpenStack を構築する内容を話して きました。その時に説明出来なかった詳細についてブログに書いておきます。\nOpenstack chef-repo from Tomokazu Hirai 説明で使ったスライドです。\nまえがき Essex ベースで構築することしか今は出来ません。Folsom に関しては \u0026lsquo;直ちに開発が スタートする\u0026rsquo; と記されていました。今回は Opscode と RackSpace のエンジニアが共 同で開発を進めているので期待しています。今まで個人で OpenStack の各コンポーネ ントの cookbook を開発されていた方がいらっしゃるのだけど汎用性を持たせるという 意味で非常に難しく、またどの方の開発に追従していけばよいか判断困っていました。 よって今回こそ期待。\n前提の構成 +-------------+ | chef-server | +-------------+ 10.0.0.10 | +---------------+ 10.0.0.0/24 | | +-------------+ +-------------+ | workstation | | node | +-------------+ +-------------+ 10.0.0.11 10.0.0.12 chef-server : chef API を持つ chef-server 。cookbook, role..などのデータを持つ workstation : openstack-chef-repo を使うノード。knife が使える必要がある。 node : OpenStack を構築するターゲットノード 目次 chef-server の構築 (BootStrap 使う) openstack-chef-repo を使用する準備 openstack-chef-repo 実行 chef-server の構築 Opscode の wiki に記されている通りなのですが、簡単に書いておきます。今回は bootstrap 方式で用意します。\nchef-server ノードに chef をインストールします。chef-solo を用いるからです。\nchef-server# gem install chef chef-solo を使うための設定情報を /etc/chef/solo.rb に記します。\nchef-server# mkdir /etc/chef chef-server# ${EDITOR} /etc/chef/solo.rb file_cache_path \u0026quot;/tmp/chef-solo\u0026quot; cookbook_path \u0026quot;/tmp/chef-solo/cookbooks\u0026quot; chef-solo を実行する際に使う json ファイルを用意します。attribute を上書きする ことが出来ます。今回は webui を有効にした状態にします。\nchef-server# ${EDITOR} ~/chef.json { \u0026quot;chef_server\u0026quot;: { \u0026quot;server_url\u0026quot;: \u0026quot;http://localhost:4000\u0026quot;, \u0026quot;init_style\u0026quot;: \u0026quot;runit\u0026quot; }, \u0026quot;run_list\u0026quot;: [ \u0026quot;recipe[chef-server::rubygems-install]\u0026quot; ] } bootstrap して chef-server を構築します。\nchef-server# chef-solo -c /etc/chef/solo.rb -j ~/chef.json -r http://s3.amazonaws.com/chef-solo/bootstrap-latest.tar.gz chef-server が完成したはずです。workstation で knife を使うために \u0026lsquo;client\u0026rsquo; を 作ります。chef 10.x では user ではなく client が knife を実行します。\nchef-server% mkdir ~/.chef chef-server% sudo cp /etc/chef/validation.pem ~/.chef/ chef-server% sudo cp /etc/chef/webui.pem ~/.chef/ chef-server% sudo chown \u0026lt;my-username\u0026gt;:\u0026lt;my-group\u0026gt; ~/.chef knife を使うために下記の操作を行います。\nchef-server% cd ~ chef-server% knife configure -i Where should I put the config file? [~/.chef/knife.rb] Please enter the chef server URL: [http://10.0.0.1:4000] Please enter a clientname for the new client: [jedipunkz] Please enter the existing admin clientname: [chef-webui] Please enter the location of the existing admin client's private key:[/etc/chef/webui.pem] /home/jedipunkz/.chef/webui.pem Please enter the validation clientname: [chef-validator] Please enter the location of the validation key:[/etc/chef/validation.pem] /home/jedipunkz/.chef/validation.pem Please enter the path to a chef repository (or leave blank): Creating initial API user... Created client[jedipunkz] Configuration file written to /home/jedipunkz/.chef/knife.rb workstaion ノードで knife を操作するための \u0026lsquo;worker\u0026rsquo; client を作成します。\nchef-server% export EDITOR=vim chef-server% knife client create worker -a -f worker.pem chef-server% knife client list chef-validator chef-webui chefserver.example.com jedipunkz worker openstack-chef-repo を使用する準備 workstation で openstack-chef-repo を使うための準備をします。\nまずは knife を使えるように下記のように knife.rb や pem の手元への転送を行います。\nworkstation% gem install chef librarian spiceweasel workstation% cd ~ workstation% git clone git://github.com/opscode/openstack-chef-repo.git workstation% cd openstack-chef-repo workstation% vim .chef/knife.rb log_level :info log_location STDOUT node_name 'worker' client_key '/home/jedipunkz/openstack-chef-repo/.chef/worker.pem' validation_client_name 'chef-validator' validation_key '/home/jedipunkz/openstack-chef-repo/.chef/validation.pem' chef_server_url 'http://10.0.0.10:4000' cache_type 'BasicFile' cache_options( :path =\u0026gt; '/home/jedipunkz/openstack-chef-repo/.chef/checksums' ) cookbook_path '/home/jedipunkz/openstack-chef-repo/cookbooks' workstation% scp 10.0.0.10:~/worker.pem .chef/ workstation% scp 10.0.0.10:~/.chef/validation.pem .chef/ librarian を使って OpenStack 構築に必要な cookbooks をダンロードします。Cheffile というファイルに何をどこから取得するのか記されいてそれにしたがってダウンロードされます。\nworkstation% libratrian-chef update 環境に合わせて production.yml を修正します。今回必要最低限の箇所のみ修正します。 \u0026ldquo;osops_networks\u0026rdquo; という箇所に nova-network に渡すネットワーク情報があるので今回の 環境 10.0.0.0/24 に修正します。全体はこのような内容になります。\nworkstation% ${EDITOR} production.yml name \u0026quot;production\u0026quot; description \u0026quot;Defines the network and database settings you're going to use with OpenStack. The networks will be used in the libraries provided by the osops-utils cookbook. This example is for FlatDHCP with 2 physical networks.\u0026quot; override_attributes( \u0026quot;glance\u0026quot; =\u0026gt; { \u0026quot;image_upload\u0026quot; =\u0026gt; true, \u0026quot;images\u0026quot; =\u0026gt; [\u0026quot;precise\u0026quot;,\u0026quot;cirros\u0026quot;], }, \u0026quot;mysql\u0026quot; =\u0026gt; { \u0026quot;allow_remote_root\u0026quot; =\u0026gt; true, \u0026quot;root_network_acl\u0026quot; =\u0026gt; \u0026quot;%\u0026quot; }, \u0026quot;osops_networks\u0026quot; =\u0026gt; { \u0026quot;public\u0026quot; =\u0026gt; \u0026quot;10.0.0.0/24\u0026quot;, \u0026quot;management\u0026quot; =\u0026gt; \u0026quot;10.0.0.0/24\u0026quot;, \u0026quot;nova\u0026quot; =\u0026gt; \u0026quot;10.0.0.0/24\u0026quot; }, \u0026quot;nova\u0026quot; =\u0026gt; { \u0026quot;network\u0026quot; =\u0026gt; { \u0026quot;fixed_range\u0026quot; =\u0026gt; \u0026quot;192.168.100.0/24\u0026quot;, \u0026quot;public_interface\u0026quot; =\u0026gt; \u0026quot;eth0\u0026quot; }, \u0026quot;networks\u0026quot; =\u0026gt; [ { \u0026quot;label\u0026quot; =\u0026gt; \u0026quot;public\u0026quot;, \u0026quot;ipv4_cidr\u0026quot; =\u0026gt; \u0026quot;192.168.100.0/24\u0026quot;, \u0026quot;num_networks\u0026quot; =\u0026gt; \u0026quot;1\u0026quot;, \u0026quot;network_size\u0026quot; =\u0026gt; \u0026quot;255\u0026quot;, \u0026quot;bridge\u0026quot; =\u0026gt; \u0026quot;br100\u0026quot;, \u0026quot;bridge_dev\u0026quot; =\u0026gt; \u0026quot;eth0\u0026quot;, \u0026quot;dns1\u0026quot; =\u0026gt; \u0026quot;8.8.8.8\u0026quot;, \u0026quot;dns2\u0026quot; =\u0026gt; \u0026quot;8.8.4.4\u0026quot; } ] } ) infrastructure.yml という spiceweasel が参照するファイルの修正を行います。 cookbooks, roles, environments, data bags, node とパラメータがあり node のみ記されていないので今回用意したターゲットノードの情報を追記します。\nnode には予め SSH 公開鍵を設置する必要があります。また下記は root ユーザでログ インしていますが、sudo してもらっても構わないです。role で指定した \u0026lsquo;allinone\u0026rsquo; は OpenStack の全てのコンポーネントを一台のノードにインストールするためのもの です。これを他の role 例えば \u0026lsquo;keystone\u0026rsquo; 等を指定して複数のノードに OpenStack を展開することも可能なはずです。試していませんが。\nworkstation% vim infrastructure.yml ... 省略 nodes: - 10.0.0.12: run_list: role[allinone] options: -i ~/.ssh/id_rsa -x root -E production openstack-chef-repo 実行 準備が整ったので spiceweasel を使って knife コマンドの出力チェックと実行をして みます。先ほど追記した infrastructure.yml を使います。\nworkstation% spiceweasel infrastructure.yml knife cookbook upload apache2 knife cookbook upload apt knife cookbook upload aws knife cookbook upload build-essential knife cookbook upload ntp knife cookbook upload openssh knife cookbook upload openssl knife cookbook upload postgresql knife cookbook upload selinux knife cookbook upload xfs knife cookbook upload yum knife cookbook upload erlang knife cookbook upload mysql knife cookbook upload rabbitmq knife cookbook upload database knife cookbook upload omnibus_updater knife cookbook upload lxc knife cookbook upload sysctl knife cookbook upload osops-utils knife cookbook upload mysql-openstack knife cookbook upload rabbitmq-openstack knife cookbook upload keystone knife cookbook upload glance knife cookbook upload nova knife cookbook upload horizon knife environment from file production.rb knife role from file base.rb knife role from file lxc.rb knife role from file mysql-master.rb knife role from file rabbitmq-server.rb knife role from file keystone.rb knife role from file glance-api.rb knife role from file glance-registry.rb knife role from file glance.rb knife role from file nova-setup.rb knife role from file nova-scheduler.rb knife role from file nova-api-ec2.rb knife role from file nova-api-os-compute.rb knife role from file nova-volume.rb knife role from file nova-vncproxy.rb knife role from file horizon-server.rb knife role from file single-controller.rb knife role from file single-compute.rb knife role from file allinone.rb knife bootstrap 10.0.0.12 -i ~/.ssh/id_rsa -x root -E production -r 'role[allinone]' この操作で spiceweasel は role ファイルの中身と yml ファイルを読み、依存関係を チェックしてくれます。\nではいよいよ実行。\nworkstaion% spiceweasel -e infrastructure.yml 数分で OpenStack 環境が構築出来ると思います。\n所感 現時点ではまだ essex ベースの OpenStack しか構築出来ない。folsom 以降について は直ちに行われる。また先にも記したが Opscode と RackSpace のエンジニアが共同で 作業に入ったので今後に期待。複数の OpenStack cookboooks を見てきたが個人で開発 するのは厳しいと感じています。fedra, centos, ubuntu, debian \u0026hellip; いろんなプラッ トフォームを前提に開発するのは厳しい\u0026hellip;。pull request する余力があればやってみ たい。また合わせて各コンポーネントの cookbooks についても同様に参加していきた い。\nchef は繰り返し実行されるので継続的にデプロイが可能であり、chef Resources が我々 の操作を抽象化してくれる。尚且つ API で開発も容易になる。学習コストは若干高い し、cookbooks の開発も正直しんどい。だけどインフラを chef でコントロールする意 味はとても大きい。その他のメリットとして属人的な操作に依存したシステムを無くす という事もある。\nOpenStack のドキュメントにはデプロイ方法として dodai-deploy と puppet が掲載さ れている。chef は載っていない。これは cookbooks の開発が遅れている状況が理由に あると思う。今回 Opscode の中の Matt Ray さんの下記の資料\nhttp://www.slideshare.net/mattray/chef-11-previewchef-for-openstack\nを見て、これからに期待したいと感じた。\n","permalink":"https://jedipunkz.github.io/post/2013/02/10/di-11hui-openstack-study11-openstack-chef-repo/","summary":"\u003cp\u003e2013年2月9日に行われた OpenStack 勉強会第11回で話してきました。\u003c/p\u003e\n\u003cp\u003eopenstack-chef-repo と言う、Opscode Chef で OpenStack を構築する内容を話して\nきました。その時に説明出来なかった詳細についてブログに書いておきます。\u003c/p\u003e\n\u003ciframe src=\"http://www.slideshare.net/slideshow/embed_code/16434817\"\nwidth=\"427\" height=\"356\" frameborder=\"0\" marginwidth=\"0\" marginheight=\"0\"\nscrolling=\"no\" style=\"border:1px solid #CCC;border-width:1px 1px\n0;margin-bottom:5px\" allowfullscreen webkitallowfullscreen mozallowfullscreen\u003e\n\u003c/iframe\u003e \u003cdiv style=\"margin-bottom:5px\"\u003e \u003cstrong\u003e \u003ca\nhref=\"http://www.slideshare.net/tomokazubobhirai/openstack-chefrepo\"\ntitle=\"Openstack chef-repo\" target=\"_blank\"\u003eOpenstack chef-repo\u003c/a\u003e \u003c/strong\u003e\nfrom \u003cstrong\u003e\u003ca href=\"http://www.slideshare.net/tomokazubobhirai\"\ntarget=\"_blank\"\u003eTomokazu Hirai\u003c/a\u003e\u003c/strong\u003e \u003c/div\u003e\n\u003cp\u003e説明で使ったスライドです。\u003c/p\u003e\n\u003ch2 id=\"まえがき\"\u003eまえがき\u003c/h2\u003e\n\u003cp\u003eEssex ベースで構築することしか今は出来ません。Folsom に関しては \u0026lsquo;直ちに開発が\nスタートする\u0026rsquo; と記されていました。今回は Opscode と RackSpace のエンジニアが共\n同で開発を進めているので期待しています。今まで個人で OpenStack の各コンポーネ\nントの cookbook を開発されていた方がいらっしゃるのだけど汎用性を持たせるという\n意味で非常に難しく、またどの方の開発に追従していけばよいか判断困っていました。\nよって今回こそ期待。\u003c/p\u003e\n\u003ch2 id=\"前提の構成\"\u003e前提の構成\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003e+-------------+\n| chef-server |\n+-------------+ 10.0.0.10\n|\n+---------------+ 10.0.0.0/24\n|               |\n+-------------+ +-------------+\n| workstation | |    node     |\n+-------------+ +-------------+\n10.0.0.11       10.0.0.12\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003echef-server : chef API を持つ chef-server 。cookbook, role..などのデータを持つ\u003c/li\u003e\n\u003cli\u003eworkstation : openstack-chef-repo を使うノード。knife が使える必要がある。\u003c/li\u003e\n\u003cli\u003enode        : OpenStack を構築するターゲットノード\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"目次\"\u003e目次\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003echef-server の構築 (BootStrap 使う)\u003c/li\u003e\n\u003cli\u003eopenstack-chef-repo を使用する準備\u003c/li\u003e\n\u003cli\u003eopenstack-chef-repo 実行\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"chef-server-の構築\"\u003echef-server の構築\u003c/h2\u003e\n\u003cp\u003eOpscode の wiki に記されている通りなのですが、簡単に書いておきます。今回は\nbootstrap 方式で用意します。\u003c/p\u003e","title":"第11回OpenStack勉強会で話してきた"},{"content":"Spiceweasel https://github.com/mattray/spiceweasel#cookbooks を使ってみた。\nSpiceweasel は Chef の cookbook のダウンロード, role/cookbook の chef server へのアップロード, ブートストラップ等をバッチ処理的に行なってくれる(もしくはコ マンドラインを出力してくれる)ツールで、自分的にイケてるなと感じたのでブログに 書いておきます。\nクラウドフェデレーション的サービスというかフロントエンドサービスというか、複数 のクラウドを扱えるサービスは増えてきているけど、chef を扱えるエンジニアであれ ば、この Spiceweasel で簡単・一括デプロイ出来るので良いのではないかと。\n早速だけど chef-repo にこんな yamp ファイルを用意します。\ncookbooks: - apt: - nginx: roles: - base: nodes: - 172.24.17.3: run_list: role[base] options: -i ~/.ssh/testkey01 -x root -N webset01 - 172.24.17.4: run_list: role[base] options: -i ~/.ssh/testkey01 -x root -N webset02 上から説明すると\u0026hellip;\n\u0026lsquo;apt\u0026rsquo;, \u0026rsquo;nginx\u0026rsquo; の cookbook を opscode レポジトリからダウンロード \u0026lsquo;apt\u0026rsquo;, \u0026rsquo;nginx\u0026rsquo; の cookbook を chef-server へアップロード roles/base.rb を chef-server へアップロード 2つのノードに対して bootstrap 仕掛ける ってことをやるためのファイルです。予め chef-repo と roles は用意してあげる必要 があります。この辺りは knife の操作のための準備と全く同じ。また Spiceweasel は、 この yaml フィアル内の各パラメータや指定した role の内容の依存関係をチェックし てくれます。\nでは、このファイルに対して\n% spiceweasel \u0026lt;yaml ファイル名\u0026gt; すると、結果として下記のようなバッチが取得出来る。\nknife cookbook site download apt --file cookbooks/apt.tgz tar -C cookbooks/ -xf cookbooks/apt.tgz rm -f cookbooks/apt.tgz knife cookbook upload apt knife cookbook site download test --file cookbooks/test.tgz tar -C cookbooks/ -xf cookbooks/test.tgz rm -f cookbooks/test.tgz knife cookbook upload test knife role from file base.rb knife bootstrap 172.24.17.3 -i ~/.ssh/testkey01 -x root -N webset01 -r 'role[base]' knife bootstrap 172.24.17.4 -i ~/.ssh/testkey01 -x root -N webset02 -r 'role[base]' 尚且つ -e オプションを指定すると、実際にこれらのバッチを実行出来る。cookbooks ディレクトリに予めレシピが存在すればダウンロードのバッチは省略されるぽいし、-d を指定すると逆にノード削除バッチ処理、-r でリビルドのためのバッチ処理が得られ る。\n削除系・リビルド系は現時点では不具合が見られました。実行すると他の環境にも影響 出るので注意が必要。\n個人的にはプライベートレポジトリの cookbooks も取ってこれるようになると嬉しい。 まぁ、Berkshelf 使えばいいのだけど。http://berkshelf.com/\n開発者の Matt Ray さんの資料によれば Chef for OpenStack もこの Spiceweasel を 使う方向で修正が掛かったらしい。個人的には一番興味あるところ。ちなみに Chef for OpenStack は folsom ベースが現在 \u0026lsquo;active development\u0026rsquo; 状態らしい。\nOpscode のサイトで紹介されているサンプルは古いバージョンでの指定方法らしく、注 意が必要です。\nhttp://wiki.opscode.com/display/chef/Spiceweasel\n所感 chef, knife 周りはいろんな関連技術があるので技術を選定する上で迷ってしまうこと が多いのだけど、この Spiceweasel には可能性を感じました。って言うのは、Chef が 実装出来ていないインテグレーションやオーケストレーションっていう所まで踏み込め る可能性があるから。ノード間の関連付けが出来るんです。Swift-Storage と Swift-Proxy の関連付け、または Load-Balancer と HTTP-Server の関連付け等。継続 的デリバリなインテグレーションって Chef を使ってどう実現するんだ？って思ってい た時があったのですが、こういったラッパーツールの登場で解決されそうな気がします。\n","permalink":"https://jedipunkz.github.io/post/2013/02/01/spiceweasel-knife-bootstrap/","summary":"\u003cp\u003eSpiceweasel \u003ca href=\"https://github.com/mattray/spiceweasel#cookbooks\"\u003ehttps://github.com/mattray/spiceweasel#cookbooks\u003c/a\u003e を使ってみた。\u003c/p\u003e\n\u003cp\u003eSpiceweasel は Chef の cookbook のダウンロード, role/cookbook の chef server\nへのアップロード, ブートストラップ等をバッチ処理的に行なってくれる(もしくはコ\nマンドラインを出力してくれる)ツールで、自分的にイケてるなと感じたのでブログに\n書いておきます。\u003c/p\u003e\n\u003cp\u003eクラウドフェデレーション的サービスというかフロントエンドサービスというか、複数\nのクラウドを扱えるサービスは増えてきているけど、chef を扱えるエンジニアであれ\nば、この Spiceweasel で簡単・一括デプロイ出来るので良いのではないかと。\u003c/p\u003e\n\u003cp\u003e早速だけど chef-repo にこんな yamp ファイルを用意します。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecookbooks:\n- apt:\n- nginx:\n\nroles:\n- base:\n\nnodes:\n- 172.24.17.3:\n    run_list: role[base]\n    options: -i ~/.ssh/testkey01 -x root -N webset01\n- 172.24.17.4:\n    run_list: role[base]\n    options: -i ~/.ssh/testkey01 -x root -N webset02\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e上から説明すると\u0026hellip;\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u0026lsquo;apt\u0026rsquo;, \u0026rsquo;nginx\u0026rsquo; の cookbook を opscode レポジトリからダウンロード\u003c/li\u003e\n\u003cli\u003e\u0026lsquo;apt\u0026rsquo;, \u0026rsquo;nginx\u0026rsquo; の cookbook を chef-server へアップロード\u003c/li\u003e\n\u003cli\u003eroles/base.rb を chef-server へアップロード\u003c/li\u003e\n\u003cli\u003e2つのノードに対して bootstrap 仕掛ける\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eってことをやるためのファイルです。予め chef-repo と roles は用意してあげる必要\nがあります。この辺りは knife の操作のための準備と全く同じ。また Spiceweasel は、\nこの yaml フィアル内の各パラメータや指定した role の内容の依存関係をチェックし\nてくれます。\u003c/p\u003e","title":"Spiceweasel で knife バッチ処理"},{"content":"(2013/08/31 修正しました)\n自宅のノート PC にいつも Debian Gnu/Linux unstable を入れて作業してたのだけど、 Arch Linux が試したくなって入れてみた。すごくイイ。ミニマル思考で常に最新。端 末に入れる OS としては最適かも!と思えてきた。Ubuntu はデスクトップ環境で扱う にはチト大きすぎるし。FreeBSD のコンパイル待ち時間が最近耐えられないし\u0026hellip;。\n前リリースの Arch Linux には /arch/setup という簡易インストーラがあったのだけ ど、それすら最近無くなった。環境作る方法を自分のためにもメモしておきます。\nOS イメージ iso 取得とインストール用 USB スティック作成 Linux, Windows, Mac で作り方が変わるようだけど、自分は Mac OSX を使ってインス トール USB スティックを作成した。\ndiskutil で USB スティック装着前後の disk デバイス番号を覚える\n% diskutil list (ここでは /dev/rdisk4 として進める。)\nアンマウントする。\n% sudo diskutil unmountDisk /dev/rdisk4 ダウンロードした iso を USB スティックに書き込む。\n% sudo dd if=/path/to/downloaded/iso of=/dev/rdisk4 bs=8192 % sudo diskutil eject /dev/rdisk4 USB スティック装着しインストール開始 起動するとメニューが表示されるので x86_64 を選んで起動。プロンプトが表示される。\nまず disk のパーティション作成。\n# fdisk /dev/sda # 環境に合わせてデバイス名変更 # fdisk -l /dev/sda デバイス ブート 始点 終点 ブロック Id システム /dev/sda1 2048 4196351 2097152 82 Linux スワップ / Solaris /dev/sda2 * 4196352 156301487 76052568 83 Linux 上記のように切りました。fdisk の使い方は\u0026hellip;ぐぐってください。また /boot にあたるパーティション にブートフラグを必ず付けること。\nファイルシステムを作ってマウント。\n# mkfs.ext4 /dev/sda2 # mount -t ext4 /dev/sda2 /mnt コアシステムのインストール\n# pacstrap /mnt base base-devel fstab の配置。必要に応じて /dev/sda1 (上記で作った) をスワップとして加筆。\n# genfstab -p /mnt \u0026gt;\u0026gt; /mnt/etc/fstab chroot して /dev/sda2 内コアシステムで作業 chroot する。\n# arch-chroot /mnt キーマップを us 配列で CTRL と Caps をスワップしたいので下記の作業を実施。\n# cd /usr/share/kbd/keymaps/i386/qwerty # cp us.map.gz usx.map.gz # gunzip usx.map.gz # vi usx.map keycode 58 = Control # Caps_Lock を Control に置き換え # gzip usx.map # loadkeys usx # キーマップをロード # vi /etc/vconsole.conf KEYMAP=usx # 追記 ロケールの生成。\n# vi /etc/locale.gen # ja_JP.UTF-8 のコメントアウトを削除 # locale-gen タイムゾーンの設定。\n# ln -s /usr/share/zoneinfo/Asia/Tokyo /etc/localtime ホスト名修正。\n# echo \u0026quot;\u0026lt;ホスト名\u0026gt;\u0026quot; \u0026gt; /etc/hostname # vi /etc/hosts # 適宜修正 ネットワークの設定。まずは有線。\n# systemctl enable dhcpcd@eth0.service ブートローダとして syslinux を使う。ディスクデバイス名だけ修正。syslinux のインストーラが うまくデバイス名を拾ってくれない。\n# pacman -S syslinux # vi /boot/syslinux/syslinux.cfg LABEL arch MENU LABEL Arch Linux LINUX ../vmlinuz-linux APPEND root=/dev/sda2 ro INITRD ../initramfs-linux.img パスワード設定。\n# passwd 再起動し システムをインストールした /dev/sda2 で起動する。 再起動し USB スティックを抜く。起動したら root ユーザでログイン。\n無線デバイスの設定。\n# pacman -S dialog wpa_supplicant # wifi-menu 検知されたアクセスポイント名が表示されるので希望のモノを選択しパスフレーズを入力。\n一般ユーザの作成と sudo 設定。\n# useradd -u \u0026lt;UID) -d /home/\u0026lt;UESRNAME\u0026gt; -m \u0026lt;USERNAME\u0026gt; # passwd \u0026lt;USERNAME\u0026gt; # pacman -S sudo # vigr # wheel グループに自分を追記 # visudo # wheel のところのコメントアウトを削除 %wheel ALL=(ALL) ALL X 周りの設定 X とウィンドウマネージャのインストール。ウィンドウマネージャは好きなものを。私 は awesome を選びました。\n# pacman -S xorg xorg-xinit xterm rxvt-unicode xf86-video-intel awesome 日本語フォントのインストールと日本語インプットメソッドの設定。ibus と mozc を選んだ。 結構賢い。とりあえずのフォントとして sazanami を。\n# vi /etc/pacman.conf # 下記のレポジトリを追記 [pnsft-pur] SigLevel = Optional TrustAll Server = http://downloads.sourceforge.net/project/pnsft-aur/pur/$arch # pacman -Syy # pacman -S ttf-sazanami ibus-mozc mozc 一般ユーザの ~/.xinitrc に下記を追記。\n% vi ~/.xinitrc xrdb -merge $HOME/.Xresources export LANG=ja_JP.UTF-8 export LANGUAGE=ja_JP.UTF-8 export LC_ALL=ja_JP.UTF-8 export LC_CTYPE=ja_JP.UTF-8 export GTK_IM_MODULE=ibus export QT_IM_MODULE=xim export XMODIFIERS=@im=ibus ibus-daemon --daemonize --xim \u0026amp; setxkbmap -layout us -option ctrl:nocaps exec awesome X を起動。\n% startx 私は awesome を選びましたが、たまに enlightenment も選択します。この時点で enlightenment の メニューなど、日本語化されているはずだが、厳しければ\nhttps://wiki.archlinux.org/index.php/Input_Japanese_using_uim_(%E6%97%A5%E6%9C%AC%E8%AA%9E)\nにある IPA フォント、VL ゴシックなどを入れる。こちらのほうが数段綺麗。\nIPA フォントの入れ方だけメモっておく。\n% wget https://aur.archlinux.org/packages/ot/otf-ipafont/otf-ipafont.tar.gz % tar zxvf otf-ipafont.tar.gz % cd otf-ipafont % makepkg -s % sudo pacman -U \u0026lt;生成された pkg.tar.xz ファイル\u0026gt; Thinkpad の真ん中ボタンでスクロールする。\n# vi /etc/X11/xorg.conf.d/10-thinkpad.conf # 新規生成 Section \u0026quot;InputClass\u0026quot; Identifier \u0026quot;Trackpoint Wheel Emulation\u0026quot; MatchProduct \u0026quot;TPPS/2 IBM TrackPoint|DualPoint Stick|Synaptics Inc. Composite TouchPad / TrackPoint|ThinkPad USB Keyboard with TrackPoint|USB Trackpoint pointing device|Composite TouchPad / TrackPoint\u0026quot; MatchDevicePath \u0026quot;/dev/input/event*\u0026quot; Option \u0026quot;EmulateWheel\u0026quot; \u0026quot;true\u0026quot; Option \u0026quot;EmulateWheelButton\u0026quot; \u0026quot;2\u0026quot; Option \u0026quot;Emulate3Buttons\u0026quot; \u0026quot;false\u0026quot; Option \u0026quot;XAxisMapping\u0026quot; \u0026quot;6 7\u0026quot; Option \u0026quot;YAxisMapping\u0026quot; \u0026quot;4 5\u0026quot; EndSection X の再起動をして終了。\nまとめ 今回はブログというより雑多なまとめになったけど、メモするだけでも意味があるので 残しておいた。Debian / Ubuntu より手間は掛かるけど、エンジニアが何をやっている か掴めるのでいいカンジ。無駄なプロセスいないので起動もめちゃ速いし。何より最新 のソフトウェア (安定したもの) が簡単に使えるのは嬉しい。ここに書いた手順は時間 がすぎるに連れて意味がないものになっていくだろう。開発が活発でここ数年でも結構 劇的にシステムが変更になってるみたい。systemd .. とか。理解するのに苦労する所 はないので、エンジニアであれば誰でも扱えそうなところも魅力。\nOS なんて何でもいい時代 (抽象化されつつあるから) だけど、手元の端末に入れる OS だけは Arch Linux がいいなぁ。\n","permalink":"https://jedipunkz.github.io/post/2013/01/14/arch-linux-setup/","summary":"\u003cp\u003e(2013/08/31 修正しました)\u003c/p\u003e\n\u003cp\u003e自宅のノート PC にいつも Debian Gnu/Linux unstable を入れて作業してたのだけど、\nArch Linux が試したくなって入れてみた。すごくイイ。ミニマル思考で常に最新。端\n末に入れる OS としては最適かも!と思えてきた。Ubuntu はデスクトップ環境で扱う\nにはチト大きすぎるし。FreeBSD のコンパイル待ち時間が最近耐えられないし\u0026hellip;。\u003c/p\u003e\n\u003cp\u003e前リリースの Arch Linux には /arch/setup という簡易インストーラがあったのだけ\nど、それすら最近無くなった。環境作る方法を自分のためにもメモしておきます。\u003c/p\u003e\n\u003ch4 id=\"os-イメージ-iso-取得とインストール用-usb-スティック作成\"\u003eOS イメージ iso 取得とインストール用 USB スティック作成\u003c/h4\u003e\n\u003cp\u003eLinux, Windows, Mac で作り方が変わるようだけど、自分は Mac OSX を使ってインス\nトール USB スティックを作成した。\u003c/p\u003e\n\u003cp\u003ediskutil で USB スティック装着前後の disk デバイス番号を覚える\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e% diskutil list\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e(ここでは /dev/rdisk4 として進める。)\u003c/p\u003e\n\u003cp\u003eアンマウントする。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e% sudo diskutil unmountDisk /dev/rdisk4\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eダウンロードした iso を USB スティックに書き込む。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e% sudo dd if=/path/to/downloaded/iso of=/dev/rdisk4 bs=8192\n% sudo diskutil eject /dev/rdisk4\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4 id=\"usb-スティック装着しインストール開始\"\u003eUSB スティック装着しインストール開始\u003c/h4\u003e\n\u003cp\u003e起動するとメニューが表示されるので x86_64 を選んで起動。プロンプトが表示される。\u003c/p\u003e","title":"Arch Linux セットアップまとめ"},{"content":"以前紹介した OpenStack Folsom 構築 bash スクリプトなのだけど quantum の代わり に nova-network も使えるようにしておいた。\n構築 bash スクリプトは、\nhttps://github.com/jedipunkz/openstack_folsom_deploy/blob/master/README_jp.md\nに詳しい使い方を書いておきました。またパラメータを修正して実行するのだけどパラ メータについては、\nhttps://github.com/jedipunkz/openstack_folsom_deploy/blob/master/README_parameters_jp.md\nに書いておきました。\n手持ちの Thinkpad に OpenStack folsom 入れた写真。この写真の OpenStack Folsom を構築した時の手順を書いておくよ。\nOS をインストール OS をインストールします。12.10 を使いました。(12.04 LTS でも可)。/dev/sda6 等、 Cinder 用に一つパーティションを作ってマウントしないでおきます。また固定 IP ア ドレスを NIC に付与しておきます。\nスクリプト取得 スクリプトを取得する。\n% sudo apt-get update; sudo apt-get install git-core % git clone https://github.com/jedipunkz/openstack_folsom_deploy.git % cd openstack_folsom_deploy パラメータ修正 deploy_with_nova-network.conf 内のパラメータを修正します。オールインワン構成な ので、ほぼほぼ修正せずに実行しますが\nHOST_IP='\u0026lt;Thinkpad の IP アドレス\u0026gt;' だけ修正。\n実行\u0026hellip; 実行する。\n% sudo ./deploy.sh allinone nova-network \u0026hellip; 最近 ./deploy.sh create_network nova-network を実行しなくて済むようにしました。\nHorizon にアクセスする ブラウザで http://localhost/horizon にアクセスすれば horizon にアクセス出来る。 ユーザ情報は\u0026hellip;\nuser : demo pass : demo です。Horizon から後でユーザ情報変更してもらって構わないです。\nコマンドラインからアクセスする。 実行したユーザのホームディレクトリに ~/openstackrc がある。\n~/openstackrc # admin アカウント用 ~openstackrc-demo # demo ユーザ用 読み込んで OpenStack のコマンドを実行する。下記は例です。\n% source ~/openstackrc-demo % nova list +--------------------------------------+----------+--------+---------------------------------+ | ID | Name | Status | Networks | +--------------------------------------+----------+--------+---------------------------------+ | 81730be5-f2b3-411c-bfef-9a879e3d7d56 | grievous | ACTIVE | private=10.0.0.5, 192.168.1.195 | | 0ba8416a-58bd-476d-96a8-2ecc37ae53e6 | luke | ACTIVE | private=10.0.0.2, 192.168.1.194 | | 5b1f23b3-c15e-4260-bf6c-f3e965bcd546 | r2d2 | ACTIVE | private=10.0.0.4, 192.168.1.193 | +--------------------------------------+----------+--------+---------------------------------+ 次のリリース版は\u0026hellip; bash で書いても\u0026hellip;満足感得られないしコード汚くなるし、人に読んでもらえないし。 ruby で書いても perl で書いても同じ。コマンドをバシバシ打つインフラ系のコード は汚くなるしまともにテストも出来ない。これからはインフラ系もコードを書く時代だっ て言っておいてこれじゃアカンぉ。\nならば情報が整理されるって意味だけでも chef のようなフレームワークを使う意味は 大きい。資源を他の人に有効活用してもらえるチャンスも増えるし。chef のクックブッ クなんてコードじゃないってアプリエンジニアに言われようが、やっぱりフレームワー ク使うべきだし使いたい。次の OpenStack のリリースの時は Chef のクックブック作 るって決めたよぉー。why-run 出来たり、テストも出来るしね。\n","permalink":"https://jedipunkz.github.io/post/2013/01/12/openstack-on-thinkpad/","summary":"\u003cp\u003e以前紹介した OpenStack Folsom 構築 bash スクリプトなのだけど quantum の代わり\nに nova-network も使えるようにしておいた。\u003c/p\u003e\n\u003cp\u003e構築 bash スクリプトは、\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/jedipunkz/openstack_folsom_deploy/blob/master/README_jp.md\"\u003ehttps://github.com/jedipunkz/openstack_folsom_deploy/blob/master/README_jp.md\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eに詳しい使い方を書いておきました。またパラメータを修正して実行するのだけどパラ\nメータについては、\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/jedipunkz/openstack_folsom_deploy/blob/master/README_parameters_jp.md\"\u003ehttps://github.com/jedipunkz/openstack_folsom_deploy/blob/master/README_parameters_jp.md\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eに書いておきました。\u003c/p\u003e\n\u003cimg src=\"http://jedipunkz.github.com/pix/openstack_folsom_thinkpad.jpg\"\u003e\n\u003cp\u003e手持ちの Thinkpad に OpenStack folsom 入れた写真。この写真の OpenStack Folsom\nを構築した時の手順を書いておくよ。\u003c/p\u003e\n\u003ch4 id=\"os-をインストール\"\u003eOS をインストール\u003c/h4\u003e\n\u003cp\u003eOS をインストールします。12.10 を使いました。(12.04 LTS でも可)。/dev/sda6 等、\nCinder 用に一つパーティションを作ってマウントしないでおきます。また固定 IP ア\nドレスを NIC に付与しておきます。\u003c/p\u003e\n\u003ch4 id=\"スクリプト取得\"\u003eスクリプト取得\u003c/h4\u003e\n\u003cp\u003eスクリプトを取得する。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e% sudo apt-get update; sudo apt-get install git-core\n% git clone https://github.com/jedipunkz/openstack_folsom_deploy.git\n% cd openstack_folsom_deploy\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4 id=\"パラメータ修正\"\u003eパラメータ修正\u003c/h4\u003e\n\u003cp\u003edeploy_with_nova-network.conf 内のパラメータを修正します。オールインワン構成な\nので、ほぼほぼ修正せずに実行しますが\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eHOST_IP='\u0026lt;Thinkpad の IP アドレス\u0026gt;'\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eだけ修正。\u003c/p\u003e","title":"OpenStack Folsom on Thinkpad"},{"content":"FreeBSD を OpenStack で管理したいなぁと思って自宅に OpenStack 環境作ってました。\nお正月なのに\u0026hellip;\n使ったのは folsom ベースの OpenStack (nova-network) と FreeBSD 9.1 です。8 系 の FreeBSD でも大体同じ作業で実現出来るぽいです。あと nova-network でって書い たのは自宅に quantum だと少し厳しいからです。FlatDHCPManager が調度良かった。\n今回のポイントは FreeBSD の HDD, NIC のドライバに virtio を使うように修正する ところです。OpenStack (KVM) は virtio 前提なので、そうせざるを得なかったです。\n今回使ったソフトウェア OpenStack Folsom (nova-network) Ubuntu Server 12.10 FreeBSD 9.1 amd64 作業方法 準備としてこれらが必要になります。事前に行なってください。\nFreeBSD-9.1-RELEASE-amd64-disc1.iso ダウンロード 作業ホスト (Ubuntu Server 12.10) に qemu-kvm をインストール freebsd9.img として qcow2 イメージを作成します。\n% kvm-img create -f qcow2 freebsd9.img 8G 作成したイメージファイルに FreeBSD 9.1 をインストールします。\n% kvm -m 256 -cdrom ./FreeBSD-9.1-RELEASE-amd64-disc1.iso \\ -drive file=./freebsd9.img -boot d -net nic -net user -nographic -vnc :10 VNC のディスプレイ番号は空いているものを使ってください。空いていれば他でも構い ません。\nVNC viewer をインストールし localhost:10 に接続します。ここでは作業ホストに X Window System が入っていることを前提に書いていますが、Non-X な方は他のホストか らその他の VNC ソフトウェアを使ってアクセスしてもらっても構わないです。\n% sudo apt-get update % sudo apt-get gvncviewer %gvncviewer localhost:10 インストーラが起動しているのでインストール行なってください。気をつける点として は一つ。src をインストールしてください。後に virtio をインストールするのに必要 になってくるからです。kmod ファイルをコンパイルしてインストールすることになります。\nインストールが終わったら今度は仮想マシンを HDD から起動します。\n% kvm -m 256 -drive file=./freebsd9.img -boot c -net nic -net user \\ -nographic -vnc :10 VNC で再度接続し仮想マシン上で emulators/virtio-kmod をインストールします。\nfreebsd9% cd /usr/ports/emulators/virtio-kmod/ freebsd9% su freebsd9# make install clean 次に virtio ドライバを扱うように HDD, NIC ドライバ周りの設定を変更します。 virtio のインストールが終わった後に「こうしろ」とメッセージが出てきますので それを参考に行います。一部、僕の環境ではそのままではダメだったので修正して 使いました。\nインストールした virtio を起動時に読み込むために下記を追記します。\nfreebsd9# vi /boot/loader.conf virtio_load=\u0026quot;YES\u0026quot; virtio_pci_load=\u0026quot;YES\u0026quot; virtio_blk_load=\u0026quot;YES\u0026quot; if_vtnet_load=\u0026quot;YES\u0026quot; virtio_balloon_load=\u0026quot;YES\u0026quot; HDD ドライバを virtio を使うように変更します。これによってデバイス名が変わって くるので /etc/fstab を編集します。\nfreebsd9# sed -i.bak -Ee 's|/dev/ada?|/dev/vtbd|' /etc/fstab NIC も virtio を使います。僕の環境では ifconfig_re0 でした。これを vtnet0 に変 更します。\nfreebsd9# cat /etc/rc.conf ifconfig_vtnet0=\u0026quot;DHCP\u0026quot; ..\u0026lt;snip\u0026gt;.. qemu の起動方法、もしくはデフォルトのハードウェア定義によって re0 は変わってく るかもしれません。適宜変更します。\n仮想マシンをシャットダウンします。\nfreebsd9# shutdown -p now 完成したイメージファイル freebsd9.img を openstack 環境に転送し (openstack 環 境で作業している方は必要無いです) glance に登録します。\n環境変数諸々を揃えて\u0026hellip;\n% glance add name=\u0026quot;FreeBSD 9\u0026quot; is_public=true container_format=ovf disk_format=qcow2 \u0026lt; freebsd9.img % glance image-list +--------------------------------------+------------------------+-------------+------------------+-------------+--------+ | ID | Name | Disk Format | Container Format | Size | Status | +--------------------------------------+------------------------+-------------+------------------+-------------+--------+ | 1af6f41b-1048-4d78-9715-87935c0bc6ae | FreeBSD 9 | qcow2 | ovf | 4305584128 | active | +--------------------------------------+------------------------+-------------+------------------+-------------+--------+ 以上です。\nまとめ FreeBSD は仕事場でもプライベートでもまだまだ現役だし OpenStack で扱えるように することは僕にとってとても重要でした。なので満足ｗ FreeBSD 8 系では同じ手順で 扱えるらしいのだけど、それ以前のバージョンになるとどうか\u0026hellip; virtio がポイント なのと、KVM とゲスト OS バージョンって相性がめちゃ有るのでここもポイントになり そう。ハイパーバイザに VMWare も使えるらしいので今度やってみるかな。VMWare で も相性はあるけど、KVM ほどシビアにならなくていい印象がある。FreeBSD 3系でも頑 張れば動いたし。\nあとは cloud-init 。まだ開発途中だそうです。なので metadata サーバにアクセスし て、色んな事しようと思ってもまだ難しい。\nこの年末に Windows も OpenStack に乗せてみたので、そちらの記事も時間があったら 載せようっと。\n2013/01/02 追記 FreeBSD 8.3 で試してみましたが、全く同じ手順で起動してくれました。ただ、 emulators/virtio-kmod が 8.2 or 9 に対応しているとあったので Makefile を修正し て virtio-kmod をインストールしています。今のところ何も問題出ていません。\n# diff -u /usr/ports/emulators/virtio-kmod/Makefile.org /usr/ports/emulators/virtio-kmod/Makefile --- /usr/ports/emulators/virtio-kmod/Makefile.org 2013-01-02 06:02:48.000000000 +0900 +++ /usr/ports/emulators/virtio-kmod/Makefile 2013-01-02 06:02:59.000000000 +0900 @@ -28,7 +28,7 @@ .include \u0026lt;bsd.port.pre.mk\u0026gt; -.if ${OSREL} != \u0026#34;8.2\u0026#34; \u0026amp;\u0026amp; ${OSREL} != \u0026#34;9.0\u0026#34; +.if ${OSREL} != \u0026#34;8.3\u0026#34; \u0026amp;\u0026amp; ${OSREL} != \u0026#34;9.0\u0026#34; IGNORE=not supported $${OSREL} (${OSREL}) .endif ","permalink":"https://jedipunkz.github.io/post/2013/01/01/freebsd-on-openstack/","summary":"\u003cp\u003eFreeBSD を OpenStack で管理したいなぁと思って自宅に OpenStack 環境作ってました。\u003c/p\u003e\n\u003cp\u003eお正月なのに\u0026hellip;\u003c/p\u003e\n\u003cp\u003e使ったのは folsom ベースの OpenStack (nova-network) と FreeBSD 9.1 です。8 系\nの FreeBSD でも大体同じ作業で実現出来るぽいです。あと nova-network でって書い\nたのは自宅に quantum だと少し厳しいからです。FlatDHCPManager が調度良かった。\u003c/p\u003e\n\u003cp\u003e今回のポイントは FreeBSD の HDD, NIC のドライバに virtio を使うように修正する\nところです。OpenStack (KVM) は virtio 前提なので、そうせざるを得なかったです。\u003c/p\u003e\n\u003ch2 id=\"今回使ったソフトウェア\"\u003e今回使ったソフトウェア\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eOpenStack Folsom (nova-network)\u003c/li\u003e\n\u003cli\u003eUbuntu Server 12.10\u003c/li\u003e\n\u003cli\u003eFreeBSD 9.1 amd64\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"作業方法\"\u003e作業方法\u003c/h2\u003e\n\u003cp\u003e準備としてこれらが必要になります。事前に行なってください。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFreeBSD-9.1-RELEASE-amd64-disc1.iso ダウンロード\u003c/li\u003e\n\u003cli\u003e作業ホスト (Ubuntu Server 12.10) に qemu-kvm をインストール\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003efreebsd9.img として qcow2 イメージを作成します。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e% kvm-img create -f qcow2 freebsd9.img 8G\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e作成したイメージファイルに FreeBSD 9.1 をインストールします。\u003c/p\u003e","title":"FreeBSD on OpenStack"},{"content":"今日は \u0026ldquo;OpenStack Advent Calendar 2012 JP\u0026rdquo; というイベントのために記事を書きた いと思います。Advent Calendar とはキリスト生誕を祝うため 12/25 まで毎日誰かがブログ 等で特定の話題について述べるもの、らしいです。CloudStack さん, Eucalyptus さん も今年はやっているそうですね。\nイベントサイト : http://atnd.org/events/34389\nでは早速！(ただ..CloudStack の Advent Calendar とネタがかぶり気味です..。)\n御存知の通り OpenStack は API を提供していてユーザがコードを書くことで OpenStack のコマンド・Horizon で出来ることは全て可能です。API を叩くのに幾つか フレームワークが存在します。\nfog libcloud deltacloud などです。\nここでは内部で fog を使っている knife-openstack を利用して API に触れてみよう かと思います。API を叩くことを想像してもらって、インフラエンジニアの仕事内容の 変化まで述べられたらいいなぁと思っています。\nOpenStack 環境の用意 予め OpenStack 環境は揃っているものとしますです。お持ちでなければ\nhttp://jedipunkz.github.com/blog/2012/11/10/openstack-folsom-install/\nこの記事を参考に環境を作ってみて下さい。あ、devstack でも大丈夫です。\nchef, knife-openstack の用意 chef, knife-openstack を入れるのは OpenStack 環境でも、別のノードでも構いません。\nchef が確か 1.9.2 ベースが推奨だったので今回は 1.9.2-p320 使います。 ruby は rbenv で入れるのがオススメです。knife-openstack, chef のインストールは\u0026hellip;\n% sudo apt-get install libreadline-dev libxslt1-dev libxml2-dev % gem install chef --no-rdoc --no-ri % gem install knife-openstack --no-rdoc --no-ri % rbenv rehash # rbenv を使っている際に実行.. 次に knife.rb を用意します。情報として下記を\nOS_USERNAME : :openstack_username OS_PASSWORD : :openstack_password OS_AUTH_URL : :openstack_auth_url OS_TENANT_NAME : :openstack_tenant 追加します。例として下記を参考にしてください。\n% mkdir .chef % ${EDITOR} .chef/knife.rb knife[:openstack_username] = \u0026quot;demo\u0026quot; knife[:openstack_password] = \u0026quot;demo\u0026quot; knife[:openstack_auth_url] = \u0026quot;http://172.16.1.11:5000/v2.0/tokens\u0026quot; knife[:openstack_tenant] = \u0026quot;service\u0026quot; ここで注意なのが OS_AUTH_URL がいつもコマンドラインで扱うものと違い /tokens が 付いています。fog を直に扱う時も同じですがこれが必要です。\nssh keypair の用意 ssh keypair が必要になってくるので用意します。\n% nova keypair-add testkey01 \u0026gt; testkey01 knife-openstack の操作方法 いよいよ knife-openstack を使って OpenStack を操作してみましょう。\nまずは flavor のリストを取得します。\n% knife openstack flavor list ID Name Virtual CPUs RAM Disk 1 m1.tiny 1 512 MB 0 GB 2 m1.small 1 2048 MB 20 GB 3 m1.medium 2 4096 MB 40 GB 4 m1.large 4 8192 MB 80 GB 5 m1.xlarge 8 16384 MB 160 GB image リストを取得します。id が必要になります。\n% knife openstack image list ID Name 436deba5-8fab-4bb7-9205-41e33fe22744 Cirros 0.3.0 x86_64 VM を生成してみましょー。\n% knife openstack server create -f 1 -I 436deba5-8fab-4bb7-9205-41e33fe22744 -S testkey01 -N knifetest01 Instance Name: knifetest01 Instance ID: 1e20850d-9572-46c8-a41e-fdf56f4f65a7 SSH Keypair: testkey Waiting for server............ Flavor: 1 Image: 436deba5-8fab-4bb7-9205-41e33fe22744 出来たかどうか、チェック。\n% knife openstack server list Instance ID Name Public IP Private IP Flavor Image Keypair State 1e20850d-9572-46c8-a41e-fdf56f4f65a7 knifetest01 1 436deba5-8fab-4bb7-9205-41e33fe22744 hogehoge active できました。逆に VM を削除するには\n% knife openstack server delete 1e20850d-9572-46c8-a41e-fdf56f4f65a7 Instance ID: 1e20850d-9572-46c8-a41e-fdf56f4f65a7 Instance Name: knifetest01 Flavor: 1 Image: 436deba5-8fab-4bb7-9205-41e33fe22744 Do you really want to delete this server? (Y/N) y WARNING: Deleted server 1e20850d-9572-46c8-a41e-fdf56f4f65a7 WARNING: Corresponding node and client for the 1e20850d-9572-46c8-a41e-fdf56f4f65a7 server were not deleted and remain registered with the Chef Server です。\n残念なところとしては knife-openstack 自体はまだまだ機能が充実していません。VM の基本的な操作くらいしか出来ないので、追加実装したいという方がいらっしゃいまし たら Pull リクエスト送ると良いのではないでしょうか。\nknife-openstack 公式サイト : https://github.com/opscode/knife-openstack\nまぁ、ここまで書いてアレですが.. 気がついた方もいらっしゃると思います。 knife-openstack で出来ることは openstack コマンド群で全て出来るのであまり意味 はないですよね。ただ fog を使って API を叩くことを想像して欲しくて..( -_- )\nFog 単体で操作してみる 今日はまだまだ書ける！\nknife-openstack では基本的な操作しか出来ませんでしたが fog はより多くの機能を実装しています。(2012/12/08 現在 quantum 周りの開発は未 完成らしいです。floating-ip 周りがぁ。誰かコミットして。)\nfog 単体で OpenStack API を操作するには、下記の通り実行します。fog をインストー ルし\u0026hellip;\n% gem install fog --no-rdoc --no-ri % rbenv rehash 環境変数を入力し\u0026hellip; (情報は例です)\n% cat env export OS_TENANT_NAME=service export OS_USERNAME=demo export OS_PASSWORD=demo export OS_AUTH_URL=\u0026quot;http://172.16.1.11:5000/v2.0/\u0026quot; export OS_AUTH_URL_FOG=\u0026quot;http://172.16.1.11:5000/v2.0/tokens\u0026quot; % source env コードを下記のように記述すると VM の生成が行えます。\n#!/usr/bin/env ruby require \u0026#39;fog\u0026#39; require \u0026#39;pp\u0026#39; conn = Fog::Compute.new({ :provider =\u0026gt; \u0026#39;OpenStack\u0026#39;, :openstack_api_key =\u0026gt; ENV[\u0026#39;OS_PASSWORD\u0026#39;], :openstack_username =\u0026gt; ENV[\u0026#34;OS_USERNAME\u0026#34;], :openstack_auth_url =\u0026gt; ENV[\u0026#34;OS_AUTH_URL_FOG\u0026#34;], :openstack_tenant =\u0026gt; ENV[\u0026#34;OS_TENANT_NAME\u0026#34;] }) flavor = conn.flavors.find { |f| f.name == \u0026#39;m1.tiny\u0026#39; } image_name = \u0026#39;Cirros 0.3.0 x86_64\u0026#39; image = conn.images.find { |i| i.name == image_name } puts \u0026#34;#{\u0026#39;Creating server\u0026#39;} from image #{image.name}...\u0026#34; server = conn.servers.create :name =\u0026gt; \u0026#34;fogvm-#{Time.now.strftime \u0026#39;%Y%m%d-%H%M%S\u0026#39;}\u0026#34;, :image_ref =\u0026gt; image.id, :flavor_ref =\u0026gt; flavor.id, :key_name =\u0026gt; \u0026#39;testkey01\u0026#39; server.wait_for { ready? } 実行 !\n% ruby \u0026lt;CODENAME\u0026gt;.rb Creating server from image Cirros 0.3.0 x86_64... % 出来ました。VM が生成されたか先ほどの knife-openstack で確認してみましょう。\n% knife openstack server list Instance ID Name Public IP Private IP Flavor Image Keypair State 1e20850d-9572-46c8-a41e-fdf56f4f65a7 knifetest01 1 436deba5-8fab-4bb7-9205-41e33fe22744 hogehoge active f5c314a3-e32f-498f-984f-b79078d76a5a fogvm-20121207-104743 1 aedef2a1-f820-43a6-96cd-f5361d27df3f testkey01 active まとめと 考察 今回は API をみんな叩いてるねん！ってことに気がついて欲しくて、こんな記事を書 いてみました。実は OpenStack のコマンドも \u0026ndash;debug を付けて (Quantum だけは -v) 実行すると OpenStack の API を叩いているメッセージが出力されると OpenStack ユー ザ会の方から聞きました。是非やっていてください。\n% nova --debug list % quantum -v net-list API を実装するのか、ツールを実装するのか？といった話題が以前の OpenStack Summit で常に話題になっていたらしいですが、最近は API で決まり、といったと ころでしょうか。コードを書いてインラフを定義する時代に突入です。ちなみに fog は AWS も扱えます。また Opscode Chef や Puppet, JuJu 等のデプロイフレームワー クを使えば VM 上のサービス構築もコードを書くことで出来ます。また、以前ブログに 書いたのですが OpenFlow といった技術を使うとネットワークをコードを書くことで設 計出来る、かもしれない。\nhttp://jedipunkz.github.com/blog/2012/11/21/openflow-trema-handson-report/\nつまり、コードを書くことで\nサーバ構築 ネットワーク構築 サービス構築 を一貫して行えることになります。\nインフラエンジニアの僕としては時代の変化に着いて行かねば！という焦りで一杯です。 もちろんレガシなインフラエンジニアも生き残るのでしょうが、働く場所が限られてき そう。より高度な(低レイヤからの深い？)技術を有している人が限定された場所で成果 を上げていく印象。僕らの様な一般的なインフラエンジニアは OpenStack, AWS, HP Cloud, CloudStack の様なクラウドインフラを相手にコードを書く、もしくは抽象化さ れた技術をより扱いやすいソフトウェアを介して構築・管理していくことになるのでしょ うか。これから覚えることは山積みですが、楽しい時代です。\n","permalink":"https://jedipunkz.github.io/post/2012/12/08/knife-fog-openstack-api/","summary":"\u003cp\u003e今日は \u0026ldquo;OpenStack Advent Calendar 2012 JP\u0026rdquo; というイベントのために記事を書きた\nいと思います。Advent Calendar とはキリスト生誕を祝うため 12/25 まで毎日誰かがブログ\n等で特定の話題について述べるもの、らしいです。CloudStack さん, Eucalyptus さん\nも今年はやっているそうですね。\u003c/p\u003e\n\u003cp\u003eイベントサイト : \u003ca href=\"http://atnd.org/events/34389\"\u003ehttp://atnd.org/events/34389\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eでは早速！(ただ..CloudStack の Advent Calendar とネタがかぶり気味です..。)\u003c/p\u003e\n\u003cp\u003e御存知の通り OpenStack は API を提供していてユーザがコードを書くことで\nOpenStack のコマンド・Horizon で出来ることは全て可能です。API を叩くのに幾つか\nフレームワークが存在します。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003efog\u003c/li\u003e\n\u003cli\u003elibcloud\u003c/li\u003e\n\u003cli\u003edeltacloud\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eなどです。\u003c/p\u003e\n\u003cp\u003eここでは内部で fog を使っている knife-openstack を利用して API に触れてみよう\nかと思います。API を叩くことを想像してもらって、インフラエンジニアの仕事内容の\n変化まで述べられたらいいなぁと思っています。\u003c/p\u003e\n\u003ch2 id=\"openstack-環境の用意\"\u003eOpenStack 環境の用意\u003c/h2\u003e\n\u003cp\u003e予め OpenStack 環境は揃っているものとしますです。お持ちでなければ\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://jedipunkz.github.com/blog/2012/11/10/openstack-folsom-install/\"\u003ehttp://jedipunkz.github.com/blog/2012/11/10/openstack-folsom-install/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eこの記事を参考に環境を作ってみて下さい。あ、devstack でも大丈夫です。\u003c/p\u003e\n\u003ch2 id=\"chef-knife-openstack-の用意\"\u003echef, knife-openstack の用意\u003c/h2\u003e\n\u003cp\u003echef, knife-openstack を入れるのは OpenStack 環境でも、別のノードでも構いません。\u003c/p\u003e\n\u003cp\u003echef が確か 1.9.2 ベースが推奨だったので今回は 1.9.2-p320 使います。\nruby は rbenv で入れるのがオススメです。knife-openstack, chef のインストールは\u0026hellip;\u003c/p\u003e","title":"OpenStack API を理解しインフラエンジニアの仕事の変化を感じる"},{"content":"InternetWeek2012 で開かれた \u0026ldquo;OpenFlow Trema ハンズオン\u0026rdquo; に参加してきました。\n講師 : Trema 開発チーム 鈴木一哉さま, 高宮安仁さま 開催日 : 2012年11月21日 OpenStack の Quantum Plugin として Trema が扱えるという話だったので興味を持っ たのがきっかけです。また Ruby で簡潔にネットワークをコード化出来る、という点も 個人的に非常に興味を持ちました。OpenStack, CloudStack 等のクラウド管理ソフトウェ アが提供する API といい、Opscode Chef, Puppet 等のインフラソフトウェア構築フレー ムワークといい、この OpenFlow もインフラを形成する技術を抽象化し、技術者がコー ドを書くことでインフラ構築を行える、という点ではイマドキだなと思います。\nGoogle は既にデータセンター間の通信を 100% 、OpenFlow の仕様に沿った機器・ソフ トウェアをを独自に実装しさばいているそうですし、我々が利用する日も近いと想像し ます。\nOpenFlow のモチベーション OpenFlow の登場には理由が幾つかあって、既存のネットワークの抱えている下記の幾 つかの問題を解決するためです。\n装置仕様の肥大化 多様なプロトコルが標準化 装置のコスト増大 ある意味、自律したシステムが招く複雑さ 一方、OpenFlow を利用すると..\nコモディティ化された HW の利用が可能 OpenFlow はコントローラ (神) が集中管理するので楽な場合もある ネットワーク運用の自動化が図れる アプリケーションに合わせた最適化 柔軟な自己修復 等のメリットが。\nOpenFlow と Trema とは? OpenFlow は \u0026lsquo;OpenFlow コントローラ\u0026rsquo;, \u0026lsquo;OpenFlow スイッチ\u0026rsquo; から成る。OpenFlow コ ントローラと OpenFlow スイッチの間の通信は OpenFlow プロトコルでされる。今日の 話題 Trema はこの..\nOpenFlow コントローラのフレームワーク エミュレータ trema コマンド のセットである。自宅の PC 一台で OpenFlow プログラムが行え、エミュレーションも 行える手軽さ、また Ruby による簡潔な記述が可能でプログラミング初心者でも扱いや すい、という趣味ユーザにはもってのほかだ。\nHellow, Trema ! 早速 プログラミングの初歩、Hello World から。\nコード hello-trema.rb は\nclass HelloTrema \u0026lt; Controller def start puts \u0026quot;Hello, Trema!\u0026quot; end end 実行\u0026hellip;\n% trema run hello-trema.rb Hello, Trema! Trema が提供する Controller クラスを \u0026lsquo;継承\u0026rsquo; し HelloTrema クラスを定義した。 Controller クラスには幾つものハンドラが用意されていて start ハンドラもその一つ。 他にも色んなメソッドが用意されている。詳しくは後ほど。\nTrema によるスイッチの起動 次にスイッチを起動してみる。Trema は ruby の DSL でコンフィギュレーションを定 義出来る。hello-switch.conf として下記の内容\u0026hellip;\nvswitch { dpid \u0026quot;0xabc\u0026quot; } vswitch { dpid \u0026quot;0x1\u0026quot; } vswitch { dpid \u0026quot;0x2\u0026quot; } hello-switch.rb として\nclass HelloSwitch \u0026lt; Controller def switch_ready dpid puts \u0026quot;Hello #{ dpid.to_hex }!\u0026quot; end def switch_disconnected dpid puts \u0026quot;Killed ! :D #{ dpid.to_hex }!\u0026quot; end end 実行すると\n% trema run hello-switch.rb -c hello-switch.conf Hello 0xabc! Hello 0x1! Hello 0x2! となる。スイッチを3つ起動したわけだ。switch_ready とは Trema が提供するコント ローラで定義された \u0026ldquo;スイッチが稼働した時に実行されるハンドラ\u0026rdquo; だ。スイッチが起 動したため \u0026ldquo;Hello ..\u0026rdquo; なるメッセージが出力された、と理解すればいい。\nまたこの起動中に\n% trema kill 0x1 Killed ! :D 0x1! と実行することでスイッチを停止出来る。この際 switch_disconnected ハンドラが実 行され上記のメッセージを出力したというわけだ。また逆に再稼働させるには trema up コマンドを用いる。\nその他のハンドラ一覧 紹介した start, switch_ready 等のハンドラ以外にも下記のモノがある。\nstart switch_ready switch_disconnected packet_in flow_removed port_status openflow_error features_reply stats_reply barrier_reply get_config_reply queue_get_config_reply vendor ドキュメントは\nhttp://rubydoc.info/github/trema/trema/master/frames にあるので参照すると良い。\nL2 スイッチの実装 OpenFlow はネットワーク機器を実装出来るモノなので、ここで L2 スイッチを実装し てみたい。L2 スイッチの動作は\n既に ARP テーブルを持っていればパケットを受け流す 自分の ARP テーブルで管理されていないパケットは学習してからパケットを受け流 す が基本だ。これを実装する。\nl2-switch.conf として\nvswitch { dpid \u0026quot;0xabc\u0026quot; } vhost (\u0026quot;host1\u0026quot;) { ip \u0026quot;192.168.0.1\u0026quot; netmask \u0026quot;255.255.0.0\u0026quot; mac \u0026quot;00:00:00:01:00:01\u0026quot; } vhost (\u0026quot;host2\u0026quot;) { ip \u0026quot;192.168.0.2\u0026quot; netmask \u0026quot;255.255.0.0\u0026quot; mac \u0026quot;00:00:00:01:00:02\u0026quot; } link \u0026quot;0xabc\u0026quot;, \u0026quot;host1\u0026quot; link \u0026quot;0xabc\u0026quot;, \u0026quot;host2\u0026quot; ここでは detapath_id \u0026ldquo;0xabc\u0026rdquo; なる仮想スイッチを一つ定義し、サーバホスト host1, host2 を定義した。それぞれで IP アドレス・MAC アドレスを定義している。また link により仮想スイッチとサーバホストの I/F を接続している。\nいよいよ L2 スイッチのコード。l2-switch.rb として、下記を記述する。\nrequire \u0026quot;fdb\u0026quot; class LearningSwitch \u0026lt; Controller def start @fdb = FDB.new end def packet_in dpid, message @fdb.learn message.macsa, message.in_port port_no = @fdb.lookup( message.macda ) if port_no flow_mod dpid, message, port_no packet_out dpid, message, port_no else flood dpid, message end end def flow_mod dpid, message, port_no send_flow_mod_add( dpid, :match =\u0026gt; ExactMatch.from( message ), :actions =\u0026gt; ActionOutput.new( port_no ) ) end def flow_mod dpid, message, port_no send_flow_mod_add( dpid, :match =\u0026gt; ExactMatch.from( message ), :actions =\u0026gt; ActionOutput.new( port_no ) ) end def packet_out dpid, message, port_no send_packet_out( dpid, :packet_in =\u0026gt; message, :actions =\u0026gt; ActionOutput.new( port_no ) ) end def flood dpid, message packet_out dpid, message, OFPP_FLOOD end end まず実行してみる。\n% trema run l2-switch.rb -c l2-switch.conf 異なる shell でパケットの送信と状態表示を行う。\n% trema send_packet --source host1 --dest host2 % trema show_stats host1 ip_dst,tp_dst,ip_src,tp_src,n_pkts,n_octets 192.168.0.2,1,192.168.0.1,1,1,50 % trema send_packet --source host1 --dest host2 % trema send_packet --source host2 --dest host1 % trema dump_flows 0xabc NXST_FLOW reply (xid=0x4): cookie=0x2, duration=67.343s, table=0, n_packets=0, n_bytes=0, priority=65535,udp,in_port=1,vlan_tci=0x0000,dl_src=00:00:00:01:00:02,dl_dst=00:00:00:01:00:01,nw_src=192.168.0.2,nw_dst=192.168.0.1,nw_tos=0,tp_src=1,tp_dst=1 actions=output:2 cookie=0x1, duration=70.339s, table=0, n_packets=0, n_bytes=0,\tpriority=65535,udp,in_port=2,vlan_tci=0x0000,dl_src=00:00:00:01:00:01,dl_dst=00:00:00:01:00:01,nw_src=192.168.0.1,nw_dst=192.168.0.1,nw_tos=0,tp_src=1,tp_dst=1 actions=output:2 host1 から host2 に対して trema send_packet で通信を行い、host1 の状態を表示し た。また交互に通信をさせフローをダンプしたのが上記だ。\n一番基本なところらしいので、コードを詳しく解説。l2-switch.rb の下記の部分。\ndef packet_in dpid, message # ---(1) @fdb.learn message.macsa, message.in_port # ---(2) port_no = @fdb.lookup( message.macda ) # ---(3) if port_no # ---(4) flow_mod dpid, message, port_no packet_out dpid, message, port_no else # ---(5) flood dpid, message end end (1) では packet_in ハンドラを利用した。(2) で fdb (floating DB) の learn メソッ ドで port と mac アドレスの学習を行った。(3) で @fdb にすでに mac アドレスの記 述があれば port_no に値が入る。値が入っていれば (4) を。スイッチのフローテーブ\nルを更新しパケットを packet_out する。入っていなければ (5)を実行しパケットを flood する。\nflow_mod, packet_out, flood はこのプログラム内で定義しているプライベートなメソッ ドだ。\nこの時のソフトウェア構成 このコードを動作させた際に実行したホスト上のプロセスを見てみた。\novs-openflowd phost switch_manager が居た。siwtch_manager が 6633 番ポート待ち受けているコントローラ自身だろう。 phost は仮想サーバで ovs-openflowd (OpenvSwitch) が仮想スイッチとして動作して いると想像出来る。またこの時にネットワークインターフェースが\ntrema0-0 trema0-1 trema1-0 trema1-1 と起動していた。これは 0xabc スイッチと host1, host2 との接続で使われている I/F だろう。\nまとめ Ruby で記述出来るので技術者にとって Trema は優しい。また簡潔な記述が行えるとい うのも Ruby ならではだろう。github.com には高度な利用サンプルが幾つか掲載され ている。\nhttps://github.com/trema/apps また簡単なサンプル集ということであれば\nhttps://github.com/trema/trema/tree/develop/src/examples がうってつけのリファレンスになる。\n冒頭でも書いたがスイッチのエミュレータが同封されているので、技術者はすぐに開発 に入ることが出来る。\nOpenStack は仮想マシンを管理・構成するため API を提供し、そしてネットワークを管理・ 構成するため OpenFlow がある。OpenStack の API を叩くコードを書くのと同様に OpenFlow を Trema という OpenFlow コントローラフレームワークを用いてコードを書 くことが出来る。インフラエンジニアの仕事の範囲は確実にここ数年で変化し、その変 化に追いつくには \u0026ldquo;コードを書く\u0026rdquo; ことを念頭に置かなくてはならないだろう。\n最後に、この機会を与えてくださった 鈴木様・高宮様にお礼を申し上げます。\n","permalink":"https://jedipunkz.github.io/post/2012/11/21/openflow-trema-handson-report/","summary":"\u003cp\u003eInternetWeek2012 で開かれた \u0026ldquo;OpenFlow Trema ハンズオン\u0026rdquo; に参加してきました。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e講師   : Trema 開発チーム 鈴木一哉さま, 高宮安仁さま\n開催日 : 2012年11月21日\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOpenStack の Quantum Plugin として Trema が扱えるという話だったので興味を持っ\nたのがきっかけです。また Ruby で簡潔にネットワークをコード化出来る、という点も\n個人的に非常に興味を持ちました。OpenStack, CloudStack 等のクラウド管理ソフトウェ\nアが提供する API といい、Opscode Chef, Puppet 等のインフラソフトウェア構築フレー\nムワークといい、この OpenFlow もインフラを形成する技術を抽象化し、技術者がコー\nドを書くことでインフラ構築を行える、という点ではイマドキだなと思います。\u003c/p\u003e\n\u003cp\u003eGoogle は既にデータセンター間の通信を 100% 、OpenFlow の仕様に沿った機器・ソフ\nトウェアをを独自に実装しさばいているそうですし、我々が利用する日も近いと想像し\nます。\u003c/p\u003e\n\u003ch2 id=\"openflow-のモチベーション\"\u003eOpenFlow のモチベーション\u003c/h2\u003e\n\u003cp\u003eOpenFlow の登場には理由が幾つかあって、既存のネットワークの抱えている下記の幾\nつかの問題を解決するためです。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e装置仕様の肥大化\u003c/li\u003e\n\u003cli\u003e多様なプロトコルが標準化\u003c/li\u003e\n\u003cli\u003e装置のコスト増大\u003c/li\u003e\n\u003cli\u003eある意味、自律したシステムが招く複雑さ\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e一方、OpenFlow を利用すると..\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eコモディティ化された HW の利用が可能\u003c/li\u003e\n\u003cli\u003eOpenFlow はコントローラ (神) が集中管理するので楽な場合もある\u003c/li\u003e\n\u003cli\u003eネットワーク運用の自動化が図れる\u003c/li\u003e\n\u003cli\u003eアプリケーションに合わせた最適化\u003c/li\u003e\n\u003cli\u003e柔軟な自己修復\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e等のメリットが。\u003c/p\u003e\n\u003ch2 id=\"openflow-と-trema-とは\"\u003eOpenFlow と Trema とは?\u003c/h2\u003e\n\u003cp\u003eOpenFlow は \u0026lsquo;OpenFlow コントローラ\u0026rsquo;, \u0026lsquo;OpenFlow スイッチ\u0026rsquo; から成る。OpenFlow コ\nントローラと OpenFlow スイッチの間の通信は OpenFlow プロトコルでされる。今日の\n話題 Trema はこの..\u003c/p\u003e","title":"OpenFlow Trema ハンズオン参加レポート"},{"content":"※2012/12/04 に内容を修正しました。Network Node を切り出すよう修正。 ※213/01/09 に内容を修正しました。パラメータ修正です。\nOpenStack の Folsom リリースからメインコンポーネントの仲間入りした Quantum を 理解するのに時間を要してしまったのだけど、もう数十回とインストールを繰り返して だいぶ理解出来てきました。手作業でインストールしてると日が暮れてしまうのでと思っ て自分用に bash で構築スクリプトを作ったのだけど、それを公開しようと思います。\nOpenStack Folsom の構築に四苦八苦している方に使ってもらえたらと思ってます。\nhttp://jedipunkz.github.com/openstack_folsom_deploy/\nchef や puppet, juju などデプロイのフレームワークは今流行です。ただどれも環境 を予め構築しなくてはいけないので、誰でもすぐに使える環境ってことで bash スクリ プトで書いています。時間があれば是非 chef の cookbook を書いていきたいです。と いうか予定です。でも、もうすでに opscode 等は書き始めています。(汗\nではでは、紹介を始めます。\n前提の構成 management segment 172.16.1.0/24 +--------------------------------------------+------------------+----------------- | | | | | | | eth2 172.16.1.13 | eth2 172.16.1.12 | eth2 172.24.1.11 +------------+ +-----------+ +------------+ | | eth1 ------------------- eth1 | | | | | network | vlan/gre seg = 172.24.17.0/24 | compute | | controller | | node | data segment = 172.16.2.0/24 | node | | node | +------------+ 172.16.2.13 172.16.2.12 +-----------+ +------------+ | eth0 10.200.8.13 | eth0 10.200.8.11 | | | | +--------------------------------------------+------------------+----------------- | public segment 10.200.8.0/24 | | 10.200.8.1 +-----------+ | GW Router |-\u0026gt; The Internet +-----------+ Quantum は、\nAPI network Public network Data network Management Network の4つのネットワークセグメントを前提に設計されています。4つ用意するのが大変なの で今回は\nAPI / Management network Public Network Data Network と API と Management を兼務させた3つのネットワークセグメントを前提に話続けます。 もちろん Management を追加で切り出しても構わないです。NIC を追加するだけで OK 。\n詳しくは、\nhttp://docs.openstack.org/trunk/openstack-network/admin/content/connectivity.html\nに掲載されています。\nまた今回は public network に 10.200.8/24 を使ってます。サービス環境ではここが グローバルセグメントになります。\nインストール対象 OS は Ubuntu Server 12.04 LTS もしくは 12.10 で動作します。\n3つの NIC があるマシンを1台、NIC 2つのマシンを2台を用意します。 controller node x 1台 network node x 1台 + comupte node x n台 の構成で す。\ncontroller : glance, keystone, mysql, horizon, quantum server が稼働する Node network : quantum dhcp agent, quantum l3 agent, quantum openvswitch agent が稼働する Node compute : nova, quntum openvswitch agent が稼働する Node 上記の構成では下記の通り /etc/network/interface を設定します。controller 側の 設定です。\nauto lo iface lo inet loopback auto eth0 iface eth0 inet static address 10.200.8.11 netmask 255.255.255.0 dns-nameservers 8.8.8.8 8.8.4.4 dns-search cpi.ad.jp auto eth2 iface eth2 inet static address 172.16.1.11 netmask 255.255.255.0 gateway 172.16.1.1 Network Node は\u0026hellip;\nauto lo iface lo inet loopback auto eth0 iface eth0 inet static up ifconfig $IFACE 0.0.0.0 up up ip link set $IFACE promisc on down ip link set $IFACE promisc off down ifconfig $IFACE down address 10.200.8.21 netmask 255.255.255.0 #gateway 10.200.8.1 # dns-* options are implemented by the resolvconf package, if installed dns-nameservers 8.8.8.8 8.8.4.4 dns-search cpi.ad.jp auto eth1 iface eth1 inet static address 172.16.2.13 netmask 255.255.255.0 auto eth2 iface eth2 inet static address 172.16.1.13 netmask 255.255.255.0 gateway 172.16.1.1 dns-nameservers 8.8.8.8 8.8.4.4 compute Node は\u0026hellip;\nauto eth1 iface eth1 inet static address 172.16.2.12 netmask 255.255.255.0 auto eth2 iface eth2 inet static address 172.16.1.12 netmask 255.255.255.0 gateway 172.16.1.1 dns-nameservers 8.8.8.8 8.8.4.4 です。下記のコマンドでネットワークインターフェースを再起動してください。 ここまで用意出来たらいよいよ実行するのみです。\ncontroller% sudo /etc/init.d/networking restart network % sudo /etc/init.d/networking restart compute % sudo /etc/init.d/networking restart スクリプト実行 下記の通りスクリプトを取得して\u0026hellip;\ncontroller% git clone https://github.com/jedipunkz/openstack_folsom_deploy.git controller% cd openstack_folsom_deploy それぞれの構成に合わせて deploy.conf を修正します。上記の構成の場合下記のようになります。\nBASE_DIR=`pwd` CONTROLLER_NODE_IP='172.16.1.11' CONTROLLER_NODE_PUB_IP='10.200.8.11' NETWORK_NODE_IP='172.16.1.12' COMPUTE_NODE_IP='172.16.1.13' DATA_NIC_COMPUTE='eth1' MYSQL_PASS='secret' CINDER_VOLUME='/dev/sda6' DATA_NIC='eth1' PUBLIC_NIC='eth0' NETWORK_TYPE='gre' INT_NET_GATEWAY='172.24.17.254' INT_NET_RANGE='172.24.17.0/24' EXT_NET_GATEWAY='10.200.8.1' EXT_NET_START='10.200.8.36' EXT_NET_END='10.200.8.40' EXT_NET_RANGE='10.200.8.0/24' OS_IMAGE_URL=\u0026quot;https://launchpad.net/cirros/trunk/0.3.0/+download/cirros-0.3.0-x86_64-disk.img\u0026quot; OS_IMAGE_NAME=\u0026quot;Cirros 0.3.0 x86_64\u0026quot; 環境に合わせて設定すれば OK です。上記の前提の構成の場合このような値を入れてい きます。重要なところだけ説明すると..\nCONTROLLER_NODE_IP : Controller Node の IP アドレス NETWROK_NODE_IP : Network Node の IP アドレス COMPUTE_NODE_IP : Compute Node の IP アドレス INT_NET_.. : Quantum で管理する内部ネットワーク EXT_NET-.. : Quantum で管理する外部ネットワーク です。\ndeploy.conf を修正したら、各 Node にディレクトリをコピーします。\nnetwork% scp -r \u0026lt;CONTROLLER_NODE_IP\u0026gt;:~/openstack_folsom_deploy . compute% scp -r \u0026lt;CONTROLLER_NODE_IP\u0026gt;:~/openstack_folsom_deploy . いよいよ実行。それぞれの Node で順にスクリプトを実行します。\ncontroller% sudo ./deploy.sh controller quantum network % sudo ./deploy.sh network quantum compute % sudo ./deploy.sh compute quantum また構築が終了したら quantum 上にネットワークを作成します。\ncontroller% sudo ./deploy.sh create_network qunatum 完成です。http://${CONTROLLER_NODE_IP}/horizon/ にアクセスすれば管 理画面が表示されるはずです。\n更に compute node を追加したければ\u0026hellip;\ncompute02% scp -r \u0026lt;CONTROLLER_NODE_IP\u0026gt;:~/openstack_folsom_deploy . compute02% cd openstack_folsom_deploy compute02% vim deploy.conf # $COMPUTE_NODE_IP を修正する。その他はそのまま。 compute02% sudo ./deploy.sh compute quantum と deploy.conf 内 $COMPUTE_NODE_IP を更新して実行すれば OK です。\nFloating IP の利用 現在、folsom リリース版には floating ip が horizon 経由で利用できない問題があ ります。コマンドラインでは利用できるのでその方法を。\n% source $HOME/openstackrc % quantum net-list % quantun floatingip-create \u0026lt;ext_net_id\u0026gt; % quantun floatingip-list % quantum port-list % quantum floatingip-associate \u0026lt;floatingip_id\u0026gt; \u0026lt;vm_port_id\u0026gt; Quantum と I/O と..所感 http://www.readability.com/read?url=http://docs.openstack.org/trunk/openstack-network/admin/content/services.html\n最近、メーリングリストで挙がっている話題。今回の構成だと Quantum は controller 上で稼働し全ての VM がこの Quantum を利用することになります。つまり単一障害点っ ていうだけではなく I/O が集中するので負荷も上昇する。前リリース版 ESSEX の nova-network の時は追加する compute node 上全てで nova-network を稼働させ、 node が増えるにつれ I/O も拡張出来るシンプルな構成が組めるモノだったのですが、 Quantum の構成になって、そう簡単にいかなくなった。\nオールインワン構成等、ほかの構成について 2013/01/09 に nova-network にも対応しました。\nオールインワン構成や quantum に代わって nova-network を使う構成等、その他の構成構築方法 については下記のドキュメントを参考にしてください。\nhttps://github.com/jedipunkz/openstack_folsom_deploy/blob/master/README_jp.md\n","permalink":"https://jedipunkz.github.io/post/2012/11/10/openstack-folsom-install/","summary":"\u003cp\u003e※2012/12/04 に内容を修正しました。Network Node を切り出すよう修正。\n※213/01/09 に内容を修正しました。パラメータ修正です。\u003c/p\u003e\n\u003cp\u003eOpenStack の Folsom リリースからメインコンポーネントの仲間入りした Quantum を\n理解するのに時間を要してしまったのだけど、もう数十回とインストールを繰り返して\nだいぶ理解出来てきました。手作業でインストールしてると日が暮れてしまうのでと思っ\nて自分用に bash で構築スクリプトを作ったのだけど、それを公開しようと思います。\u003c/p\u003e\n\u003cp\u003eOpenStack Folsom の構築に四苦八苦している方に使ってもらえたらと思ってます。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://jedipunkz.github.com/openstack_folsom_deploy/\"\u003ehttp://jedipunkz.github.com/openstack_folsom_deploy/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003echef や puppet, juju などデプロイのフレームワークは今流行です。ただどれも環境\nを予め構築しなくてはいけないので、誰でもすぐに使える環境ってことで bash スクリ\nプトで書いています。時間があれば是非 chef の cookbook を書いていきたいです。と\nいうか予定です。でも、もうすでに opscode 等は書き始めています。(汗\u003c/p\u003e\n\u003cp\u003eではでは、紹介を始めます。\u003c/p\u003e\n\u003ch2 id=\"前提の構成\"\u003e前提の構成\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003emanagement segment 172.16.1.0/24\n+--------------------------------------------+------------------+-----------------\n|                                            |                  |\n|                                            |                  |\n| eth2 172.16.1.13                           | eth2 172.16.1.12 | eth2 172.24.1.11\n+------------+                               +-----------+      +------------+\n|            | eth1 ------------------- eth1 |           |      |            |\n|  network   | vlan/gre seg = 172.24.17.0/24 |  compute  |      | controller |\n|    node    | data segment = 172.16.2.0/24  |   node    |      |    node    |\n+------------+ 172.16.2.13       172.16.2.12 +-----------+      +------------+      \n| eth0 10.200.8.13                                              | eth0 10.200.8.11\n|                                                               |\n|                                                               |\n+--------------------------------------------+------------------+-----------------\n|       public segment 10.200.8.0/24\n|\n| 10.200.8.1\n+-----------+\n| GW Router |-\u0026gt; The Internet\n+-----------+\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eQuantum は、\u003c/p\u003e","title":"OpenStack Folsom 構築スクリプト"},{"content":"最近、OpenStack にどっぷり浸かってる @jedipunkz です。\nFolsom がリリースされて Quantum を理解するのにめちゃ苦労して楽しい真っ最中なのだけど、 今日は OpenStack の中でも最も枯れているコンポーネント Swift を使ったオブジェクトストレー ジ構築について少し書こうかなぁと思ってます。\n最近は OpenStack を構築・デプロイするのに皆、Swift 入れてないのね。仲間はずれ 感たっぷりだけど、一番安定して動くと思ってる。\nこれを読んで、自宅にオブジェクトストレージを置いちゃおぅ。\n構成は ?\u0026hellip; +--------+ | client | +--------+ | +-------------+ | swift-proxy | +-------------+ 172.16.0.10 | +-------------------+-------------------+ 172.16.0.0/24 | | | +-----------------+ +-----------------+ +-----------------+ | swift-storage01 | | swift-storage02 | | swift-storage03 | +-----------------+ +-----------------+ +-----------------+ 172.16.0.11 172.16.0.12 172.16.0.13 となる。IP アドレスは\u0026hellip;\nclient : 172.16.0.0/24 のどこか swift-proxy : 172.16.0.10 swift-storage01 : 172.16.0.11 swift-storage02 : 172.16.0.12 swift-storage03 : 172.16.0.13 これはサンプル。自宅の環境に合わせて読み替えてください。\n全て同じネットワークセグメントに。なので上の図は概念的な図です。通信の流れだけ 把握出来ればいいなぁと。向き書いてないけど..。四角で囲まれているのがノード (サー バ) です。物理サーバでも仮想マシンでも大丈夫！\nマシンの準備 Ubuntu Server 12.04 もしくは 12.10 を用意。swift-storage のマシン3台だけは /dev/sda6 など Disk デバイスを swift 用に用意してあげてください。普通にハード ディスクのパーティションを切ってあげるだけでいいです。高価な Disk を使うまでも ないので。それが分散ストレージの良いところ！ Disk ・ノードが壊れてもデータが失 われないんです！\nswift-proxy の構築 今回は簡易認証機能の tempauth を使います。Keystone を使った構成を説明しようか 迷ったのだけど、Keystone の構築だけで一つ記事が書けるくらいになるので..、諦め ました。tempauth は Swift 本体に実装されていますよ。\n構築は簡単、まず下記をコピペしてください。\napt-get update apt-get install swift python-swift apt-get install python-keystoneclient python-keystone mkdir /etc/swift chown -R swift:swift /etc/swift export PROXY_LOCAL_NET_IP=10.200.4.133 export POUND_NET=10.200.4.138 apt-get install swift-proxy memcached perl -pi -e \u0026quot;s/-l 127.0.0.1/-l $PROXY_LOCAL_NET_IP/\u0026quot; /etc/memcached.conf service memcached restart cat \u0026gt;/etc/swift/proxy-server.conf \u0026lt;\u0026lt;EOF [DEFAULT] #cert_file = /etc/swift/cert.crt #key_file = /etc/swift/cert.key bind_port = 8080 workers = 8 user = swift [pipeline:main] pipeline = healthcheck cache tempauth proxy-server [app:proxy-server] use = egg:swift#proxy allow_account_management = true account_autocreate = true [filter:tempauth] use = egg:swift#tempauth user_system_root = testpass .admin https://$PROXY_LOCAL_NET_IP:8080/v1/AUTH_system [filter:healthcheck] use = egg:swift#healthcheck [filter:cache] use = egg:swift#memcache memcache_servers = $PROXY_LOCAL_NET_IP01:11211 EOF 次に swift.conf とリング情報 (バランシングのための情報) を swift-proxy 上に用意します。これらは、 あとで各すべてのノードに配置するので重要です。これもコピペしてください。\ncat \u0026gt; /etc/swift/swift.conf \u0026lt;\u0026lt; EOF [swift-hash] # random unique string that can never change (DO NOT LOSE) swift_hash_path_suffix = `od -t x8 -N 8 -A n \u0026lt;/dev/random` EOF cd /etc/swift swift-ring-builder account.builder create 18 3 1 swift-ring-builder container.builder create 18 3 1 swift-ring-builder object.builder create 18 3 1 export STORAGE_LOCAL_NET_IP01=172.16.0.11 export STORAGE_LOCAL_NET_IP02=172.16.0.12 export STORAGE_LOCAL_NET_IP03=172.16.0.13 export ZONE01=1 export ZONE02=2 export ZONE03=3 export WEIGHT=100 export DEVICE=sda6 swift-ring-builder account.builder add z$ZONE01-$STORAGE_LOCAL_NET_IP01:6002/$DEVICE $WEIGHT swift-ring-builder container.builder add z$ZONE01-$STORAGE_LOCAL_NET_IP01:6001/$DEVICE $WEIGHT swift-ring-builder object.builder add z$ZONE01-$STORAGE_LOCAL_NET_IP01:6000/$DEVICE $WEIGHT swift-ring-builder account.builder add z$ZONE02-$STORAGE_LOCAL_NET_IP02:6002/$DEVICE $WEIGHT swift-ring-builder container.builder add z$ZONE02-$STORAGE_LOCAL_NET_IP02:6001/$DEVICE $WEIGHT swift-ring-builder object.builder add z$ZONE02-$STORAGE_LOCAL_NET_IP02:6000/$DEVICE $WEIGHT swift-ring-builder account.builder add z$ZONE03-$STORAGE_LOCAL_NET_IP03:6002/$DEVICE $WEIGHT swift-ring-builder container.builder add z$ZONE03-$STORAGE_LOCAL_NET_IP03:6001/$DEVICE $WEIGHT swift-ring-builder object.builder add z$ZONE03-$STORAGE_LOCAL_NET_IP03:6000/$DEVICE $WEIGHT swift-ring-builder account.builder swift-ring-builder container.builder swift-ring-builder object.builder swift-ring-builder account.builder rebalance swift-ring-builder container.builder rebalance swift-ring-builder object.builder rebalance で、起動。\nswift-proxy# chown -R swift:swift /etc/swift swift-proxy# swift-init proxy start おしまい。\nswift-storage 構築 swift-storage の構築は各ノードで下記の内容をコピペしてください。今回は3台分だ けど、マシンが余ってたら4台でも5台でも OK ！\napt-get update apt-get install swift python-swift mkdir /etc/swift chown -R swift:swift /etc/swift export STORAGE_LOCAL_NET_IP=172.16.0.11 apt-get install swift-account swift-container swift-object xfsprogs mkfs.xfs -i size=1024 /dev/sda6 echo \u0026quot;/dev/sda6 /srv/node/sda6 xfs noatime,nodiratime,nobarrier,logbufs=8 0 0\u0026quot; \u0026gt;\u0026gt; /etc/fstab mkdir -p /srv/node/sda6 mount /srv/node/sda6 chown -R swift:swift /srv/node cat \u0026gt;/etc/rsyncd.conf \u0026lt;\u0026lt;EOF uid = swift gid = swift log file = /var/log/rsyncd.log pid file = /var/run/rsyncd.pid address = $STORAGE_LOCAL_NET_IP [account] max connections = 2 path = /srv/node/ read only = false lock file = /var/lock/account.lock [container] max connections = 2 path = /srv/node/ read only = false lock file = /var/lock/container.lock [object] max connections = 2 path = /srv/node/ read only = false lock file = /var/lock/object.lock EOF perl -pi -e 's/RSYNC_ENABLE=false/RSYNC_ENABLE=true/' /etc/default/rsync service rsync start cat \u0026gt;/etc/swift/account-server.conf \u0026lt;\u0026lt;EOF [DEFAULT] bind_ip = $STORAGE_LOCAL_NET_IP workers = 2 [pipeline:main] pipeline = account-server [app:account-server] use = egg:swift#account [account-replicator] [account-auditor] [account-reaper] EOF cat \u0026gt;/etc/swift/container-server.conf \u0026lt;\u0026lt;EOF [DEFAULT] bind_ip = $STORAGE_LOCAL_NET_IP workers = 2 [pipeline:main] pipeline = container-server [app:container-server] use = egg:swift#container [container-replicator] [container-updater] [container-auditor] [container-sync] EOF cat \u0026gt;/etc/swift/object-server.conf \u0026lt;\u0026lt;EOF [DEFAULT] bind_ip = $STORAGE_LOCAL_NET_IP workers = 2 [pipeline:main] pipeline = object-server [app:object-server] use = egg:swift#object [object-replicator] [object-updater] [object-auditor] EOF 環境変数 ${STORAGE_LOCAL_NET_IP} を3台毎に変えて、各台で流し込んであげたら、 swift-proxy で生成した /etc/swift.conf とリング情報達を各 swift-storage に 配置します。\nswift-proxy # scp /etc/swift/swift.conf /etc/swift/*ring.gz 172.16.0.11:/tmp/ swift-proxy # scp /etc/swift/swift.conf /etc/swift/*ring.gz 172.16.0.12:/tmp/ swift-proxy # scp /etc/swift/swift.conf /etc/swift/*ring.gz 172.16.0.13:/tmp/ swift-storage01 # mv /tmp/swift.conf /tmp/*ring.gz /etc/swift/ swift-storage01 # chown -R swift:swift /etc/swift swift-storage02 # mv /tmp/swift.conf /tmp/*ring.gz /etc/swift/ swift-storage02 # chown -R swift:swift /etc/swift swift-storage03 # mv /tmp/swift.conf /tmp/*ring.gz /etc/swift/ swift-storage03 # chown -R swift:swift /etc/swift swift-storage を各台で起動します。\nswift-storage01 # swift-init all start swift-storage02 # swift-init all start swift-storage03 # swift-init all start アクセスしてみる 完成したので、swift クライアントでアクセスしてみる。\n% swift -A http://172.16.0.10:8080/auth/v1.0 -U system:root -K testpass stat Account: AUTH_system Containers: 4 Objects: 15 Bytes: 23866252 Connection: keep-alive Accept-Ranges: bytes ファイルをアップロード・ダウンロードしてみる。\n% swift -A http://172.16.0.10:8080/auth/v1.0 -U system:root -K testpass upload test /etc/hosts % swift -A http://172.16.0.10:8080/auth/v1.0 -U system:root -K testpass list % test % cd /tmp/ % swift -A http://172.16.0.10:8080/auth/v1.0 -U system:root -K testpass download test % ls /tmp/etc/hosts もちろん、HTTP な API なので curl 等の HTTP ブラウザを使ってもアクセスできる！\n% curl -k -v -H 'X-Storage-User: system:root' -H 'X-Storage-Pass: testpass' http://172.16.0.10:8080/auth/v1.0 \u0026lt; HTTP/1.1 200 OK \u0026lt; Server: nginx/1.1.19 \u0026lt; Date: Mon, 03 Sep 2012 04:58:30 GMT \u0026lt; Content-Length: 0 \u0026lt; Connection: keep-alive \u0026lt; X-Storage-Url: https://172.16.0.10:8080/v1/AUTH_system \u0026lt; X-Storage-Token: AUTH_tk8a19f76c9bce4077aee02aef76257020 \u0026lt; X-Auth-Token: AUTH_tk8a19f96c9bce4077aee02aef76257020 \u0026lt; * Connection #0 to host 172.16.0.10 left intact * Closing connection #0 * SSLv3, TLS alert, Client hello (1): 得られた X-Auth-Token, X-Storage-Url を使ってアクセスする。\n% curl -k -v -H 'X-Auth-Token: \u0026lt;token-from-x-auth-token-above\u0026gt;' \u0026lt;url-from-x-storage-url-above\u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; Server: nginx/1.1.19 \u0026lt; Date: Mon, 03 Sep 2012 04:59:47 GMT \u0026lt; Content-Type: text/plain; charset=utf-8 \u0026lt; Content-Length: 34 \u0026lt; Connection: keep-alive \u0026lt; X-Account-Object-Count: 15 \u0026lt; X-Account-Bytes-Used: 23866252 \u0026lt; X-Account-Container-Count: 4 \u0026lt; Accept-Ranges: bytes \u0026lt; test * Connection #0 to host 172.16.0.10 left intact * Closing connection #0 * SSLv3, TLS alert, Client hello (1): さっき放り込んだ \u0026rsquo;test\u0026rsquo; が swift 上にあることが確認できる。\nhttp://cyberduck.ch/ ここにある CyberDuck という GUI なツールを使ってもアクセス出来るよ！ HTTPS が必須になるので一工夫する必要があるのだけど、そのあたりは頑張ってみてください。\nオンラインでのノードの追加・削除なんてことも出来ます。次回時間があったら解説しますね。\n今回は swift-ring-builder コマンドでレプリカ数 \u0026lsquo;3\u0026rsquo; を指定したので、アップロー ドしたファイル(オブジェクトと言う) は必ず 3 個配置される。なので 1 台の swift-storage が故障しても大丈夫。ノードを増やせばストレージ全体の容量も増やせ る。また、swift-proxy は単純な HTTP なので負荷分散機・もしくはソフトウェアのロー ドバランサを入れれば swift-proxy 自体の冗長も組めるほか、ストレージ I/O の拡張 にもつながる。pound, nginx などを使って冗長組んでみてください。その時に memcached の内容は共有させてあげる必要があるので、これまた一工夫が必要なのだけ ど。時間があったら今度解説します。\n自宅でも簡単に分散オブジェクトストレージが組める swift。使わない手は無いですよぉ。\n","permalink":"https://jedipunkz.github.io/post/2012/11/04/swift-tempauth/","summary":"\u003cp\u003e最近、OpenStack にどっぷり浸かってる \u003ca href=\"https://twitter.com/jedipunkz\"\u003e@jedipunkz\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003eFolsom がリリースされて Quantum を理解するのにめちゃ苦労して楽しい真っ最中なのだけど、\n今日は OpenStack の中でも最も枯れているコンポーネント Swift を使ったオブジェクトストレー\nジ構築について少し書こうかなぁと思ってます。\u003c/p\u003e\n\u003cp\u003e最近は OpenStack を構築・デプロイするのに皆、Swift 入れてないのね。仲間はずれ\n感たっぷりだけど、一番安定して動くと思ってる。\u003c/p\u003e\n\u003cp\u003eこれを読んで、自宅にオブジェクトストレージを置いちゃおぅ。\u003c/p\u003e\n\u003ch2 id=\"構成は-\"\u003e構成は ?\u0026hellip;\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003e                        +--------+\n                        | client |\n                        +--------+\n                             |\n                      +-------------+\n                      | swift-proxy |\n                      +-------------+ 172.16.0.10\n                             |\n         +-------------------+-------------------+ 172.16.0.0/24\n         |                   |                   |\n+-----------------+ +-----------------+ +-----------------+\n| swift-storage01 | | swift-storage02 | | swift-storage03 |\n+-----------------+ +-----------------+ +-----------------+\n172.16.0.11         172.16.0.12         172.16.0.13\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eとなる。IP アドレスは\u0026hellip;\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eclient : 172.16.0.0/24 のどこか\u003c/li\u003e\n\u003cli\u003eswift-proxy : 172.16.0.10\u003c/li\u003e\n\u003cli\u003eswift-storage01 : 172.16.0.11\u003c/li\u003e\n\u003cli\u003eswift-storage02 : 172.16.0.12\u003c/li\u003e\n\u003cli\u003eswift-storage03 : 172.16.0.13\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eこれはサンプル。自宅の環境に合わせて読み替えてください。\u003c/p\u003e","title":"Swift で簡単に分散オブジェクトストレージ"},{"content":"昨日、開かれた \u0026ldquo;Opscode Chef のシークレットトレーニング\u0026rdquo; に参加してきました。\n場所はうちの会社で KDDI ウェブコミュニケーションズ。主催はクリエーションオンラ インさんでした。講師は Sean OMeara (@someara) さん。今後 Chef のトレーニングを 日本で開くため、事前に内容についてフィードバックが欲しかったそうで、オープンな レッスンではありませんでしたが、次回以降、日本でも期待できそうです。\n内容は chef の基本・メリット・考え方などを網羅した資料で1時間程進められ、その 後はハンズオンがメインでした。今日は実際にハンズオンの内容を書いていこうかと思 います。\nchef workstation 環境は揃っている前提にします。また chef server として opscode の hosted chef (opscode が提供している chef のホスティングサービス, chef-server として動作します) を使います。またターゲットホストは当日は ec2 イ ンスタンスを使いましたが、chef ワークステーションから到達できるホストであれば 何でも良いでしょう。\nまずは chef-repo のクローン。講習会で使われたものです。\ngit clone https://github.com/opscode/chef-repo-workshop-sysadmin.git chef-repo 予め cookbook が入っています。\n次に、manage.opscode.com へアクセスしアカウントを作ります。Free アカウントが誰 でも作れるようになっています。\nhttps://manage.opscode.com へアクセス -\u0026gt; Sign Up をクリック -\u0026gt; アカウント情報 を入力 -\u0026gt; submit -\u0026gt; メールにて verify -\u0026gt; 自分のアカウント名をクリック -\u0026gt; Get a new key をクリックし \u0026lt;アカウント名\u0026gt;.pem をダウンロード -\u0026gt; create a organization をクリックし Free を選択し、適当な名前で organization を作成。 validation key と knife.rb をダウンロード\nこれらで得た3つのファイル (2つの pem とknife.rb) を chef-repo/.chef/ 配下に置 きます。これで準備 OK。knife.rb を見ると、opscode の chef ホスティングにアクセ スするよう記述があります。\n% cp \u0026lt;somewhere\u0026gt;/knife.rb \u0026lt;somewhere\u0026gt;/\u0026lt;account\u0026gt;.pem \u0026lt;somewhere\u0026gt;/\u0026lt;account\u0026gt;-validation.pem chef-repo/.chef/ トレーニング用 chef-repo には予め幾つかの cookbook と role, data_bag が入って います。これらを knife を使ってアップロードします。\n% cd chef-repo % knife cookbook upload -a % knife role from file role/*.rb % knife data bag create users % knife data bag from file users nagiosadmin.json そして bootstrap を実行。.chef/bootstrap ディレクトリに chef-full.erb ファイル が入っていて bash スクリプトになっている。knife bootstrap でこの bash スクリプ トをターゲットホスト上で実行することになる。中身はと言うと chef 環境をインストー ルしているようだ。\nchef 環境をどうやってノードに入れているかなぁ。preseed 使うかな。手作業じゃ意 味ないしなぁと考えていたのですが、こうやればいいのですね。参考になります。これ は簡単。\nではいよいよ bootstrap を実行。\n% knife bootstrap \u0026lt;IPADDRESS\u0026gt; -r 'role[base],role[monitoring]' --sudo -x \u0026lt;USER\u0026gt; -P \u0026lt;PASSWD\u0026gt; IP アドレス、ssh ユーザ名・パスワードは適宜入れてください。これで cookbooks ディ レクトリ配下の各種 cookbook が実行されました。中身は nagios とそれに依存する apache2, openssl, mysql, php 等。\n次に cookbook を新たにダウンロードし knife upload してみます。\n% knife cookbook site download chef-client % tar zxvf chef-client-1.2.0.tar.gz -C cookbooks/ % knife cookbook upload chef-client 「アカウントの pem がない場合 validation が用いられるが、一旦サーバと通信出来た ら validation は必要なくなる。それどころか wi-fi パスワードのようなものなので validation は削除することを強くすすめる」と Sean が言っていました。chef 環境は すでに bootstrap で入っているものの、validation を削除するための recipe を base という role に追加し、実行してみます。\nSean の言っていたこと、間違っていたら指摘してください( \u0026gt;\u0026lt; ) 英語だったので自信 ないです。\n% vim role/base.rb name \u0026quot;base\u0026quot; description \u0026quot;Base role applied to all nodes.\u0026quot; run_list( \u0026quot;recipe[apt]\u0026quot;, \u0026quot;recipe[nagios::client]\u0026quot; \u0026quot;recipe[chef-client::delete_validation]\u0026quot; ) default_attributes( \u0026quot;nagios\u0026quot; =\u0026gt; { \u0026quot;server_role\u0026quot; =\u0026gt; \u0026quot;monitoring\u0026quot; } ) role をアップロード。\n% knife role from file roles/base.rb ターゲットホストで chef-client を実行。ここで バージョン 10.14 から登場した why-run を試してから実行。\ntarget# chef-client -Fdoc -lfatal --color --why-run target# chef-client -Fdoc -lfatal --color もしくはワークステーションから knife ssh を使っても良い。\n% knife search node \u0026quot;role:base\u0026quot; -a cloud.public_ipv4 % knife ssh \u0026quot;role:base\u0026quot; \u0026quot;sudo chef-client -Fmin\u0026quot; -x ubuntu -P opscodechef -a cloud.public_ipv4 -a cloud.public_ipv4 とは、AWS EC2 や OpenStack, CloudStack 環境ではインスタン スの eth0 インターフェースにプライベート IP アドレスが付与されているケースが 殆どなため、グローバル IP アドレスを検索し実行している。\n大きな流れはこれでおしまい。\nトレーニングでは bento, minitest, cucumber-chef 等の紹介があったが、詳細な内容 については見送られた。昼休み1時間をはさんで計7時間の長丁場だったが、chef 初級 を脱するのには最適なトレーニングだった。これからトレーニング内容や種別を組んで いくそうなので、内容は変わってくるかもしれない。あとでフィードバックをしなく ちゃ。\n個人的には OpenStack に興味を持っているので chef を使って OpenStack をデプロイ したい。今年6月には \u0026lsquo;Chef for OpenStack\u0026rsquo; というアナウンスがあり、DELL, RackSpace, HP 等の大企業がパートナーとして参加することになったと発表があった。 今まで openstack cookbook は長くメンテナンスされてこなかったので、これには期待 している。\n最後にこの機会を作って下さった クリエーションオンラインさん、opscode の Sean さん、mr.devops さん、ありがとうございましたー。貴重な体験でした。\n","permalink":"https://jedipunkz.github.io/post/2012/10/06/secret-training-of-opscode-chef/","summary":"\u003cp\u003e昨日、開かれた \u0026ldquo;Opscode Chef のシークレットトレーニング\u0026rdquo; に参加してきました。\u003c/p\u003e\n\u003cp\u003e場所はうちの会社で KDDI ウェブコミュニケーションズ。主催はクリエーションオンラ\nインさんでした。講師は Sean OMeara (@someara) さん。今後 Chef のトレーニングを\n日本で開くため、事前に内容についてフィードバックが欲しかったそうで、オープンな\nレッスンではありませんでしたが、次回以降、日本でも期待できそうです。\u003c/p\u003e\n\u003cp\u003e内容は chef の基本・メリット・考え方などを網羅した資料で1時間程進められ、その\n後はハンズオンがメインでした。今日は実際にハンズオンの内容を書いていこうかと思\nいます。\u003c/p\u003e\n\u003cp\u003echef workstation 環境は揃っている前提にします。また chef server として opscode\nの hosted chef (opscode が提供している chef のホスティングサービス,\nchef-server として動作します) を使います。またターゲットホストは当日は ec2 イ\nンスタンスを使いましたが、chef ワークステーションから到達できるホストであれば\n何でも良いでしょう。\u003c/p\u003e\n\u003cp\u003eまずは chef-repo のクローン。講習会で使われたものです。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egit clone https://github.com/opscode/chef-repo-workshop-sysadmin.git chef-repo\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e予め cookbook が入っています。\u003c/p\u003e\n\u003cp\u003e次に、manage.opscode.com へアクセスしアカウントを作ります。Free アカウントが誰\nでも作れるようになっています。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://manage.opscode.com\"\u003ehttps://manage.opscode.com\u003c/a\u003e へアクセス -\u0026gt; Sign Up をクリック -\u0026gt; アカウント情報\nを入力 -\u0026gt; submit -\u0026gt; メールにて verify -\u0026gt; 自分のアカウント名をクリック -\u0026gt; Get a\nnew key をクリックし \u0026lt;アカウント名\u0026gt;.pem をダウンロード -\u0026gt; create a\norganization をクリックし Free を選択し、適当な名前で organization を作成。\nvalidation key と knife.rb をダウンロード\u003c/p\u003e","title":"Secret Training of Opscode Chef"},{"content":"第7回 OpenStack 勉強会に参加してきました。\n開催日 : 2012年08月28日 開催場所 : 天王洲アイル ビットアイル 1年以上前から OpenStack, CloudStack 界隈はウォッチしていたのだけど、実際に構築 してってなると、今月始めばかりで、OpenStack も先週4日間掛けてやっとこさ構築出来たっ てところ\u0026hellip;orz。前回のブログ記事でへなちょこスクリプト公開しちゃったのを後悔しつ つ現地に向かいましたw あと、その他に Opscode Chef 等の技術にも興味持って調査し ていたので、今回の勉強会はまさに直ぐに活かせる内容だった。\nでは早速、報告があった内容と自分の感想を交えつつ書いていきます。\nHP さんのクラウドサービス HP Cloud Services 日本 HP 真壁さま HP さんは既に Public クラウドサービスを提供し始めていて Ojbect Storage, CDN 部 分は既にリリース済みだそうだ。compute, block storage 等はベータ版状態でこれか らリリース。OpenStack ベースな構成で Horizon 部分は自前で開発したもの。既 にサーバ数は万の桁まで到達！ MySQL な DaaS も登場予定だとか。\nあと HP だけにクラウドサービスに特化したサーバ機器も出していて、それが HP Project Moonshot 。ARM/Atom 搭載のサーバで 2,880 nodes/rack が可能だとか！す げぇ。もちろん電源等のボトルネックとなるリソースは他にも出てきそうだけど。\nノード数って増えると嬉しいのかな？コア数が増えるのは嬉しいけど。\nCanonical JuJu Canonical 松本さま JuJu は Canonical が提供しているデプロイツールで charms と呼ばれるレシピ集 (っ て言うと語弊があるのか) に従ってソフトウェアの配布を行うツール。MAAS という物 理サーバのプロビジョニングツールと組み合わせればハードウェアを設置した後のプロ ビジョニング操作は一気通貫出来る、といったもの。具体的な操作例を挙げてくれたの で添付してきます。\n% juju deploy --repository=/tmp swift-proxy % juju deploy --repository=/tmp swift-storage % juju add-relation swift-storage:swift-proxy % swift-proxy:swift-proxy % juju add-unit swift-storage # node 追加 swift-proxy, swift-storage をデプロイし、その後それぞれを関係付けているのが add-relation。また swift-proxy に対して swift-storage node を追加してくといっ た操作が add-unit らしい。\nCharms と呼ばれるモノの中をのぞかせてもらったが Shell Script と json ファイル 集になっていた。インフラ系のエンジニアに操作してもらうにはこれがベスト、といっ たところなのだろう。Opscode Chef の様な自由度があるかどうかは、触ってみないと 分からない。時間を見つけて調べてみるかぁ。\nちなにみ今日 MAAS について調べたのですが、これは PXE Boot と DHCP のコンフィギュ レーションを GUI でするってものなのですね。後に出てくる Crowbar とはだいぶ違う。 間違っていたら指摘してください..。\n参考 URL : https://wiki.ubuntu.com/ServerTeam/MAAS\nRedHat の OpenStack への取り組み RedHat 中井さま OpenStack ディストリビューションを提供し始めたことで最近話題になっていたが、い よいよ今年2012年10月リリース予定の folsom をベースとしたリリースを2013年に控え ているそうだ。ここで初めて有償サポートが開始されるそう。より簡単に構築が出来て、 技術的な不安定を持っているユーザを取り込んでいくのだろう。面白いネタも貰えた。 将来、Swift 代替で Gluster-FS が扱えるようになる可能性があるそうだ。また KVM にコミットしているエンジニアを抱えている彼らだが、KVM から直接 Gluster-FS 上の VM イメージを操作出来るように修正加える案も出ているそうだ。これが実現すれば、 nova ノードのサーバリソースに依存しない大きな Disk イメージを扱うことも可能に なるだろう。\nまた、Canonical JuJu, Opscode Chef に並ぶツールの紹介もあった。CloudForms がそ れなのだが、各社と共通するコンセプトを持っているようだ。開発環境・本番環境への シームレスなデプロイ、と。\nDELL Crowbar DELL 増月さま ある意味、僕にとって一番の収穫だったのが DELL の Crowbar。DELL サーバに依存せ ず使えるデプロイツールで、IPMI, RAID, BIOS 等のハードウェア構成も自動構築が出 来るそう！また Opscode Chef がベースになっていて barclamp と呼ばれる Chef で 言う Cookbooks を元にソフトウェアをデプロイしていくそうだ。Chef Server 環境が 必須で chef-solo のような操作には対応していなそうなのが残念だった。Web ベース の GUI インターフェースで操作するらしい。デモも当日見れました。\nハードウェア構成も自動構築出来るツールは Canonical の MAAS があるが、一歩踏み 込んだ構成が組めそう。尚、Opscode の Cookbooks を再利用するのは少し難しい状況 のようだ。この辺りは 2.0 バージョンで改善されるそうだ。Chef との依存関係をより シンプルなものにするそう。\n他ベンダのと差別化を図るのかと思いきや、サーバ機器に依存しないツールを出してく る DELL さんの思いは、どこにあるのだろう。あと BIOS, RAID 周りをソフトウェアで プロビジョニングしていく受け口の I/F は IPMI なのかな？質問すればよかった。\nGMO お名前 KVM 先月の OSC で発表になった資料をベースに説明して頂いた。diablo ベースで CentOS 上に構築されているらしい。また griddynamic.net のパッケージ (知らなかった) を 利用して (なぜ素直に Ubuntu 使わないのかな？) 構築したそうだ。griddynamic.net のパッケージは既にエキスパイアしているらしい\u0026hellip;。nova ノードは既に200台規模。 libvirt, kvm 周りにパッチを独自で当てているそうで、その具体的内容が聞けた。た だこの辺は OpenStack のアップデートに追いつく作業がめちゃ大変になるだろうなぁ と想像する。..\n全体を通して Opscode Chef に並ぶ技術が出始めてきた。OpenStack で nodes を追加するところまで 自動化したとしても vm 上の操作を手作業するわけにはいかないし、必然なのだろう。 よりディストリビューションに依存しない、インフラ系・アプリ系共に理解出来る、自 由度のある、汎用性のある技術を我々が選びながら使っていく必要がありそう。僕らイ ンフラ系エンジニアの仕事内容も、この辺りにシフトしていく時代はもう目の前まで来 ているだろう。OpenStack を構築する手順が JuJu 等でコマンド一発なのを見て唖然と したのも確か。誰でもできる操作になるのも必然で、ただどういう操作がされているか を理解し、必要に応じて改変して開発していく力は身に着けておかないと、インフラ系 は特に、仕事内容が単純化していく一方になる気がする。危機を感じつつチャンスに結 びつけるいい機会なのかなぁ。あとは監視周りも自動コンフィギュレーションされない と、真の自動化には至らないなぁ。\n当日は BitIsle スタッフのみなさん、コミュニティのみなさん、ありがとうございま したぁー。\n","permalink":"https://jedipunkz.github.io/post/2012/08/29/7th-openstack-meetup/","summary":"\u003cp\u003e第7回 OpenStack 勉強会に参加してきました。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e開催日   : 2012年08月28日\n開催場所 : 天王洲アイル ビットアイル\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e1年以上前から OpenStack, CloudStack 界隈はウォッチしていたのだけど、実際に構築\nしてってなると、今月始めばかりで、OpenStack も先週4日間掛けてやっとこさ構築出来たっ\nてところ\u0026hellip;orz。前回のブログ記事でへなちょこスクリプト公開しちゃったのを後悔しつ\nつ現地に向かいましたw あと、その他に Opscode Chef 等の技術にも興味持って調査し\nていたので、今回の勉強会はまさに直ぐに活かせる内容だった。\u003c/p\u003e\n\u003cp\u003eでは早速、報告があった内容と自分の感想を交えつつ書いていきます。\u003c/p\u003e\n\u003ch2 id=\"hp-さんのクラウドサービス-hp-cloud-services\"\u003eHP さんのクラウドサービス HP Cloud Services\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003e日本 HP 真壁さま\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eHP さんは既に Public クラウドサービスを提供し始めていて Ojbect Storage, CDN 部\n分は既にリリース済みだそうだ。compute, block storage 等はベータ版状態でこれか\nらリリース。OpenStack ベースな構成で Horizon 部分は自前で開発したもの。既\nにサーバ数は万の桁まで到達！ MySQL な DaaS も登場予定だとか。\u003c/p\u003e\n\u003cp\u003eあと HP だけにクラウドサービスに特化したサーバ機器も出していて、それが HP\nProject Moonshot 。ARM/Atom 搭載のサーバで 2,880 nodes/rack が可能だとか！す\nげぇ。もちろん電源等のボトルネックとなるリソースは他にも出てきそうだけど。\u003c/p\u003e\n\u003cp\u003eノード数って増えると嬉しいのかな？コア数が増えるのは嬉しいけど。\u003c/p\u003e\n\u003ch2 id=\"canonical-juju\"\u003eCanonical JuJu\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003eCanonical 松本さま\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eJuJu は Canonical が提供しているデプロイツールで charms と呼ばれるレシピ集 (っ\nて言うと語弊があるのか) に従ってソフトウェアの配布を行うツール。MAAS という物\n理サーバのプロビジョニングツールと組み合わせればハードウェアを設置した後のプロ\nビジョニング操作は一気通貫出来る、といったもの。具体的な操作例を挙げてくれたの\nで添付してきます。\u003c/p\u003e","title":"第7回 OpenStack 勉強会参加レポート"},{"content":"OpenStack のインストールってしんどいなぁ、って感じて devstack http://devstack.org/ とかで構築して中を覗いていたのですが、そもそも devstack って再起動してしまえば何も起動してこないし、swift がインストールされないしで。 やっぱり公式のマニュアル見ながらインストールするしかないかぁって\u0026hellip;。感じてい たのですが\u0026hellip;。\nhttp://docs.openstack.org/essex/openstack-compute/starter/os-compute-starterguide-trunk.pdf\nこのマニュアルの前提は、ネットワーク2セグメント・server1, server2 の計2台が前 提なのですが、環境作るのがしんどいので、オールインワンな構築がしたい！サーバ1台で OpenStack ESSEX を インストールしたい！で、シェルスクリプトを作ったのでそれを使ったインストール方法を紹介します。\nぼくの Thinkpad に OpenStack ESSEX をインストールしてブラウザで localhost に接続して いる画面です。ちゃんと KVM VM が起動して noVNC で接続できています。自己満足やぁ。\n前提条件 Ubuntu Server 12.04 LTS amd64 がインストールされていること Intel-VT もしくは AMD−Vなマシン NIC が一つ以上ついているマシン /dev/sda6, /dev/sda7 (デバイス名は何でもいい) の2つが未使用で空いていること です。\n構成 1 NIC を前提に eth0 と eth0:0 の2つを想定すると、こんな構成になります。eth0:0 は完全にダミーで IP アドレスは何でもいいです。br100 ブリッジデバイス上で VM が NW I/F を持ちます。floating range ってのは OpenStack で言うグローバル IP レン ジ。グローバルである必要は無いですが eth0 と同じレンジの IP アドレスを VM に付 与出来ます。/dev/sda6 が nova-volumes で /dev/sda7 が swift 。なので OS インス トール時に2つのデバイスを未使用で空けておいてください。\n+--+--+--+ |VM|VM|VM| 192.168.4.32/27 +--+--+--+.. +----------+ +--------+ | | | br100 | 192.168.4.33/27 -\u0026gt; floating range : 10.200.8.32/27 | | +--------+ | | | eth0:0 | 192.168.3.1 disk devices | Host | +--------+ (dummy) +------------------------+ | | | /dev/sda6 nova-volumes | | | +--------+ +------------------------+ | | | eth0 | ${HOST_IP} | /dev/sda7 swift | +----------+ +--------+ +------------------------+ | nw I/Fs +----------+ | CPE | +----------+ インストール手順 インストール手順は簡単です。\n% sudo -i # git clone git://github.com/jedipunkz/openstack_install.git して取得したスクリプトを環境に合わせて環境変数設定します。スクリプト上部のこれ らの内容を、環境に合わせて設定。${HOST_IP} と ${NOVA_VOLUMES_DEV}, ${SWIFT_DEV} だけ気をつければ OK です。その他は内部ネットワークの設定なので何 でもつながります。\n# ----------------------------------------------------------------- # Environment Parameter # ----------------------------------------------------------------- HOST_IP='10.200.8.15' HOST_MASK='255.255.255.0' HOST_NETWORK='10.200.8.0' HOST_BROADCAST='10.200.8.255' GATEWAY='10.200.8.1' MYSQL_PASS='secret' FIXED_RANGE='192.168.4.1/27' FLOATING_RANGE='10.200.8.32/27' FLAT_NETWORK_DHCP_START='192.168.4.33' ISCSI_IP_PREFIX='192.168.4' NOVA_VOLUMES_DEV='/dev/sda6' SWIFT_DEV='/dev/sda7' で実行。\n# chmod +x openstack_install/openstack_install.sh # ./openstak_install/openstack_install.sh allinone ( wait some minutes...) マシンによりますが、10分弱すると OpenStack が構築されているはずです。\nOS イメージと SSH キーペアのインストール 上記の手順で OpenStack は構築されるのですが、VM を起動するには OS イメージが必 要ですよね。これは自分で用意するしかないです。ただ、これは簡単で下記の手順で出 来ます。\nOS イメージ作成 サンプルで Ubuntu Server 12.04 LTS amd64 なイメージをここで作ってみます。\n# kvm-image create -f qcow2 server.img 5G # wget http://gb.releases.ubuntu.com//precise/ubuntu-12.04-server-amd64.iso # kvm -m 256 -cdrom ubuntu-12.04-server-amd64.iso -drive file=server.img,if=virtio,index=0 -boot d -net nic -net user -nographic -vnc :0 手元の端末の VNC ツールで ${HOST_IP}:0 に接続し OS のインストールを済ませます。 その後、下記のコマンドで HDD から起動してあげて\u0026hellip;\n# kvm -m 256 -drive file=server.img,if=virtio,index=0 -boot c -net nic -net user -nographic -vnc :0 再度、VNC で VM に接続して..\n# sudo rm -rf /etc/udev/rules.d/70-persistent-net.rules # shutdown -h now 上記の操作をしたら OS イメージ作成は終わり。その他のディストリビューションでの イメージ作成については公式マニュアルに書いてあります。\nGlance に OS イメージをインストール 作成した OS イメージを Glance に追加します。先ほどのスクリプトで生成された /root/.openstack が Glance に接続するために必要なので zsh の場合 source してから\u0026hellip;\n# source /root/.openstack # glance add name=\u0026quot;Ubuntu Server 12.04LTS\u0026quot; is_public=true container_format=ovf disk_format=qcow2 \u0026lt; server.img で追加出来ます。.openstack は bash でも取得できるのでその際は\n# . /root/.openstack してください。root ユーザ以外でも操作出来ます。\nSSH キーペアの生成とインストール VM に割り当てる SSH キーペアを作ってインストールする手順です。\n# ssh-keygen # nova keypair-add --pub_key .ssh/id_rsa.pub mykey # nova keypair-list Horizon へ接続 いよいよ Horizon へ接続です。Horizon は OpenStack ESSEX から取り込まれた Web UI です。VM の作成・削除、ネットワークの設定等がブラウザで操作出来ます。\nhttp://${HOST_IP に接続してユーザ : \u0026lsquo;admin\u0026rsquo;, パスワード \u0026lsquo;admin\u0026rsquo; でログインしてください。\nまとめ OpenStack はしんどいｗ ですが来月 2012/09 リリース予定の Folsom は \u0026lsquo;Easy Setup\u0026rsquo; がフューチャされてるそうです。期待。手動で構築していると glance のとこ ろで ID 地獄にハマりますｗ 今回の手順で all in one な環境ができたら、色々覗い てみてコンポーネント毎に Node を切り出すってことも考えないといけないと思います。 それぞれは HTTP ベースの API で接続できれば OK なので切り出すこと自体は簡単。 冗長を組む方法は.. これから調べます。keystone, glance を拡張・冗長させるって出 来るのか？難しそう。CloudStack と違って rabbitmq-server でキューイングしてくれ るので、Node が増えた時の対処は考えれれているよう。\nあと、OS イメージではなくて AMI で VM を作る方法もあるのですが AMI の作成方法は Web を見ていると沢山載っていますので参考にして作ってみてください。\n","permalink":"https://jedipunkz.github.io/post/2012/08/26/all-in-one-openstack-installation/","summary":"\u003cp\u003eOpenStack のインストールってしんどいなぁ、って感じて devstack\n\u003ca href=\"http://devstack.org/\"\u003ehttp://devstack.org/\u003c/a\u003e とかで構築して中を覗いていたのですが、そもそも devstack\nって再起動してしまえば何も起動してこないし、swift がインストールされないしで。\nやっぱり公式のマニュアル見ながらインストールするしかないかぁって\u0026hellip;。感じてい\nたのですが\u0026hellip;。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://docs.openstack.org/essex/openstack-compute/starter/os-compute-starterguide-trunk.pdf\"\u003ehttp://docs.openstack.org/essex/openstack-compute/starter/os-compute-starterguide-trunk.pdf\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eこのマニュアルの前提は、ネットワーク2セグメント・server1, server2 の計2台が前\n提なのですが、環境作るのがしんどいので、オールインワンな構築がしたい！サーバ1台で OpenStack ESSEX を\nインストールしたい！で、シェルスクリプトを作ったのでそれを使ったインストール方法を紹介します。\u003c/p\u003e\n\u003cimg src=\"http://jedipunkz.github.com/pix/openstack_thinkpad.jpg\"\u003e\n\u003cp\u003eぼくの Thinkpad に OpenStack ESSEX をインストールしてブラウザで localhost に接続して\nいる画面です。ちゃんと KVM VM が起動して noVNC で接続できています。自己満足やぁ。\u003c/p\u003e\n\u003ch2 id=\"前提条件\"\u003e前提条件\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eUbuntu Server 12.04 LTS amd64 がインストールされていること\u003c/li\u003e\n\u003cli\u003eIntel-VT もしくは AMD−Vなマシン\u003c/li\u003e\n\u003cli\u003eNIC が一つ以上ついているマシン\u003c/li\u003e\n\u003cli\u003e/dev/sda6, /dev/sda7 (デバイス名は何でもいい) の2つが未使用で空いていること\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eです。\u003c/p\u003e\n\u003ch2 id=\"構成\"\u003e構成\u003c/h2\u003e\n\u003cp\u003e1 NIC を前提に eth0 と eth0:0 の2つを想定すると、こんな構成になります。eth0:0\nは完全にダミーで IP アドレスは何でもいいです。br100 ブリッジデバイス上で VM が\nNW I/F を持ちます。floating range ってのは OpenStack で言うグローバル IP レン\nジ。グローバルである必要は無いですが eth0 と同じレンジの IP アドレスを VM に付\n与出来ます。/dev/sda6 が nova-volumes で /dev/sda7 が swift 。なので OS インス\nトール時に2つのデバイスを未使用で空けておいてください。\u003c/p\u003e","title":"OpenStack ESSEX オールインワン インストール"},{"content":"仕事で Opesocd Chef の情報収集をしてたのですが、僕が感じるにこれはインフラエン ジニアの未来だと。逆に言うとインフラエンジニアの危機。AWS のようなクラウドサー ビスがあればアプリケーションエンジニアが今までインフラエンジニアが行っていた作 業を自ら出来てしまうからです。\nインフラエンジニアなら身に付けるしかない！って僕が感じる Chef について chef-solo を通して理解するために情報まとめました。\nchef には chef-server 構成で動作するものと chef-solo というサーバ無しで動作す るものがある。chef-server は構築するのが少し大変 (後に方法をブログに書きたい) なので今回は chef-solo を使ってみる。ちなみに Opscode が chef-server のホスティ ングサービスを展開している。彼らとしてはこちらがメイン。\nchef-solo の入れ方 opscode が推奨している ruby-1.9.2 をインストールする。rvm は色々問題を招き寄せ るので rbenv を使って環境整えます。root ユーザ環境内に入れてください。\n必要なパッケージをインストール\n% sudo apt-get update % sudo apt-get install build-essential zlib1g-dev libssl-dev root ユーザにてrbenv をインストール\n% sudo -i # cd ~ # git clone git://github.com/sstephenson/rbenv.git .rbenv # echo 'export PATH=\u0026quot;$HOME/.rbenv/bin:$PATH\u0026quot;' \u0026gt;\u0026gt; ~/.zshrc # echo 'eval \u0026quot;$(rbenv init -)\u0026quot;' \u0026gt;\u0026gt; ~/.zshrc ruby-build をインストール\n# mkdir -p ~/.rbenv/plugins # cd ~/.rbenv/plugins # git clone git://github.com/sstephenson/ruby-build.git opscode が推奨している ruby バージョン 1.9.2 をインストール\n# rbenv install 1.9.2-p290 # rbenv global 1.9.2-p290 # rbenv rehash capistrano, chef のインストールを gem を使い行う\n# gem install chef # rbenv rehash これらは \u0026lsquo;root\u0026rsquo; ユーザ環境内に構築する必要がある。chef がそれを前提としている からだ。また、perl と違い ruby は後方互換性がないので将来のことを考え rbenv で バージョンを管理し続ける必要がある、と思う。\nchef-solo の設定 /etc/chef/chef.json ファイルを修正することで、chef-solo で実行する recipe の追 加を行う。これは複数指定することが可能。\n{ \u0026quot;run_list\u0026quot;: [ \u0026quot;recipe[ntp]\u0026quot;, ] } 上記は ntp レシピ を追加した例。次に /etc/chef/solo.rb を生成する。これは chef-solo 動作に必要な PATH 指定を主に行う。\nfile_cache_path \u0026quot;/tmp/chef-solo\u0026quot; cookbook_path [\u0026quot;/home/jedipunkz/cookbooks\u0026quot;] role_path \u0026quot;/home/jedipunkz/role\u0026quot; log_level :debug 上記パラメータの説明は下記の通り。\nfile_cache_path : cache 用のディレクトリ指定 cookbook_path : cookbook を配置するディレクトリ指定 role_path : role ディレクトリ指定 log_level : Log Level の指定 サンプルクックブックのダウンロードと理解 サンプルとして opscode が提供している \u0026rsquo;ntp\u0026rsquo; を持ってくる。中身が簡単に理解出来 るものなので最初理解するために持ってくるものとしては最適。\n% cd /home/jedipunkz/cookbooks % git clone https://github.com/opscode-cookbooks/ntp.git 持ってきたクックブックの構造は\nntp . |- attributes |- templates |- recipes |- meradata.rb |-.. |-.. となっている。attribute/default.rb の一部分を抜粋を記してみた。\ndefault['ntp']['servers'] = %w{ 0.pool.ntp.org 1.pool.ntp.org 2.pool.ntp.org 3.pool.ntp.org } default['ntp']['packages'] = %w{ ntp ntpdate } default['ntp']['service'] = \u0026quot;ntp\u0026quot; default['ntp']['varlibdir'] = \u0026quot;/var/lib/ntp\u0026quot; default['ntp']['conf_owner'] = \u0026quot;root\u0026quot; default['ntp']['conf_group'] = \u0026quot;root\u0026quot; default['ntp']['var_owner'] = \u0026quot;ntp\u0026quot; default['ntp']['var_group'] = \u0026quot;ntp\u0026quot; chef は ruby の DSL で記述するが template や recipe 内で指定するパラメータ集と なるのが attribute となる。上を見てみると ntp のパッケージ名やディレクトリのオー ナー情報等が記されている。\ntemplates/default/ntp.conf.erb を見てみると\u0026hellip;\ndriftfile \u0026lt;%= node['ntp']['driftfile'] %\u0026gt; statsdir \u0026lt;%= node['ntp']['statsdir'] %\u0026gt; statistics loopstats peerstats clockstats filegen loopstats file loopstats type day enable filegen peerstats file peerstats type day enable filegen clockstats file clockstats type day enable \u0026lt;%# If ntp.peers is not empty %\u0026gt; \u0026lt;% unless node['ntp']['peers'].empty? -%\u0026gt; \u0026lt;%# Loop through defined peers, but don't peer with ourself %\u0026gt; \u0026lt;% node['ntp']['peers'].each do |ntppeer| -%\u0026gt; \u0026lt;% if node['ipaddress'] != ntppeer and node['fqdn'] != ntppeer %\u0026gt; peer \u0026lt;%= ntppeer %\u0026gt; iburst restrict \u0026lt;%= ntppeer %\u0026gt; nomodify \u0026lt;% end -%\u0026gt; \u0026lt;% end -%\u0026gt; \u0026lt;% end -%\u0026gt; これはインストールする /etc/ntp.conf (recipde で後に指定する) の内容そのままだ。 先程も書いたが chef は ruby の DSL 記述が基本なので attribute で指定したパラメー タを持ってきて、こういったコンフィグファイルを生成出来る。では最後に recipe を 見てみる。この recipe が chef の本体と言っていいところですね。上部の抜粋です。\nnode['ntp']['packages'].each do |ntppkg| package ntppkg end この [\u0026rsquo;ntp\u0026rsquo;][\u0026lsquo;packages\u0026rsquo;] は attributes/default.rb に %w{ ntp ntpdate } と書い てある。つまり ntp, ntpdate の配列を ntppkg として回して chef resources の \u0026lsquo;package\u0026rsquo; を使ってインストールしている。resources については chef の公式ドキュ メントを読むと良い。recipe で使える記述全てが1ページにまとまっている。\nhttp://wiki.opscode.com/display/chef/Resources\n次に recipe の下部を抜粋してみた。chef resources の template によって ntp.conf をインストールしている。\ntemplate \u0026quot;/etc/ntp.conf\u0026quot; do source \u0026quot;ntp.conf.erb\u0026quot; owner node['ntp']['conf_owner'] group node['ntp']['conf_group'] mode \u0026quot;0644\u0026quot; notifies :restart, resources(:service =\u0026gt; node['ntp']['service']) end source によって template/default/ntp.conf.erb を呼び出し owner, group でファイ ルのオーナー情報を、mode でパーミッションを指定している。また修正が入った際に ntp サービスの再起動を行っているのが最終行だ。\n抜粋で例を挙げながらだったが、Resources の記述方法さえ理解してしまえば全てが理 解出来るだろうし、自分でクックブックを作ることも簡単だろう。\nchef-solo の実行 ではいよいよ chef-solo の実行。\n上記で生成した chef.json と solo.rb を指定し chef-solo を実行することで上記 run_list で指定した recipe \u0026rsquo;ntp\u0026rsquo; が実行される。\n% sudo -i # chef-solo -c /etc/chef/solo.rb -j /etc/chef/chef.json まとめ chef-server 構成の組み方は後日ブログで書いてみたい。先日 #DevLOVE に参加した際 にも話題になったが、chef-solo を使うか chef-server 構成を組むか、まだ議論が必 要そう。chef-server 構成を組むことは簡単では無いが普通にエンジニアなら組めるだ ろう。が、組んだところで拡張性・冗長性・を考えた構成を組むにはまだまだノウハウ が足りない。また couchDB, rabbitmq など比較的新しいミドルウェアが使われている ので、これから経験積まないと難しいだろう。それに比べて chef-solo は上記の通り とてもシンプル。しかも chef-solo を実行する node 自身は必然的に数が増え拡張す るし、それを受ける apt レポジトリは単純な HTTP なので拡張・冗長は簡単だろう。\nまた、capistrano と chef-solo を組み合わせることで、role といった概念をもたせ たり、workstation で一括操作といった利便性も持たせることが出来る。ある意味 chef-solo を使うなら必然的な点になりそう。capistrano との組み合わせについても 後日ブログで書いてみたい。\nchef を理解すること自体はそんなには難しくないし、これからの時代に必要になるこ とは眼に見えているので、学んでおいて損はしないだろう。\n","permalink":"https://jedipunkz.github.io/post/2012/08/18/chef-solo/","summary":"\u003cp\u003e仕事で Opesocd Chef の情報収集をしてたのですが、僕が感じるにこれはインフラエン\nジニアの未来だと。逆に言うとインフラエンジニアの危機。AWS のようなクラウドサー\nビスがあればアプリケーションエンジニアが今までインフラエンジニアが行っていた作\n業を自ら出来てしまうからです。\u003c/p\u003e\n\u003cp\u003eインフラエンジニアなら身に付けるしかない！って僕が感じる Chef について\nchef-solo を通して理解するために情報まとめました。\u003c/p\u003e\n\u003cp\u003echef には chef-server 構成で動作するものと chef-solo というサーバ無しで動作す\nるものがある。chef-server は構築するのが少し大変 (後に方法をブログに書きたい)\nなので今回は chef-solo を使ってみる。ちなみに Opscode が chef-server のホスティ\nングサービスを展開している。彼らとしてはこちらがメイン。\u003c/p\u003e\n\u003ch2 id=\"chef-solo-の入れ方\"\u003echef-solo の入れ方\u003c/h2\u003e\n\u003cp\u003eopscode が推奨している ruby-1.9.2 をインストールする。rvm は色々問題を招き寄せ\nるので rbenv を使って環境整えます。root ユーザ環境内に入れてください。\u003c/p\u003e\n\u003cp\u003e必要なパッケージをインストール\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e% sudo apt-get update\n% sudo apt-get install build-essential zlib1g-dev libssl-dev\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eroot ユーザにてrbenv をインストール\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e% sudo -i\n# cd ~\n# git clone git://github.com/sstephenson/rbenv.git .rbenv\n# echo 'export PATH=\u0026quot;$HOME/.rbenv/bin:$PATH\u0026quot;' \u0026gt;\u0026gt; ~/.zshrc\n# echo 'eval \u0026quot;$(rbenv init -)\u0026quot;' \u0026gt;\u0026gt; ~/.zshrc\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eruby-build をインストール\u003c/p\u003e","title":"chef-solo で学ぶ chef の基本動作"},{"content":"chef-server の構築は少し面倒だと前回の記事 http://jedipunkz.github.com/blog/2012/08/18/chef-solo/ に書いたのですが、 opscode が提供している bootstrap を用いると、構築作業がほぼ自動化出来ます。 今回はこの手順を書いていきます。\nchef のインストール 前回同様に rbenv を使って ruby をインストールし chef を gem でインストールして いきます。\n% sudo apt-get update % sudo apt-get install zlib1g-dev build-essential libssl-dev % sudo -i # cd ~ # git clone git://github.com/sstephenson/rbenv.git .rbenv # echo 'export PATH=\u0026quot;$HOME/.rbenv/bin:$PATH\u0026quot;' \u0026gt;\u0026gt; ~/.zshrc # echo 'eval \u0026quot;$(rbenv init -)\u0026quot;' \u0026gt;\u0026gt; ~/.zshrc ruby-build をインストールします。\n# mkdir -p ~/.rbenv/plugins # cd ~/.rbenv/plugins # git clone git://github.com/sstephenson/ruby-build.git ruby-1.9.2 インストール\n# rbenv install 1.9.2-p290 # rbenv global 1.9.2-p290 # rbenv rehash gem を使って、chef, chef-server-api をインストールします\n# gem install chef # gem install chef-server-api opscode bootstrap を使った chef-server の構築 まずは chef-solo の環境整備。この bootstrap は chef-solo で実行出来るクックブッ ク集になっています。中を覗くと \u0026lsquo;apache2, chef, chef-server, couchdb, erlang, rabgitmq\u0026rsquo; などなど必要なミドルウェア群のクックブックが入っていることが判ります。\nまずは chef-solo の環境を整備します。\n# mkdir /etc/chef # vi /etc/chef/solo.rb # cat /etc/chef/solo.rb file_cache_path \u0026quot;/tmp/chef-solo\u0026quot; cookbook_path \u0026quot;/tmp/chef-solo/cookbooks\u0026quot; # vi /etc/chef/chef.json # cat /etc/chef/chef.json { \u0026quot;chef_server\u0026quot;: { \u0026quot;server_url\u0026quot;: \u0026quot;http://localhost:4000\u0026quot; }, \u0026quot;run_list\u0026quot;: [ \u0026quot;recipe[chef-server::rubygems-install]\u0026quot; ] } solo.rb で指定したパスは任意のもので構いません。\nAmazon S3 上に opscode bootstrap があります。いよいよ chef-solo で bootstrap を実行します。\n# chef-solo -c /etc/chef/solo.rb -j /etc/chef/chef.json -r http://s3.amazonaws.com/chef-solo/bootstrap-latest.tar.gz これで chef-server 構築は終わりです。実行したホストに couchDB, Erlang などが構 成されていることが分かると思います。\n次に knife の環境を整備します。knife は chef の操作を行うためのコマンドライン ツールです。\n# mkdir -p ~/.chef # cp /etc/chef/validation.pem /etc/chef/webui.pem ~/.chef # sudo chown -R root ~/.chef # knife configure -i Where should I put the config file? [~/.chef/knife.rb] Please enter the chef server URL: [http://localhost:4000] Please enter a clientname for the new client: [root] Please enter the existing admin clientname: [chef-webui] Please enter the location of the existing admin client's private key: # [/etc/chef/webui.pem] /root/.chef/webui.pem Please enter the validation clientname: [chef-validator] Please enter the location of the validation key: # [/etc/chef/validation.pem] /root/.chef/validation.pem Please enter the path to a chef repository (or leave blank): WARN: Creating initial API user... INFO: Created (or updated) client[root] WARN: Configuration file written to /home/root/.chef/knife.rb パラメータはほぼそのまま入力してください。pem ファイルのパス指定だけ先ほど root ユーザの HOME ディレクトリにインストールしたモノを指定してください。\nこれで下記のように knife が使えるようになっています。\n# knife client list chef-validator chef-webui root 今回は以上です。この chef-server の構築を knife::server というツールで実現する 方法もあります。\nhttp://fnichol.github.com/knife-server/\n同じく bootstrap を指定して構築するツールなのですが、これを使う利点として\nchef-server 構成のバックアップが取れる バックアップを元に復元できる があります。これは便利。ただし knife が最初から使える環境に限ります。\nこれらツールを使うことを前提にしないと、作業が複雑化するため、必須だと思います。 手作業はミスの元ですし。ただ、一度は手作業でやってみて構成を理解することはしな くてはならないでしょう。couchDB, Rabbitmq などの新しいミドルウェアについての理 解も深めなくてはならないでしょうし、これらをスケールさせることを前提にしておか ないとノードの数が増える度に負荷が上昇していくので将来困るでしょう。前回の記事 でも触れましたが chef-solo を用いた場合はそれらの心配が無くなります。\n","permalink":"https://jedipunkz.github.io/post/2012/08/18/opscode-bootstrap-chef-server/","summary":"\u003cp\u003echef-server の構築は少し面倒だと前回の記事\n\u003ca href=\"http://jedipunkz.github.com/blog/2012/08/18/chef-solo/\"\u003ehttp://jedipunkz.github.com/blog/2012/08/18/chef-solo/\u003c/a\u003e に書いたのですが、\nopscode が提供している bootstrap を用いると、構築作業がほぼ自動化出来ます。\n今回はこの手順を書いていきます。\u003c/p\u003e\n\u003ch2 id=\"chef-のインストール\"\u003echef のインストール\u003c/h2\u003e\n\u003cp\u003e前回同様に rbenv を使って ruby をインストールし chef を gem でインストールして\nいきます。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e% sudo apt-get update\n% sudo apt-get install zlib1g-dev build-essential libssl-dev\n% sudo -i\n# cd ~\n# git clone git://github.com/sstephenson/rbenv.git .rbenv\n# echo 'export PATH=\u0026quot;$HOME/.rbenv/bin:$PATH\u0026quot;' \u0026gt;\u0026gt; ~/.zshrc\n# echo 'eval \u0026quot;$(rbenv init -)\u0026quot;' \u0026gt;\u0026gt; ~/.zshrc\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eruby-build をインストールします。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# mkdir -p ~/.rbenv/plugins\n# cd ~/.rbenv/plugins\n# git clone git://github.com/sstephenson/ruby-build.git\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eruby-1.9.2 インストール\u003c/p\u003e","title":"Opscode Bootstrap を使った Chef-Server 構築"},{"content":"2012年07月21日に大崎のフィーチャーアーキテクトさんで行われた #DevLOVE (Chef De DevOps) に参加してきました。\n開催日 : 2012年07月21日(土曜日) 15:00 - 20:40 場所 : 大崎 フューチャーアーキテクトさま URL : http://www.zusaar.com/event/314003 仕事場でも Chef の利用を考え始めていて今回いい機会でした。半年前と比べるとだい ぶ揃って来ましたが、まだまだ資料の少ない Chef。貴重な機会でした。\nプログラムは下の通り。\n*『Chefの下準備』by Ryutaro YOSHIBA [ @ryuzee ] *『Chef自慢のレシピ披露』 by 中島弘貴さん [ @nakashii_ ] * ワークショップ『みんなでCooking』 * dialog『試食会』 * Niftyさんのスーパー宣伝タイム!!! *『渾身会』 @ryuzee さんの \u0026ldquo;Chef の下準備\u0026rdquo; は SlideShare に使った資料が公開されています。\n20120721 chefの下準備 #devlove from Ryuzee YOSHIBA 印象的だったのが、VirtualBox + Vagrant という環境。実際に使っていらっしゃいま した。MacBook の中に仮想環境とそのインターフェースである Vegrant を使って、デ プロイのテスト等が実施できるそうです。また、Vegrant 設定ファイルは Chef のレシ ピを自動読込して、常に本番環境と同じ状態にしているそうです。CloudFormation と いうキーワードや Capistrano というキーワードが出てきました。最近よく耳にするワー ドです。また CI は Jenkins だよね、だったり。\n3番目のプログラム \u0026ldquo;ワークショップ、みんなで Cooking\u0026rdquo; は 4-6 人の組みになり、実 際に Chef のレシピを書いてみようというもの。今回は wordpress の構築のためのレ シピを書いていきました。僕らの組みも時間ギリギリでレシピの投入を終えました。 wordpress の設定ファイル wp-config.php と mysql への wordpress 用データベース の作成を実際に attribute, template, script ら Resources を使ってレシピを書きま した。簡単な例でしたが、みなとコミュニケーションがとれたのでいい機会でした。\ndialog \u0026ldquo;試食会\u0026rdquo; では、グループ毎、またグループを入れ糧のディスカッション。使っ てみてどうだったか？を話し合ってそこから議論を伸ばしていきました。\nやっぱり僕も参加する前からそうだったのですが、一番皆が気になっているのが、\n\u0026quot;chef-solo + capistrano ? それとも chef-server がいいの？\u0026quot; でした。実際に使っていらっしゃる CA さんの話によると、20台までは chef-solo + capistrano の構成が良いと。そして100台規模になると chef-server が良いそうです。 具体的に何が chef-solo で問題になるのかが聞けなかったのですが、ちょっと僕らの 課題にしてみようかと思います。また、CA さんでは puppet を使っていらっしゃる部 署もあるそうで、chef-server の動作が不安定？とかで400台規模になると puppet が 良くなると。そして chef-server はいまは rabbitmq, couchDB, erlang などの構成で 出来ているのですが、そのうちどれかが (erlang の solr ?) が mysql に置き換わる と噂を聞きました。\nまた、nifty さんが現在 opscode chef wiki を翻訳されているのですが、僕も非常に 興味があります。参加させてもらおうかな。また nifty さんが nifty cloud 用 knife-ec2 を公開しているそうで説明がありました。自社にクラウドシステムがあると、 色々楽しめるよなぁ\u0026hellip;。いいな。\n[2012/07/23 15:00 追記] nifty の @tily さんが当日使った資料を公開なさったので貼り付けておきます。\nニフティ社内の Chef 利用について from tidnlyam 以上が \u0026ldquo;Chef De DevOps\u0026rdquo; の参加レポートで、ここからは僕らの課題でありここ1週間 くらいで取り組もうとしている点。\nchef-server の構築ですが、ubuntu + opscode の deb package を使う方法もあるので すが、これだと opscode が推奨している ruby のバージョン 1.9.2 ではなく 1.8.7 になってしまいます。これは ubuntu, debian の stable release もしくは development release が frozen しているからです。なので、今僕が考えているのは、 chef-solo と opscode bootstrap の組み合わせで構築する方法です。\nhttp://wiki.opscode.com/display/chef/Installing+Chef+Server+using+Chef+Solo\nこの bootstrap は opscode が chef-server 構築用レシピとして公開しているもので す。これを使えば chef client さえ入っていれば構築できてしまうという。この手順 は一度手元でも確認してみました。(Ubuntu Server 12.04 LTS amd64).\nまた Chef のメーリングリストで聞いてみたところ、やはり推奨の ruby バージョンは 1.9.2 であって、この bootstrap の方法と、あとは knife::server というものです。\nまだ僕は手元で確認できていないのですが、こちらにあるようです。\nhttp://fnichol.github.com/knife-server/\nchef-server 構成だと rabbitmq に queue が溜まったり、couchDB のディスクを溢れ させたりとトラブル起きた時の切り分けだったり対処方法だったり、まだまだ情報少な いので苦労するだろうなぁと個人的には感じました。chef-solo + capistrano の構成 も考慮しつつ、このあたり調べて行こうと思います。\n","permalink":"https://jedipunkz.github.io/post/2012/07/22/number-devlove-nican-jia-sitekimasita.-chef-de-devops/","summary":"\u003cp\u003e2012年07月21日に大崎のフィーチャーアーキテクトさんで行われた #DevLOVE (Chef De\nDevOps) に参加してきました。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e開催日 : 2012年07月21日(土曜日) 15:00 - 20:40\n場所   : 大崎 フューチャーアーキテクトさま\nURL    : http://www.zusaar.com/event/314003\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e仕事場でも Chef の利用を考え始めていて今回いい機会でした。半年前と比べるとだい\nぶ揃って来ましたが、まだまだ資料の少ない Chef。貴重な機会でした。\u003c/p\u003e\n\u003cp\u003eプログラムは下の通り。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e*『Chefの下準備』by Ryutaro YOSHIBA [ @ryuzee ]\n*『Chef自慢のレシピ披露』 by 中島弘貴さん [ @nakashii_ ]\n* ワークショップ『みんなでCooking』\n* dialog『試食会』\n* Niftyさんのスーパー宣伝タイム!!!\n*『渾身会』\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e@ryuzee さんの \u0026ldquo;Chef の下準備\u0026rdquo; は SlideShare に使った資料が公開されています。\u003c/p\u003e\n\u003ciframe src=\"http://www.slideshare.net/slideshow/embed_code/13712176\"\nwidth=\"427\" height=\"356\" frameborder=\"0\" marginwidth=\"0\" marginheight=\"0\"\nscrolling=\"no\" style=\"border:1px solid #CCC;border-width:1px 1px\n0;margin-bottom:5px\" allowfullscreen\u003e \u003c/iframe\u003e \u003cdiv\nstyle=\"margin-bottom:5px\"\u003e \u003cstrong\u003e \u003ca\nhref=\"http://www.slideshare.net/Ryuzee/20120721-chef-devlove\" title=\"20120721\nchefの下準備 #devlove\" target=\"_blank\"\u003e20120721 chefの下準備 #devlove\u003c/a\u003e\n\u003c/strong\u003e from \u003cstrong\u003e\u003ca href=\"http://www.slideshare.net/Ryuzee\"\ntarget=\"_blank\"\u003eRyuzee YOSHIBA\u003c/a\u003e\u003c/strong\u003e \u003c/div\u003e\n\u003cp\u003e印象的だったのが、VirtualBox + Vagrant という環境。実際に使っていらっしゃいま\nした。MacBook の中に仮想環境とそのインターフェースである Vegrant を使って、デ\nプロイのテスト等が実施できるそうです。また、Vegrant 設定ファイルは Chef のレシ\nピを自動読込して、常に本番環境と同じ状態にしているそうです。CloudFormation と\nいうキーワードや Capistrano というキーワードが出てきました。最近よく耳にするワー\nドです。また CI は Jenkins だよね、だったり。\u003c/p\u003e","title":"#DevLOVE に参加してきました。Chef De DevOps"},{"content":"interop 2012 で \u0026lsquo;DNS の猛威とその対策\u0026rsquo; を傍聴してきました。簡単にレポート書い ておきます。ちょっと油断しただけで大きな問題のアップデートが出てくる DNS。怖い です..。\n本講義の概要 ブラジルで大規模な ISP への DNS ポイゾニング攻撃が発生。それら猛威について理解 すると同時に技術的対応策や具体的な対応プロセスについて説明。\nイントロダクション : インターネットエクスチェンジ 石田さん DNS を取り巻く状況\nDNS を乗っ取って悪事を働く試み : DNS Changer DNS そのものをセキュアにする方向性 : DNSSEC DNS に様々な制御を任せる方向性 : SPF, AAAA, DANE, 児童ポルノブロッキング 事例 : IIJ 松崎さん 2011/11 ブラジルの事例 著名サイトへのアクセスを行うと malware が仕込まれたサイトへ誘導。ホームルータ のキャッシュがポイゾニングされた。\nとあるホームルータの問題 admin パスワードが管理 web で見える wan からのアクセスが有効 同じチップセットを使っている製品で同様の問題.. wan からルータへアクセスすると html ソースにアカウント情報が平文で書かれてい た これは怖い..。\n攻撃活動の実施 攻撃者が行う手順。\n脆弱性な CPE 発見 パスワード書き換え CPE が参照する DNS の書き換え 著名サイト向け DNS への応答を書き換え malware サイトへ誘導 銀行の安全客員ツールを無効にする malware をインストールさせる 幾つかの銀行向け DNS 応答を書き換え、目的のフィッシングサイトに誘導。DNS 書 き換えは短い期間のみであった。 規模 2011年時点、450万の CPE の DNS が書き換えられていた。今年も 30万以上の CPE が 影響を受けたまま。\nその後 攻撃者が脆弱な CPE を探す試みはまだ続いている。ブラジル以外でも被害報告が。\nその他の事例 DNS Changer LAN 内の DHCP サーバに攻撃して配布する DNS を書き換えるとも言われている。 その後、FBI が参照用 DNS(攻撃者管理) を差し押さえ。いきなり止めると、ユーザが インターネットへの接続ができなくなるので、同じ IP アドレスでキャッシュサーバを 運用。まだ35万程度の感染ホストがあるので、キャッシュ DNS の運用を 2012/7/9 ま で延長決定。\nJPCERT が用意している、確認サイト このサイトにアクセスすることで自環境が汚染されているかどうかが判ります。\nhttp://www.dns-ok.jpcert.or.jp/\n幽霊ドメイン名について : JPRS 坂口さん 近年注目されている猛威 プロトコルによる定義が明確でない部分を突いた攻撃 (幽霊ドメイン名) プロトコルの脆弱性をついた攻撃 (キャッシュポイゾニング) 傾向と対策 1980年代から利用されていた 性善説から生まれた 利用者用途が大きく変動 DNS の重要性は未だ変わっていない DNS プロトコルを正しく理解が対策の第一歩!\n幽霊ドメイン名について 2012年2月8日 中国 Haixin Duan さんによる論文として発表 その前日にISC が緊急セキュリティアドバイザリを発表 幽霊ドメインが引き起こす問題 ドメインを管理していたものからそのドメインを引き剥がすことができる 強制的にドメイン名を使用不可にしても使われ続ける 強制的に移転させても使われ続ける 動作原理 キャッシュ -\u0026gt; Root -\u0026gt; JP DNS -\u0026gt; 権威 DNS の場合、キャッシュ上の TTL が切れたレコードに対して権威 DNS に問い合わせするが、 TTL が切れていないレコードについても問い合わせる。その際に新しい情報が入ってき た場合上書きするか？破棄するか？は、実装次第。NS ホストを定期的に問い合わせす ることで TTL を巻き戻し、永遠にキャッシュを利用させる攻撃手法。悪意のあるサイ トのドメインを差し押さえても、使用され続ける可能性が出てくる。\n対策 幽霊ドメインが発生しない実装へ書き換え (bind9, unbound は実装済み) 幽霊ドメイン名のキャッシュ情報をクリアする (ISC が推奨) Unbound, Bind9 で取られた対策 権威 DNS に問い合わせはするが、TTL については上書きしない\n問題の対策 : IIJ 山本さん 前提\nブラジルの事例についてはキャッシュサーバで対策しても無益。 従来 従来のキャッシュ DNS への攻撃はキャッシュポイゾニング ISP のキャッシュ DNS サーバが標的 歴史的経緯でアクセスコントロールが緩い プロトコル/実装の脆弱性を突き、偽のレコードをキャッシュさせる 理論的には20年前から知られていたが、実際の攻撃が困難だった 近年 2008, カミンスキー攻撃が発表される キャッシュしたレコードは TTL が来るまで再度検索されないが、これを可能にした 理論的な猛威から現実的なそれへ 対策 問い合わせる際の Port 番号をランダム化 (queryID(16bit) x port (16bit)) 一部商用サーバでは攻撃を検知する機能がアリ (攻撃を検知したら問い合わせを tcp に切り替える) ISP では ingress-filtering を可能な限り実施する Open Recursive はやめる -\u0026gt; 攻撃の機会を大幅に低減できる DNSSEC の導入をすすめる -\u0026gt; 中長期的な対策 DNSSEC が短期的対策にならない理由 検証するキャッシュサーバがまだ少ない DNSSEC で署名しているドメイン名がまだほとんど無い 鶏と卵問題 DNSSEC そのものの複雑さ -\u0026gt; 理解しているエンジニアの数が少ない DNS Changer への対策 CPE ベンダで対策するしか無い DNS サーバでできることは無い ISP がやるとしたら\u0026hellip;\nOP25B ならぬ OP53B 対策を行うか？ DNS トラフィックを見張る (Google Public DNS など例外はあるが、自社キャッシュ 以外への問い合わせは誤差の範囲のはず) DNS の課題 : IIJ 松崎さま 児童ポルノフィルタ : 問題のあるサイトのドメイン名を書き換える DNS64 : A へ書き換える -\u0026gt; DNSSEC との併用が難しいことが取り上げられている \u0026lt; RFC \u0026lsquo;アクセス制御\u0026rsquo;・\u0026lsquo;書き換え\u0026rsquo;, これらは攻撃者が行うことと同じ!\n所感, まとめ 以上簡単でしたが、レポートでした。こういったイベントに1年出席しないだけで、知 らないことが出てくる DNS 界隈。毎年大きな脆弱性の出る Bind。使えて当たり前、障 害が起こると大問題になる DNS。管理者にとってつらい状況だけど、ユーザにとって無 くてはならないシステムなので、運用・開発に携わっている人間は、近年の状況をウォッ チし続けていく必要がある。この講義を傍聴しているなかで、自社の環境の組み換えを ぼんやり考えていました。\n","permalink":"https://jedipunkz.github.io/post/2012/06/16/dns-report-interop2012/","summary":"\u003cp\u003einterop 2012 で \u0026lsquo;DNS の猛威とその対策\u0026rsquo; を傍聴してきました。簡単にレポート書い\nておきます。ちょっと油断しただけで大きな問題のアップデートが出てくる DNS。怖い\nです..。\u003c/p\u003e\n\u003ch2 id=\"本講義の概要\"\u003e本講義の概要\u003c/h2\u003e\n\u003cp\u003eブラジルで大規模な ISP への DNS ポイゾニング攻撃が発生。それら猛威について理解\nすると同時に技術的対応策や具体的な対応プロセスについて説明。\u003c/p\u003e\n\u003ch2 id=\"イントロダクション--インターネットエクスチェンジ-石田さん\"\u003eイントロダクション : インターネットエクスチェンジ 石田さん\u003c/h2\u003e\n\u003cp\u003eDNS を取り巻く状況\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDNS を乗っ取って悪事を働く試み : DNS Changer\u003c/li\u003e\n\u003cli\u003eDNS そのものをセキュアにする方向性 : DNSSEC\u003c/li\u003e\n\u003cli\u003eDNS に様々な制御を任せる方向性 : SPF, AAAA, DANE, 児童ポルノブロッキング\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"事例--iij-松崎さん\"\u003e事例 : IIJ 松崎さん\u003c/h2\u003e\n\u003ch4 id=\"201111-ブラジルの事例\"\u003e2011/11 ブラジルの事例\u003c/h4\u003e\n\u003cp\u003e著名サイトへのアクセスを行うと malware が仕込まれたサイトへ誘導。ホームルータ\nのキャッシュがポイゾニングされた。\u003c/p\u003e\n\u003ch4 id=\"とあるホームルータの問題\"\u003eとあるホームルータの問題\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eadmin パスワードが管理 web で見える\u003c/li\u003e\n\u003cli\u003ewan からのアクセスが有効\u003c/li\u003e\n\u003cli\u003e同じチップセットを使っている製品で同様の問題..\u003c/li\u003e\n\u003cli\u003ewan からルータへアクセスすると html ソースにアカウント情報が平文で書かれてい\nた\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eこれは怖い..。\u003c/p\u003e\n\u003ch4 id=\"攻撃活動の実施\"\u003e攻撃活動の実施\u003c/h4\u003e\n\u003cp\u003e攻撃者が行う手順。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e脆弱性な CPE 発見\u003c/li\u003e\n\u003cli\u003eパスワード書き換え\u003c/li\u003e\n\u003cli\u003eCPE が参照する DNS の書き換え\u003c/li\u003e\n\u003cli\u003e著名サイト向け DNS への応答を書き換え\u003c/li\u003e\n\u003cli\u003emalware サイトへ誘導\u003c/li\u003e\n\u003cli\u003e銀行の安全客員ツールを無効にする malware をインストールさせる\u003c/li\u003e\n\u003cli\u003e幾つかの銀行向け DNS 応答を書き換え、目的のフィッシングサイトに誘導。DNS 書\nき換えは短い期間のみであった。\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"規模\"\u003e規模\u003c/h4\u003e\n\u003cp\u003e2011年時点、450万の CPE の DNS が書き換えられていた。今年も 30万以上の CPE が\n影響を受けたまま。\u003c/p\u003e","title":"DNS の猛威とその対策, 参加レポ #interop2012"},{"content":"interop 2012 で開催された \u0026ldquo;仮想ルータ Vyatta を使ったネットワーク構築法\u0026rdquo; に参 加してきました。簡単ですがレポートを書いておきます。\n開催日 : 2012/06/14(木) 場所 : 幕張メッセ Interop 2012 最初に所感。反省です。サーバエンジニアの視点でしか感じられていなかった。次期バー ジョンの vPlane 実装などエンタープライズ向けとしても利用出来る可能性を感じる し、比較的小さなリソースでハンズオン参加者30人程度の vyatta ルータを動かしてサ クサク動いているのを見て、簡単にパフォーマンスがどうとか 前回の記事で言うべ きじゃなかったぁ。ホント反省。\nではハンズオンの内容。\n最初に、基本情報の話 有償版と無償版の違い\nメジャーリリースのみの無償版に対して有償版はマイナーリリースもあり 有償版は保守あり 仮想 image template 機能が有償版であり API, Web GUI, Config Sync, Systen Image Cloning が有償版であり image template, config sync, cloning など、有償版ではあると嬉しい機能がモリモ リ。\n最近の利用ケース Vyatta + VM で VPN 接続環境構築 キャンパスネットワーク キャンパスネットワークのユースケースが一番多いそうだ。また、会場内にアンケート をとった結果、仮想環境での構築を想定されている方が多数だった。\n構成と特徴 Debian Gnu/Linux ベース Quagga, StrongSwam が内部で動作 apt-get など馴染み深いコマンドが使えます。 http://www.vyatta4people.org/tag/vybuddy/ ここに色々ノウハウがあるらしい。僕はまだ見ていません。まほろば工房の近藤さんに 教えて頂きました。あとでチェックします。\nバージョン stable リリースは 6.4 。次期バージョン 6.5 では vPlane の実装が予定されている。これ は IA サーバのコア毎に役割を変えるといったモノ。ノードが忙しくなった時に例えば 転送を司るコアには影響を与えない等。これは重要だ。サーバと違ってネットワーク機 器は何があっても死守しなくちゃいけない機能があるもんなぁ。6.5 に期待。\nここで、ハンズオンで課題になった IPSec の話を少しだけ説明します。\n192.168.18.0/24 +------+ +----------+ | vm01 |-------| vyatta01 |pppoe XXX.XXX.XXX.XXX +------+ +----------+ | |192.168.20.0/24 | IPSec +------+ +----------+ | | vm02 |-------| vyatta02 |pppoe YYY.YYY.YYY.YYY +------+ +----------+ 192.168.19.0/24 上図の環境で vm01 が vm02 に IPSec を用いて接続するための方法を記しています。 vyatta 同士は 192.168.20.0/24 のネットワークで接続されていますが、互いにルーティ ングテーブルは書いていないものとします。まずは vyatta01 に対しての設定。\nあ、pppoe による NAT の設定は予めしてあるものとします。\n# set vpn ipsec ipsec-interfaces interface pppoe1 # set vpn ipsec ike-group IKE-G proposal 1 encryption aes256 # set vpn ipsec ike-group IKE-G proposal 1 hash sha1 # set vpn ipsec ike-group IKE-G lifetime 3600 鍵交換の方式 IKE の設定をします。\n# set vpn ipsec esp-group ESP-G proposal 1 encryption aes256 # set vpn ipsec esp-group ESP-G proposal 1 hash sha1 # set vpn ipsec esp-group ESP-G lifetime 1800 ESP の設定をします。鍵の破棄・生成時間 1800 を設定しています。\n# edit vpn ipsec site-to-site peer YYY.YYY.YYY.YYY # set authentication mode pre-shared-secret # set authentication pre-shared-secret hogehoge site-to-site peer を指定して接続先 vyatta のグローバル IP アドレスを指定します。 また pre-shared-secret でパスフレーズを指定します。\n# set ike-group IKE-G # set local-ip YYY.YYY.YYY.YYY # set tunnel 1 local subnet 192.168.18.0/24 # set tunnel 1 remote subnet 192.168.19.0/24 # set tunnel 1 esp-group ESP-G 自グローバル IP, 自プライベートネットワーク、リモートネットワークの情報諸々、 設定します。\n# set nat source rule 10 destination address !192.168.19.0/24 # commit また最後に IPsec で接続する先のネットワークへのパケットの場合、NAT しないよう にします。\n次は逆に vyatta01 に対しても設定を投入します。諸々の値は逆にする必要があります。 これを実施すると、vm01 から vm02 に対して pppoe interface 同士の IPsec を介し て接続できるのが確認出来ました。\n私の職場で開発した製品にも投入してみようかなと目論んでいます。楽しかった。\n最後に、この場を提供して下さった方々に感謝します。\n講演者 株式会社まほろば工房 近藤邦昭さま 有限会社銀座堂 浅間正和さま さくらインターネット 大久保修一さま 伊藤忠テクノソリューションズ 伊藤哲史さま ","permalink":"https://jedipunkz.github.io/post/2012/06/14/vyatta-handson-interop2012/","summary":"\u003cp\u003einterop 2012 で開催された \u0026ldquo;仮想ルータ Vyatta を使ったネットワーク構築法\u0026rdquo; に参\n加してきました。簡単ですがレポートを書いておきます。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e開催日 : 2012/06/14(木)\n場所   : 幕張メッセ Interop 2012\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e最初に所感。反省です。サーバエンジニアの視点でしか感じられていなかった。次期バー\nジョンの vPlane 実装などエンタープライズ向けとしても利用出来る可能性を感じる\nし、比較的小さなリソースでハンズオン参加者30人程度の vyatta ルータを動かしてサ\nクサク動いているのを見て、簡単にパフォーマンスがどうとか\n\u003ca href=\"http://jedipunkz.github.com/blog/2012/06/13/vyatta-vpn/\"\u003e前回の記事\u003c/a\u003eで言うべ\nきじゃなかったぁ。ホント反省。\u003c/p\u003e\n\u003cp\u003eではハンズオンの内容。\u003c/p\u003e\n\u003ch2 id=\"最初に基本情報の話\"\u003e最初に、基本情報の話\u003c/h2\u003e\n\u003cp\u003e有償版と無償版の違い\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eメジャーリリースのみの無償版に対して有償版はマイナーリリースもあり\u003c/li\u003e\n\u003cli\u003e有償版は保守あり\u003c/li\u003e\n\u003cli\u003e仮想 image template 機能が有償版であり\u003c/li\u003e\n\u003cli\u003eAPI, Web GUI, Config Sync, Systen Image Cloning が有償版であり\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eimage template, config sync, cloning など、有償版ではあると嬉しい機能がモリモ\nリ。\u003c/p\u003e\n\u003ch2 id=\"最近の利用ケース\"\u003e最近の利用ケース\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eVyatta + VM で VPN 接続環境構築\u003c/li\u003e\n\u003cli\u003eキャンパスネットワーク\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eキャンパスネットワークのユースケースが一番多いそうだ。また、会場内にアンケート\nをとった結果、仮想環境での構築を想定されている方が多数だった。\u003c/p\u003e\n\u003ch2 id=\"構成と特徴\"\u003e構成と特徴\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eDebian Gnu/Linux ベース\u003c/li\u003e\n\u003cli\u003eQuagga, StrongSwam が内部で動作\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eapt-get など馴染み深いコマンドが使えます。\n\u003ca href=\"http://www.vyatta4people.org/tag/vybuddy/\"\u003ehttp://www.vyatta4people.org/tag/vybuddy/\u003c/a\u003e\nここに色々ノウハウがあるらしい。僕はまだ見ていません。まほろば工房の近藤さんに\n教えて頂きました。あとでチェックします。\u003c/p\u003e","title":"Vyatta ハンズオン参加レポ #interop2012"},{"content":"Vyatta で VPN しようと思ったら信じられないくらい簡単に構築できたので共有します。\n今回は PPTP (Point-to-Point Tunneling Protocol) を用いました。\nさっそく手順に。\n$ configure # set vpn pptp remote-access authentication local-users username ${USER} password ${PASSWORD} # set vpn pptp remote-access authentication mode local # set vpn pptp remote-access client-ip-pool start ${IP_START} # set vpn pptp remote-access client-ip-pool stop ${IP_END} # set vpn pptp remote-access outside-address ${GLOBAL_IP} # set vpn pptp remote-access dns-servers server-1 8.8.8.8 # set vpn pptp remote-access dns-servers server-1 8.8.4.4 # commit # save これだけです。\n認証のためのユーザを1行目で作っています。client-ip-pool で VPN 接続端末に付与 する IP アドレスのレンジを指定して outside-address で Listen Port を指定します。 DNS リゾルバに指定させたいアドレスを dns-servers で指定したら終わりです。\n私は手元の iPhone で自宅に VPN 接続して使っています。これで iPhone か 自宅内の 機器に直接アクセスできるので便利です。\nあと、Vyatta をしばらく自宅で使ってみての所感です。\nVyatta は汎用機にインストールできるし VM としても動作させられるし便利です。が、 実際に起動しているプロセス等を見ていくと、Linux 界では古くからあるレガシなソフ トウェアが起動しているだけに見えます。Vyatta の UI はこれらソフトウェアのコン フィグレーションを簡易化するラッパー的なモノになっているのが分かります。\n汎用性がある一方、パフォーマンスはあまり期待出来ないかもしれません。実際にリッ チなコンテンツを閲覧した時のパフォーマンスは Buffalo 製コンシューマ向け機器に 劣ります。Linux なのでチューニングは出来ますが、予め幾つかのチューニングは入っ ていますし、あくまでもチューニングなので劇的なパフォーマンス改善には繋がりませ ん。\nFreeBSD 系 ? で pfsense という \u0026lsquo;open source firewall distribution\u0026rsquo; もあるよう なので、いずれ試してみたいです。\n","permalink":"https://jedipunkz.github.io/post/2012/06/13/vyatta-vpn/","summary":"\u003cp\u003eVyatta で VPN しようと思ったら信じられないくらい簡単に構築できたので共有します。\u003c/p\u003e\n\u003cp\u003e今回は PPTP (Point-to-Point Tunneling Protocol) を用いました。\u003c/p\u003e\n\u003cp\u003eさっそく手順に。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ configure\n# set vpn pptp remote-access authentication local-users username ${USER} password ${PASSWORD}\n# set vpn pptp remote-access authentication mode local\n# set vpn pptp remote-access client-ip-pool start ${IP_START}\n# set vpn pptp remote-access client-ip-pool stop ${IP_END}\n# set vpn pptp remote-access outside-address ${GLOBAL_IP}\n# set vpn pptp remote-access dns-servers server-1 8.8.8.8\n# set vpn pptp remote-access dns-servers server-1 8.8.4.4\n# commit\n# save\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eこれだけです。\u003c/p\u003e","title":"Vyatta で構築する簡単 VPN サーバ"},{"content":"普段は Mac, Linux がメインなのですが、NetBSD もたまにデスクトップ機として利用 するので、初期設定手順をまとめ。\nConsole 設定, キーリマップ Caps と Ctrl キーを入れ替えます。お約束。\n# wsconsctl -w encoding=us.swapctrlcaps # vi /etc/wscons.conf # 同様の設定を入れる キーリピートを速くします。標準では遅すぎる。値は好みで。\n# wsconsctl -w repeat.del1=300 # リピートが始まるまでの時間 # wsconsctl -w repeat.deln=40 # 反復間隔 # vi /etc/wscons.conf # 同様の値を入れる pkgsrc をインストール xx, y は次期に応じて変える。例 :2012Q1\n# cd /usr # cvs -q -z3 -d anoncvs@anoncvs.NetBSD.org:/cvsroot checkout -r pkgsrc-20xxQy -P pkgsrc 最新の状態に更新する場合。リリースタグを利用したいなら不要。\n# cd /usr/pkgsrc # cvs update -dP grub のインストール grub をインストール。デフォルトでも良いのだけど、安心したいから。他の OS とデュ アル・トリプルブートに設定することが多いので必要を感じて入れている場合もある。 もちろん Linux 側からインストールしても OK。\n# cd /usr/pkgsrc/sysutils/grub # make install # /usr/pkg/sbin/grub-install '(hd0)' # vi /grub/menu.lst # 適当に設定 X の設定 # X -configure # cp /root/Xorg.conf.new /etc/X11/xorg.conf ウィンドウマネージャは普段は twm を利用しているので、何も入れない。\n日本語フォントとして efont を使う。スグレモノフォント。\n# cd /usr/pkgsrc/fonts/efont-unicode # make install xorg.conf に FontPath を通す。\nFontPath \u0026quot;/usr/pkg/lib/XZ11/fonts/efont\u0026quot; urxvt と zsh ターミナルエミュレータとして rxvt を利用。Unicode 対応な urxvt をインストール する。zsh はマルチバイト対応な zsh-current をインストール。\n# cd /usr/pkgsrc/shells/zsh-curren # make install # cd /usr/pkgsrc/x11/rxvt-unicode # make install 日本語入力設定 日本語のインプットメソッド uim, anthy をインストール。\n# cd /usr/pkgsrc/inputmethod/uim # make install 環境変数を設定。.zshrc でも .xinitrc でも可。\nexport LANG=ja_JP.UTF-8 export LC_ALL=ja_JP.UTF-8 export LC_CTYPE=ja_JP.UTF-8 export XMODIFIERS=@im=uim export GTK_IM_MODULE=uim exec uim-xim \u0026amp; exec uim-toolbar-gtk * exec /usr/X11R7/bin/twm 所感 昔は FreeBSD を使っていたのだけど、Mac, Linux, NetBSD だけになってしまった。 NetBSD は実装が綺麗なのでスーッと入っていけるので好き。ただ、source からのビル ドは時間が掛かるので、git や chef, puppet 等の仕組みを利用してビルドする方法を 確立したいなぁと最近思ってる。\n","permalink":"https://jedipunkz.github.io/post/2012/05/12/netbsd-first-setup/","summary":"\u003cp\u003e普段は Mac, Linux がメインなのですが、NetBSD もたまにデスクトップ機として利用\nするので、初期設定手順をまとめ。\u003c/p\u003e\n\u003ch2 id=\"console-設定-キーリマップ\"\u003eConsole 設定, キーリマップ\u003c/h2\u003e\n\u003cp\u003eCaps と Ctrl キーを入れ替えます。お約束。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# wsconsctl -w encoding=us.swapctrlcaps\n# vi /etc/wscons.conf # 同様の設定を入れる\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eキーリピートを速くします。標準では遅すぎる。値は好みで。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# wsconsctl -w repeat.del1=300 # リピートが始まるまでの時間\n# wsconsctl -w repeat.deln=40  # 反復間隔\n# vi /etc/wscons.conf          # 同様の値を入れる\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id=\"pkgsrc-をインストール\"\u003epkgsrc をインストール\u003c/h2\u003e\n\u003cp\u003exx, y は次期に応じて変える。例 :2012Q1\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# cd /usr\n# cvs -q -z3 -d anoncvs@anoncvs.NetBSD.org:/cvsroot checkout -r pkgsrc-20xxQy -P pkgsrc\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e最新の状態に更新する場合。リリースタグを利用したいなら不要。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# cd /usr/pkgsrc\n# cvs update -dP\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id=\"grub-のインストール\"\u003egrub のインストール\u003c/h2\u003e\n\u003cp\u003egrub をインストール。デフォルトでも良いのだけど、安心したいから。他の OS とデュ\nアル・トリプルブートに設定することが多いので必要を感じて入れている場合もある。\nもちろん Linux 側からインストールしても OK。\u003c/p\u003e","title":"NetBSD インストール直後の初期設定まとめ"},{"content":"Hacker News で取り上げられていた emacs-nav を使ってみた。\nhttp://code.google.com/p/emacs-nav/\nインストール方法は簡単で\n% wget http://emacs-nav.googlecode.com/files/emacs-nav-20110220a.tar.gz % tar zxvf emacs-nav-20110220a.tar.gz % mv emacs-nav-20110220a ~/.emacs.d/emacs-nav して\n;; emacs-nav (add-to-list 'load-path \u0026quot;~/.emacs.d/emacs-nav/\u0026quot;) (require 'nav) するだけ。\n見た目はこんな感じ。\n起動は M-x nav と入力。ウィンドウの左にファイルブラウザが開いてファイルを選択 出来る。これだけだと、使うメリットを感じないが、面白いのがマウスで選択出来ると ころ。TextMate のブラウザのような感じだ。\n今だと anything.el が便利すぎて、こちらを利用する価値を見出せるか分からないけ ど、暫く使ってみようと思う。\n","permalink":"https://jedipunkz.github.io/post/2012/05/04/emacs-nav/","summary":"\u003cp\u003eHacker News で取り上げられていた emacs-nav を使ってみた。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://code.google.com/p/emacs-nav/\"\u003ehttp://code.google.com/p/emacs-nav/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eインストール方法は簡単で\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e% wget http://emacs-nav.googlecode.com/files/emacs-nav-20110220a.tar.gz\n% tar zxvf emacs-nav-20110220a.tar.gz\n% mv emacs-nav-20110220a ~/.emacs.d/emacs-nav\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eして\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e;; emacs-nav\n(add-to-list 'load-path \u0026quot;~/.emacs.d/emacs-nav/\u0026quot;)\n(require 'nav)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eするだけ。\u003c/p\u003e\n\u003cp\u003e見た目はこんな感じ。\u003c/p\u003e\n\u003cimg src=\"http://jedipunkz.github.com/pix/emacs-nav.png\"\u003e\n\u003cp\u003e起動は M-x nav と入力。ウィンドウの左にファイルブラウザが開いてファイルを選択\n出来る。これだけだと、使うメリットを感じないが、面白いのがマウスで選択出来ると\nころ。TextMate のブラウザのような感じだ。\u003c/p\u003e\n\u003cp\u003e今だと anything.el が便利すぎて、こちらを利用する価値を見出せるか分からないけ\nど、暫く使ってみようと思う。\u003c/p\u003e","title":"Emacs でファイルブラウザ emacs-nav を利用"},{"content":"http://www.emacswiki.org/emacs-en/PowerLine\nvim の powerline に似たモードを emacs で実現できる powerline.el を試してみた。 見た目が派手でかわいくなるので、気に入った。w すでに下記のようなサイトで紹介さ れつつある。\nhttp://d.hatena.ne.jp/kenjiskywalker/20120502/1335922233 http://n8.hatenablog.com/entry/2012/03/21/172928\nインストールすると見た目はこんな感じになる。\nうん、かわいい。\nインストール方法は簡単。auto-install 環境が構築されているとして\nauto-install-from-emacs-wiki powerline.el する。auto-install が入っていなければ http://www.emacswiki.org/emacs/powerline.el ここにある powerline.el をダウンロードして ~/.emacs.d/lisp/ 配下など path の通っ ている所に入れれば OK。\nあとは .emacs 内には下記を追記するだけだ。\n;; powerline.el (defun arrow-right-xpm (color1 color2) \u0026quot;Return an XPM right arrow string representing.\u0026quot; (format \u0026quot;/* XPM */ static char * arrow_right[] = { \\\u0026quot;12 18 2 1\\\u0026quot;, \\\u0026quot;. c %s\\\u0026quot;, \\\u0026quot; c %s\\\u0026quot;, \\\u0026quot;. \\\u0026quot;, \\\u0026quot;.. \\\u0026quot;, \\\u0026quot;... \\\u0026quot;, \\\u0026quot;.... \\\u0026quot;, \\\u0026quot;..... \\\u0026quot;, \\\u0026quot;...... \\\u0026quot;, \\\u0026quot;....... \\\u0026quot;, \\\u0026quot;........ \\\u0026quot;, \\\u0026quot;......... \\\u0026quot;, \\\u0026quot;......... \\\u0026quot;, \\\u0026quot;........ \\\u0026quot;, \\\u0026quot;....... \\\u0026quot;, \\\u0026quot;...... \\\u0026quot;, \\\u0026quot;..... \\\u0026quot;, \\\u0026quot;.... \\\u0026quot;, \\\u0026quot;... \\\u0026quot;, \\\u0026quot;.. \\\u0026quot;, \\\u0026quot;. \\\u0026quot;};\u0026quot; color1 color2)) (defun arrow-left-xpm (color1 color2) \u0026quot;Return an XPM right arrow string representing.\u0026quot; (format \u0026quot;/* XPM */ static char * arrow_right[] = { \\\u0026quot;12 18 2 1\\\u0026quot;, \\\u0026quot;. c %s\\\u0026quot;, \\\u0026quot; c %s\\\u0026quot;, \\\u0026quot; .\\\u0026quot;, \\\u0026quot; ..\\\u0026quot;, \\\u0026quot; ...\\\u0026quot;, \\\u0026quot; ....\\\u0026quot;, \\\u0026quot; .....\\\u0026quot;, \\\u0026quot; ......\\\u0026quot;, \\\u0026quot; .......\\\u0026quot;, \\\u0026quot; ........\\\u0026quot;, \\\u0026quot; .........\\\u0026quot;, \\\u0026quot; .........\\\u0026quot;, \\\u0026quot; ........\\\u0026quot;, \\\u0026quot; .......\\\u0026quot;, \\\u0026quot; ......\\\u0026quot;, \\\u0026quot; .....\\\u0026quot;, \\\u0026quot; ....\\\u0026quot;, \\\u0026quot; ...\\\u0026quot;, \\\u0026quot; ..\\\u0026quot;, \\\u0026quot; .\\\u0026quot;};\u0026quot; color2 color1)) (defconst color1 \u0026quot;#FF6699\u0026quot;) (defconst color3 \u0026quot;#CDC0B0\u0026quot;) (defconst color2 \u0026quot;#FF0066\u0026quot;) (defconst color4 \u0026quot;#CDC0B0\u0026quot;) (defvar arrow-right-1 (create-image (arrow-right-xpm color1 color2) 'xpm t :ascent 'center)) (defvar arrow-right-2 (create-image (arrow-right-xpm color2 \u0026quot;None\u0026quot;) 'xpm t :ascent 'center)) (defvar arrow-left-1 (create-image (arrow-left-xpm color2 color1) 'xpm t :ascent 'center)) (defvar arrow-left-2 (create-image (arrow-left-xpm \u0026quot;None\u0026quot; color2) 'xpm t :ascent 'center)) (setq-default mode-line-format (list '(:eval (concat (propertize \u0026quot; %b \u0026quot; 'face 'mode-line-color-1) (propertize \u0026quot; \u0026quot; 'display arrow-right-1))) '(:eval (concat (propertize \u0026quot; %m \u0026quot; 'face 'mode-line-color-2) (propertize \u0026quot; \u0026quot; 'display arrow-right-2))) ;; Justify right by filling with spaces to right fringe - 16 ;; (16 should be computed rahter than hardcoded) '(:eval (propertize \u0026quot; \u0026quot; 'display '((space :align-to (- right-fringe 17))))) '(:eval (concat (propertize \u0026quot; \u0026quot; 'display arrow-left-2) (propertize \u0026quot; %p \u0026quot; 'face 'mode-line-color-2))) '(:eval (concat (propertize \u0026quot; \u0026quot; 'display arrow-left-1) (propertize \u0026quot;%4l:%2c \u0026quot; 'face 'mode-line-color-1))) )) (make-face 'mode-line-color-1) (set-face-attribute 'mode-line-color-1 nil :foreground \u0026quot;#fff\u0026quot; :background color1) (make-face 'mode-line-color-2) (set-face-attribute 'mode-line-color-2 nil :foreground \u0026quot;#fff\u0026quot; :background color2) (set-face-attribute 'mode-line nil :foreground \u0026quot;#fff\u0026quot; :background color3 :box nil) (set-face-attribute 'mode-line-inactive nil :foreground \u0026quot;#fff\u0026quot; :background color4) 色を少し変えただけで、ほぼ emacswiki に載っているコードそのままだ。no window だと寂しいことになるが、まぁ仕方ない。vim でも powerline を使って気に入ってい たので emacs でも使えるとあって嬉しい限りだ。これから少しずつカスタマイズして いこうと思う。\n","permalink":"https://jedipunkz.github.io/post/2012/05/04/powerline.el-emacs/","summary":"\u003cp\u003e\u003ca href=\"http://www.emacswiki.org/emacs-en/PowerLine\"\u003ehttp://www.emacswiki.org/emacs-en/PowerLine\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003evim の powerline に似たモードを emacs で実現できる powerline.el を試してみた。\n見た目が派手でかわいくなるので、気に入った。w すでに下記のようなサイトで紹介さ\nれつつある。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://d.hatena.ne.jp/kenjiskywalker/20120502/1335922233\"\u003ehttp://d.hatena.ne.jp/kenjiskywalker/20120502/1335922233\u003c/a\u003e\n\u003ca href=\"http://n8.hatenablog.com/entry/2012/03/21/172928\"\u003ehttp://n8.hatenablog.com/entry/2012/03/21/172928\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eインストールすると見た目はこんな感じになる。\u003c/p\u003e\n\u003cimg src=\"http://jedipunkz.github.com/pix/powerline.png\"\u003e\n\u003cp\u003eうん、かわいい。\u003c/p\u003e\n\u003cp\u003eインストール方法は簡単。auto-install 環境が構築されているとして\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eauto-install-from-emacs-wiki powerline.el\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eする。auto-install が入っていなければ\n\u003ca href=\"http://www.emacswiki.org/emacs/powerline.el\"\u003ehttp://www.emacswiki.org/emacs/powerline.el\u003c/a\u003e\nここにある powerline.el をダウンロードして ~/.emacs.d/lisp/ 配下など path の通っ\nている所に入れれば OK。\u003c/p\u003e\n\u003cp\u003eあとは .emacs 内には下記を追記するだけだ。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e;; powerline.el\n(defun arrow-right-xpm (color1 color2)\n  \u0026quot;Return an XPM right arrow string representing.\u0026quot;\n  (format \u0026quot;/* XPM */\nstatic char * arrow_right[] = {\n\\\u0026quot;12 18 2 1\\\u0026quot;,\n\\\u0026quot;. c %s\\\u0026quot;,\n\\\u0026quot;  c %s\\\u0026quot;,\n\\\u0026quot;.           \\\u0026quot;,\n\\\u0026quot;..          \\\u0026quot;,\n\\\u0026quot;...         \\\u0026quot;,\n\\\u0026quot;....        \\\u0026quot;,\n\\\u0026quot;.....       \\\u0026quot;,\n\\\u0026quot;......      \\\u0026quot;,\n\\\u0026quot;.......     \\\u0026quot;,\n\\\u0026quot;........    \\\u0026quot;,\n\\\u0026quot;.........   \\\u0026quot;,\n\\\u0026quot;.........   \\\u0026quot;,\n\\\u0026quot;........    \\\u0026quot;,\n\\\u0026quot;.......     \\\u0026quot;,\n\\\u0026quot;......      \\\u0026quot;,\n\\\u0026quot;.....       \\\u0026quot;,\n\\\u0026quot;....        \\\u0026quot;,\n\\\u0026quot;...         \\\u0026quot;,\n\\\u0026quot;..          \\\u0026quot;,\n\\\u0026quot;.           \\\u0026quot;};\u0026quot;  color1 color2))\n\n(defun arrow-left-xpm (color1 color2)\n  \u0026quot;Return an XPM right arrow string representing.\u0026quot;\n  (format \u0026quot;/* XPM */\nstatic char * arrow_right[] = {\n\\\u0026quot;12 18 2 1\\\u0026quot;,\n\\\u0026quot;. c %s\\\u0026quot;,\n\\\u0026quot;  c %s\\\u0026quot;,\n\\\u0026quot;           .\\\u0026quot;,\n\\\u0026quot;          ..\\\u0026quot;,\n\\\u0026quot;         ...\\\u0026quot;,\n\\\u0026quot;        ....\\\u0026quot;,\n\\\u0026quot;       .....\\\u0026quot;,\n\\\u0026quot;      ......\\\u0026quot;,\n\\\u0026quot;     .......\\\u0026quot;,\n\\\u0026quot;    ........\\\u0026quot;,\n\\\u0026quot;   .........\\\u0026quot;,\n\\\u0026quot;   .........\\\u0026quot;,\n\\\u0026quot;    ........\\\u0026quot;,\n\\\u0026quot;     .......\\\u0026quot;,\n\\\u0026quot;      ......\\\u0026quot;,\n\\\u0026quot;       .....\\\u0026quot;,\n\\\u0026quot;        ....\\\u0026quot;,\n\\\u0026quot;         ...\\\u0026quot;,\n\\\u0026quot;          ..\\\u0026quot;,\n\\\u0026quot;           .\\\u0026quot;};\u0026quot;  color2 color1))\n\n\n(defconst color1 \u0026quot;#FF6699\u0026quot;)\n(defconst color3 \u0026quot;#CDC0B0\u0026quot;)\n(defconst color2 \u0026quot;#FF0066\u0026quot;)\n(defconst color4 \u0026quot;#CDC0B0\u0026quot;)\n\n(defvar arrow-right-1 (create-image (arrow-right-xpm color1 color2) 'xpm t :ascent 'center))\n(defvar arrow-right-2 (create-image (arrow-right-xpm color2 \u0026quot;None\u0026quot;) 'xpm t :ascent 'center))\n(defvar arrow-left-1  (create-image (arrow-left-xpm color2 color1) 'xpm t :ascent 'center))\n(defvar arrow-left-2  (create-image (arrow-left-xpm \u0026quot;None\u0026quot; color2) 'xpm t :ascent 'center))\n\n(setq-default mode-line-format\n (list  '(:eval (concat (propertize \u0026quot; %b \u0026quot; 'face 'mode-line-color-1)\n                        (propertize \u0026quot; \u0026quot; 'display arrow-right-1)))\n        '(:eval (concat (propertize \u0026quot; %m \u0026quot; 'face 'mode-line-color-2)\n                        (propertize \u0026quot; \u0026quot; 'display arrow-right-2)))\n\n        ;; Justify right by filling with spaces to right fringe - 16\n        ;; (16 should be computed rahter than hardcoded)\n        '(:eval (propertize \u0026quot; \u0026quot; 'display '((space :align-to (- right-fringe 17)))))\n\n        '(:eval (concat (propertize \u0026quot; \u0026quot; 'display arrow-left-2)\n                        (propertize \u0026quot; %p \u0026quot; 'face 'mode-line-color-2)))\n        '(:eval (concat (propertize \u0026quot; \u0026quot; 'display arrow-left-1)\n                        (propertize \u0026quot;%4l:%2c  \u0026quot; 'face 'mode-line-color-1)))\n)) \n\n(make-face 'mode-line-color-1)\n(set-face-attribute 'mode-line-color-1 nil\n                    :foreground \u0026quot;#fff\u0026quot;\n                    :background color1)\n\n(make-face 'mode-line-color-2)\n(set-face-attribute 'mode-line-color-2 nil\n                    :foreground \u0026quot;#fff\u0026quot;\n                    :background color2)\n\n(set-face-attribute 'mode-line nil\n                    :foreground \u0026quot;#fff\u0026quot;\n                    :background color3\n                    :box nil)\n(set-face-attribute 'mode-line-inactive nil\n                    :foreground \u0026quot;#fff\u0026quot;\n                    :background color4)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e色を少し変えただけで、ほぼ emacswiki に載っているコードそのままだ。no window\nだと寂しいことになるが、まぁ仕方ない。vim でも powerline を使って気に入ってい\nたので emacs でも使えるとあって嬉しい限りだ。これから少しずつカスタマイズして\nいこうと思う。\u003c/p\u003e","title":"powerline.el で emacs モードラインを派手に"},{"content":"自宅ルータを Vyatta で運用開始したのだけど、無線ルータ化ができたのでメモしておき ます。\nまずはアクセスポイントとして稼働する無線カードの選定。下記の URL に Linux 系で 動作する無線カードチップ名の一覧が載っている。Vyatta は Linux 系ドライバを利用 しているので、この一覧が有効なはずだ。\nhttp://linuxwireless.org/en/users/Drivers\nIntel 系も結構アクセスポイントとしては動作しないことが判ったので Atheros の AR9k のカードを購入した。私が買ったのはAR9280 チップの mini PCI-E 無線カード。\n早速装着してみると、認識した！\n% dmesg | grep ath [ 11.528390] ath9k 0000:01:00.0: PCI INT A -\u0026gt; GSI 16 (level, low) -\u0026gt; IRQ 16 [ 11.528398] ath9k 0000:01:00.0: setting latency timer to 64 [ 11.961619] ath: EEPROM regdomain: 0x37 [ 11.961620] ath: EEPROM indicates we should expect a direct regpair map [ 11.961622] ath: Country alpha2 being used: AW [ 11.961623] ath: Regpair used: 0x37 [ 12.031676] ieee80211 phy0: Selected rate control algorithm 'ath9k_rate_control' 早速設定に入る。有線側有線 NIC と無線 NIC をブリッジ接続する。br0 デバイスにの み IP アドレスを振り、eth1, wlan0 は IP アドレスを振らない構成にする。\nまずはブリッジインターフェースを作る。\nset interfaces bridge br0 address ${IP_ADDRESS} set interfaces bridge br0 description LOCAL_NET eth1 (ローカル側有線 NIC) をブリッジ br0 に含める。\nset interfaces ethernet eth1 bridge-group bridge br0 set interfaces ethernet eth1 description LOCAL_NET wlan0 の設定。同じく br0 に含めて、諸々の設定を投入。\nset interfaces wireless wlan0 bridge-group bridge br0 set interfaces wireless wlan0 channel 2 set interfaces wireless wlan0 description LOCAL_NET set interfaces wireless wlan0 mode n set interfaces wireless wlan0 security wpa mode wpa2 set interfaces wireless wlan0 security wpa passphrase ${PASS} set interfaces wireless wlan0 ssid ${SSID} set interfaces wireless wlan0 type access-point commit save ${IP_ADDRESS}, ${SSID}, ${PASS} は任意です。読み替えてください。\nここでコツ。これらの設定をして commit してもうまく無線接続出来なかった。vyatta を再起動させる (予め save すること) と、無線接続ができた。\n購入した無線モジュールは mini PCE-I Atheros　AR5BXB92 (AR9280 チップ)、 PCI-E に変換するカード 、あとバッファローのアンテナだ。\n今のところ利用できているが、無線の強度が少し弱い気がする。これから少し調べてみ ます。\n","permalink":"https://jedipunkz.github.io/post/2012/05/04/vyatta-wireless-ap/","summary":"\u003cp\u003e自宅ルータを Vyatta で運用開始したのだけど、無線ルータ化ができたのでメモしておき\nます。\u003c/p\u003e\n\u003cp\u003eまずはアクセスポイントとして稼働する無線カードの選定。下記の URL に Linux 系で\n動作する無線カードチップ名の一覧が載っている。Vyatta は Linux 系ドライバを利用\nしているので、この一覧が有効なはずだ。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://linuxwireless.org/en/users/Drivers\"\u003ehttp://linuxwireless.org/en/users/Drivers\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eIntel 系も結構アクセスポイントとしては動作しないことが判ったので Atheros の\nAR9k のカードを購入した。私が買ったのはAR9280 チップの mini PCI-E 無線カード。\u003c/p\u003e\n\u003cp\u003e早速装着してみると、認識した！\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e% dmesg | grep ath\n[   11.528390] ath9k 0000:01:00.0: PCI INT A -\u0026gt; GSI 16 (level, low) -\u0026gt; IRQ\n16\n[   11.528398] ath9k 0000:01:00.0: setting latency timer to 64\n[   11.961619] ath: EEPROM regdomain: 0x37\n[   11.961620] ath: EEPROM indicates we should expect a direct regpair map\n[   11.961622] ath: Country alpha2 being used: AW\n[   11.961623] ath: Regpair used: 0x37\n[   12.031676] ieee80211 phy0: Selected rate control algorithm 'ath9k_rate_control'\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e早速設定に入る。有線側有線 NIC と無線 NIC をブリッジ接続する。br0 デバイスにの\nみ IP アドレスを振り、eth1, wlan0 は IP アドレスを振らない構成にする。\u003c/p\u003e","title":"Vyatta で無線アクセスポイント"},{"content":"Vyatta を自宅ルータで使い始めて感じたのは、PS3 などのゲーム機や IP 電話など UPnP 接続が必要なことがあるってこと。ただ Vyatta は UPnP に対応していないので、 どうしようかと思っていたら、有志の方が作ってくれたソフトウェアがあり、うちでも これを使うことにした。今回はその方法を記していきます。\nhttps://github.com/kiall/vyatta-upnp\n上記のソースを取得して生成するのだが、vyatta 上で構築する環境を作りたくないの で、私は Debian Gnu/Linux マシン上で行いました。Ubuntu でも大丈夫だと思います。\ndebian% sudo apt-get \u0026amp;\u0026amp; sudo apt-get install build-essential debian% git clone https://github.com/kiall/vyatta-upnp.git debian% cd vyatta-upnp debian% dpkg-buildpackage -us -uc -d 一つ上のディレクトリに vyatta-upnp_0.2_all.deb という .deb ファイルができあがっ ているはずで、これが UPnP パッケージファイル vyatta-upnp_0.2_all.deb です。\n次に vyatta 上での作業。packages.vyatta.com から libupnp4 と linux-igd を取得、 その後先ほど生成した vyatta-upnp_0.2_all.deb を vyatta 上に持ってきてからイン ストールします。\nvyatta# cd /tmp/ vyatta# wget http://packages.vyatta.com/debian/pool/main/libu/libupnp4/libupnp4_1.8.0~svn20100507-1_amd64.deb vyatta# wget http://packages.vyatta.com/debian/pool/main/l/linux-igd/linux-igd_1.0+cvs20070630-3_amd64.deb vyatta# scp ${DEBIAN}:/${SOMEWHERE}/vyatta-upnp_0.2_all.deb . # 先ほど生成したファイル vyatta# dpkg -i libupnp4_1.8.0~svn20100507-1_amd64.deb linux-igd_1.0+cvs20070630-3_amd64.deb vyatta# dpkg -i vyatta-upnp_0.2_all.deb これで設定が可能になりました。設定してみます。\nvyatta# configure vyatta# set service upnp listen-on eth1 vyatta# commit vyatta# save これで完了です。eth1 は 自宅環境に合わせて下さい。ルータのプライベート側インター フェース名です。私の家は wlan0 と eth1 をブリッジしているので listen-on br0 に しました。\n私の環境では PS3 の \u0026ldquo;アンチャーテッド3\u0026rdquo; で動作確認しています。\n","permalink":"https://jedipunkz.github.io/post/2012/04/29/vyatta-upnp/","summary":"\u003cp\u003eVyatta を自宅ルータで使い始めて感じたのは、PS3 などのゲーム機や IP 電話など\nUPnP 接続が必要なことがあるってこと。ただ Vyatta は UPnP に対応していないので、\nどうしようかと思っていたら、有志の方が作ってくれたソフトウェアがあり、うちでも\nこれを使うことにした。今回はその方法を記していきます。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/kiall/vyatta-upnp\"\u003ehttps://github.com/kiall/vyatta-upnp\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e上記のソースを取得して生成するのだが、vyatta 上で構築する環境を作りたくないの\nで、私は Debian Gnu/Linux マシン上で行いました。Ubuntu でも大丈夫だと思います。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edebian% sudo apt-get \u0026amp;\u0026amp; sudo apt-get install build-essential\ndebian% git clone https://github.com/kiall/vyatta-upnp.git\ndebian% cd vyatta-upnp\ndebian% dpkg-buildpackage -us -uc -d\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e一つ上のディレクトリに vyatta-upnp_0.2_all.deb という .deb ファイルができあがっ\nているはずで、これが UPnP パッケージファイル vyatta-upnp_0.2_all.deb です。\u003c/p\u003e\n\u003cp\u003e次に vyatta 上での作業。packages.vyatta.com から libupnp4 と linux-igd を取得、\nその後先ほど生成した vyatta-upnp_0.2_all.deb を vyatta 上に持ってきてからイン\nストールします。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003evyatta# cd /tmp/\nvyatta# wget http://packages.vyatta.com/debian/pool/main/libu/libupnp4/libupnp4_1.8.0~svn20100507-1_amd64.deb\nvyatta# wget http://packages.vyatta.com/debian/pool/main/l/linux-igd/linux-igd_1.0+cvs20070630-3_amd64.deb\nvyatta# scp ${DEBIAN}:/${SOMEWHERE}/vyatta-upnp_0.2_all.deb . # 先ほど生成したファイル\nvyatta# dpkg -i libupnp4_1.8.0~svn20100507-1_amd64.deb linux-igd_1.0+cvs20070630-3_amd64.deb\nvyatta# dpkg -i vyatta-upnp_0.2_all.deb\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eこれで設定が可能になりました。設定してみます。\u003c/p\u003e","title":"vyatta で UPnP 接続"},{"content":"自宅ルータを Vyatta で構築してみたくなり、秋葉原の ark でマシンを調達しました。 Broadcom の BCM57780 チップが搭載された NIC がマザーボード J\u0026amp;W MINIX™ H61M-USB3 だったのですが、Vyatta.org によると Broadcom の NIC が Certificated Hardware に載っていなくて心配でした。まぁ定評のある NIC メーカだから動くだろう と楽観視していたのですけど、案の定動きました。vyatta.org の Certificated Hardware にコミットしたら \u0026ldquo;user tested\u0026rdquo; として掲載してもらえました。\nhttp://www.vyatta.org/hardware/interfaces\nこんな感じに見えています。\n# dmesg | grep Broadcom [ 3.284646] tg3 0000:03:00.0: eth0: attached PHY driver [Broadcom BCM57780](mii_bus:phy_addr=300:01) [ 3.524122] tg3 0000:05:00.0: eth1: attached PHY driver [Broadcom BCM57780](mii_bus:phy_addr=500:01) 今回は、基本的な設定 (PPPoE, NAT, DHCP) 周りを記していきます。\n環境は\u0026hellip;\n+--------+ | Modem | +--------+ | | pppoe0 +--eth0--+ | vyatta | +--eth1--+ | 192.168.1.0/24 +----------+ | |192.168.1.10 +--------+ +--------+ | CPE | | DNS | +--------+ +--------+ として記します。\nまずインストール。http://www.vyatta.org/downloads から 64bit VC6.3 Live CD iso をダウンロードしてきます。(2012/04/28現在最新). インストール対象のマシンに挿入 して CDROM ブートすると Vyatta が立ち上がるので、ユーザ vyatta, パスワード vyatta でログインし\n# install-system します。インタラクティブに問い合わせられるので答えていってください。インストールが 終わったらマシンを再起動します。\neth1 にプライベート IP アドレスを振ります。IP アドレスは適当に読み替えてくださ い。また基本的な設定も行います。\n# set service ssh port 10022 # firewall 設定するまではこうしたほうが安心です # set system host-name ${HOSTNAME} # set system time-zone Asia/Tokyo # set system name-server 8.8.8.8 # set interface ethernet eth1 address 192.168.1.254 eth0 を PPPoE デバイスとして利用して PPPoE 接続を実際にします。\n# set interface ethernet eth0 pppoe 0 # set interface ethernet eth0 pppoe 0 user-id ${PPPoE_username} # set interface ethernet eth0 pppoe 0 password ${PPPoE_password} # set interface ethernet eth0 pppoe 0 name-server auto # set interface ethernet eth0 pppoe 0 defaultroute auto # set interface ethernet eth0 pppoe 0 local-address XXX.XXX.XXX.XXX # 固定IPがある場合 # commit ここまでで vyatta ノードからインターネットに接続出来るようになります。\n次に、ローカルネットワーク上の CPE だったりサーバノードからのインターネット接 続のために NAT 設定をします。\n# set service nat rule 1 # set service nat rule 1 type masquerade # set service nat rule 1 source address 192.168.1.0/24 # set service nat rule 1 outbound-interface pppoe0 # commit CPE からのインターネットへの接続が出来るようになりました。\nあとは自宅ルータなので DHCP サービスがあったほうが便利だよね、とうことで。\n# set service dhcp-server shared-network-name HOME subnet 198.168.1.0/24 start 192.168.1.100 stop 192.168.1.150 # set service dhcp-server shared-network-name HOME subnet 192.168.1.0/24 default-router 192.168.1.254 # set service dhcp-server shared-network-name HOME subnet 198.168.1.0/24 dns-server 8.8.8.8 # set service dhcp-server shared-network-name HOME subnet 198.168.1.0/24 dns-server 8.8.4.4 # commit ローカルネットワーク上の CPE から DHCP リクエストを出してみてください。IP が取得できると思います。\nここまでで基本的な有線自宅ルータとしての構築はほぼ完了ですが、最後にローカルネットワーク上のサーバ を DNS サーバとして稼働させるための NAT 設定方法を記しておきます。\n# set service nat rule 2 # set service nat rule 2 destination # set service nat rule 2 type destination # set service nat rule 2 type destination # set service nat rule 2 inbound-interface pppoe0 # set service nat rule 2 protocol udp # set service nat rule 2 destination port 53 # set service nat rule 2 inside-address address 192.168.1.10 # commit DNS は稀に TCP にフォールバックするので同様に TCP ようの NAT ルールも追記します。\n# set service nat rule 3 # set service nat rule 3 destination # set service nat rule 3 type destination # set service nat rule 3 type destination # set service nat rule 3 inbound-interface pppoe0 # set service nat rule 3 protocol tcp # set service nat rule 3 destination port 53 # set service nat rule 3 inside-address address 192.168.1.10 set service nat rule 3 destinationset service nat rule 3 destination # commit\n最後に設定を保存するために\u0026hellip;\n# save して終わりです。次回は UPnP な設定方法を書いて行きたいと思います。\nちなみに、私の自宅ルータはこんなモノを使いました。\n","permalink":"https://jedipunkz.github.io/post/2012/04/28/vyattarouter/","summary":"\u003cp\u003e自宅ルータを Vyatta で構築してみたくなり、秋葉原の ark でマシンを調達しました。\nBroadcom の BCM57780 チップが搭載された NIC がマザーボード J\u0026amp;W MINIX™\nH61M-USB3 だったのですが、Vyatta.org によると Broadcom の NIC が Certificated\nHardware に載っていなくて心配でした。まぁ定評のある NIC メーカだから動くだろう\nと楽観視していたのですけど、案の定動きました。vyatta.org の Certificated\nHardware にコミットしたら \u0026ldquo;user tested\u0026rdquo; として掲載してもらえました。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://www.vyatta.org/hardware/interfaces\"\u003ehttp://www.vyatta.org/hardware/interfaces\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eこんな感じに見えています。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# dmesg | grep Broadcom\n[    3.284646] tg3 0000:03:00.0: eth0: attached PHY driver [Broadcom BCM57780](mii_bus:phy_addr=300:01)\n[    3.524122] tg3 0000:05:00.0: eth1: attached PHY driver [Broadcom BCM57780](mii_bus:phy_addr=500:01)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e今回は、基本的な設定 (PPPoE, NAT, DHCP) 周りを記していきます。\u003c/p\u003e\n\u003cp\u003e環境は\u0026hellip;\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e+--------+\n|  Modem |\n+--------+\n|\n| pppoe0\n+--eth0--+\n| vyatta |\n+--eth1--+\n|           192.168.1.0/24\n+----------+\n|          |192.168.1.10\n+--------+ +--------+\n|  CPE   | |  DNS   |\n+--------+ +--------+\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eとして記します。\u003c/p\u003e","title":"vyatta で自宅ルータ構築"},{"content":"今週/先週？、Hacker News で取り上げられた Mosh を自宅 と会社で使い始めた。SSH 代替なソフトウェアで、SSP (State Synchronization Protocol)over UDP で動作している。MIT が開発したそうだ。\n動作は、クライアントがシーケンス番号と共にデータグラムをサーバに送信し、同期し 続ける。クライアントがローミングし IP アドレスが代わる等した時、以前より大きい シーケンス番号と共に正当なパケットが送信されたとサーバが認識した場合のみ、サー バは新しいソース IP アドレスを新たなクライアントだと認識する。もちろん、この場 合のローミングは NAT 越しの IP 再アサイン時やクライアントのネットワークインター フェース切り替えやノート PC を新たな無線アクセスポイント配下へ移動した場合も同 様に動作する。めちゃ便利やん。Mosh は SSP を2経路持ち、1つはクライアントからサー バへユーザの打ったキーの同期を取る。もう一方はサーバからクライアントへで、スク リーンの状態をクライアントへ同期を取るためだ。\nつまり、ノート PC やその他モバイル機器の IP アドレスが変わったとしても接続性は 担保され、また ノート PC のスリープ解除後にも接続性は確保され続ける。また、UDP で動作しているので、フルスクリーンの vim や emacs 等での再描画の遅延等も起こり にくそうだ。あと Ctrl-C 。TCP だと、キータイプがサーバプログラムに伝わらない状 況はプログラムプロセスが混雑しているとよくあるのだが、SSP over UDP での Ctrl-C はそういうことが無いそうだ。\nまた、認証機構は SSH に任せているので sshd は引き続き稼働させておく必要がある。 mosh は接続する先のユーザが一般ユーザ権限で動作させるプログラムでしかない。つ まり mosh daemon は必要ないようだ。\n実際にインストールしてみた。Mac の場合、homebrew で\n% brew update % brew install mobile-shell で完了。私はサーバに Debian と Ubuntu を使っているのだが、Debian の場合は testing, unstable でパッケージが容易されている。が、testing のパッケージを使っ た所、動作が不安定だった。文字を削除しても一文字消えない等。よって、github か ら最新のソースを取得。(unstable のパッケージでもイイかもしれない、私は試してな いです。)\n% sudo apt-get install protobuf-compiler libprotobuf-dev pkg-config \\ libboost-dev libncurses5-dev % cd gitwork % git clone https://github.com/keithw/mosh.git % cd mosh % ./autogen.sh \u0026amp;\u0026amp; ./configure \u0026amp;\u0026amp; make % sudo make install Ubuntu の場合は、\n% sudo apt-get install python-software-properties % sudo add-apt-repository ppa:keithw/mosh % sudo apt-get update % sudo apt-get install mosh だ。その他の destribution, OS の場合は下記のリンクを参照してみてください。\nhttp://mosh.mit.edu/#getting\n次にクライアント \u0026lt;-\u0026gt; サーバ間の経路が変わっても接続したままになるのか、テスト してみた。\nMacBook wireless nic \u0026lt;-\u0026gt; server を MacBook ethernet nic \u0026lt;-\u0026gt; server に接続しな おしてみた。また、その接続しなおしのタイミングの間、\n% while(true) do echo \u0026quot;is mosh alive ?\u0026quot; ;sleep 3; done とシェル上で実行し続けた。結果、サーバに再接続され、また上のシェルプロセスも継 続され続けた！これには驚いた。\nということで、使い始めて1週間だが今のところ快適。\n会社の方に protobuf は Google が自社ネットワーク内のトラフィック軽減のために開 発したものだ、と言っていた。こんなところでも Google の技術力がすんごいことに改 めて気がついたよ。\n","permalink":"https://jedipunkz.github.io/post/2012/04/14/ssh-mosh/","summary":"\u003cp\u003e今週/先週？、Hacker News で取り上げられた \u003ca href=\"http://mosh.mit.edu/\"\u003eMosh\u003c/a\u003e を自宅\nと会社で使い始めた。SSH 代替なソフトウェアで、SSP (State Synchronization\nProtocol)over UDP で動作している。MIT が開発したそうだ。\u003c/p\u003e\n\u003cp\u003e動作は、クライアントがシーケンス番号と共にデータグラムをサーバに送信し、同期し\n続ける。クライアントがローミングし IP アドレスが代わる等した時、以前より大きい\nシーケンス番号と共に正当なパケットが送信されたとサーバが認識した場合のみ、サー\nバは新しいソース IP アドレスを新たなクライアントだと認識する。もちろん、この場\n合のローミングは NAT 越しの IP 再アサイン時やクライアントのネットワークインター\nフェース切り替えやノート PC を新たな無線アクセスポイント配下へ移動した場合も同\n様に動作する。めちゃ便利やん。Mosh は SSP を2経路持ち、1つはクライアントからサー\nバへユーザの打ったキーの同期を取る。もう一方はサーバからクライアントへで、スク\nリーンの状態をクライアントへ同期を取るためだ。\u003c/p\u003e\n\u003cp\u003eつまり、ノート PC やその他モバイル機器の IP アドレスが変わったとしても接続性は\n担保され、また ノート PC のスリープ解除後にも接続性は確保され続ける。また、UDP\nで動作しているので、フルスクリーンの vim や emacs 等での再描画の遅延等も起こり\nにくそうだ。あと Ctrl-C 。TCP だと、キータイプがサーバプログラムに伝わらない状\n況はプログラムプロセスが混雑しているとよくあるのだが、SSP over UDP での Ctrl-C\nはそういうことが無いそうだ。\u003c/p\u003e\n\u003cp\u003eまた、認証機構は SSH に任せているので sshd は引き続き稼働させておく必要がある。\nmosh は接続する先のユーザが一般ユーザ権限で動作させるプログラムでしかない。つ\nまり mosh daemon は必要ないようだ。\u003c/p\u003e\n\u003cp\u003e実際にインストールしてみた。Mac の場合、homebrew で\u003c/p\u003e","title":"Mosh を使う"},{"content":"ノート PC を購入するといつも Debian Gnu/Linux sid をインストールするのだけれど も、X Window System や InputMethod をインストールして利用し始められるところま での手順っていつも忘れる。メモとしてブログに載せておきます。\nconsole 上での ctrl:caps swap 設定 取りあえずこの設定をしないと、何も操作出来ない。Caps Lock と Control キーを入 れ替える設定です。\n/etc/default/keyboard を下記のように修正\nXKBOPTIONS=\u0026quot;ctrl:swapcaps\u0026quot; 下記のコマンドで設定を反映。\n% sudo /etc/init.d/console-setup restart sid の sources.list 設定 Debian Gnu/Linux をノート PC にインストールする時は必ず sid を入れます。新し目 のソフトウェアを使いたいから。\ndeb http://ftp.riken.jp/Linux/debian/debian/ unstable main contrib non-free deb-src http://ftp.riken.jp/Linux/debian/debian/ unstable main contrib non-free 下記のコマンドで dist-upgrade\n% sudo apt-get update % sudo apt-get dist-upgrade iwlwifi のインストールとネットワーク設定 買うノート PC はいつも ThinkPad。大体 intel チップな Wi-Fi モジュールが搭載さ れているので、iwlwifi を使う。\n% sudo apt-get install firmware-iwlwifi % sudo modprobe iwl4965 # 各々の端末に合わせる。lspci コマンドで確認 % sudo modprobe iwlagn % sudo iwconfig % sudo ifconfig wlan0 up 次に /etc/network/interfaces の設定\nauto wlan0 iface wlan0 inet dhcp pre-up /sbin/wpa_supplicant -B -Dwext -c/etc/wpa_supplicant/wpa_supplicant.conf -iwlan0 pre-up iwconfig wlan0 essid ${SSID} pre-up ip link set wlan0 up pre-up iwconfig wlan0 ap any /etc/wpa_supplicant/wpa_supplicant.conf の設定\n% sudo wpa_passphrase ${SSID} ${PASSPHRASE} \u0026gt; /etc/wpa_supplicant/wpa_supplicant.conf 生成した上記のファイルを修正・加筆\nnetwork={ proto=WPA WPA2 key_mgmt=WPA-PSK pairwise=CCMP TKIP ssid=\u0026quot;${SSD}\u0026quot; psk=..... } ネットワークインターフェースの再起動で接続\n% sudo service networking restart X Window 周りの設定 必要なパッケージをインストールする。ビデオカードドライバは端末に合わせてインス トールする。thinkpad は Intel なビデオカードの場合が多いので下記のように指定。\nまた、昔はよく enlightenment を使っていたのだけど、e17 が何時まで経っても完成 度上がらないので諦めて openbox を使うことに。\n% sudo apt-get insatll openbox obmenu xorg xserver-xorg-video-intel console 同様に ctrl:caps を swap する。xorg.conf が無い(2012/04時点)ので、sid では /etc/X11/Xsession.d/10setxkbmap を下記のように生成して対応。\n#!/bin/sh setxkbmap -option ctl:nocaps 日本語入力 input method のインストール 日本語入力に uim-mozc を利用することに。今のところこれが一番快適。\n% sudo apt-get install uim-mozc uim-xim uim-utils mozc-utils-gui mozc-server uim-qt /etc/X11/openbox/autostart に下記の行を追記\nif type uim-xim \u0026amp;\u0026gt; /dev/null ; then uim-xim \u0026amp; uim-toolbar-qt4 \u0026amp; fi uim-toolbar-qt4 の preference で \u0026ldquo;オン・オフキー\u0026rdquo; 等もろもろを好みに設定。\nalsa によるサウンドの出力 昔は苦労したサウンド出力も、今ではこんなに簡単な操作で出来るようになりましたｗ\n% sudo apt-get install alsa-utils alsa-base % sudo modprobe snd-pcm-oss % sudo alsactl init % sudo alsactl store 以上です。ここまで来れば、あとは好みで設定していけばいい。もちろん Ubuntu 等の dist を使えば、こんな操作は必要無いのだけど、自分でやらないとどうも気持ちが悪 いし、万が一トラブルが起きても自分で設定していれば治せるし。\nまぁ、最近では MacBook を使うことがほとんどなのですが..。or2\n","permalink":"https://jedipunkz.github.io/post/2012/04/07/debian-sid-on-thinkpad-first-setup/","summary":"\u003cp\u003eノート PC を購入するといつも Debian Gnu/Linux sid をインストールするのだけれど\nも、X Window System や InputMethod をインストールして利用し始められるところま\nでの手順っていつも忘れる。メモとしてブログに載せておきます。\u003c/p\u003e\n\u003ch2 id=\"console-上での-ctrlcaps-swap-設定\"\u003econsole 上での ctrl:caps swap 設定\u003c/h2\u003e\n\u003cp\u003e取りあえずこの設定をしないと、何も操作出来ない。Caps Lock と Control キーを入\nれ替える設定です。\u003c/p\u003e\n\u003cp\u003e/etc/default/keyboard を下記のように修正\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eXKBOPTIONS=\u0026quot;ctrl:swapcaps\u0026quot;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e下記のコマンドで設定を反映。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e% sudo /etc/init.d/console-setup restart\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id=\"sid-の-sourceslist-設定\"\u003esid の sources.list 設定\u003c/h2\u003e\n\u003cp\u003eDebian Gnu/Linux をノート PC にインストールする時は必ず sid を入れます。新し目\nのソフトウェアを使いたいから。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edeb http://ftp.riken.jp/Linux/debian/debian/ unstable main contrib non-free\ndeb-src http://ftp.riken.jp/Linux/debian/debian/ unstable main contrib non-free\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e下記のコマンドで dist-upgrade\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e% sudo apt-get update\n% sudo apt-get dist-upgrade\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id=\"iwlwifi-のインストールとネットワーク設定\"\u003eiwlwifi のインストールとネットワーク設定\u003c/h2\u003e\n\u003cp\u003e買うノート PC はいつも ThinkPad。大体 intel チップな Wi-Fi モジュールが搭載さ\nれているので、iwlwifi を使う。\u003c/p\u003e","title":"debian sid on thinkpad"},{"content":"長年 Gnu screen 愛用者だったのだけど完全に tmux に移行しました。\n愛用している iterm2 との相性も良く、好都合な点が幾つかあり移行する価値がありました。\nただ、サーバサイドでの利用は諦めました。問題だったコピペ問題をクリアしている tmux のバージョンが Debian sid から取得出来たのだけど、まだまだ完成度高くなく..。\nよって、Mac に tmux をインストールして作業するようになりました。インストール方法はこれ。\n予め https://github.com/mxcl/homebrew/wiki/installation に したがって homebrew をインストールする必要あり。\n% brew update % brew install tmux インストールしたら .tmux.conf の作成に入る。prefix キーは C-t にしたかった。screen 時代から これを使っていて指がそう動くから。\n# prefix key set-option -g prefix C-t またステータスライン周りの設定。色なども自分で選択すると良い。\n# view set -g status-interval 5 set -g status-left-length 16 set -g status-right-length 50 # status set -g status-fg white set -g status-bg black set -g status-left-length 30 set -g status-left '#[fg=white,bg=black]#H#[fg=white]:#[fg=white][#S#[fg=white]][#[default]' set -g status-right '#[fg=white,bg=red,bold] [%Y-%m-%d(%a) %H:%M]#[default]' # window-status-current setw -g window-status-current-fg white setw -g window-status-current-bg red setw -g window-status-current-attr bold#,underscore # pane-active-border set -g pane-active-border-fg black set -g pane-active-border-bg blue UTF-8 有効化やキーバインド設定等は\u0026hellip;\n# Option set-window-option -g utf8 on #set-window-option -g mode-keys vi 私は emacs 使いなので vi モードは無効にした。これで emacs モードが有効になる。\nマウス選択を有効にすると、便利かもしれない。\n#set-option -g mouse-select-pane on #set-option -g mouse-resize-pane が、PANE (ペイン) を横分割した際にこれらの設定を入れているとマウスでのコピーペーストが出来ない ため、私は無効にした。\nその他の設定。.tmux.conf を tmux 開いたまま設定リロードだったり、ペインの移動キーバインド等。 特に hjkl キーの vi キーバインドをペイン移動のためにアサインするため、その他のキーを下記のように 設定している。ウィンドウの移動は C-n, C-p にも追加でアサイン (default は n, p) し、ペインの移動 には hjkl をアサインした。\nbind C-r source-file ~/.tmux.conf bind C-n next-window bind C-p previous-window bind c new-window bind | split-window -h bind C-k kill-pane bind K kill-window bind i display-panes bind y copy-mode bind h select-pane -L bind j select-pane -D bind k select-pane -U bind l select-pane -R ウィンドウの中にペインの概念が入ったことで、作業効率が非常に上がったし、Mac 上のアプリなので、Mac を 開いた瞬間に作業に入れる。(今までは debian 上の screen で生活していた) debian 上の tmux に移行する必要 があるかどうか、これから使い込んでみたいと思う。その時にまたレビューします。\n","permalink":"https://jedipunkz.github.io/post/2012/04/01/switching-screen-tmux/","summary":"\u003cp\u003e長年 Gnu screen 愛用者だったのだけど完全に tmux に移行しました。\u003c/p\u003e\n\u003cp\u003e愛用している iterm2 との相性も良く、好都合な点が幾つかあり移行する価値がありました。\u003c/p\u003e\n\u003cp\u003eただ、サーバサイドでの利用は諦めました。問題だったコピペ問題をクリアしている tmux のバージョンが\nDebian sid から取得出来たのだけど、まだまだ完成度高くなく..。\u003c/p\u003e\n\u003cp\u003eよって、Mac に tmux をインストールして作業するようになりました。インストール方法はこれ。\u003c/p\u003e\n\u003cp\u003e予め \u003ca href=\"https://github.com/mxcl/homebrew/wiki/installation\"\u003ehttps://github.com/mxcl/homebrew/wiki/installation\u003c/a\u003e に\nしたがって homebrew をインストールする必要あり。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e% brew update\n% brew install tmux\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eインストールしたら .tmux.conf の作成に入る。prefix キーは C-t にしたかった。screen 時代から\nこれを使っていて指がそう動くから。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# prefix key\nset-option -g prefix C-t\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eまたステータスライン周りの設定。色なども自分で選択すると良い。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# view\nset -g status-interval 5\nset -g status-left-length 16\nset -g status-right-length 50\n# status\nset -g status-fg white\nset -g status-bg black\nset -g status-left-length 30\nset -g status-left '#[fg=white,bg=black]#H#[fg=white]:#[fg=white][#S#[fg=white]][#[default]'\nset -g status-right '#[fg=white,bg=red,bold] [%Y-%m-%d(%a) %H:%M]#[default]'\n# window-status-current\nsetw -g window-status-current-fg white\nsetw -g window-status-current-bg red\nsetw -g window-status-current-attr bold#,underscore\n# pane-active-border\nset -g pane-active-border-fg black\nset -g pane-active-border-bg blue\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eUTF-8 有効化やキーバインド設定等は\u0026hellip;\u003c/p\u003e","title":"switching screen-\u003etmux"},{"content":"pages.github.com は github.com の WEB ホスティングサービスです。これを利用して octopress のブログを 構築する方法をメモしていきます。\nまず、github.com に \u0026ldquo;${好きな名前}.github.com\u0026rdquo; という名前のレポジトリを github.com 上で作成します。 レポジトリの作成は普通のレポジトリ作成と同じ方法で行えます。しばらくすると \u0026ldquo;${好きな名前}.github.com のページがビルド出来ました\u0026rdquo; という内容でメールが送られてきます。\npages.github.com によると、レポジトリページで \u0026ldquo;GitHub Page\u0026rdquo; にチェックを入れろと書いてありますが、情報が古いようです。 2012/03/20 現在、この操作の必要はありませんでした。\n次に octopress の環境構築。\noctopress は、jekyll ベースのブログツールです。markdown 形式で記事を書くのですが、emacs や vim 等 好きなエディタを使って記事を書けるので便利です。最近 \u0026ldquo;Blogging with Emacs\u0026rdquo; なんてブログをよく目にしたと 思うのですが、まさにソレですよね。エンジニアにとっては嬉しいブログ環境です。\nまずは、rvm の環境構築を。octopress は ruby 1.9.2 以上が必要なので用意するのですが rvm を使うと 手軽に用意出来るので、今回はその方法を記します。\n参考 URL は http://octopress.org/docs/setup/rvm/ です。\nまずは準備から。私の環境は Ubuntu Server 10.04 LTS なのですが、下記のパッケージが必用になります。\n% sudo apt-get install gcc make zlib1g-dev libssl-dev 下記のコマンドを実行すると、rvm がインストールされます。\n% bash -s stable \u0026lt; \u0026lt;(curl -s https://raw.github.com/wayneeseguin/rvm/master/binscripts/rvm-installer) 次に使っている shell に合わせて rc ファイルを設定します。\nbash なら\u0026hellip;\n% echo '[[ -s \u0026quot;$HOME/.rvm/scripts/rvm\u0026quot; ]] \u0026amp;\u0026amp; . \u0026quot;$HOME/.rvm/scripts/rvm\u0026quot; # Load RVM function' \u0026gt;\u0026gt; ~/.bash_profile % source ~/.bash_profile zsh なら\u0026hellip;\n% echo '[[ -s $HOME/.rvm/scripts/rvm ]] \u0026amp;\u0026amp; source $HOME/.rvm/scripts/rvm' \u0026gt;\u0026gt; ~/.zshrc % source ~/.zshrc ruby 1.9.2 をインストールします。\n% rvm install 1.9.2 \u0026amp;\u0026amp; rvm use 1.9.2 % rvm rubygems latest 次に octopress のインストールと環境設定です。\n% mkdir ${好きな名前}.github.com % cd ${好きな名前}.github.com % git clone git://github.com/imathis/octopress.git octopress % cd octopress % gem install bundler % bundle install % rake install # classic テーマのインストール ここまで出来たら github.com へデプロイするだけです。\n% rake setup_github_pages Enter the read/write url for your repository: と表示されるので、作成した github.com ページの情報を入力します。私の場合は\ngit@github.com:chobiwan/chobiwan.github.com.git この操作で、.git 内の情報諸々を更新してくれます。実行した後に覗いて見て下さい。\nです。最後にブログ生成とデプロイを実行します。この作業で ${好きな名前}.github.com へのデプロイが 実行されます。\n% rake gen_deploy 暫く時間がかかりますが (数分) 、${好きな名前}.github.com のサイトにアクセスできるようになっている はずです。\n次に、必要最低限の設定を行います。_config.yml ファイル内の下記の情報を満たしていきましょう。\nurl: # For rewriting urls for RSS, etc title = # Used in the header and title tags title = # A description used in the header simple_search: # Search engine for simple site search description = # A default meta description for your site subscribe_rss: # Url for your blog\u0026rsquo;s feed, defauts to /atom.xml subscribe_email: # Url to subscribe by email (service required) email: # Email address for the RSS feed if you want it.\nではいよいよ、最初の投稿を。\n% rake new_post[\u0026quot;title\u0026quot;] % rake new_post\\[\u0026quot;title\u0026quot;\\] # zsh の場合 source/_posts/2012-03-20-test-post.markdown という日付付きファイルが生成されるので、このファイルを 好きなエディタで編集します。\nmarkdown 形式で記すことが出来ます。上記の操作でテンプレートらしきファイルになっているので \u0026ldquo;\u0026mdash;\u0026rdquo; の配下から記事を書いていきます。また categories 等の情報は自分で修正出来ます。\n書き終わったら、ブログを生成してデプロイです。\n% rake gen_deploy 先ほど作った ${好きな名前}.github.com へアクセスしてみましょう。octopress のブログが完成している はずです。\n","permalink":"https://jedipunkz.github.io/post/2012/03/20/github-dot-com-de-octopress-gou-zhu/","summary":"\u003cp\u003epages.github.com は github.com の WEB ホスティングサービスです。これを利用して octopress のブログを\n構築する方法をメモしていきます。\u003c/p\u003e\n\u003cp\u003eまず、github.com に \u0026ldquo;${好きな名前}.github.com\u0026rdquo; という名前のレポジトリを github.com 上で作成します。\nレポジトリの作成は普通のレポジトリ作成と同じ方法で行えます。しばらくすると\n\u0026ldquo;${好きな名前}.github.com のページがビルド出来ました\u0026rdquo; という内容でメールが送られてきます。\u003c/p\u003e\n\u003cp\u003epages.github.com によると、レポジトリページで \u0026ldquo;GitHub Page\u0026rdquo; にチェックを入れろと書いてありますが、情報が古いようです。\n2012/03/20 現在、この操作の必要はありませんでした。\u003c/p\u003e\n\u003cp\u003e次に octopress の環境構築。\u003c/p\u003e\n\u003cp\u003eoctopress は、jekyll ベースのブログツールです。markdown 形式で記事を書くのですが、emacs や vim 等\n好きなエディタを使って記事を書けるので便利です。最近 \u0026ldquo;Blogging with Emacs\u0026rdquo; なんてブログをよく目にしたと\n思うのですが、まさにソレですよね。エンジニアにとっては嬉しいブログ環境です。\u003c/p\u003e\n\u003cp\u003eまずは、rvm の環境構築を。octopress は ruby 1.9.2 以上が必要なので用意するのですが rvm を使うと\n手軽に用意出来るので、今回はその方法を記します。\u003c/p\u003e\n\u003cp\u003e参考 URL は \u003ca href=\"http://octopress.org/docs/setup/rvm/\"\u003ehttp://octopress.org/docs/setup/rvm/\u003c/a\u003e です。\u003c/p\u003e\n\u003cp\u003eまずは準備から。私の環境は Ubuntu Server 10.04 LTS なのですが、下記のパッケージが必用になります。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e% sudo apt-get install gcc make zlib1g-dev libssl-dev\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e下記のコマンドを実行すると、rvm がインストールされます。\u003c/p\u003e","title":"github.com で octopress 構築"},{"content":"Carbon な API は排除していくべきと Apple も言っているようですし、自宅も会社も Cocoa な emacs を使うようになりました。\nその手順を書いていきます。\nソースとパッチでビルドも出来るのですが、homebrew 使うとメチャ楽なので今回はそれを使います。 homebrew は公式サイトに詳しいことが書いてありますけどインストールがワンラインで済みます。 あと、事前に AppStore で Xcode を入れてください。\n% /usr/bin/ruby -e \u0026quot;$(curl -fsSL https://raw.github.com/gist/323731)\u0026quot; だけです。\nそして Cocoa な emacs インストール。\n% brew install --cocoa emacs % sudo mv /usr/local/Cellar/emacs/24.1/Emacs.app /Applications/ 以上です..。簡単すぎる。先人たちのおかげですね。\n次は時間見つけて anything.el のことを書こうかなぁと思ってます。\n参考 URL : http://mxcl.github.com/homebrew/\n","permalink":"https://jedipunkz.github.io/post/2012/03/07/cocoa-na-emacs-insutoru/","summary":"\u003cp\u003eCarbon な API は排除していくべきと Apple も言っているようですし、自宅も会社も Cocoa な emacs を使うようになりました。\u003c/p\u003e\n\u003cp\u003eその手順を書いていきます。\u003c/p\u003e\n\u003cp\u003eソースとパッチでビルドも出来るのですが、\u003ca href=\"http://mxcl.github.com/homebrew/\" target=\"_blank\"\u003ehomebrew\u003c/a\u003e 使うとメチャ楽なので今回はそれを使います。\nhomebrew は公式サイトに詳しいことが書いてありますけどインストールがワンラインで済みます。\nあと、事前に AppStore で Xcode を入れてください。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e% /usr/bin/ruby -e \u0026quot;$(curl -fsSL https://raw.github.com/gist/323731)\u0026quot;                                                     \n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eだけです。\u003c/p\u003e\n\u003cp\u003eそして Cocoa な emacs インストール。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e% brew install --cocoa emacs \n% sudo mv /usr/local/Cellar/emacs/24.1/Emacs.app /Applications/\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e以上です..。簡単すぎる。先人たちのおかげですね。\u003c/p\u003e\n\u003cp\u003e次は時間見つけて anything.el のことを書こうかなぁと思ってます。\u003c/p\u003e\n\u003cp\u003e参考 URL : \u003ca href=\"http://mxcl.github.com/homebrew/\" title=\"homebrew\" target=\"_blank\"\u003e\u003ca href=\"http://mxcl.github.com/homebrew/\"\u003ehttp://mxcl.github.com/homebrew/\u003c/a\u003e\u003c/a\u003e\u003c/p\u003e","title":"cocoa な emacs インストール"},{"content":"\n上の画像は conky というツールのキャプチャです。\nconky は x window で使える linux マシンのステータスを文字・グラフ描画で表現してくれるツールです。\n透明にしたりグラフ表示を派手にすることも出来るのだけど、わたしは上図のようにステータスバーとして使ってます。Window Manager に openbox という素っ気ないものを使うようにしてるので、これ自体がファイラーもステータスバーも無いんです。なので conky を利用して \u0026lsquo;時間\u0026rsquo;, \u0026lsquo;バッテリ残量\u0026rsquo;, \u0026lsquo;AC アダプタ有無\u0026rsquo;, \u0026lsquo;ネットワーク使用量\u0026rsquo; 等を表示してます。\ndeviantart に rent0n86 さんという方が投稿した作品があって、それをこちょこちょ自分用にいじって使ってます。\ndebian gnu/linux な GUI 環境があれば\u0026hellip;\n% sudo apt-get conky-all % cd $HOME % wget https://raw.github.com/chobiwan/dotfiles/master/.conkyrc で、この環境を作れます。\n表示する内容は環境に合わせて修正すると楽しいです。幅は minimum_size パラメータで合わせてください。\nパラメータ一覧は、公式 Wiki サイト に正しい情報が載っています。\n話変わるけど、enlightenment 17 が完成度高くならない理由ってなんなのでしょうかね？ 16 を愛用していただけに残念。\n","permalink":"https://jedipunkz.github.io/post/2012/03/07/conky-statusbar/","summary":"\u003cp\u003e\u003ca href=\"http://files.chobiwan.me/pix/conky_capture.png\"\u003e\u003cimg src=\"http://files.chobiwan.me/pix/conky_capture.png\" alt=\"\" title=\"conky_capture\" width=\"721\" height=\"274\" class=\"alignnone size-full wp-image-60\" /\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e上の画像は conky というツールのキャプチャです。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://conky.sourceforge.net/\" target=\"_blank\"\u003econky\u003c/a\u003e は x window で使える linux マシンのステータスを文字・グラフ描画で表現してくれるツールです。\u003c/p\u003e\n\u003cp\u003e透明にしたりグラフ表示を派手にすることも出来るのだけど、わたしは上図のようにステータスバーとして使ってます。Window Manager に openbox という素っ気ないものを使うようにしてるので、これ自体がファイラーもステータスバーも無いんです。なので conky を利用して \u0026lsquo;時間\u0026rsquo;, \u0026lsquo;バッテリ残量\u0026rsquo;, \u0026lsquo;AC アダプタ有無\u0026rsquo;, \u0026lsquo;ネットワーク使用量\u0026rsquo; 等を表示してます。\u003c/p\u003e\n\u003cp\u003edeviantart に \u003ca href=\"http://rent0n86.deviantart.com/art/My-horizontal-conkyrc-122604863\" target=\"_blank\"\u003erent0n86\u003c/a\u003e さんという方が投稿した作品があって、それをこちょこちょ自分用にいじって使ってます。\u003c/p\u003e\n\u003cp\u003edebian gnu/linux な GUI 環境があれば\u0026hellip;\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e% sudo apt-get conky-all\n% cd $HOME\n% wget https://raw.github.com/chobiwan/dotfiles/master/.conkyrc\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eで、この環境を作れます。\u003c/p\u003e\n\u003cp\u003e表示する内容は環境に合わせて修正すると楽しいです。幅は minimum_size パラメータで合わせてください。\u003c/p\u003e\n\u003cp\u003eパラメータ一覧は、\u003ca href=\"http://wiki.conky.be/index.php?title=Configuration_Settings\" target=\"_blank\"\u003e公式 Wiki サイト\u003c/a\u003e に正しい情報が載っています。\u003c/p\u003e\n\u003cp\u003e話変わるけど、enlightenment 17 が完成度高くならない理由ってなんなのでしょうかね？ 16 を愛用していただけに残念。\u003c/p\u003e","title":"conky statusbar"},{"content":"github.com は便利なのだけどプライベートなレポジトリを作るのにお金払うのはもったいないので自宅サーバに SSH 経由の Git サーバを構築した。その時の手順をメモしておきます。\ngitosis という便利なツールがあって、これを使うとあっという間に環境構築できます。私の環境は debian Gnu/Linux Squeeze なのですが apt-get で必要なモノを入れました。gitosis は git で持ってきます。\nremote% sudo apt-get update remote% sudo apt-get install git git-core python python-setuptools remote% cd $HOME/usr/src remote% git clone git://eagain.net/gitosis.git remote% cd gitosis remote% sudo python setup.py install SSH でアクセスする先のユーザを作ります。\nremote% sudo adduser --shell /bin/sh -gecos --group \\ --disable-password --home /home/git git 作業端末で rsa な SSH 公開鍵を生成して ${remote} サーバは転送する。\nlocal% ssh-keygen -t rsa ... インタラクティブに答える local% scp .ssh/id_dsa.pub ${remote}:/tmp/ 転送した鍵を元に ${remote} サーバ上で git レポジトリを初期化する。\nremote% sudo -H -u git gitosis-init \u0026lt; /tmp/id_rsa.pub もし実行権が付いていなかったら\nremote% sudo chmod 755 /home/git/repositories/gitosis-admin.git/hooks/post-update これで環境構築完了。ただ、このままだとローカルの作業端末からレポジトリの生成が出来ない。\nローカルの作業端末でレポジトリを生成する手順は、 gitosis-admin.git を clone してきて、gitosis.conf を修正し commit/push する。\nlocal% git clone git@${remote}:gitosis-admin.git local% vi gitosis.conf 今回はテストで test グループに test レポジトリを作るための config を書いてみる。members は複数人書ける。\n[gitosis] [group gitosis-admin] writable = gitosis-admin members = chobiwan@${local} [group test] writable = test members = chobiwan@${local} 修正した内容を ${remote} サーバへ git push して反映させる。\nlocal% git add . local% git commit -m \u0026quot;added test repo\u0026quot; local% git push origin master これで \u0026rsquo;test\u0026rsquo; レポジトリをローカルの作業端末から作ってコミットする準備完了。 試しにファイルを add, commit, push してみる。\nlocal% mkdir ~/gitwork local% cd ~/gitwork local% touch README local% git add README local% git commit -m \u0026quot;my first commi\u0026quot; local% git remote add origin git@obi.chobiwan.me:test.git local% git push origin master (2012/05/01 追記) 新規ユーザの追加方法は下記の通り。\nlocal% ssh-keygen -t rsa local% cd ~/gitwork local% git clone git@${remote}:gitosis-admin.git local% cp ~/.ssh/id_rsa.pub ./gitosis-admin/keydir/user@hostname.pub local% vi ./gitosis-admin/gitosis.conf \u0026lt;参加したいリポジトリの membersに user@hostname.pub を追加(スペース区切り) local% git gitosis-admin \u0026amp;\u0026amp; git add . \u0026amp;\u0026amp; git commit -m \u0026quot;added a user\u0026quot; local% git push origin master 公開鍵のアップやレポジトリ・グループの生成の仕方を覚えれば、OK なのかなぁと。 次はレポジトリのバックアップとレカバリについてまとめていきたい。リカバリできないと死ねるから。\n","permalink":"https://jedipunkz.github.io/post/2012/03/07/gitosis-ssh-plus-git-saba/","summary":"\u003cp\u003egithub.com は便利なのだけどプライベートなレポジトリを作るのにお金払うのはもったいないので自宅サーバに SSH 経由の Git サーバを構築した。その時の手順をメモしておきます。\u003c/p\u003e\n\u003cp\u003egitosis という便利なツールがあって、これを使うとあっという間に環境構築できます。私の環境は debian Gnu/Linux Squeeze なのですが apt-get で必要なモノを入れました。gitosis は git で持ってきます。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eremote% sudo apt-get update\nremote% sudo apt-get install git git-core python python-setuptools\nremote% cd $HOME/usr/src\nremote% git clone git://eagain.net/gitosis.git\nremote% cd gitosis\nremote% sudo python setup.py install\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSSH でアクセスする先のユーザを作ります。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eremote% sudo adduser --shell /bin/sh -gecos --group \\\n        --disable-password --home /home/git git\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e作業端末で rsa な SSH 公開鍵を生成して ${remote} サーバは転送する。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003elocal% ssh-keygen -t rsa\n... インタラクティブに答える\nlocal% scp .ssh/id_dsa.pub ${remote}:/tmp/\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e転送した鍵を元に ${remote} サーバ上で git レポジトリを初期化する。\u003c/p\u003e","title":"gitosis ssh+git サーバ"},{"content":"白金台のクックパッドさんで行われた \u0026ldquo;heroku jp meet up #3\u0026rdquo; に参加してきました。\n東京マラソン参加のため来日されていた Christopher Stolt さんや Ruby コミッタの相澤さんなどの話を聞けました。\nChristopher さんからは、基本的な使い方や heroku で動作させたアプリケーションをローカル環境で動作させる foreman、また皆が意外と気にするアプリのログを tail する方法などの説明がありました。PaaS での皆の懸念点が結構解決されたんじゃないかなぁ。\n相澤さんからは NY マラソンでの実績など、比較的エンタープライズな使われ方もされ初めていると説明がありました。あと、呼び名なのですが heroku は \u0026ldquo;へろく\u0026rdquo; と発音するそうです。確かに about.heroku.com には \u0026ldquo;Heroku (pronounced her-OH-koo) is a cloud application platform\u0026rdquo; と書いてあるのだが、\u0026ldquo;へろく\u0026rdquo; が正しいそうです。w\nそのた LT が幾つあって、ちょうど気になっていた Lokka の話があったので、自宅に帰ってから自分の heroku アカウントで lokka を動かしてみました。lokka の公式サイトに手順が書いてあって、そのままなのですが行ったのは、\n% gem install heroku bundler % git clone git://github.com/komagata/lokka.git % cd lokka % heroku create % git push heroku master % heroku rake db:setup % heroku open すれば OK。\n私の環境では公開鍵認証がうまくいかなかったので、下記の対処をしました。\n% heroku keys:add 試しに wordpress のデータを移行してみようと思ったのですが、\u0026ldquo;Application Error\u0026rdquo; が発生。今のところうまくいっていません。コードとブログコンテンツを git で管理出来るので、今時ですよね。初めてみたい。\nまた、今回会場を提供してくださったクックパッドさんが、バレンタインデーということもあり皆に料理を提供して下さいました。感謝ー。\n本当は heroku システムがどう構成されているか？が知りたかったのですが、ユーザ視点で使われ方が分からないと何も出来ないと感じていたため、今回はとても良い機会でした。個人的にも heroku は使い続けたい、と感じたサービスでした。みなさん、当日はありがとうございました。\n","permalink":"https://jedipunkz.github.io/post/2012/03/07/heroku-jp-meetup-number-3/","summary":"\u003cp\u003e白金台のクックパッドさんで行われた \u0026ldquo;heroku jp meet up #3\u0026rdquo; に参加してきました。\u003c/p\u003e\n\u003cp\u003e東京マラソン参加のため来日されていた \u003ca href=\"http://twitter.com/stolt45\"\u003eChristopher Stolt\u003c/a\u003e さんや Ruby コミッタの\u003ca href=\"http://twitter.com/ayumin\"\u003e相澤\u003c/a\u003eさんなどの話を聞けました。\u003c/p\u003e\n\u003cp\u003eChristopher さんからは、基本的な使い方や heroku で動作させたアプリケーションをローカル環境で動作させる foreman、また皆が意外と気にするアプリのログを tail する方法などの説明がありました。PaaS での皆の懸念点が結構解決されたんじゃないかなぁ。\u003c/p\u003e\n\u003cp\u003e相澤さんからは NY マラソンでの実績など、比較的エンタープライズな使われ方もされ初めていると説明がありました。あと、呼び名なのですが heroku は \u0026ldquo;へろく\u0026rdquo; と発音するそうです。確かに about.heroku.com には \u0026ldquo;Heroku (pronounced her-OH-koo) is a cloud application platform\u0026rdquo; と書いてあるのだが、\u0026ldquo;へろく\u0026rdquo; が正しいそうです。w\u003c/p\u003e\n\u003cp\u003eそのた LT が幾つあって、ちょうど気になっていた \u003ca href=\"http://lokka.org/\"\u003eLokka\u003c/a\u003e の話があったので、自宅に帰ってから自分の heroku アカウントで lokka を動かしてみました。lokka の公式サイトに手順が書いてあって、そのままなのですが行ったのは、\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e% gem install heroku bundler\n% git clone git://github.com/komagata/lokka.git\n% cd lokka\n% heroku create\n% git push heroku master\n% heroku rake db:setup\n% heroku open\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eすれば OK。\u003c/p\u003e","title":"Heroku JP Meetup #3"},{"content":"2012年2月6日、西新宿にある株式会社ニフティさんで行われた \u0026ldquo;OpenFlow 勉強会\u0026rdquo; に参加したので簡単なレポメモを書いておきます。\nまずは OpenFlow の基本動作。\nメッセージの切り出し ハンドシェイク コネクションの維持 スイッチから送られてくるメッセージへの応答 構成は\nOpenFlow コントローラ, スイッチから成る コントローラは通信制御 スイッチはフロールールをコントローラに問い合わせ通信を受け流し(packet/frame 転送) コントローラは L2 - L4 フィールドを見て制御する そしてコントローラ、スイッチは各社・団体から提供されている。今月も Nicira Networks さんが自社システムの構成を抽象的ではありますが公開され、HP さんも OpenFlow 対応スイッチを12製品ほど発表されました。\nコントローラの種別、\nBeacon (Java) NOX (Python) Ryu NodeFlow Trema (C/ruby) Nicira Networks Big Switch Networks Midokura NTT Data スイッチの種別、\ncisco nexus 3000 IBM BNT rackSwitch G8264 NEC Univerge PF5240/PF5820 Pronto Systems 3240/3290 HP 3500/5400 HP OpenFlow 化 firmware Reference Implementation (Software) Open vSwitch (Software) NEC さんの PF ほにゃららは、GUI なインターフェースと API を持った製品で OpenFlow 1.0.0 仕様に準拠。スイッチ25台までを管理するコントローラ。価格は、コントローラ : 1000万, スイッチ : 250万 だそうです。スイッチ25台はあくまでもソフトリミットらしいです。\nTrema は開発者が日本人で OpenFlow 1.1.0 に準拠。Ruby / C を使ってフロールールが掛けるフレームワーク型のソフトウェアで、github にある Wiki を見ても分かりますが、とても簡潔なルール記述が可能です。あと、特徴なのがテストフレームワーク(シュミレータ)が付いているので、OpenFlow 対応スイッチを購入しなくても自分で書いたルールがテスト出来ます。これはとても大きな意味がある。自宅でもあれこれルールを書けて動作を知るにはとても良い機能です。\nNicira Networks の NVP (Nacira Virtual Platform) はハイパーバイザ上の OpenvSwitch やアプライアンスなスイッチを管理するコントローラから成ると今月公開されました。ただ抽象的な言葉ばかりが並んでいて ( web, datasheet 共に ) まだまだ、仕組みを理解するには情報が足らないのかなぁと感じています。特に distributed controller はどう冗長取れている？など。software designed network の ML が日本でも管理され始めて議論されていますが、ハイパーバイザ上の OpenvSwitch で、カプセル化時のオーバーヘッドをどう攻略するか？が問題じゃないかと話し合われています。\nMidokura さんも会場にいらっしゃったのですが、印象的な言葉は \u0026ldquo;OpenFlow は夢の技術ではないし、既存のネットワーク技術を置き換えるものでもない\u0026rdquo; でした。サービスを形成するサーバファーム周りで OpenFlow を使ったシステムを組むには良いですけどね。\n最後に参考 URL を記しておきます。\nsdnstudy google group trema Nicira Virtual Platform ","permalink":"https://jedipunkz.github.io/post/2012/03/07/openflow-mian-qiang-hui/","summary":"\u003cp\u003e2012年2月6日、西新宿にある株式会社ニフティさんで行われた \u0026ldquo;OpenFlow 勉強会\u0026rdquo; に参加したので簡単なレポメモを書いておきます。\u003c/p\u003e\n\u003cp\u003eまずは OpenFlow の基本動作。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eメッセージの切り出し\u003c/li\u003e\n\u003cli\u003eハンドシェイク\u003c/li\u003e\n\u003cli\u003eコネクションの維持\u003c/li\u003e\n\u003cli\u003eスイッチから送られてくるメッセージへの応答\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e構成は\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eOpenFlow コントローラ, スイッチから成る\u003c/li\u003e\n\u003cli\u003eコントローラは通信制御\u003c/li\u003e\n\u003cli\u003eスイッチはフロールールをコントローラに問い合わせ通信を受け流し(packet/frame 転送)\u003c/li\u003e\n\u003cli\u003eコントローラは L2 - L4 フィールドを見て制御する\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eそしてコントローラ、スイッチは各社・団体から提供されている。今月も Nicira Networks さんが自社システムの構成を抽象的ではありますが公開され、HP さんも OpenFlow 対応スイッチを12製品ほど発表されました。\u003c/p\u003e\n\u003cp\u003eコントローラの種別、\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eBeacon (Java)\u003c/li\u003e\n\u003cli\u003eNOX (Python)\u003c/li\u003e\n\u003cli\u003eRyu\u003c/li\u003e\n\u003cli\u003eNodeFlow\u003c/li\u003e\n\u003cli\u003eTrema (C/ruby)\u003c/li\u003e\n\u003cli\u003eNicira Networks\u003c/li\u003e\n\u003cli\u003eBig Switch Networks\u003c/li\u003e\n\u003cli\u003eMidokura\u003c/li\u003e\n\u003cli\u003eNTT Data\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eスイッチの種別、\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ecisco nexus 3000\u003c/li\u003e\n\u003cli\u003eIBM BNT rackSwitch G8264\u003c/li\u003e\n\u003cli\u003eNEC Univerge PF5240/PF5820\u003c/li\u003e\n\u003cli\u003ePronto Systems 3240/3290\u003c/li\u003e\n\u003cli\u003eHP 3500/5400\u003c/li\u003e\n\u003cli\u003eHP OpenFlow 化 firmware\u003c/li\u003e\n\u003cli\u003eReference Implementation (Software)\u003c/li\u003e\n\u003cli\u003eOpen vSwitch (Software)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eNEC さんの PF ほにゃららは、GUI なインターフェースと API を持った製品で OpenFlow 1.0.0 仕様に準拠。スイッチ25台までを管理するコントローラ。価格は、コントローラ : 1000万, スイッチ : 250万 だそうです。スイッチ25台はあくまでもソフトリミットらしいです。\u003c/p\u003e","title":"OpenFlow 勉強会"},{"content":"MS Windows なツールを使う方法だったり、vmlinuz, initrd をファイラーでコピーしたり、何故かいつもインターネットで調べると USB スティックを利用した debian のインストール方法が\u0026rsquo;面倒\u0026rsquo;, \u0026lsquo;不確か\u0026rsquo; なので、忘れないようにメモ。\n手元に linux 端末用意して、USB スティック挿す。\n% wget ftp://ftp.jp.debian.org/pub/Linux/debian-cd/6.0.3/amd64/iso-cd/debian-6.0.3-amd64-netinst.iso % sudo cat debian-6.0.3-amd64-netinst.iso \u0026gt; /dev/sdb # 挿した USB スティックのデバイス名 で終わり。\nただ弱点があって、フルイメージの iso は利用できないでの、今回みたいに netinstall だったり businesscard な iso を利用しかない。 他のディストリビューションもだけど、インストールする環境はネットに繋がっていないと不都合があるって時代だからいいかなぁ。\n一方、Ubuntu Server は賢い子なので 公式サイト に行くと USB スティック用の iso がダウンロードできたり iso を焼く環境に合わせて手順まで教えてくれる\u0026hellip;。\n","permalink":"https://jedipunkz.github.io/post/2012/03/07/usb-stick-de-debian-gnu-slash-linux-insutoru/","summary":"\u003cp\u003eMS Windows なツールを使う方法だったり、vmlinuz, initrd をファイラーでコピーしたり、何故かいつもインターネットで調べると USB スティックを利用した debian のインストール方法が\u0026rsquo;面倒\u0026rsquo;, \u0026lsquo;不確か\u0026rsquo; なので、忘れないようにメモ。\u003c/p\u003e\n\u003cp\u003e手元に linux 端末用意して、USB スティック挿す。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e% wget ftp://ftp.jp.debian.org/pub/Linux/debian-cd/6.0.3/amd64/iso-cd/debian-6.0.3-amd64-netinst.iso\n% sudo cat debian-6.0.3-amd64-netinst.iso \u0026gt; /dev/sdb # 挿した USB スティックのデバイス名\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eで終わり。\u003c/p\u003e\n\u003cp\u003eただ弱点があって、フルイメージの iso は利用できないでの、今回みたいに netinstall だったり businesscard な iso を利用しかない。\n他のディストリビューションもだけど、インストールする環境はネットに繋がっていないと不都合があるって時代だからいいかなぁ。\u003c/p\u003e\n\u003cp\u003e一方、Ubuntu Server は賢い子なので \u003ca href=\"http://www.ubuntu.com/download/ubuntu/download\" target=\"blank\"\u003e公式サイト\u003c/a\u003e に行くと USB スティック用の iso がダウンロードできたり iso を焼く環境に合わせて手順まで教えてくれる\u0026hellip;。\u003c/p\u003e","title":"USB Stick で Debian Gnu/Linux インストール"},{"content":"ブログを始めるにあたり、wordpress 環境を構築する必要が出てきました。いつもの apache2 + mysql5 + PHP じゃつまらないので、nginx と fastcgi を使って少しだけ高速化してみました。メモですけど、ここに手順を記していきます。\n※ wordpress から octopress に移行しました\u0026hellip; (2012/03/07)\nただ、今回は nginx や mysql の基本的なオペレーション手順は割愛させてもらいます。\n私の環境について\u0026hellip;\n% lsb_release -a No LSB modules are available. Distributor ID: Debian Description: Debian GNU/Linux 6.0.3 (squeeze) Release: 6.0.3 Codename: squeeze インストールしたもの\u0026hellip; メタパッケージを指定したのでその他必要なモノはインストールされます。\n% sudo apt-get update % sudo apt-get install spawn-fcgi php5 php5-mysql php5-cgi mysql-server nginx まずはお決まりの gzip 圧縮転送。IE の古いモノ以外は対応しているので心配なし。今回のテーマと関係無いですけど、一応入れておきます。\n% diff -u /etc/nginx/nginx.conf.org /etc/nginx/nginx.conf --- /etc/nginx/nginx.conf.org 2012-01-14 15:27:45.000000000 +0900 +++ /etc/nginx/nginx.conf 2012-01-14 15:28:58.000000000 +0900 @@ -22,6 +22,10 @@ tcp_nodelay on; gzip on; + gzip_http_version 1.0; + gzip_vary on; + gzip_comp_level 6; + gzip_types text/html text/xml text/css application/xhtml+xml application/xml application/rss+xml application/atom_xml application/x-javascript application/x-httpd-php; gzip_disable \u0026quot;MSIE [1-6]\\.(?!.*SV1)\u0026quot;; include /etc/nginx/conf.d/*.conf; spawn-fcgi を稼働させるスクリプトを生成する。/usr/bin/php-fastcgi として下記の内容で保存する。\n#! /bin/sh /usr/bin/spawn-fcgi -a 127.0.0.1 -p 9000 -C 6 -u www-data -f /usr/bin/php5-cgi % sudo chmod 755 /usr/bin/php-fastcgi 次にこれを実行する起動スクリプトの用意と実行。/etc/init.d/php-fastcgi\n#!/bin/bash ### BEGIN INIT INFO # Required-Start: $local_fs $remote_fs $network $syslog # Required-Stop: $local_fs $remote_fs $network $syslog # Default-Start: 2 3 4 5 # Default-Stop: 0 1 6 # Short-Description: php-fastcgi script # Description: php-fastcgi script ### END INIT INFO # env SCRIPT=/usr/bin/php-fastcgi USER=www-data RETVAL=0 PIDFILE=/var/run/php5-cgi.pid # start or stop case \u0026quot;$1\u0026quot; in start) su - $USER -c $SCRIPT pidof php5-cgi \u0026gt; $PIDFILE RETVAL=$? ;; stop) killall -9 php5-cgi echo '' \u0026gt; $PIDFILE RETVAL=$? ;; restart) killall -9 php5-cgi su - $USER -c $SCRIPT pidof php5-cgi \u0026gt; $PIDFILE RETVAL=$? ;; *) echo \u0026quot;Usage: php-fastcgi {start|stop|restart}\u0026quot; exit 1 ;; esac 起動すると php-cgi のプロセスが立ち上がり localhost:9000 で LISTEN された状態になっているはずです。nginx はここへのプロキシのような動作をすることになります。 下記の手順で起動と起動スクリプトへの組み込みを行なってください。\n% sudo chmod 755 /etc/init.d/php-fastcgi % sudo update-rc.d php-fastcgi defaults % sudo service php-fastcgi start 次に nginx の virtualhost を掘ります。変数 ${FQDN_HOSTNAME}, ${DOCUMENT_ROOT}は自分 の環境情報に読み替えてください。\nserver { listen 80; server_name ${FQDN_HOSTNAME} access_log /var/log/nginx/${FQDN_HOSTNAME}.access.log; error_log /var/log/nginx/${FQDN_HOSTNAME}.error.log; location / { root ${DOCUMENT_ROOT}; index index.html index.php; } location ~ \\.php$ { include /etc/nginx/fastcgi_params; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME ${DOCUMENT_ROOT}$fastcgi_script_name; } } nginx を stop/start してこれらの設定を有効にします。\n% sudo service nginx stop % sudo service nginx start 以上です。\nその他にも proxy cache を有効にして wordpress の静的な出力をキャッシュするチューニング方法もあるそうなので、次回試してみます。\n","permalink":"https://jedipunkz.github.io/post/2012/03/07/wordpress-wo-nginx-plus-fastcgi-degao-su-hua/","summary":"\u003cp\u003eブログを始めるにあたり、wordpress 環境を構築する必要が出てきました。いつもの apache2 + mysql5 + PHP じゃつまらないので、nginx と fastcgi を使って少しだけ高速化してみました。メモですけど、ここに手順を記していきます。\u003c/p\u003e\n\u003cp\u003e※ wordpress から octopress に移行しました\u0026hellip; (2012/03/07)\u003c/p\u003e\n\u003cp\u003eただ、今回は nginx や mysql の基本的なオペレーション手順は割愛させてもらいます。\u003c/p\u003e\n\u003cp\u003e私の環境について\u0026hellip;\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e% lsb_release -a\nNo LSB modules are available.\nDistributor ID: Debian\nDescription:    Debian GNU/Linux 6.0.3 (squeeze)\nRelease:        6.0.3\nCodename:       squeeze\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eインストールしたもの\u0026hellip;\nメタパッケージを指定したのでその他必要なモノはインストールされます。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e% sudo apt-get update\n% sudo apt-get install spawn-fcgi php5 php5-mysql php5-cgi mysql-server nginx \n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eまずはお決まりの gzip 圧縮転送。IE の古いモノ以外は対応しているので心配なし。今回のテーマと関係無いですけど、一応入れておきます。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e% diff -u /etc/nginx/nginx.conf.org /etc/nginx/nginx.conf\n--- /etc/nginx/nginx.conf.org   2012-01-14 15:27:45.000000000 +0900\n+++ /etc/nginx/nginx.conf       2012-01-14 15:28:58.000000000 +0900\n@@ -22,6 +22,10 @@\n     tcp_nodelay        on;\n  \n     gzip  on;\n+    gzip_http_version 1.0;\n+    gzip_vary         on;\n+    gzip_comp_level   6;\n+    gzip_types        text/html text/xml text/css application/xhtml+xml application/xml application/rss+xml application/atom_xml application/x-javascript application/x-httpd-php;\n     gzip_disable \u0026quot;MSIE [1-6]\\.(?!.*SV1)\u0026quot;;\n \n     include /etc/nginx/conf.d/*.conf;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003espawn-fcgi を稼働させるスクリプトを生成する。/usr/bin/php-fastcgi として下記の内容で保存する。\u003c/p\u003e","title":"WordPress を nginx + fastcgi で高速化"}]