+++
title = "JTF2014 で Ceph について話してきた！"
date = "2014-06-22"
slug = "2014/06/22/jtf2014-ceph"
Categories = ["infrastructure", "report"]
description = "July Tech Festa 2014 での Ceph-Deploy を使った Ceph 構築についての発表レポート"
+++
こんにちは。<a href="https://twitter.com/jedipunkz">@jedipunkz</a> です。

今日、JTF2014 (July Tech Festa 2014) というイベントで Ceph のことを話してきま
した。Ceph ユーザ会の会員として話してきたのですが Ceph ユーザ会は実は最近発足
したばかりのユーザ会で、まだまだ活動が活発ではありません。もし興味がある方いらっ
しゃいましたら是非参加よろしくお願いしますー。下記の Google Groups になります。

<https://groups.google.com/forum/#!forum/ceph-jp>

ユーザ会としての勉強会として初になるのですが、今回このイベントで自分は
Ceph-Deploy について話してきました。とりあえず皆さんに使ってもらいたかったので
この話をしてきました。が、予定時間がメチャ短かったので超絶早口で頑張った分、皆
さんに理解してもらえなかった気がしてちょっと反省...。なので、このブログを利用
して少し細くさせてもらいます。

今日の発表資料はこちらです！

<script async class="speakerdeck-embed"
data-id="592a0b90ceb30131a5d25ae3f95c3a1a" data-ratio="1.33333333333333" src="//speakerdeck.com/assets/embed.js"></script>

今日のテーマは 「Ceph-Deploy を使って Ceph を構築してみる」だったのですが、下
記のテーマを持って資料を作っています。

* 単にミニマム構成ではなく運用を考慮した実用性のある構成
* OSD, MON, MDS の各プロセスとノード・ディスクの数の関係を知ってもらう

特に「実用性のある..」は意識したつもりでした。そのために前提とした構成に下記の
特徴を持たせています。(資料 6 ページ目に構成図があります。確認してみてください。)

* オブジェクト格納用ディスクは複数/ノードを前提
* OSD レプリケーションのためのクラスタネットワークを用いる構成
* OSD の扱うジャーナル格納用ディスクは高速な SSD を用いる
* MDS は利用する HW リソースの特徴が異なるので別ノードへ配置

ストレージ全体を拡張したければ

* 図中 ceph01-03 の様なノードを増設する
* ceph01-03 にディスクとそれに対する OSD を増設する

ですが、前者がベストでしょう。ノード増設の場合 ceph-deploy を用いて

* ceph-deploy mon create <新規ホスト名> で MON を稼働
* ceph-dploy disk zap, osd create で OSD を稼働

で簡単に可能です。MDS の増設も負荷状況を見ながらするといいでしょう。自分はまだ
Ceph を運用していないので、各プロセスがどのようなリソースの消費の仕方をするの
か知りません。MDS がどのような数で運用していくべきなのか。早く運用から得たノウ
ハウが共有されないかなぁと期待しています。

また今回話すのを忘れたのですが SSD をジャーナル格納用ディスクとして用いたのは
ハードディスクに対して高速でアクセス出来ること・またメタデータはファイルオブジェ
クトに対して小容量で済む、といった理由からです。メタデータを扱うのに適している
と思います。また将来的には幾つかの KVS データベースソフトウェアをメタデータ管
理に使う実装がされるそうです。

以上です。皆さん、是非 Ceph を使ってみてください！ また興味のある方はユーザ会
への加入をご検討くださいー。
